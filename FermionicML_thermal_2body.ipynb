{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aguschanchu/FermionicML/blob/main/FermionicML_thermal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXz5cOlVwrzZ"
      },
      "source": [
        "# FermionicML:\n",
        "\n",
        "Code based on aguschanchu/Bosonic.py\n",
        "\n",
        "A diferencia del código anterior, este modelo trabaja sobre estados térmicos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLD2Yai55rMm"
      },
      "source": [
        "## Código base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgf9ExZN4jA7"
      },
      "source": [
        "Cargamos el código de Bosonic.py básico, branch fermionic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Gydz4kCH4l5w"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/tmp/ipykernel_5719/3558866515.py:248: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
            "  def gamma_lamba_inv(x):\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.special import binom\n",
        "from scipy.sparse import dok_matrix, linalg\n",
        "from scipy import linalg as linalg_d\n",
        "from joblib import Memory\n",
        "import random\n",
        "import plotly.graph_objects as go\n",
        "from joblib import Parallel, delayed\n",
        "from numba import jit, prange, njit\n",
        "import numba as nb\n",
        "import pickle\n",
        "import math\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "# Funciones auxiliares optimiadas\n",
        "@nb.jit(nopython=True, parallel=True)\n",
        "def int_to_tuple_arr(ni,nf, b, digits=None):\n",
        "    sol = np.zeros((nf-ni, digits), dtype=np.int64)\n",
        "    for n in prange(ni, nf):\n",
        "        r = np.zeros(digits, dtype=np.int64)\n",
        "        ncop = n\n",
        "        idx = 0\n",
        "        while n != 0:\n",
        "            r[idx] = n % b\n",
        "            n = n // b\n",
        "            idx += 1\n",
        "        if digits is not None:\n",
        "            if idx < digits:\n",
        "                for i in range(idx, digits):\n",
        "                    r[i] = 0\n",
        "                idx = digits\n",
        "        sol[ncop-ni,:] = r[:idx]\n",
        "    return sol\n",
        "\n",
        "def tuple_to_int(t, d):\n",
        "    b = d-1\n",
        "    l = len(t)\n",
        "    s = [t[k]*b**(l-k-1) for k in range(0,l)]\n",
        "    return sum(s)\n",
        "\n",
        "def create_basis_(m, d, size):\n",
        "    base = []\n",
        "    index = 0\n",
        "    chunk_size = 1000000\n",
        "    for x in range(0,(m+1)**d, chunk_size):\n",
        "        start_index = x\n",
        "        end_index = min(x + chunk_size, (m+1)**d)\n",
        "        arr = int_to_tuple_arr(start_index, end_index, m+1, d)\n",
        "        sums = np.sum(arr, axis=1)\n",
        "        rows = np.where(sums == m)[0]\n",
        "        for row in [arr[i] for i in rows]:\n",
        "            if np.all(np.logical_or(row == 0, row == 1)):\n",
        "                base.append(row)\n",
        "\n",
        "    # Como consecuencia de la paralelizacion, es necesario reordenar la base\n",
        "    sorted_base = sorted(base, key=lambda x: tuple_to_int(x, d), reverse=True)\n",
        "    assert len(base) == size\n",
        "\n",
        "    return sorted_base\n",
        "\n",
        "class fixed_basis:\n",
        "\n",
        "    # Convierte a un enterno n a su escritura en base b\n",
        "    def _int_to_tuple(self, n, b, digits = None):\n",
        "        rep = np.base_repr(n, b)\n",
        "        rep_int = [int(x,b) for x in rep]\n",
        "        if digits is not None:\n",
        "            zeros = [0 for i in range(0,digits-len(rep))]\n",
        "            return zeros + rep_int\n",
        "        else:\n",
        "            return rep_int\n",
        "\n",
        "    # Revierte la transformacion anterior\n",
        "    def tuple_to_int(self, t):\n",
        "        b = self.d-1\n",
        "        l = len(t)\n",
        "        s = [t[k]*b**(l-k-1) for k in range(0,l)]\n",
        "        return sum(s)\n",
        "\n",
        "    # Convierte el vector en su representacion\n",
        "    def vect_to_repr(self, vect):\n",
        "        for i, k in enumerate(vect):\n",
        "            if k == 1. or k == 1:\n",
        "                break\n",
        "        else:\n",
        "            return 0\n",
        "        return self.base[i,:]\n",
        "\n",
        "    def rep_to_vect(self, rep):\n",
        "        rep = list(rep)\n",
        "        for i, r in [(j, self.base[j,:]) for j in range(0,self.size)]:\n",
        "            if list(r) == rep:\n",
        "                return self.canonicals[:,i]\n",
        "        else:\n",
        "            None\n",
        "\n",
        "    def rep_to_index(self, rep):\n",
        "        return self.base.tolist().index(list(rep))\n",
        "\n",
        "    @staticmethod\n",
        "    def rep_to_exi(rep):\n",
        "        r = []\n",
        "        for i, k in enumerate(rep):\n",
        "            r += [i for x in range(0,k)]\n",
        "        return r\n",
        "\n",
        "    # Crea base de M particulas en D estados (repr y base canonica)\n",
        "    def create_basis(self, m, d):\n",
        "        #print(\"Creating basis: \", m, d)\n",
        "        length = int(binom(d,m))\n",
        "        base = np.array(create_basis_(m, d, length))\n",
        "        # Asignamos a cada uno de ellos un canónico\n",
        "        canonicals = np.eye(length)\n",
        "        return base, canonicals\n",
        "\n",
        "    def __init__(self, m, d):\n",
        "        self.m = m\n",
        "        self.d = d\n",
        "        self.size = int(binom(d,m))\n",
        "        (self.base, self.canonicals) = self.create_basis(m, d)\n",
        "\n",
        "\n",
        "# Matrices de aniquilación y creación endomórficas. Estan fuera de la clase para poder ser cacheadas\n",
        "#@memory.cache\n",
        "def bdb(basis, i, j):\n",
        "    mat = dok_matrix((basis.size, basis.size), dtype=np.float32)\n",
        "    if i != j:\n",
        "        for k, v in enumerate(basis.base):\n",
        "            if v[j] != 0 and v[i] != 1:\n",
        "                dest = list(v.copy())\n",
        "                dest[j] -= 1\n",
        "                dest[i] += 1\n",
        "                tar = basis.rep_to_index(dest)\n",
        "                mat[tar, k] = np.sqrt(v[i]+1)*np.sqrt(v[j])\n",
        "    else:\n",
        "        for k, v in enumerate(basis.base):\n",
        "            if v[j] != 0:\n",
        "                mat[k, k] = v[i]\n",
        "    return mat\n",
        "\n",
        "#@memory.cache\n",
        "def bbd(basis, i, j):\n",
        "    mat = dok_matrix((basis.size, basis.size), dtype=np.float32)\n",
        "    if i != j:\n",
        "        for k, v in enumerate(basis.base):\n",
        "            if v[i] != 0 and v[j] != 1:\n",
        "                dest = list(v.copy())\n",
        "                dest[i] -= 1\n",
        "                dest[j] += 1\n",
        "                tar = basis.rep_to_index(dest)\n",
        "                mat[tar, k] = np.sqrt(v[j]+1)*np.sqrt(v[i])\n",
        "    else:\n",
        "        for k, v in enumerate(basis.base):\n",
        "            if v[i] != 1:\n",
        "                mat[k, k] = v[i]+1\n",
        "    return mat\n",
        "\n",
        "# Matrices de aniquilación y creación.Toman la base de origen y destino (basis_o, basis_d) resp\n",
        "#@nb.jit(nopython=True, parallel=True)\n",
        "@nb.jit(nopython=True)\n",
        "def b_aux(basis_o, basis_d, i):\n",
        "    mat = np.zeros((len(basis_d), len(basis_o)), dtype=np.float32)\n",
        "    for k in prange(len(basis_o)):\n",
        "        if basis_o[k][i] != 0:\n",
        "            dest = list(basis_o[k].copy())\n",
        "            dest[i] -= 1\n",
        "            for j in prange(len(basis_d)):\n",
        "                if list(basis_d[j]) == dest:\n",
        "                    tar = j\n",
        "                    mat[tar, k] = np.sqrt(basis_o[k][i])\n",
        "    return mat\n",
        "\n",
        "def b(basis_o, basis_d, i):\n",
        "    return b_aux(basis_o.base, basis_d.base, i)\n",
        "\n",
        "#@nb.jit(nopython=True, parallel=True)\n",
        "@nb.jit(nopython=True)\n",
        "def bd_aux(basis_o, basis_d, i):\n",
        "    mat = np.zeros((len(basis_d), len(basis_o)), dtype=np.float32)\n",
        "    for k in prange(len(basis_o)):\n",
        "        if basis_o[k][i] != 1:\n",
        "            dest = list(basis_o[k].copy())\n",
        "            dest[i] += 1\n",
        "            for j in prange(len(basis_d)):\n",
        "                if list(basis_d[j]) == dest:\n",
        "                    tar = j\n",
        "                    mat[tar, k] = np.sqrt(basis_o[k][i]+1)\n",
        "    return mat\n",
        "\n",
        "def bd(basis_o, basis_d, i):\n",
        "    return bd_aux(basis_o.base, basis_d.base, i)\n",
        "\n",
        "\n",
        "# Acepta una lista de indices a crear\n",
        "@nb.jit(nopython=True, parallel=True)\n",
        "def bd_gen_aux(basis_o, basis_d, gen_list):\n",
        "    mat = np.zeros((len(basis_d), len(basis_o)), dtype=np.float32)\n",
        "    for k in prange(len(basis_o)):\n",
        "        conds = np.zeros(len(gen_list), dtype=np.int64)\n",
        "        for i in range(len(gen_list)):\n",
        "            if basis_o[k][gen_list[i]] != 1:\n",
        "                conds[i] = 1\n",
        "        if np.all(conds):\n",
        "            dest = list(basis_o[k].copy())\n",
        "            for i in gen_list:\n",
        "                dest[i] += 1\n",
        "            for j in prange(len(basis_d)):\n",
        "                if list(basis_d[j]) == dest:\n",
        "                    tar = j\n",
        "                    mat[tar, k] = np.sqrt(basis_o[k][i]+1)\n",
        "    return mat\n",
        "\n",
        "def bd_gen(basis_o, basis_d, i):\n",
        "    return bd_gen_aux(basis_o.base, basis_d.base, np.array(i))\n",
        "\n",
        "def b_gen(basis_o, basis_d, i):\n",
        "    return np.transpose(bd_gen(basis_d, basis_o, i))\n",
        "\n",
        "# Volvemos a definir la función para compilarla\n",
        "@nb.jit(forceobj=True)\n",
        "def _rep_to_index(base, rep):\n",
        "    return base.tolist().index(list(rep))\n",
        "\n",
        "# Funciones auxiliares para calcular rho2kkbar y gamma_p\n",
        "@nb.jit(nopython=True)\n",
        "def rep_to_exi(rep):\n",
        "    r = []\n",
        "    for i in range(len(rep)):\n",
        "        for j in range(rep[i]):\n",
        "            r.append(i)\n",
        "    return r\n",
        "\n",
        "@nb.njit\n",
        "def factorial(n):\n",
        "    result = 1\n",
        "    for i in range(1, n + 1):\n",
        "        result *= i\n",
        "    return result\n",
        "\n",
        "@nb.njit\n",
        "def gamma_lamba(x):\n",
        "    res = 1.0\n",
        "    for o in x:\n",
        "        res *= math.sqrt(factorial(o))\n",
        "    return res\n",
        "\n",
        "@nb.jit\n",
        "def gamma_lamba_inv(x):\n",
        "    res = 1.0\n",
        "    for o in x:\n",
        "        res *= 1.0 / np.sqrt(factorial(o))\n",
        "    return res\n",
        "\n",
        "@nb.njit\n",
        "def rep_to_index_np(base, rep):\n",
        "    for i in range(len(base)):\n",
        "        if np.all(base[i] == rep):\n",
        "            return i\n",
        "    return -1\n",
        "\n",
        "\n",
        "def gamma_p(basis, m, vect, m_basis = None, nm_basis = None):\n",
        "    d = basis.d\n",
        "    if not m_basis or not nm_basis:\n",
        "        m_basis = fixed_basis(m, d)\n",
        "        nm_basis = fixed_basis(basis.m-m,d)\n",
        "    return gamma_p_aux(basis.base, vect, m_basis.base, nm_basis.base)\n",
        "\n",
        "@nb.njit()\n",
        "def gamma_p_aux(basis, vect, m_basis, nm_basis):\n",
        "    mat = np.zeros((len(m_basis), len(nm_basis)), dtype=np.float32)\n",
        "    for i in prange(len(m_basis)):\n",
        "        v = m_basis[i]\n",
        "        for j in prange(len(nm_basis)):\n",
        "            w = nm_basis[j]\n",
        "            targ = v + w\n",
        "            index = rep_to_index_np(basis, targ)\n",
        "            if index != -1:\n",
        "                coef = vect[index]\n",
        "                if coef != 0:\n",
        "                    coef = coef * gamma_lamba_inv(v) * gamma_lamba_inv(w) * gamma_lamba(targ)\n",
        "                mat[i, j] = coef\n",
        "    return mat\n",
        "# Devuelve la matriz rho M asociada al vector\n",
        "def rho_m(basis, m, vect, m_basis = None, nm_basis = None):\n",
        "    g = gamma_p(basis, m, vect, m_basis, nm_basis)\n",
        "    return np.dot(g,np.transpose(g))\n",
        "\n",
        "# Devuelve la matriz gamma asociada a la descomposición (M,N-M) del vector\n",
        "@jit(forceobj=True)\n",
        "def gamma(basis, m, vect, m_basis = None, nm_basis = None):\n",
        "    d = basis.d\n",
        "    if not m_basis or not nm_basis:\n",
        "        m_basis = fixed_basis(m, d)\n",
        "        nm_basis = fixed_basis(basis.m-m,d)\n",
        "    mat = dok_matrix((m_basis.size, nm_basis.size), dtype=np.float32)\n",
        "    for i, v in enumerate(m_basis.base):\n",
        "        for j, w in enumerate(nm_basis.base):\n",
        "            targ = v+w\n",
        "            # Revisamos que sea un estado fermionico valido\n",
        "            arr = np.asarray(targ)\n",
        "            if not np.all(np.logical_or(arr == 0, arr == 1)):\n",
        "                continue\n",
        "            index = _rep_to_index(basis.base, targ)\n",
        "            coef = vect[index]\n",
        "            if coef != 0:\n",
        "                aux = lambda x: np.prod(np.reciprocal(np.sqrt([np.math.factorial(o) for o in x])))\n",
        "                aux_inv = lambda x: np.prod(np.sqrt([np.math.factorial(o) for o in x]))\n",
        "                coef = coef * aux(v) * aux(w) * aux_inv(targ)\n",
        "                #coef = coef\n",
        "                #print(v,w,coef)\n",
        "            mat[i,j] = coef\n",
        "    return mat\n",
        "\n",
        "# Genera las matrices de rho1\n",
        "def rho_1_gen(basis):\n",
        "    d = basis.d\n",
        "    s = basis.size\n",
        "    mat = np.empty((d,d,s,s), dtype=np.float32)\n",
        "    for i in range(0, d):\n",
        "        for j in range(0, d):\n",
        "            mat[i,j,:,:] = np.array(bdb(basis,j, i).todense())\n",
        "    return mat\n",
        "\n",
        "#@jit(parallel=True, nopython=True)\n",
        "def rho_1(d, state, rho_1_arrays):\n",
        "    state_expanded = state[np.newaxis, np.newaxis, :, :]\n",
        "    product = state_expanded * rho_1_arrays\n",
        "    mat = np.sum(product, axis=(-2, -1))\n",
        "\n",
        "    return mat\n",
        "\n",
        "\n",
        "# Genera las matrices de rho2\n",
        "def rho_2_gen(basis, mll_basis, t_basis):\n",
        "    size = t_basis.size\n",
        "    s = basis.size\n",
        "    # La entrada i, j contiene C_j^\\dag C_i    i, j \\in t_basis\n",
        "    mat = np.empty((size, size, s, s), dtype=np.float32)\n",
        "    for i, v in enumerate(t_basis.base):\n",
        "        for j, w in enumerate(t_basis.base):\n",
        "            c_i = b_gen(basis, mll_basis, rep_to_exi(v))\n",
        "            cdag_j = bd_gen(mll_basis, basis, rep_to_exi(w))\n",
        "            mat[i, j, :, :] = np.dot(cdag_j, c_i)\n",
        "\n",
        "    return mat\n",
        "\n",
        "def rho_2(size, state, rho_2_arrays):\n",
        "    state_expanded = np.expand_dims(state, axis=1)\n",
        "    state_expanded = np.expand_dims(state_expanded, axis=1)\n",
        "    rho_2_arrays = rho_2_arrays[np.newaxis, :, :, :, :]\n",
        "    print(state_expanded.shape, rho_2_arrays.shape)\n",
        "    product = state_expanded * rho_2_arrays\n",
        "    mat = np.sum(product, axis=(-2, -1))\n",
        "    return mat\n",
        "\n",
        "def rho_2_kkbar_gen(m, rho_2_arrays):\n",
        "    offset = 4*m+1\n",
        "    indices = [-(k-1)*(2*k-offset) for k in range(1,m+1)] # Lo saque de Mathmatica, vos confia\n",
        "    i, j = np.meshgrid(indices, indices, indexing='ij') # Lo usamos para rellenar la mat deseada\n",
        "\n",
        "    rho_2_arrays_kkbar = rho_2_arrays[i, j, :, :]\n",
        "\n",
        "    return rho_2_arrays_kkbar\n",
        "\n",
        "# Devuelve la matriz rho 2 asociada al bloque kkbar\n",
        "def rho_2_kkbar(basis, vect, ml_basis = None, mll_basis = None, t_basis = None):\n",
        "    d = basis.d\n",
        "    # Creo las bases si no están dadas\n",
        "    if ml_basis == None or mll_basis == None or t_basis == None:\n",
        "        ml_basis = fixed_basis(m-1,d)\n",
        "        mll_basis = fixed_basis(m-2,d)\n",
        "        t_basis = fixed_basis(2,d)\n",
        "    diag = []\n",
        "    for v in t_basis.base:\n",
        "        for j in range(0, d, 2):\n",
        "            if v[j] == v[j+1]:\n",
        "                continue\n",
        "            else:\n",
        "                break\n",
        "        else:\n",
        "            diag.append(v)\n",
        "    diag = np.array(diag)\n",
        "    return rho_2_kkbar_aux(diag, vect, basis.base, ml_basis.base, mll_basis.base, t_basis.base)\n",
        "\n",
        "@nb.njit\n",
        "def rho_2_kkbar_lambda(x):\n",
        "    res = 1.0\n",
        "    for o in x:\n",
        "        res *= 1.0 / math.sqrt(factorial(o))\n",
        "    return res\n",
        "\n",
        "#@nb.njit(parallel=True)\n",
        "def rho_2_kkbar_aux(diag, vect, basis, ml_basis, mll_basis, t_basis):\n",
        "    mat = np.zeros((len(diag), len(diag)), dtype=np.float32)\n",
        "    for i in prange(len(diag)):\n",
        "        for j in prange(len(diag)):\n",
        "            v = diag[i]\n",
        "            w = diag[j]\n",
        "            # Creacion de los a\n",
        "            i_set = rep_to_exi(v)\n",
        "            b_m = b_aux(ml_basis, mll_basis, i_set[1]) @ b_aux(basis, ml_basis, i_set[0])\n",
        "            # Creacion de los ad\n",
        "            i_set = rep_to_exi(w)\n",
        "            bd_m = bd_aux(ml_basis, basis, i_set[1]) @ bd_aux(mll_basis, ml_basis, i_set[0])\n",
        "            # v1 = vect @ bd_m @ b_m @ vect Para estados puros\n",
        "            # Mult de b's y filleo de mat\n",
        "            coef = np.trace(vect @ bd_m @ b_m)\n",
        "            mat[i,j] = coef * rho_2_kkbar_lambda(v) * rho_2_kkbar_lambda(w)\n",
        "    return mat\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dga5Xx_5vDf"
      },
      "source": [
        "## Definicion de Hamiltoniano"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myiTq53L5E1U"
      },
      "source": [
        "Cargamos el código de creación y resolución de Hamiltonianos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h5FXWv849Mq",
        "outputId": "49dd47b5-8c16-4ad4-92e7-e172462229b3"
      },
      "outputs": [],
      "source": [
        "m = 3\n",
        "d = 6\n",
        "# Creo las bases para no tener que recrearlas luego\n",
        "basis = fixed_basis(m, d)\n",
        "basis_m1 = fixed_basis(m-1, d)\n",
        "basis_m2 = fixed_basis(m-2, d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PToiSs915TXw"
      },
      "outputs": [],
      "source": [
        "## Usamos este approach si queremos guardar los generadores\n",
        "# Dados 1/2 (d^2+d) elementos, genera una mat de dxd:\n",
        "eps = 0.00001\n",
        "\n",
        "def sym_mat_gen(vect, d):\n",
        "    matrix = fill_matrix(vect, d)\n",
        "    return matrix + matrix.T - np.diag(matrix.diagonal())\n",
        "\n",
        "@jit(nopython=True)\n",
        "def fill_matrix(vect, d):\n",
        "    matrix = np.zeros((d, d))\n",
        "    idx = 0\n",
        "    for i in prange(d):\n",
        "        for j in prange(i, d):\n",
        "            matrix[i, j] = vect[idx]\n",
        "            idx += 1\n",
        "    return matrix\n",
        "\n",
        "# Generamos una matrix aleatoria. Cuidado con la distribución, ver https://stackoverflow.com/questions/56605189/is-there-an-efficient-way-to-generate-a-symmetric-random-matrix\n",
        "def hamil_base_gen(d):\n",
        "    U = np.random.uniform(low=0, high=1.0, size=(d, d))\n",
        "    hamil_base = np.tril(U) + np.tril(U, -1).T\n",
        "    return hamil_base\n",
        "\n",
        "# Dada un a mat dxd simetrica, contruye el hamiltoniano de un cuerpo a_{ij} c^{dag}_i c_j\n",
        "# Alternativamente podemos construirlo a partir de rho_1_gen\n",
        "def base_hamiltonian_aux(mat, size, d, rho_1_gen):\n",
        "    # Construccion de H\n",
        "    rho_1_gen_transposed = rho_1_gen.transpose(1, 0, 2, 3)\n",
        "    mat_expanded = mat[:, :, np.newaxis, np.newaxis]\n",
        "    h = np.sum(mat_expanded * rho_1_gen_transposed[:, :, :, :], axis=(0, 1))\n",
        "    return h.astype(np.float32)\n",
        "\n",
        "def base_hamiltonian(mat, basis, rho_1_gen):\n",
        "    return base_hamiltonian_aux(mat, basis.size, basis.d, rho_1_gen)\n",
        "\n",
        "def two_body_hamiltonian(t_basis_size, m, energy, G, rho_1_arrays, rho_2_arrays):\n",
        "    # Creamos la mat diagonal de d*d con los elementos de energy\n",
        "    # cada uno de estos, se contraen con los elementos de rho_1_arrays\n",
        "    # la mat energy contiene las energias de cada termino c^\\dag_k c_k para k kbar (iguales)\n",
        "    # por ello los elementos se repiten \n",
        "    energy_matrix = np.diagflat(np.kron(energy, np.ones(2))) + eps * np.random.random((2*m,2*m))\n",
        "    \n",
        "    # Construimos la mat de energía\n",
        "    h0 = np.sum(energy_matrix[:, :, np.newaxis, np.newaxis] * rho_1_arrays[:, :, :, :], axis=(0, 1))\n",
        "\n",
        "    # Pasamos ahora a la matrix de interacción con la misma estrategia\n",
        "    # dada G que indica la interacción entre los pares k' k'bar k kbar \n",
        "    # (que son elementos particulares de t_basis)\n",
        "    # transladamos estos coeficientes a una matriz en t_basis\n",
        "    # y multiplicamos por rho_2_arrays\n",
        "    \n",
        "    # Primero determinamos, dada t_basis, cuales son los indices de pares kkbar\n",
        "    offset = 4*m+1\n",
        "    indices = [-(k-1)*(2*k-offset) for k in range(1,m+1)] # Lo saque de Mathmatica, vos confia\n",
        "    i, j = np.meshgrid(indices, indices, indexing='ij') # Lo usamos para rellenar la mat deseada\n",
        "    \n",
        "    # Contruimos la mat que contraeremos con rho_2_arrays\n",
        "    mat = np.zeros((t_basis_size, t_basis_size))\n",
        "    mat[i, j] = G\n",
        "\n",
        "    hi = -np.sum(mat[:, :, np.newaxis, np.newaxis] * rho_2_arrays[:, :, :, :], axis=(0, 1))\n",
        "\n",
        "    return (h0, hi)\n",
        "\n",
        "def solve(h, last_step = None):\n",
        "    sol = linalg.eigsh(h, which='SA',k=19)\n",
        "    eigenspace_tol = 0.0001\n",
        "    if type(last_step) != type(None):\n",
        "        # Seleccionamos todos los autovects que difieren sus autovalores menos que tol (mismo autoespacio)\n",
        "        # y tomamos la proyección en el autoespacio de la solución del paso anterior (last_step)\n",
        "        eig = sol[0].real\n",
        "        eigv = sol[1]\n",
        "        cand = [eigv[:,i].real  for (i, x) in enumerate(eig) if abs(x-min(eig)) < eigenspace_tol]\n",
        "        cand_norm = [x/np.linalg.norm(x) for x in cand]\n",
        "        fund = np.zeros(len(cand[0]))\n",
        "        for x in cand_norm:\n",
        "            fund += np.dot(last_step,x) * x\n",
        "    else:\n",
        "        argmin = np.argmin(sol[0].real)\n",
        "        fund = sol[1][:,argmin]\n",
        "    fund = fund.real / np.linalg.norm(fund)\n",
        "    return fund"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emVBTg2QD-Fg"
      },
      "source": [
        "## Modelo de ML\n",
        "Basado en matrices densidad de 1 y 2 cuerpos como input, con hamiltoniano como salida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aF_Ec_mCGX96"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-06 17:53:59.588207: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-12-06 17:54:00.346451: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-06 17:54:00.346490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-06 17:54:00.350943: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-06 17:54:00.727857: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-06 17:54:07.056335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /device:GPU:0 with 20609 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tf.test.gpu_device_name()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJDoa6LUJJ8O",
        "outputId": "73481454-fbcb-469f-d72f-cd0f8d534808"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15\n",
            "[[1 0 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " [0 0 1 0 0 0]\n",
            " [0 0 0 1 0 0]\n",
            " [0 0 0 0 1 0]\n",
            " [0 0 0 0 0 1]]\n",
            "6\n",
            "[[1 0 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " [0 0 1 0 0 0]\n",
            " [0 0 0 1 0 0]\n",
            " [0 0 0 0 1 0]\n",
            " [0 0 0 0 0 1]]\n"
          ]
        }
      ],
      "source": [
        "# Construccion de bases para calculo de rho1 y rho2\n",
        "# rho2\n",
        "m = 2\n",
        "m2_basis = fixed_basis(m, d)\n",
        "print(m2_basis.size)\n",
        "nm2_basis = fixed_basis(basis.m-m, d)\n",
        "print(nm2_basis.base)\n",
        "t_basis = fixed_basis(2, basis.d)\n",
        "# rho1\n",
        "m = 1\n",
        "m1_basis = fixed_basis(m, d)\n",
        "print(m1_basis.size)\n",
        "print(m1_basis.base)\n",
        "nm1_basis = fixed_basis(basis.m-m, d)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oapxWkD16fHg"
      },
      "source": [
        "### Algunos benchmarks y funciones auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "umCIrxCZKXQd"
      },
      "outputs": [],
      "source": [
        "# Given h calculo en rho2 y rho1 máximo\n",
        "def rho1_rho2(h, beta):\n",
        "    fund = thermal_state(h, beta)\n",
        "    rho2 = np.array(rho_2(basis, m2_basis.size, state, rho_2_arrays))\n",
        "    r = np.sort(linalg_d.eigvals(rho2).real)\n",
        "    rho_2_max = r[0]\n",
        "    rho1 = np.array(rho_1(basis, state, rho_1_arrays))\n",
        "    r = np.sort(linalg_d.eigvals(rho1).real)\n",
        "    rho_1_max = r[0]\n",
        "\n",
        "    return (rho_1_max, rho_2_max)\n",
        "\n",
        "def fill_triangular_np(x):\n",
        "    m = x.shape[0]\n",
        "    n = np.int32(np.sqrt(.25 + 2 * m) - .5)\n",
        "    x_tail = x[(m - (n**2 - m)):]\n",
        "    return np.triu(np.concatenate([x, x_tail[::-1]], 0).reshape(n, n))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QaNnIIc5bZux"
      },
      "outputs": [],
      "source": [
        "# TEST: Las funciones de TF y comunes coinciden\n",
        "\n",
        "# Dado h, \\beta, construyo el estado térmico\n",
        "from scipy.linalg import expm\n",
        "\n",
        "def thermal_state(h, beta):\n",
        "    quotient = expm(-beta*h)\n",
        "    return quotient / np.trace(quotient)\n",
        "\n",
        "## NO usar para mat no hermiticas\n",
        "@nb.jit(nopython=True)\n",
        "def thermal_state_eig(h, beta):\n",
        "    w, v = np.linalg.eigh(-beta*h)\n",
        "    D = np.diag(np.exp(w))\n",
        "    mat = v @ D @ v.T\n",
        "    mat = mat / np.trace(mat)\n",
        "    return mat\n",
        "    \n",
        "def gen_to_h(base, rho_1_arrays):\n",
        "    triag = fill_triangular_np(base)\n",
        "    body_gen = triag + np.transpose(triag)-np.diag(np.diag(triag))\n",
        "    h = np.array(base_hamiltonian(body_gen, basis, rho_1_arrays))  \n",
        "    return h \n",
        "\n",
        "def gen_to_h_1b(hamil_base):\n",
        "    triag = tfp.math.fill_triangular(hamil_base, upper=True)\n",
        "    body_gen = triag + tf.transpose(triag, perm=[0,2,1])-tf.linalg.diag(tf.linalg.diag_part(triag))\n",
        "    return body_gen\n",
        "\n",
        "def gen_to_h_tf(hamil_base, rho_1_arrays):\n",
        "    triag = tfp.math.fill_triangular(hamil_base, upper=True)\n",
        "    body_gen = triag + tf.transpose(triag, perm=[0,2,1])-tf.linalg.diag(tf.linalg.diag_part(triag)) # Simetrizamos y generamos la matriz de h\n",
        "    hamil_expanded = body_gen[:, :, :, np.newaxis, np.newaxis]\n",
        "    rho_1_gen_transposed = tf.transpose(rho_1_arrays, perm=[1, 0, 2, 3])\n",
        "    h_arr = tf.reduce_sum(hamil_expanded * rho_1_gen_transposed[np.newaxis,:,:,:,:], axis=[1,2])\n",
        "    return h_arr\n",
        "\n",
        "def thermal_state_tf(h):\n",
        "    # Assume beta=1\n",
        "    exp_hamiltonian = tf.linalg.expm(-h)\n",
        "    partition_function = tf.linalg.trace(exp_hamiltonian)\n",
        "    partition_function = tf.expand_dims(partition_function, axis=1)\n",
        "    partition_function = tf.expand_dims(partition_function, axis=1)\n",
        "    \n",
        "    rho = exp_hamiltonian / partition_function\n",
        "\n",
        "    return rho\n",
        "\n",
        "def rho_1_tf(state, rho_1_arrays):\n",
        "    state = tf.expand_dims(state, axis=1)  # Shape: (5120, 10, 1, 10)\n",
        "    state_expanded = tf.expand_dims(state, axis=1)\n",
        "    rho_1_arrays_expanded = tf.expand_dims(rho_1_arrays, axis=0)  # Shape: (1, 5, 5, 10, 10)\n",
        "    product = state_expanded * rho_1_arrays_expanded  # Shape: (5120, 10, 5, 10, 10)\n",
        "    mat = tf.reduce_sum(product, axis=[-2, -1])  # Shape: (5120, 5, 5)\n",
        "    \n",
        "    return mat\n",
        "\n",
        "def rho_2_tf(state, rho_2_arrays):\n",
        "    state = tf.expand_dims(state, axis=1)  # Shape: (5120, 10, 1, 10)\n",
        "    state_expanded = tf.expand_dims(state, axis=1)\n",
        "    rho_2_arrays_expanded = tf.expand_dims(rho_2_arrays, axis=0)  # Shape: (1, 5, 5, 10, 10)\n",
        "    product = state_expanded * rho_2_arrays_expanded  # Shape: (5120, 10, 5, 10, 10)\n",
        "    mat = tf.reduce_sum(product, axis=[-2, -1])  # Shape: (5120, 5, 5)\n",
        "    \n",
        "    return mat\n",
        "\n",
        "# NOTA: para calcular el bloque rho2kkbar, utilizar en lugar\n",
        "\n",
        "def rho_1_gc_tf(hamil_base):\n",
        "    e, v = tf.linalg.eigh(gen_to_h_1b(hamil_base))\n",
        "    result = 1 / (1 + tf.exp(e))\n",
        "    result = tf.linalg.diag(result)\n",
        "    res = tf.linalg.matmul(v,result)\n",
        "    res = tf.linalg.matmul(res,v,adjoint_b=True)\n",
        "    \n",
        "    return tf.cast(res, tf.float32)\n",
        "\n",
        "# Aux function\n",
        "def outer_product(vector):\n",
        "    return tf.einsum('i,j->ij', vector, vector)\n",
        "\n",
        "def pure_state(h):\n",
        "    e, v = tf.linalg.eigh(h)\n",
        "    fund = v[:,:,0]\n",
        "    d = tf.map_fn(outer_product, fund)\n",
        "    return d\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylpy_BCw6jxF"
      },
      "source": [
        "### Construccion de dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Version sincrónica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2is_Eo_qGpEz",
        "outputId": "9a968190-59f2-4695-ef18-b99ff5b4a212"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-06 17:54:08.361003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20609 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
            "2023-12-06 17:54:10.777593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /device:GPU:0 with 20609 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/device:GPU:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|                                                                                                                                                                                       | 0/49 [00:00<?, ?it/s]2023-12-06 17:54:17.576017: I tensorflow/core/util/cuda_solvers.cc:179] Creating GpuSolver handles for stream 0x559e76328d60\n",
            "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [01:41<00:00,  2.07s/it]\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "# Config\n",
        "num_samples = 50000\n",
        "use_gpu = True\n",
        "gpu_batch_size = 1024\n",
        "\n",
        "# Construccion de parametros y matrices auxiliares\n",
        "rho1_size = m1_basis.size\n",
        "rho2_size = m2_basis.size\n",
        "fund_size = basis.size\n",
        "hamil_base_size = basis.d*(basis.d+1)//2\n",
        "rho_1_arrays = rho_1_gen(basis)\n",
        "rho_1_arrays_tf = tf.constant(rho_1_arrays, dtype=tf.float32)\n",
        "rho_2_arrays = rho_2_gen(basis, nm2_basis, m2_basis)\n",
        "rho_2_arrays_tf = tf.constant(rho_2_arrays, dtype=tf.float32)\n",
        "rho_2_arrays_kkbar = rho_2_kkbar_gen(basis.m, rho_2_arrays)\n",
        "rho_2_arrays_kkbar_tf = tf.constant(rho_2_arrays_kkbar, dtype=tf.float32)\n",
        "\n",
        "if use_gpu:\n",
        "    print(tf.test.gpu_device_name())\n",
        "    datasets = []\n",
        "    for i in tqdm(range(num_samples//gpu_batch_size+1)):\n",
        "        size = basis.d*(basis.d+1)//2\n",
        "        # En una primera versión vamos a pasar una mat proporcional a range(0,m) para energias\n",
        "        # y como interacción una cte G por ones(m,m)\n",
        "        h_labels = [(np.random.rand(), np.random.rand()) for _ in range(0,gpu_batch_size)] # Generamos los generadores\n",
        "        hamil_base = tf.constant(h_labels, dtype=tf.float32)\n",
        "        h_arr = np.zeros((gpu_batch_size, basis.size, basis.size))\n",
        "        for i, (e, g) in enumerate(h_labels):\n",
        "            (h0, hi) = two_body_hamiltonian(t_basis.size, basis.m, e*np.arange(0, basis.m), g * np.ones((m,m)), rho_1_arrays_tf, rho_2_arrays_tf)\n",
        "            h_arr[i,:,:] = h0 + hi\n",
        "        # Estados térmicos\n",
        "        state = thermal_state_tf(h_arr) \n",
        "        state = tf.cast(state, dtype=tf.float32)\n",
        "        # Estados puros\n",
        "        #state = pure_state(h_arr)\n",
        "        #rho_1_input = rho_1_tf(state, rho_1_arrays_tf)\n",
        "        rho_2_input = rho_2_tf(state, rho_2_arrays_kkbar_tf)\n",
        "\n",
        "        datasets.append(tf.data.Dataset.from_tensor_slices(((rho_2_input), h_labels)))\n",
        "        #datasets.append(tf.data.Dataset.from_tensor_slices(((rho_1_input, rho_2_input), h_labels)))\n",
        "        #datasets.append(tf.data.Dataset.from_tensor_slices(((rho_1_input, rho_2_input, state), h_labels)))\n",
        "    ds = tf.data.Dataset.from_tensor_slices(datasets)\n",
        "    dataset = ds.interleave(\n",
        "        lambda x: x,\n",
        "        cycle_length=1,\n",
        "        num_parallel_calls=tf.data.AUTOTUNE,\n",
        "    )\n",
        "\n",
        "#batch_size = 32\n",
        "#dataset = dataset.shuffle(buffer_size=num_samples).batch(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Filleo de dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "# Save and load dataset\n",
        "save_dataset = False\n",
        "load_dataset = False\n",
        "path = \"/home/agus/TF\"\n",
        "#num_samples = 5000000\n",
        "if save_dataset:\n",
        "    tf.data.Dataset.save(dataset, path)\n",
        "    with open(\"/home/agus/\"+'/file.pkl', 'wb') as file:\n",
        "        pickle.dump(beta_input, file)\n",
        "if load_dataset:\n",
        "    dataset = tf.data.Dataset.load(path)\n",
        "    with open(\"/home/agus/\"+'file.pkl', 'rb') as file:\n",
        "        beta_input = pickle.load(file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8moZIlfabZuy"
      },
      "outputs": [],
      "source": [
        "# Dividimos los datasets\n",
        "train_size = int(0.8 * num_samples)\n",
        "\n",
        "train_dataset = dataset.take(train_size)\n",
        "val_dataset = dataset.skip(train_size)\n",
        "\n",
        "\n",
        "batch_size = 256\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "val_dataset = val_dataset.batch(batch_size)\n",
        "\n",
        "#beta_val = beta_input[train_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Dataset Size: -2\n"
          ]
        }
      ],
      "source": [
        "# Cardinality no funciona con los datasets generados por GPU\n",
        "val_size = tf.data.experimental.cardinality(val_dataset).numpy()\n",
        "print(\"Validation Dataset Size:\", val_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYEEjNB-7b8y"
      },
      "source": [
        "### Definición de modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8kkhJr5K0ZQ",
        "outputId": "f1b731f1-6a02-4181-f0b5-5677a2a85784"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rho2 (InputLayer)           [(None, 15, 15, 1)]       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 14, 14, 32)        160       \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 14, 14, 32)        128       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 13, 13, 16)        2064      \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 13, 13, 16)        64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 6, 6, 16)          0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 576)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 24)                13848     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 50        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16314 (63.73 KB)\n",
            "Trainable params: 16218 (63.35 KB)\n",
            "Non-trainable params: 96 (384.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Definicion de layers basado en Conv 2D\n",
        "\n",
        "# Factor de cantidad de filtros\n",
        "lf = 8  \n",
        "conv_limit = (rho2_size - 4) // 4 \n",
        "initial_dense = (lf*2**(conv_limit-1)*((rho2_size-(conv_limit-1))//2)**2) // 8\n",
        "## rho 1\n",
        "rho2_layer =  tf.keras.layers.Input(shape=(rho2_size,rho2_size, 1), name='rho2')\n",
        "\n",
        "# Procesamos el primer input\n",
        "conv_rho2 = tf.keras.layers.Conv2D(lf*2**conv_limit, (2, 2), activation='relu')(rho2_layer)\n",
        "conv_rho2 = tf.keras.layers.BatchNormalization()(conv_rho2)\n",
        "for j in [(2**conv_limit - 2**k) for k in range(1,conv_limit)]:\n",
        "    conv_rho2 = tf.keras.layers.Conv2D(lf*j, (2, 2), activation='relu')(conv_rho2 if 2**j != 1 else rho1_layer)\n",
        "    conv_rho2 = tf.keras.layers.BatchNormalization()(conv_rho2)\n",
        "\n",
        "conv_rho2 = tf.keras.layers.MaxPooling2D((2, 2))(conv_rho2)\n",
        "\n",
        "flatten_rho2 = tf.keras.layers.Flatten()(conv_rho2)\n",
        "#flatten_rho1 = tf.keras.layers.Dense(rho1_size*rho1_size, activation='relu')(flatten_rho1)\n",
        "\n",
        "#local_size = basis.size*basis.size\n",
        "local_size = hamil_base_size\n",
        "\n",
        "#dense1 = tf.keras.layers.Dense(8*8*4*4, activation='relu')(dense1)\n",
        "#dense1 = tf.keras.layers.Dense(512, activation='relu')(flatten_rho1)\n",
        "#dense1 = tf.keras.layers.Dense(128, activation='relu')(flatten_rho1)\n",
        "dense1 = tf.keras.layers.Dense(initial_dense // 4, activation='relu')(flatten_rho2)\n",
        "#dense1 = tf.keras.layers.Dense(initial_dense//2, activation='relu')(dense1)\n",
        "\n",
        "output = tf.keras.layers.Dense(2)(dense1)\n",
        "\n",
        "\n",
        "# Creamos el modelo y compulamos\n",
        "#model = tf.keras.models.Model(inputs=[rho1_layer, rho2_layer, fund_layer], outputs=output)\n",
        "#model = tf.keras.models.Model(inputs=[rho1_layer, rho2_layer], outputs=output)\n",
        "model = tf.keras.models.Model(inputs=[rho2_layer], outputs=output)\n",
        "\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZBtonvGbZuz",
        "outputId": "f197277e-a84b-4ffd-c81f-c81581707fb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rho1 (InputLayer)           [(None, 3, 3, 1)]         0         \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 9)                 0         \n",
            "                                                                 \n",
            " concatenate_7 (Concatenate  (None, 9)                 0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 16)                160       \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 32)                544       \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 64)                2112      \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 2)                 130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2946 (11.51 KB)\n",
            "Trainable params: 2946 (11.51 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Modelo denso + fundamental\n",
        "rho2_layer =  tf.keras.layers.Input(shape=(basis.m,basis.m, 1), name='rho1')\n",
        "flatten_rho2 = tf.keras.layers.Flatten()(rho2_layer)\n",
        "#rho2_layer =  tf.keras.layers.Input(shape=(rho2_size,rho2_size, 1), name='rho2')\n",
        "#flatten_rho2 = tf.keras.layers.Flatten()(rho2_layer)\n",
        "#fund_layer =  tf.keras.layers.Input(shape=(fund_size, fund_size, 1 ), name='fund')\n",
        "#flatten_fund = tf.keras.layers.Flatten()(fund_layer)\n",
        "\n",
        "dense1 = tf.keras.layers.concatenate([flatten_rho2])\n",
        "#dense1 = tf.keras.layers.concatenate([dense1, flatten_fund])\n",
        "#dense1 = tf.keras.layers.Dense(rho1_size*rho1_size, activation='relu')(dense1)\n",
        "\n",
        "local_size = 2\n",
        "l=4\n",
        "layer_s = [128//2**i for i in reversed(range(1,l))]\n",
        "for i in range(0,l-1):\n",
        "    dense1 = tf.keras.layers.Dense(layer_s[i], activation='relu')(dense1)\n",
        "\n",
        "output = tf.keras.layers.Dense(local_size)(dense1)\n",
        "# Creamos el modelo y compulamos\n",
        "model = tf.keras.models.Model(inputs=[rho2_layer], outputs=output)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[10, 16, 32]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "layer_s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RgoMlCyyfBe-"
      },
      "outputs": [],
      "source": [
        "# LOSS FUNCTIONS\n",
        "r_size = basis.size\n",
        "\n",
        "# Custom loss function based on GS MSE\n",
        "def gs_loss(h_pred, h_true):\n",
        "    h_pred = tf.reshape(h_pred, shape=(-1,r_size,r_size))\n",
        "    e, v = tf.linalg.eigh(h_pred)\n",
        "    gs_pred = v[:, 0]\n",
        "\n",
        "    h_true = tf.reshape(h_true, shape=(-1,r_size,r_size))\n",
        "    e, v = tf.linalg.eigh(h_true)\n",
        "    gs_true = v[:, 0]\n",
        "\n",
        "    gs_diff = tf.norm(gs_true - gs_pred)\n",
        "\n",
        "    return gs_diff + tf.reduce_mean(tf.square(h_true - h_pred)) * 100\n",
        "\n",
        "def distance_to_hermitian(matrix):\n",
        "    hermitian_part = 0.5 * (matrix + tf.linalg.adjoint(matrix))\n",
        "    distance = tf.norm(matrix - hermitian_part, ord='euclidean')\n",
        "    return distance\n",
        "\n",
        "# Custom loss function based on MSE + non-hermitian penalization\n",
        "def herm_loss(h_pred, h_true):\n",
        "    h_pred_arr = tf.reshape(h_pred, shape=(-1,r_size,r_size))\n",
        "    return tf.reduce_mean(tf.square(h_true - h_pred)) + distance_to_hermitian(h_pred_arr)\n",
        "\n",
        "# Custom loss function based on h eigenvalues\n",
        "def eig_loss(h_pred, h_true):\n",
        "    eig_true = tf.sort(tf.math.real(tf.linalg.eigvals(tf.reshape(h_true, (-1, fund_size, fund_size)))))\n",
        "    eig_pred = tf.sort(tf.math.real(tf.linalg.eigvals(tf.reshape(h_pred, (-1, fund_size, fund_size)))))\n",
        "    return tf.reduce_mean(tf.square(eig_true - eig_pred))\n",
        "\n",
        "# MSE with a factor\n",
        "def mse_f(h_pred, h_true):\n",
        "    f = 100\n",
        "    return tf.reduce_mean(tf.square(h_true - h_pred))*f\n",
        "\n",
        "# Spectral radius loss\n",
        "def spectral_loss(h_pred, h_true):\n",
        "    eig = tf.math.real(tf.linalg.eigvals(tf.reshape(h_true-h_pred, (-1, fund_size, fund_size))))\n",
        "    return tf.math.reduce_max(tf.abs(eig))\n",
        "\n",
        "# Hamiltonian MSE loss (using generators)\n",
        "def base_mse_loss(base_pred, base_true):\n",
        "    h_pred = gen_to_h_tf(base_pred, rho_1_arrays_tf)\n",
        "    h_true = gen_to_h_tf(base_true, rho_1_arrays_tf)\n",
        "    mat = tf.reshape(h_pred-h_true, (-1, fund_size, fund_size))\n",
        "    return tf.norm(mat, ord='fro', axis=[-1, -2])\n",
        "\n",
        "# Custom loss function based on h eigenvalues (using generators)\n",
        "def base_eig_loss(base_pred, base_true):\n",
        "    h_pred = gen_to_h_tf(base_pred, rho_1_arrays_tf)\n",
        "    h_true = gen_to_h_tf(base_true, rho_1_arrays_tf)\n",
        "    eig_true = tf.sort(tf.math.real(tf.linalg.eigvals(tf.reshape(h_true, (-1, fund_size, fund_size)))))\n",
        "    eig_pred = tf.sort(tf.math.real(tf.linalg.eigvals(tf.reshape(h_pred, (-1, fund_size, fund_size)))))\n",
        "    return tf.reduce_mean(tf.square(eig_true - eig_pred))\n",
        "\n",
        "# Custom loss function based on rho1 eigenvals\n",
        "## Auxiliary function\n",
        "def base_to_rho_1_tf(base_pred):\n",
        "    h = gen_to_h_tf(base_pred, rho_1_arrays_tf)\n",
        "    h = tf.reshape(h, (-1, fund_size, fund_size))\n",
        "    state = thermal_state_tf(h)\n",
        "    rho1 = rho_1_tf(state, rho_1_arrays_tf)\n",
        "    return rho1\n",
        "    \n",
        "def rho1_loss(base_pred, base_true):\n",
        "    mat = base_to_rho_1_tf(base_pred) - base_to_rho_1_tf(base_true)\n",
        "    return tf.norm(mat, ord='fro', axis=[-1, -2])\n",
        "\n",
        "# Custom loss function based on rho1 eigenvals (using generators)\n",
        "def base_rho1_loss(base_pred, base_true):\n",
        "    h_pred = gen_to_h_tf(base_pred, rho_1_arrays_tf)\n",
        "    h_true = gen_to_h_tf(base_true, rho_1_arrays_tf)\n",
        "    return tf.reduce_mean(tf.square(rho_1_eig_tf(h_pred) - rho_1_eig_tf(h_true)))*1000\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiWk9piJtNIZ"
      },
      "source": [
        "## Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhJCHf0fQdRl",
        "outputId": "1821cf27-9ff5-4d67-e9f5-956d20eda5e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-06 17:57:54.805562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /device:GPU:0 with 20609 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
            "2023-12-06 17:57:56.017390: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559ec4d2aff0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2023-12-06 17:57:56.017421: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA L4, Compute Capability 8.9\n",
            "2023-12-06 17:57:56.030604: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2023-12-06 17:57:56.365453: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8900\n",
            "2023-12-06 17:57:56.479764: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    146/Unknown - 3s 3ms/step - loss: 0.0434 - accuracy: 0.8078 - mean_squared_error: 0.0434"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-06 17:57:57.716537: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9743857298035749497\n",
            "2023-12-06 17:57:57.716594: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 13445621741850301056\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 3s 6ms/step - loss: 0.0408 - accuracy: 0.8187 - mean_squared_error: 0.0408 - val_loss: 0.0020 - val_accuracy: 0.9760 - val_mean_squared_error: 0.0020\n",
            "Epoch 2/10\n",
            " 57/157 [=========>....................] - ETA: 0s - loss: 6.5063e-04 - accuracy: 0.9816 - mean_squared_error: 6.5063e-04"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-06 17:57:58.179602: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9743857298035749497\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 1s 5ms/step - loss: 4.3003e-04 - accuracy: 0.9856 - mean_squared_error: 4.3003e-04 - val_loss: 2.1912e-04 - val_accuracy: 0.9922 - val_mean_squared_error: 2.1912e-04\n",
            "Epoch 3/10\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.5835e-04 - accuracy: 0.9940 - mean_squared_error: 1.5835e-04 - val_loss: 1.1159e-04 - val_accuracy: 0.9972 - val_mean_squared_error: 1.1159e-04\n",
            "Epoch 4/10\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 9.0027e-05 - accuracy: 0.9954 - mean_squared_error: 9.0027e-05 - val_loss: 6.6300e-05 - val_accuracy: 0.9968 - val_mean_squared_error: 6.6300e-05\n",
            "Epoch 5/10\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.6953e-05 - accuracy: 0.9965 - mean_squared_error: 5.6953e-05 - val_loss: 4.6116e-05 - val_accuracy: 0.9977 - val_mean_squared_error: 4.6116e-05\n",
            "Epoch 6/10\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.9948e-05 - accuracy: 0.9974 - mean_squared_error: 3.9948e-05 - val_loss: 3.1641e-05 - val_accuracy: 0.9980 - val_mean_squared_error: 3.1641e-05\n",
            "Epoch 7/10\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.8794e-05 - accuracy: 0.9978 - mean_squared_error: 2.8794e-05 - val_loss: 2.2994e-05 - val_accuracy: 0.9980 - val_mean_squared_error: 2.2994e-05\n",
            "Epoch 8/10\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.2145e-05 - accuracy: 0.9975 - mean_squared_error: 2.2145e-05 - val_loss: 2.4332e-05 - val_accuracy: 0.9968 - val_mean_squared_error: 2.4332e-05\n",
            "Epoch 9/10\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.8483e-05 - accuracy: 0.9974 - mean_squared_error: 1.8483e-05 - val_loss: 1.7527e-05 - val_accuracy: 0.9976 - val_mean_squared_error: 1.7527e-05\n",
            "Epoch 10/10\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.5923e-05 - accuracy: 0.9977 - mean_squared_error: 1.5923e-05 - val_loss: 1.3511e-05 - val_accuracy: 0.9979 - val_mean_squared_error: 1.3511e-05\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.optimizers import RMSprop, Adam, Nadam, Lion\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(),\n",
        "              loss='MSE',  \n",
        "              metrics=['accuracy', 'mean_squared_error'])\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "    history = model.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "cvpE_X1iTXcB",
        "outputId": "eff0e5f5-5b26-46ea-ec6b-491d1de9944c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "#plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.plot(history.history['mean_squared_error'], label='Training MSE')\n",
        "plt.plot(history.history['val_mean_squared_error'], label='Validation MSE')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE')\n",
        "plt.title('MSE vs. Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# MIN LOSS = 0.0128 c/fund 50epochs MSE\n",
        "##         = 0.0118 s/fund 50epochs MSE\n",
        "##         = 0.0039 s/fund 50epochs MSE m=4 d=6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRlZuRUNa6Yb",
        "outputId": "85850559-311b-4cf4-ea5b-465a9ee8a7af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8/8 [==============================] - 0s 1ms/step\n"
          ]
        }
      ],
      "source": [
        "# Assuming you have a validation dataset (val_dataset)\n",
        "iterador = iter(val_dataset)\n",
        "sample = next(iterador)\n",
        "next_sample = next(iterador)\n",
        "input_data = sample[0]  # Assuming your dataset provides input data as the first element\n",
        "actual_values = sample[1]  # Assuming your dataset provides actual labels as the second element\n",
        "\n",
        "# Predict using the model\n",
        "predictions = model.predict(input_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(256, 3, 3), dtype=float32, numpy=\n",
              "array([[[0.53074   , 0.0716527 , 0.08921236],\n",
              "        [0.0716527 , 0.240579  , 0.07165284],\n",
              "        [0.08921236, 0.07165284, 0.09665501]],\n",
              "\n",
              "       [[0.6273009 , 0.05147061, 0.07807299],\n",
              "        [0.05147061, 0.20872231, 0.05147063],\n",
              "        [0.07807299, 0.05147063, 0.05382173]],\n",
              "\n",
              "       [[0.58406126, 0.07111894, 0.09698345],\n",
              "        [0.07111894, 0.23221023, 0.07111884],\n",
              "        [0.09698345, 0.07111884, 0.08192606]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0.49247777, 0.08055238, 0.09435193],\n",
              "        [0.08055238, 0.25239825, 0.08055242],\n",
              "        [0.09435193, 0.08055242, 0.11943947]],\n",
              "\n",
              "       [[0.2809168 , 0.07624207, 0.07636237],\n",
              "        [0.07624207, 0.26088822, 0.07624208],\n",
              "        [0.07636237, 0.07624208, 0.24196701]],\n",
              "\n",
              "       [[0.29098335, 0.03432403, 0.03487515],\n",
              "        [0.03432403, 0.22659275, 0.03432403],\n",
              "        [0.03487515, 0.03432403, 0.17278518]]], dtype=float32)>"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([0.7000782  0.80663306], shape=(2,), dtype=float32)\n",
            "[0.70169055 0.8077759 ]\n",
            "tf.Tensor([0.9917695  0.71810097], shape=(2,), dtype=float32)\n",
            "[0.9901303  0.72209466]\n",
            "tf.Tensor([0.848388  0.8957594], shape=(2,), dtype=float32)\n",
            "[0.8478959  0.89457726]\n",
            "tf.Tensor([0.09022991 0.8853695 ], shape=(2,), dtype=float32)\n",
            "[0.09031808 0.884185  ]\n",
            "0.0035659713 0.40514684\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Vemos algunos valores\n",
        "for e in val_dataset:\n",
        "    for i in range(0, 4):\n",
        "        print(e[1][i])\n",
        "        print(predictions[i])\n",
        "    break\n",
        "    \n",
        "\n",
        "RMSE_pred = mean_squared_error(actual_values, predictions, squared=False)\n",
        "RMSE_rand = mean_squared_error(actual_values, next_sample[1], squared=False)\n",
        "print(RMSE_pred, RMSE_rand)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ds5iD1OMbZu3"
      },
      "outputs": [],
      "source": [
        "m_size = basis.size\n",
        "rho_1_pred = []\n",
        "rho_1_actual = []\n",
        "norm = []\n",
        "norm_rand = []\n",
        "printear =  False\n",
        "\n",
        "for e in val_dataset:\n",
        "    for i in range(0, 3 if printear else batch_size):\n",
        "        # Valores actuales\n",
        "        #h = e[1][i].numpy().reshape(basis.size,basis.size)\n",
        "        h_true = gen_to_h(e[1][i], rho_1_arrays)\n",
        "        #print(h) if printear else 0\n",
        "        r = max(np.linalg.eigvals(e[0][i]))\n",
        "        rho_1_actual.append(r)\n",
        "\n",
        "        print(h_true) if printear else 0\n",
        "        print(r) if printear else 0\n",
        "\n",
        "        # Valores predichos\n",
        "        #h = predictions[i].reshape(basis.size,basis.size)\n",
        "        h_pred = gen_to_h(predictions[i], rho_1_arrays)\n",
        "        beta = 1\n",
        "        # Estado térmico\n",
        "        state = thermal_state(h_pred, beta)\n",
        "        # Estado puro\n",
        "        #state = pure_state(h_pred)\n",
        "        rho1 = np.array(rho_1(basis.d, state, rho_1_arrays))\n",
        "        r = max(np.sort(linalg_d.eigvals(rho1).real))\n",
        "        rho_1_pred.append(r)\n",
        "\n",
        "        print(h_pred) if printear else 0\n",
        "        print(r) if printear else 0\n",
        "        \n",
        "\n",
        "        # Normas\n",
        "        norm.append(np.linalg.norm(h_true-h_pred, ord='fro'))\n",
        "        print(f'Norma {norm[-1]}') if printear else 0\n",
        "        ## Vamos a comparar con un h aleatorio\n",
        "        size = basis.d*(basis.d+1)//2\n",
        "        base = np.random.uniform(low=0, high=1.0, size=(size,))\n",
        "        h_rand = gen_to_h(base, rho_1_arrays)\n",
        "        norm_rand.append(np.linalg.norm(h_true-h_rand, ord='fro'))\n",
        "        #print(f'Norma random {norm_rand[-1]}') if printear else 0\n",
        "        print('') if printear else 0\n",
        "        \n",
        "\n",
        "\n",
        "    # e contiene todo el batch y nos basta con uno\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(e[1][10])\n",
        "predictions[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "AL2EC9Ci-0HG",
        "outputId": "545ebe57-d3de-490f-f076-709d5c47b5f2"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "f=1\n",
        "rho_1_actual = np.array(rho_1_actual)\n",
        "rho_1_pred = np.array(rho_1_pred)\n",
        "#print(mean_squared_error(rho_1_pred, rho_1_actual))\n",
        "\n",
        "print('Rho1 based statistics')\n",
        "print(np.mean(np.abs(rho_1_actual-rho_1_pred)))\n",
        "print(np.mean(rho_1_actual)*f)\n",
        "print('std')\n",
        "print(np.std(rho_1_actual-rho_1_pred)*f)\n",
        "print(np.std(rho_1_actual)*f)\n",
        "print(np.std(rho_1_pred)*f)\n",
        "plt.hist(np.array(rho_1_pred-rho_1_actual), bins=50)\n",
        "plt.show()\n",
        "print('H based statistics')\n",
        "print(np.mean(norm), np.mean(norm_rand))\n",
        "print(np.mean(norm_rand)/np.mean(norm))\n",
        "\n",
        "\n",
        "# BEST: FACTOR 1/8 c/fund\n",
        "## 500 epochs, 10M dataset\n",
        "# BEST: FACTOR 1/9 s/fund\n",
        "## 50 epochs, 5M dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "6.25/1.30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 25 epochs d = m*2\n",
        "res = {}\n",
        "res[5] = 35/8.19 \n",
        "res[4] = 15/2.47\n",
        "res[3] = 6.2/1.73"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YioVllOX3M1N",
        "outputId": "b7715c37-1400-4c04-8be3-dd247b4b9db9"
      },
      "outputs": [],
      "source": [
        "# Get the weights of all dense layers in the model\n",
        "dense_weights = []\n",
        "for layer in model.layers:\n",
        "    if isinstance(layer, tf.keras.layers.Dense):\n",
        "        weights = layer.get_weights()\n",
        "        if len(weights) > 0:\n",
        "            dense_weights.append(weights[0])\n",
        "\n",
        "# Visualize the weights of each dense layer\n",
        "for i, weights in enumerate(dense_weights):\n",
        "    plt.figure()\n",
        "    plt.imshow(weights, cmap='viridis', aspect='auto')\n",
        "    plt.colorbar()\n",
        "    plt.title(f\"Dense Layer {i+1} Weights Visualization\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rho_2_arrays[9,0,0,9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 0 0 0 1 1] [0 1 1 0 0 0]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Genera las matrices de rho2\n",
        "def rho_2_gen(basis, mll_basis, t_basis):\n",
        "    size = t_basis.size\n",
        "    s = basis.size\n",
        "    # La entrada i, j contiene C_j^\\dag C_i    i, j \\in t_basis\n",
        "    mat = np.empty((size, size, s, s), dtype=np.float32)\n",
        "    for i, v in enumerate(t_basis.base):\n",
        "        for j, w in enumerate(t_basis.base):\n",
        "            c_i = b_gen(basis, mll_basis, rep_to_exi(v))\n",
        "            cdag_j = bd_gen(mll_basis, basis, rep_to_exi(w))\n",
        "            mat[i, j, :, :] = np.dot(cdag_j, c_i)\n",
        "            if mat[i,j,0,9] != 0:\n",
        "                print(v,w)\n",
        "\n",
        "    return mat\n",
        "\n",
        "r = rho_2_gen(basis, basis_m2, t_basis)\n",
        "r[9,0,0,9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "print(b_gen(basis, mll_basis, rep_to_exi([0,0,0,1,1,0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 1, 1, 0, 0, 0],\n",
              "       [1, 1, 0, 1, 0, 0],\n",
              "       [1, 1, 0, 0, 1, 0],\n",
              "       [1, 1, 0, 0, 0, 1],\n",
              "       [1, 0, 1, 1, 0, 0],\n",
              "       [1, 0, 1, 0, 1, 0],\n",
              "       [1, 0, 1, 0, 0, 1],\n",
              "       [1, 0, 0, 1, 1, 0],\n",
              "       [1, 0, 0, 1, 0, 1],\n",
              "       [1, 0, 0, 0, 1, 1],\n",
              "       [0, 1, 1, 1, 0, 0],\n",
              "       [0, 1, 1, 0, 1, 0],\n",
              "       [0, 1, 1, 0, 0, 1],\n",
              "       [0, 1, 0, 1, 1, 0],\n",
              "       [0, 1, 0, 1, 0, 1],\n",
              "       [0, 1, 0, 0, 1, 1],\n",
              "       [0, 0, 1, 1, 1, 0],\n",
              "       [0, 0, 1, 1, 0, 1],\n",
              "       [0, 0, 1, 0, 1, 1],\n",
              "       [0, 0, 0, 1, 1, 1]])"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "basis.base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(6, 20)\n",
            "[array([0, 1, 0, 1, 1, 0])] [0 1 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "col = 1\n",
        "b = b_gen(basis, mll_basis, rep_to_exi([0,0,0,1,1,0]))\n",
        "print(b.shape)\n",
        "for x in range(0,b.shape[1]):\n",
        "    if b[col,x] != 0:\n",
        "        ind = x\n",
        "        break\n",
        "else:\n",
        "    ind = NaN\n",
        "\n",
        "print([basis.base[ind]], mll_basis.base[col])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "m = 3\n",
        "d = 2*m\n",
        "basis = fixed_basis(m, d)\n",
        "t_basis = fixed_basis(2, d)\n",
        "basis_m1 = fixed_basis(m-1, d)\n",
        "basis_m2 = fixed_basis(m-2, d)\n",
        "ml_basis = basis_m1\n",
        "mll_basis = basis_m2\n",
        "rho_1_arrays = rho_1_gen(basis)\n",
        "rho_2_arrays = rho_2_gen(basis, mll_basis, t_basis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "t_basis = fixed_basis(2, d)\n",
        "mll_basis = fixed_basis(basis.m-2, d)\n",
        "rho_1_arrays = rho_1_gen(basis)\n",
        "rho_2_arrays = rho_2_gen(basis, mll_basis, t_basis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def two_body_hamiltonian(t_basis_size, m, energy, G, rho_1_arrays, rho_2_arrays):\n",
        "    # Creamos la mat diagonal de d*d con los elementos de energy\n",
        "    # cada uno de estos, se contraen con los elementos de rho_1_arrays\n",
        "    # la mat energy contiene las energias de cada termino c^\\dag_k c_k para k kbar (iguales)\n",
        "    # por ello los elementos se repiten \n",
        "    energy_matrix = np.diagflat(np.kron(energy, np.ones(2)))\n",
        "    \n",
        "    # Construimos la mat de energía\n",
        "    h0 = np.sum(energy_matrix[:, :, np.newaxis, np.newaxis] * rho_1_arrays[:, :, :, :], axis=(0, 1))\n",
        "\n",
        "    # Pasamos ahora a la matrix de interacción con la misma estrategia\n",
        "    # dada G que indica la interacción entre los pares k' k'bar k kbar \n",
        "    # (que son elementos particulares de t_basis)\n",
        "    # transladamos estos coeficientes a una matriz en t_basis\n",
        "    # y multiplicamos por rho_2_arrays\n",
        "    \n",
        "    # Primero determinamos, dada t_basis, cuales son los indices de pares kkbar\n",
        "    offset = 4*m+1\n",
        "    indices = [-(k-1)*(2*k-offset) for k in range(1,m+1)] # Lo saque de Mathmatica, vos confia\n",
        "    i, j = np.meshgrid(indices, indices, indexing='ij') # Lo usamos para rellenar la mat deseada\n",
        "    \n",
        "    # Contruimos la mat que contraeremos con rho_2_arrays\n",
        "    mat = np.zeros((t_basis_size, t_basis_size))\n",
        "    mat[i, j] = G\n",
        "\n",
        "    hi = -np.sum(mat[:, :, np.newaxis, np.newaxis] * rho_2_arrays[:, :, :, :], axis=(0, 1))\n",
        "\n",
        "    return (h0, hi)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "(h02,hi2) = two_body_hamiltonian(t_basis.size, m, [0,1,2], np.ones((3,3)), rho_1_arrays, rho_2_arrays)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[[[1., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 1., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 1., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [1., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 1., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [1., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 1., 0., ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[1., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [1., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 1., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 1., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "       ...,\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 1.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., ..., 1., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 1.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 1., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 1.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 1., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., ..., 0., 1., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 1.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 1., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 1., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 1.]]]], dtype=float32)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rho_2_arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0\n"
          ]
        }
      ],
      "source": [
        "print(rho_2_arrays[9,0,0,9])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "rho_1_arrays = rho_1_gen(basis)\n",
        "\n",
        "A = np.array([0, 1, 2])  # Your list with d elements\n",
        "\n",
        "# Create a diagonal matrix with each element repeated twice\n",
        "result_matrix = np.diagflat(np.kron(A, np.ones(2)))\n",
        "\n",
        "print(result_matrix)\n",
        "np.kron(A, np.ones(2))\n",
        "\n",
        "mat = np.zeros((basis.size, basis.size))\n",
        "for i in range(0,2*d):\n",
        "    for j in range(0, 2*d):\n",
        "        mat += result_matrix[i,j] * rho_1_arrays[i,j,:,:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mat = np.sum(result_matrix[:, :, np.newaxis, np.newaxis] * rho_1_arrays[:, :, :, :], axis=(0, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "h0 == mat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1 1 0 0 0 0]\n",
            "[0 0 1 1 0 0]\n",
            "[0 0 0 0 1 1]\n",
            "[0, 9, 14]\n",
            "[0, 9, 14]\n"
          ]
        }
      ],
      "source": [
        "d = 3\n",
        "t_basis = fixed_basis(2, 2*d)\n",
        "basis = fixed_basis(d, 2*d)\n",
        "size = t_basis.size\n",
        "#basis = fixed_basis(d, 2*d)\n",
        "diag_elem = []\n",
        "for x in t_basis.base:\n",
        "    if all([x[i] == x[i+1] for i in range(0, 2*d, 2)]):\n",
        "        print(x)\n",
        "        diag_elem.append(t_basis.rep_to_index(x))\n",
        "\n",
        "print(diag_elem)\n",
        "# Veamos el GALERAZO de Wolfram\n",
        "n = 4*d+1\n",
        "print([-(k-1)*(2*k-n) for k in range(1,d+1)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "m2_basis = fixed_basis(2, d)\n",
        "print(m2_basis.size)\n",
        "nm2_basis = fixed_basis(basis.m-2, d)\n",
        "print(nm2_basis.base)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "W = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
        "index = [0,9,14]\n",
        "mat = np.zeros((size,size))\n",
        "for i in range(0,3):\n",
        "    for j in range(0,3):\n",
        "        mat[index[i], index[j]] = W[i,j]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "rho_2_arrays = rho_2_gen(basis, nm2_basis, m2_basis)\n",
        "#rho_2_arrays_tf = tf.constant(rho_2_arrays, dtype=tf.float32)\n",
        "\n",
        "W = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "W = np.ones((3,3))\n",
        "index = [0, 9, 14]\n",
        "size = 15  # Assuming size is the size of the matrix\n",
        "\n",
        "# Create a meshgrid of indices\n",
        "i, j = np.meshgrid(index, index, indexing='ij')\n",
        "\n",
        "# Use the meshgrid indices to assign values from W to the specified positions in mat\n",
        "mat = np.zeros((size, size))\n",
        "mat[i, j] = W\n",
        "\n",
        "# La mat... mat corresponde a los coeficientes en t_basis\n",
        "inte = np.zeros((basis.size, basis.size))\n",
        "for i in range(0, t_basis.size):\n",
        "    for j in range(0, t_basis.size):\n",
        "        inte += - mat[i, j] * rho_2_arrays[i,j,:,:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inte == hi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "m = 3\n",
        "basis = fixed_basis(m, d)\n",
        "basis_m1 = fixed_basis(m-1, d)\n",
        "basis_m2 = fixed_basis(m-2, d)\n",
        "\n",
        "from numba import njit\n",
        "\n",
        "# Parametros hamiltoniano\n",
        "e = 1\n",
        "eps = 0\n",
        "e0 = np.zeros(2*d)\n",
        "eigenspace_tol = 0.0001\n",
        "for k in range(0, d):\n",
        "    r = random.random() * eps * 0\n",
        "    e0[2*k] = k*e+r\n",
        "    e0[2*k+1] = k*e+r\n",
        "\n",
        "@njit(parallel=True)\n",
        "def base_hamiltonian_aux(basis, size, d, basis_m1, basis_m2):\n",
        "    # Construccion de H\n",
        "    d = d//2\n",
        "    h0 = np.zeros((size,size), dtype=np.float32)\n",
        "    for k in prange(0,2*d):\n",
        "        h0 += e0[k] * np.dot(bd_aux(basis_m1, basis, k),b_aux(basis, basis_m1, k))\n",
        "    hi = np.zeros((size, size), dtype=np.float32)\n",
        "    for k in prange(0,d):\n",
        "        for kb in prange(0,d):\n",
        "            bd_terms = np.dot(bd_aux(basis_m1, basis, 2*k),bd_aux(basis_m2, basis_m1, 2*k+1))\n",
        "            b_terms = np.dot(b_aux(basis_m1, basis_m2, 2*kb+1),b_aux(basis, basis_m1, 2*kb))\n",
        "            hi += -1*np.dot(bd_terms,b_terms)\n",
        "\n",
        "    return (h0, hi)\n",
        "\n",
        "def base_hamiltonian(basis, basis_m1, basis_m2):\n",
        "    return base_hamiltonian_aux(basis.base, basis.size, basis.d, basis_m1.base, basis_m2.base)\n",
        "\n",
        "h0, hi = base_hamiltonian(basis, basis_m1, basis_m2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "oapxWkD16fHg"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
