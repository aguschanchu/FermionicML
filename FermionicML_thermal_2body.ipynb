{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXz5cOlVwrzZ"
      },
      "source": [
        "# FermionicML:\n",
        "\n",
        "Basado en aguschanchu/RhoGC. Permite utilizar nuevas bases, es más simple y mejor optimizado. Basado en matrices sparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zd1lq-RmHHN",
        "outputId": "95df2244-4df2-4386-d099-1caeeda6dcdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openfermion in /usr/local/lib/python3.10/dist-packages (1.6.1)\n",
            "Requirement already satisfied: sparse in /usr/local/lib/python3.10/dist-packages (0.15.4)\n",
            "Requirement already satisfied: ray in /usr/local/lib/python3.10/dist-packages (2.37.0)\n",
            "Requirement already satisfied: cirq-core~=1.0 in /usr/local/lib/python3.10/dist-packages (from openfermion) (1.4.1)\n",
            "Requirement already satisfied: deprecation in /usr/local/lib/python3.10/dist-packages (from openfermion) (2.1.0)\n",
            "Requirement already satisfied: h5py>=2.8 in /usr/local/lib/python3.10/dist-packages (from openfermion) (3.11.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from openfermion) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from openfermion) (1.26.4)\n",
            "Requirement already satisfied: pubchempy in /usr/local/lib/python3.10/dist-packages (from openfermion) (1.0.4)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.10/dist-packages (from openfermion) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from openfermion) (1.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from openfermion) (1.13.3)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.10/dist-packages (from sparse) (0.60.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray) (3.16.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray) (24.1)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray) (6.0.2)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray) (1.4.1)\n",
            "Requirement already satisfied: attrs>=21.3.0 in /usr/local/lib/python3.10/dist-packages (from cirq-core~=1.0->openfermion) (24.2.0)\n",
            "Requirement already satisfied: duet>=0.2.8 in /usr/local/lib/python3.10/dist-packages (from cirq-core~=1.0->openfermion) (0.2.9)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from cirq-core~=1.0->openfermion) (3.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from cirq-core~=1.0->openfermion) (2.2.2)\n",
            "Requirement already satisfied: sortedcontainers~=2.0 in /usr/local/lib/python3.10/dist-packages (from cirq-core~=1.0->openfermion) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.10/dist-packages (from cirq-core~=1.0->openfermion) (4.12.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from cirq-core~=1.0->openfermion) (4.66.5)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->sparse) (0.43.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18->openfermion) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18->openfermion) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18->openfermion) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18->openfermion) (2024.8.30)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (0.20.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->openfermion) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->cirq-core~=1.0->openfermion) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->cirq-core~=1.0->openfermion) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->cirq-core~=1.0->openfermion) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->cirq-core~=1.0->openfermion) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->cirq-core~=1.0->openfermion) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->cirq-core~=1.0->openfermion) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->cirq-core~=1.0->openfermion) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->cirq-core~=1.0->openfermion) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->cirq-core~=1.0->openfermion) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->cirq-core~=1.0->openfermion) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install openfermion sparse ray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "o6KSP-u7l8Up"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import openfermion as of\n",
        "from tqdm import tqdm\n",
        "from itertools import combinations\n",
        "from openfermion.utils import commutator, count_qubits, hermitian_conjugated\n",
        "import functools\n",
        "import concurrent.futures\n",
        "from numba import njit\n",
        "import tensorflow as tf\n",
        "import scipy\n",
        "import sparse\n",
        "import itertools\n",
        "\n",
        "# Generación de base\n",
        "class fixed_basis:\n",
        "    @staticmethod\n",
        "    def int_to_bin(k, d):\n",
        "        return np.base_repr(k, 2).zfill(d)\n",
        "\n",
        "    @staticmethod\n",
        "    def bin_to_op(b):\n",
        "        tups = [(i, 1) for i, k in list(enumerate(list(b))) if k == '1']\n",
        "        return of.FermionOperator(tups)\n",
        "\n",
        "    def idx_to_repr(self, idx):\n",
        "        return self.canonicals[idx]\n",
        "\n",
        "    def opr_to_idx(self, opr):\n",
        "        return self.base.index(opr)\n",
        "\n",
        "    # Calcula el valor medio a partir del indice del vector y el operador\n",
        "    def idx_mean_val(self, idx: int, op: of.FermionOperator):\n",
        "        vec = self.idx_to_repr(idx)\n",
        "        return np.real(np.transpose(vec) @ of.get_sparse_operator(op, n_qubits=self.d) @ vec)\n",
        "\n",
        "    # Calcula el valor medio a partir de un estado y el operador\n",
        "    def mean_val(self, vec, op):\n",
        "        idx = self.opr_to_idx(vec)\n",
        "        return self.idx_mean_val(idx, op)\n",
        "\n",
        "    # Calcula la contracción de un operador sobre dos estados dados\n",
        "    def idx_contraction(self, idx_1, idx_2, op):\n",
        "        rep = lambda x: self.idx_to_repr(x)\n",
        "        return np.real(np.transpose(rep(idx_1)) @ of.get_sparse_operator(op, n_qubits=self.d) @ rep(idx_2))\n",
        "\n",
        "    def create_basis(self, d, num = None, pairs = False):\n",
        "        basis = []\n",
        "        num_ele = []\n",
        "        for k in range(0,2**d):\n",
        "            b = self.int_to_bin(k, d)\n",
        "            if num != None:\n",
        "                if b.count('1') == num:\n",
        "                    if pairs:\n",
        "                        if np.all(b[::2] == b[1::2]):\n",
        "                            oper = self.bin_to_op(b)\n",
        "                            basis.append(oper)\n",
        "                            num_ele.append(k)\n",
        "                    else:\n",
        "                        oper = self.bin_to_op(b)\n",
        "                        basis.append(oper)\n",
        "                        num_ele.append(k)\n",
        "            else:\n",
        "                oper = self.bin_to_op(b)\n",
        "                basis.append(oper)\n",
        "        return basis, num_ele\n",
        "\n",
        "    def __init__(self, d, num = None, pairs = False):\n",
        "        self.d = d\n",
        "        self.num = num\n",
        "        self.m = num\n",
        "        self.base, self.num_ele = self.create_basis(d, num, pairs)\n",
        "        self.size = len(self.base)\n",
        "        self.canonicals = np.eye(self.size)\n",
        "        self.pairs = pairs\n",
        "\n",
        "    @staticmethod\n",
        "    def cdc(i, j):\n",
        "        return of.FermionOperator(((i,1),(j,0)))\n",
        "\n",
        "    @staticmethod\n",
        "    def cc(i, j):\n",
        "        return of.FermionOperator(((i,0),(j,0)))\n",
        "\n",
        "    # Del indice, cuenta el número de partículas\n",
        "    def num_idx(self, idx):\n",
        "        b = self.int_to_bin(idx, basis.d)\n",
        "        return b.count('1')\n",
        "\n",
        "    # Calculo de rho1 (via directa, lento, y solo definido en la base por ahora)\n",
        "    def rho_1(self, op):\n",
        "        # Necesitamos un índice, es?\n",
        "        if type(op) != int:\n",
        "            op = self.opr_to_idx(op)\n",
        "        mat = np.zeros((self.d, self.d))\n",
        "        for i in range(self.d):\n",
        "            for j in range(self.d):\n",
        "                cdc = self.cdc(j, i)\n",
        "                mat[i,j] = self.idx_mean_val(op, cdc)\n",
        "        return mat\n",
        "\n",
        "# Calculo de generadores de rho1\n",
        "def rho_1_gen(basis):\n",
        "    # Vamos a crear un hipersparse de TF, almacenamos los valores acá\n",
        "    indices = []\n",
        "    values = []\n",
        "    shape = (basis.d, basis.d, basis.size, basis.size)\n",
        "    d = basis.d\n",
        "    for i in tqdm(range(0, d)):\n",
        "        for j in range(0, d):\n",
        "            # Generamos el operador\n",
        "            op = basis.cdc(j, i)\n",
        "            if basis.num == None:\n",
        "                mat = np.real(of.get_sparse_operator(op, n_qubits=d))\n",
        "            else:\n",
        "                mat = np.real(of.get_sparse_operator(op, n_qubits=d))[np.ix_(basis.num_ele, basis.num_ele)]\n",
        "            # Extraemos la información\n",
        "            n_r, n_c = mat.nonzero()\n",
        "            data = mat.data\n",
        "            for r, c, v in zip(n_r, n_c, data):\n",
        "                indices.append([i, j, r, c])\n",
        "                values.append(v)\n",
        "    indices_t = np.array(indices).T\n",
        "    s_t = sparse.COO(indices_t, values, shape=shape)\n",
        "    return s_t\n",
        "\n",
        "# Calculo de rho1 (via generadores) de un vector en la base canonica\n",
        "def rho_1(vect, rho_1_arrays):\n",
        "    if len(vect.shape) == 1: # vectores\n",
        "        return sparse.einsum('k,ijkl,l->ij', vect, rho_1_arrays, vect)\n",
        "    elif len(vect.shape) == 2: # mat densidad\n",
        "        return sparse.einsum('ijkl,kl->ij', rho_1_arrays, vect)\n",
        "    else: # mat densidad batcheadas\n",
        "        return sparse.einsum('bkl,ijkl->bij', vect, rho_1_arrays)\n",
        "\n",
        "# Calculo de indices de rho2kkbar\n",
        "def get_kkbar_indices(t_basis):\n",
        "    indices = []\n",
        "    for i, ind in enumerate(t_basis.num_ele):\n",
        "        v = t_basis.int_to_bin(ind, t_basis.d)\n",
        "        if np.all(v[::2] == v[1::2]):\n",
        "            indices.append(i)\n",
        "    return indices\n",
        "\n",
        "# Calculo de generadores de rho2\n",
        "def rho_2_gen(basis, t_basis, idx_list = []):\n",
        "    # Vamos a crear un hipersparse de TF, almacenamos los valores acá\n",
        "    indices = []\n",
        "    values = []\n",
        "    if len(idx_list) == basis.m:\n",
        "        idx_list = idx_list\n",
        "    elif len(idx_list) == basis.m**4:\n",
        "        idx_list = np.unique(idx_list[:,0])\n",
        "    else:\n",
        "        idx_list = range(t_basis.size)\n",
        "    shape = (len(idx_list), len(idx_list), basis.size, basis.size)\n",
        "    for i, ii in tqdm(enumerate(idx_list), total=len(idx_list)):\n",
        "        for j, jj in enumerate(idx_list):\n",
        "            # Generamos el operador\n",
        "            op = t_basis.base[jj]*of.utils.hermitian_conjugated(t_basis.base[ii])\n",
        "            if basis.num == None:\n",
        "                mat = np.real(of.get_sparse_operator(op, n_qubits=d))\n",
        "            else:\n",
        "                mat = np.real(of.get_sparse_operator(op, n_qubits=d))[np.ix_(basis.num_ele, basis.num_ele)]\n",
        "            # Extraemos la información\n",
        "            n_r, n_c = mat.nonzero()\n",
        "            data = mat.data\n",
        "            for r, c, v in zip(n_r, n_c, data):\n",
        "                indices.append([i, j, r, c])\n",
        "                values.append(v)\n",
        "\n",
        "    indices_t = np.array(indices).T\n",
        "    s_t = sparse.COO(indices_t, values, shape=shape)\n",
        "    return s_t\n",
        "\n",
        "\n",
        "# A partir de rho_2_arrays, toma el bloque c^d_i c^d_{/bar{j}} c_{/bar{k}} c_l\n",
        "def get_block_indices(basis, rho_2_arrays):\n",
        "    # Generamos los operadores\n",
        "    ops = []\n",
        "    opss = []\n",
        "    for i, j, k, l in tqdm(itertools.product(range(basis.m), repeat=4), total=basis.m**4): # Ojo que están los términos i=l j=k, removemos dps\n",
        "        op = of.FermionOperator(((2 * i, 1), (2 * j + 1, 1), (2 * k + 1, 0), (2 * l, 0)))\n",
        "        sparse_op = of.get_sparse_operator(op, n_qubits=basis.d)[np.ix_(basis.num_ele, basis.num_ele)]\n",
        "        ops.append(sparse_op.T)\n",
        "        opss.append(op)\n",
        "\n",
        "    # Seleccionamos el conjunto de índices en t_basis (alpha, alpha')\n",
        "    res = []\n",
        "    for idx in range(len(ops)):\n",
        "        matches = np.all(rho_2_arrays == ops[idx], axis=[2,3])\n",
        "        matches += np.all(rho_2_arrays == -ops[idx], axis=[2,3])\n",
        "        matching_indices = np.where(matches.todense())\n",
        "        res.extend(list(zip(matching_indices[0], matching_indices[1])))\n",
        "\n",
        "    return np.array(res)\n",
        "\n",
        "# Calculo de rho2 (via generadores) de un estado en la base canonica\n",
        "def rho_2(vect, rho_2_arrays):\n",
        "    if len(vect.shape) == 1: # vectores SOLO RHO2 COMPLETA\n",
        "        return sparse.einsum('k,ijkl,l->ij', vect, rho_2_arrays, vect)\n",
        "    elif len(vect.shape) == 2: # mat densidad SOLO RHO2 COMPLETA\n",
        "        return sparse.einsum('ijkl,kl->ij', rho_2_arrays, vect)\n",
        "    else: # mat densidad batcheadas\n",
        "        return sparse.einsum('bkl,ijkl->bij', vect, rho_2_arrays)\n",
        "\n",
        "# Calculo de generadores de K (usado para quasiparticles) WIP SPARSE\n",
        "def k_gen(basis):\n",
        "    mat = np.zeros((basis.d, basis.d, basis.size, basis.size))\n",
        "    d = basis.d\n",
        "    for i in tqdm(range(0, d), total=d):\n",
        "        for j in range(0, d):\n",
        "            op = basis.cc(j, i)\n",
        "            if basis.num == None:\n",
        "                mat[i,j,::] = np.real(of.get_sparse_operator(op, n_qubits=d)).todense()\n",
        "            else:\n",
        "                mat[i,j,::] = np.real(of.get_sparse_operator(op, n_qubits=d)).todense()[np.ix_(basis.num_ele, basis.num_ele)]\n",
        "    return mat\n",
        "\n",
        "def k_vect(vect, k_gen):\n",
        "    return np.einsum('k,ijkl,l->ij', vect, k_gen, vect)\n",
        "\n",
        "# Calculo la matrix rho de cuasipartículas  WIP SPARSE\n",
        "def rho_qsp(vect, rho_1_arrays, k_arrays, rho1 = None):\n",
        "    if type(rho1) == None:\n",
        "        rho1 = rho_1(vect, rho_1_arrays)\n",
        "    k = k_vect(vect, k_arrays)\n",
        "\n",
        "    mat = np.block([[rho1, k], [-np.conjugate(k), np.eye(rho_1_arrays.shape[0])-np.conjugate(rho1)]])\n",
        "    return mat\n",
        "\n",
        "# Devuelve los indices que tienen a level ocupado\n",
        "def level_proy(d, level):\n",
        "    ids = []\n",
        "    for k in range(0,2**d):\n",
        "        b = fixed_basis.int_to_bin(k, d)\n",
        "        if b[level] == '1':\n",
        "            ids.append(k)\n",
        "    arr = np.zeros(2**d)\n",
        "    arr[np.array(ids)] = 1\n",
        "    return arr, ids\n",
        "\n",
        "def parity_levels(d):\n",
        "    rng = range(2**d)\n",
        "    binary_repr = np.vectorize(np.binary_repr)(rng)\n",
        "    ones_c = np.char.count(binary_repr, '1')\n",
        "    return np.array(rng)[ones_c % 2 == 1] # seleccionamos estados impares\n",
        "\n",
        "# Devuelve el vector postmedido\n",
        "def measure(basis, vect, level = 1):\n",
        "    l_arr, l_ids = level_proy(basis.d, level)\n",
        "    proy_v = vect * l_arr\n",
        "    comp_arr = np.logical_not(l_arr).astype(int)\n",
        "    comp_v = vect * comp_arr\n",
        "    norm = lambda v: v / np.linalg.norm(v)\n",
        "    return norm(proy_v), norm(comp_v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dga5Xx_5vDf"
      },
      "source": [
        "## Definicion de Hamiltoniano"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myiTq53L5E1U"
      },
      "source": [
        "Configuramos, y creamos los arrays pertinentes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "fPWF0wiucd-7",
        "outputId": "cb771ce9-d8f4-4666-fcaa-acd139215956",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 17%|█▋        | 1/6 [00:00<00:00,  7.76it/s]\u001b[A\n",
            " 33%|███▎      | 2/6 [00:00<00:00,  8.00it/s]\u001b[A\n",
            " 50%|█████     | 3/6 [00:00<00:00,  7.81it/s]\u001b[A\n",
            " 67%|██████▋   | 4/6 [00:00<00:00,  7.79it/s]\u001b[A\n",
            " 83%|████████▎ | 5/6 [00:00<00:00,  7.69it/s]\u001b[A\n",
            "100%|██████████| 6/6 [00:00<00:00,  7.77it/s]\n",
            "\n",
            "  0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "  7%|▋         | 1/15 [00:00<00:04,  2.98it/s]\u001b[A\n",
            " 13%|█▎        | 2/15 [00:00<00:04,  3.11it/s]\u001b[A\n",
            " 20%|██        | 3/15 [00:00<00:03,  3.12it/s]\u001b[A\n",
            " 27%|██▋       | 4/15 [00:01<00:03,  2.99it/s]\u001b[A\n",
            " 33%|███▎      | 5/15 [00:01<00:03,  2.99it/s]\u001b[A\n",
            " 40%|████      | 6/15 [00:01<00:02,  3.07it/s]\u001b[A\n",
            " 47%|████▋     | 7/15 [00:02<00:02,  3.04it/s]\u001b[A\n",
            " 53%|█████▎    | 8/15 [00:02<00:02,  2.99it/s]\u001b[A\n",
            " 60%|██████    | 9/15 [00:02<00:01,  3.01it/s]\u001b[A\n",
            " 67%|██████▋   | 10/15 [00:03<00:01,  2.92it/s]\u001b[A\n",
            " 73%|███████▎  | 11/15 [00:03<00:01,  2.87it/s]\u001b[A\n",
            " 80%|████████  | 12/15 [00:04<00:01,  2.75it/s]\u001b[A\n",
            " 87%|████████▋ | 13/15 [00:04<00:00,  2.71it/s]\u001b[A\n",
            " 93%|█████████▎| 14/15 [00:04<00:00,  2.72it/s]\u001b[A\n",
            "100%|██████████| 15/15 [00:05<00:00,  2.89it/s]\n",
            "\n",
            "  0%|          | 0/81 [00:00<?, ?it/s]\u001b[A\n",
            "  6%|▌         | 5/81 [00:00<00:01, 48.24it/s]\u001b[A\n",
            " 12%|█▏        | 10/81 [00:00<00:01, 46.49it/s]\u001b[A\n",
            " 19%|█▊        | 15/81 [00:00<00:01, 39.55it/s]\u001b[A\n",
            " 25%|██▍       | 20/81 [00:00<00:01, 37.19it/s]\u001b[A\n",
            " 30%|██▉       | 24/81 [00:00<00:01, 36.63it/s]\u001b[A\n",
            " 36%|███▌      | 29/81 [00:00<00:01, 37.96it/s]\u001b[A\n",
            " 41%|████      | 33/81 [00:00<00:01, 37.53it/s]\u001b[A\n",
            " 47%|████▋     | 38/81 [00:00<00:01, 38.99it/s]\u001b[A\n",
            " 52%|█████▏    | 42/81 [00:01<00:00, 39.13it/s]\u001b[A\n",
            " 57%|█████▋    | 46/81 [00:01<00:00, 37.50it/s]\u001b[A\n",
            " 62%|██████▏   | 50/81 [00:01<00:00, 34.98it/s]\u001b[A\n",
            " 67%|██████▋   | 54/81 [00:01<00:00, 35.12it/s]\u001b[A\n",
            " 73%|███████▎  | 59/81 [00:01<00:00, 37.47it/s]\u001b[A\n",
            " 79%|███████▉  | 64/81 [00:01<00:00, 38.03it/s]\u001b[A\n",
            " 85%|████████▌ | 69/81 [00:01<00:00, 39.14it/s]\u001b[A\n",
            " 91%|█████████▏| 74/81 [00:01<00:00, 40.00it/s]\u001b[A\n",
            "100%|██████████| 81/81 [00:02<00:00, 37.64it/s]\n",
            "\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 3/3 [00:00<00:00, 12.61it/s]\n",
            "\n",
            "  0%|          | 0/9 [00:00<?, ?it/s]\u001b[A\n",
            " 11%|█         | 1/9 [00:00<00:01,  4.33it/s]\u001b[A\n",
            " 22%|██▏       | 2/9 [00:00<00:01,  4.62it/s]\u001b[A\n",
            " 33%|███▎      | 3/9 [00:00<00:01,  4.95it/s]\u001b[A\n",
            " 44%|████▍     | 4/9 [00:00<00:01,  4.95it/s]\u001b[A\n",
            " 56%|█████▌    | 5/9 [00:01<00:00,  4.71it/s]\u001b[A\n",
            " 67%|██████▋   | 6/9 [00:01<00:00,  4.80it/s]\u001b[A\n",
            " 78%|███████▊  | 7/9 [00:01<00:00,  4.88it/s]\u001b[A\n",
            " 89%|████████▉ | 8/9 [00:01<00:00,  5.03it/s]\u001b[A\n",
            "100%|██████████| 9/9 [00:01<00:00,  4.78it/s]\n"
          ]
        }
      ],
      "source": [
        "d = 6\n",
        "num = d//2 # En caso de ser None, es GC\n",
        "pairs = False\n",
        "\n",
        "# Bases\n",
        "basis = fixed_basis(d, num=num, pairs=pairs)\n",
        "t_basis = fixed_basis(d, num=2, pairs=pairs)\n",
        "# Arrays\n",
        "rho_1_arrays = rho_1_gen(basis)\n",
        "rho_2_arrays = rho_2_gen(basis, t_basis)\n",
        "# Indices\n",
        "k_indices = get_kkbar_indices(t_basis)\n",
        "block_indices = get_block_indices(basis, rho_2_arrays)\n",
        "# Arrays reducidos\n",
        "rho_2_kkbar_arrays = rho_2_gen(basis, t_basis, idx_list = k_indices)\n",
        "rho_2_block_arrays = rho_2_gen(basis, t_basis, idx_list = block_indices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "n7Bo2KUzl8Ut"
      },
      "outputs": [],
      "source": [
        "def two_body_hamiltonian_sp(energy_seed, G_batched, rho_1_arrays, rho_2_arrays, indices = None):\n",
        "    # SECCIÓN ENERGIAS\n",
        "    ## Dado un seed de niveles diagonal construimos la mat simétrica dxd que multiplicara a c^dag_i c_j\n",
        "    diagonal = np.zeros((gpu_batch_size, basis.d, basis.d))\n",
        "    diagonal[:, np.arange(basis.d), np.arange(basis.d)] = energy_seed\n",
        "    ## Convertimos en sparse la energia y la expandimos\n",
        "    energy_matrix = sparse.COO.from_numpy(diagonal)\n",
        "    energy_matrix_expanded = energy_matrix[:, :, :, np.newaxis, np.newaxis]\n",
        "    rho_1_gen_transposed = rho_1_arrays.transpose(axes=[1, 0, 2, 3])\n",
        "    # Multiplicamos por los operadores C^dag C\n",
        "    matprod = energy_matrix_expanded * rho_1_gen_transposed[np.newaxis,:,:,:,:]\n",
        "    h0_arr = matprod.sum(axis=[1,2])\n",
        "\n",
        "    # SECCIÓN INTERACCIÓN\n",
        "    # Ya tenemos los indices de updates, ahora tomamos la mat en t_basis (una de zeros)\n",
        "    # y updateamos de acuerdo a la lista de G's cada uno flatteneados\n",
        "\n",
        "    # Creamos la mat de t_basis, nada más que hacer! los coeficientes están dados. Bueno, y simetrizar\n",
        "    int_mat = np.zeros((gpu_batch_size, t_basis.size, t_basis.size))\n",
        "\n",
        "    # Si nos dieron indices, debemos llevar el G_arr (d,d) -> (t_basis.size, t_basis.size)\n",
        "    ## Caso kkbar\n",
        "    if len(indices) == basis.m:\n",
        "        indices = np.array(indices)\n",
        "        int_mat[:, indices[:, None], indices[None, :]] = G_batched\n",
        "    ## Caso genérico\n",
        "    elif indices is None:\n",
        "        idx = np.triu_indices(t_basis.size)\n",
        "        int_mat[:, idx[0], idx[1]] = G_batched\n",
        "    ## Caso reducido\n",
        "    elif len(indices) == basis.m**4:\n",
        "        # Previamente tenemos que remover los términos diagonales, pues esperamos basis.m**2(basis.m**2-1) coeficientes\n",
        "        fil_indices = []\n",
        "        for i in indices:\n",
        "            if i[0] != i[1]:\n",
        "                fil_indices.append(i)\n",
        "        indices = np.array(fil_indices)\n",
        "        batch_indices = np.arange(gpu_batch_size)[:, np.newaxis]\n",
        "        row_indices = indices[:, 0]\n",
        "        col_indices = indices[:, 1]\n",
        "        int_mat[batch_indices, row_indices, col_indices] = G_batched\n",
        "    else:\n",
        "        raise ValueError\n",
        "    diagonal = np.einsum('ijk,ijk->ijk', int_mat, np.eye(t_basis.size)[np.newaxis,::])\n",
        "    int_mat = int_mat + np.transpose(int_mat, axes=(0,2,1)) - diagonal\n",
        "    int_mat = sparse.COO.from_numpy(int_mat)\n",
        "\n",
        "    # Preparamos las dimensiones y multiplicamos\n",
        "    int_mat_expanded = int_mat[:, :, :, np.newaxis, np.newaxis]\n",
        "    rho_2_gen_transposed = rho_2_arrays.transpose(axes=[1, 0, 2, 3])\n",
        "    matprod = int_mat_expanded * rho_2_gen_transposed[np.newaxis,:,:,:,:]\n",
        "    hi_arr = matprod.sum(axis=[1,2])\n",
        "\n",
        "    return h0_arr - hi_arr\n",
        "\n",
        "\n",
        "def state_energy(state, h_arr):\n",
        "    return tf.linalg.trace(tf.matmul(state, h_arr))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "mwVKrpPcl8Uu"
      },
      "outputs": [],
      "source": [
        "# TEST: Las funciones de TF y comunes coinciden\n",
        "\n",
        "# Dado h, \\beta, construyo el estado térmico\n",
        "def thermal_state(h):\n",
        "    quotient = scipy.linalg.expm(-h)\n",
        "    return quotient / np.trace(quotient, axis1=-1, axis2=-2)[:, np.newaxis, np.newaxis]\n",
        "\n",
        "## NO usar para mat no hermiticas\n",
        "def thermal_state_eig(h, beta):\n",
        "    w, v = np.linalg.eigh(-beta*h)\n",
        "    D = np.diag(np.exp(w))\n",
        "    mat = v @ D @ v.T\n",
        "    mat = mat / np.trace(mat)\n",
        "    return mat\n",
        "\n",
        "def gen_to_h(base, rho_1_arrays):\n",
        "    triag = fill_triangular_np(base)\n",
        "    body_gen = triag + np.transpose(triag)-np.diag(np.diag(triag))\n",
        "    h = np.array(base_hamiltonian(body_gen, basis, rho_1_arrays))\n",
        "    return h\n",
        "\n",
        "def gen_to_h_1b(hamil_base):\n",
        "    triag = tfp.math.fill_triangular(hamil_base, upper=True)\n",
        "    body_gen = triag + tf.transpose(triag, perm=[0,2,1])-tf.linalg.diag(tf.linalg.diag_part(triag))\n",
        "    return body_gen\n",
        "\n",
        "def gen_to_h_tf(hamil_base, rho_1_arrays):\n",
        "    triag = tfp.math.fill_triangular(hamil_base, upper=True)\n",
        "    body_gen = triag + tf.transpose(triag, perm=[0,2,1])-tf.linalg.diag(tf.linalg.diag_part(triag)) # Simetrizamos y generamos la matriz de h\n",
        "    hamil_expanded = body_gen[:, :, :, np.newaxis, np.newaxis]\n",
        "    rho_1_gen_transposed = tf.transpose(rho_1_arrays, perm=[1, 0, 2, 3])\n",
        "    h_arr = tf.reduce_sum(hamil_expanded * rho_1_gen_transposed[np.newaxis,:,:,:,:], axis=[1,2])\n",
        "    return h_arr\n",
        "\n",
        "def thermal_state_tf(h):\n",
        "    # Assume beta=1\n",
        "    exp_hamiltonian = tf.linalg.expm(-h)\n",
        "    partition_function = tf.linalg.trace(exp_hamiltonian)\n",
        "    partition_function = tf.expand_dims(partition_function, axis=1)\n",
        "    partition_function = tf.expand_dims(partition_function, axis=1)\n",
        "\n",
        "    rho = exp_hamiltonian / partition_function\n",
        "\n",
        "    return rho\n",
        "\n",
        "def rho_1_tf(state, rho_1_arrays):\n",
        "    state = tf.expand_dims(state, axis=1)  # Shape: (5120, 10, 1, 10)\n",
        "    state_expanded = tf.expand_dims(state, axis=1)\n",
        "    rho_1_arrays_expanded = tf.expand_dims(rho_1_arrays, axis=0)  # Shape: (1, 5, 5, 10, 10)\n",
        "    product = state_expanded * rho_1_arrays_expanded  # Shape: (5120, 10, 5, 10, 10)\n",
        "    mat = tf.reduce_sum(product, axis=[-2, -1])  # Shape: (5120, 5, 5)\n",
        "\n",
        "    return mat\n",
        "\n",
        "def rho_2_tf(state, rho_2_arrays):\n",
        "    state = tf.expand_dims(state, axis=1)  # Shape: (5120, 10, 1, 10)\n",
        "    state_expanded = tf.expand_dims(state, axis=1)\n",
        "    rho_2_arrays_expanded = tf.expand_dims(rho_2_arrays, axis=0)  # Shape: (1, 5, 5, 10, 10)\n",
        "    product = state_expanded * rho_2_arrays_expanded  # Shape: (5120, 10, 5, 10, 10)\n",
        "    mat = tf.reduce_sum(product, axis=[-2, -1])  # Shape: (5120, 5, 5)\n",
        "\n",
        "    return mat\n",
        "\n",
        "# NOTA: para calcular el bloque rho2kkbar, utilizar en lugar\n",
        "\n",
        "def rho_1_gc_tf(hamil_base):\n",
        "    e, v = tf.linalg.eigh(gen_to_h_1b(hamil_base))\n",
        "    result = 1 / (1 + tf.exp(e))\n",
        "    result = tf.linalg.diag(result)\n",
        "    res = tf.linalg.matmul(v,result)\n",
        "    res = tf.linalg.matmul(res,v,adjoint_b=True)\n",
        "\n",
        "    return tf.cast(res, tf.float32)\n",
        "\n",
        "def pure_state(h):\n",
        "    mat = np.zeros((gpu_batch_size, basis.size, basis.size))\n",
        "    for i in range(gpu_batch_size):\n",
        "        e, v = scipy.sparse.linalg.eigsh(h[i,:,:])\n",
        "        fund = v[:, 0]\n",
        "        mat[i,:,:] = np.einsum('i,j->ij', fund.ravel(), fund.ravel())\n",
        "    return mat\n",
        "\n",
        "# Casos de entrenamiento tipo mat gaussianas\n",
        "def gen_gauss_mat(G, sigma_sq, size):\n",
        "    indices = np.arange(size)\n",
        "    mat = G * np.exp(-((indices - indices[:, np.newaxis])**2) / (2 * sigma_sq))\n",
        "    return mat\n",
        "\n",
        "def gen_gauss_mat_np(G_values, sigma_sq_values, size):\n",
        "    indices = np.arange(size, dtype=np.float32)\n",
        "    indices_diff = indices - indices[:, np.newaxis]\n",
        "\n",
        "    mat = G_values[:, np.newaxis, np.newaxis] * np.exp(-np.square(indices_diff) / (2 * sigma_sq_values[:, np.newaxis, np.newaxis]))\n",
        "\n",
        "    return mat\n",
        "\n",
        "# Casos de entrenamiento tipo matriz vectorial\n",
        "def gen_vect_mat(size, g_init, g_stop, sym = True):\n",
        "    if sym:\n",
        "        vect = np.sort(np.random.uniform(g_init, g_stop, size // 2))[::-1]\n",
        "        vect = np.repeat(vect, 2)\n",
        "        if size % 2 != 0: # TODO: Agregar tipo en el medio\n",
        "            raise ValueError\n",
        "    else:\n",
        "        vect = np.sort(np.random.uniform(g_init, g_stop, size))[::-1]\n",
        "    indices = np.abs(np.arange(size)[:, np.newaxis] - np.arange(size))\n",
        "    mat = vect[indices]\n",
        "\n",
        "    return vect, mat\n",
        "\n",
        "def gen_gauss_plus_vect(G_values, sigma_sq_values, size):\n",
        "    indices = np.arange(size//2, dtype=np.float32)\n",
        "    vect = G_values[:, np.newaxis] * np.exp(-np.square(indices) / (2 * sigma_sq_values[:, np.newaxis]))\n",
        "    return vect\n",
        "\n",
        "def gen_random_arr(h_labels):\n",
        "    matrices = np.zeros((gpu_batch_size, basis.m, basis.m))\n",
        "    up_idx = np.triu_indices(basis.m, 1)\n",
        "    matrices[:, up_idx[0], up_idx[1]] = h_labels\n",
        "    matrices += matrices.transpose(0, 2, 1)\n",
        "\n",
        "    return matrices\n",
        "\n",
        "def random_fermi_arr(g_init, g_stop, s = basis.num):\n",
        "    # En primer lugar, construimos la mat simétrica con respecto a nivel de Fermi\n",
        "    seed = np.round(np.random.uniform(g_init, g_stop,(s//2, s//2)), 2)\n",
        "    mat = np.zeros((s, s))\n",
        "    for i in range(s):\n",
        "        for j in range(s):\n",
        "            conv = lambda x: s//2 - 1 - np.min([x,s-1-x])\n",
        "            mat[i,j] = seed[conv(i), conv(j)]\n",
        "\n",
        "    # Simetrizamos\n",
        "    mat = (mat + mat.T)/2\n",
        "    # Volamos la diagonal + antidiagonal\n",
        "    mat = mat - np.diag(np.diag(mat)) - np.diag(np.diag(mat))[::-1]\n",
        "\n",
        "    # Los labels se buscan de la siguiente manera\n",
        "    #up_idx = np.triu_indices(basis.m//2, 1)\n",
        "    #mat[up_idx].reshape((8,8))\n",
        "\n",
        "def random_fermi_arr_inv(seed, s=basis.num, obj = False):\n",
        "    up_idx = np.triu_indices(s//2, 1)\n",
        "    reb = np.zeros((s//2,s//2))\n",
        "    if obj:\n",
        "        reb = np.zeros((s//2,s//2), dtype=object)\n",
        "    reb[up_idx] = seed\n",
        "    reb = reb + reb.T\n",
        "    rebsymm = reb[::-1]\n",
        "    res = np.block([[reb,np.flip(rebsymm)],[rebsymm,np.flip(reb)]])\n",
        "\n",
        "    return res\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emVBTg2QD-Fg"
      },
      "source": [
        "## Modelo de ML\n",
        "Basado en matrices densidad de 1 y 2 cuerpos como input, con hamiltoniano como salida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aF_Ec_mCGX96",
        "outputId": "7e72c6c6-4e75-49a7-f11c-d464f7b55a88"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tf.test.gpu_device_name()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylpy_BCw6jxF"
      },
      "source": [
        "### Construccion de dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc9S7FZTl8Uw"
      },
      "source": [
        "#### Version sincrónica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "7TSeBjHal8Ux"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "from typing import Literal\n",
        "import ray\n",
        "\n",
        "# Config\n",
        "#num_samples = 1500\n",
        "gpu_batch_size = 256 # 256\n",
        "u_energy_seed = np.array([np.repeat(np.arange(0, basis.num) - basis.num//2 + 1/2, 2) for _ in range(0,gpu_batch_size)]) # Semilla para H equiespaciado\n",
        "en_batch = u_energy_seed\n",
        "\n",
        "# Beta\n",
        "beta = 1\n",
        "\n",
        "# Construccion de parametros y matrices auxiliares\n",
        "\n",
        "# Generación de dataset (params)\n",
        "# h_type = {const, gaussian, random}: const = proporcional a ones, gaussian = proporcional a mat gaussiana, random = full random\n",
        "# g_init, g_stop: rango de Gs (aplica a los 3 casos)\n",
        "# state_type = {thermal, gs}: tipo de estado (térmico o funalmental)\n",
        "# input_type = {rho2, rho1}: tipo de feature a calcular\n",
        "valid_h_type = Literal['const', 'gaussian', 'vect', 'gaussvect', 'random', 'vectnosymm', 'randomsymm', 'randomenerg', 'blockgen']\n",
        "valid_state_type = Literal['thermal', 'gs']\n",
        "valid_input_type = Literal['rho2', 'rho1', 'rho1+rho2']\n",
        "\n",
        "\n",
        "def gather_elements(x):\n",
        "    return tf.gather(x, indices)\n",
        "\n",
        "@ray.remote\n",
        "def gen_dataset_slice(idx, h_type: valid_h_type, g_init: float, g_stop: float, state_type: valid_state_type, input_type: valid_input_type, include_energy: bool, ph):\n",
        "\n",
        "    rho_1_arrays = ray.get(rho_1_arrays_r)\n",
        "    rho_2_arrays = ray.get(rho_2_arrays_r)\n",
        "    rho_2_kkbar_arrays = ray.get(rho_2_kkbar_arrays_r)\n",
        "    k_indices = ray.get(k_indices_r)\n",
        "\n",
        "    ## Caso G proporcional a ones\n",
        "    if h_type == 'const':\n",
        "        en_batch = u_energy_seed\n",
        "        label_size = 1\n",
        "        h_labels = [np.random.uniform(g_init, g_stop) for _ in range(0,gpu_batch_size)]\n",
        "        g_arr = [np.ones((basis.num, basis.num))*g_seed for g_seed in h_labels]\n",
        "\n",
        "    # WIP DE ACA EN ADELANTE\n",
        "    ## Caso generico\n",
        "    elif h_type == 'random':\n",
        "        en_batch = u_energy_seed\n",
        "        label_size = basis.m*(basis.m-1)// 2  # CASO GENERICO elementos independientes de una mat de m x m sin diagonal\n",
        "        h_labels = [np.random.uniform(g_init, g_stop, label_size) for _ in range(0,gpu_batch_size)]\n",
        "        # Construimos la mat G\n",
        "        g_arr = gen_random_arr(h_labels)\n",
        "\n",
        "    elif h_type == 'randomsymm':\n",
        "        en_batch = u_energy_seed\n",
        "        label_size = len(np.triu_indices(basis.m//2, 1)[0])\n",
        "        g_arr = [random_fermi_arr(g_init, g_stop) for _ in range(gpu_batch_size)]\n",
        "        # Ahora extraemos los labels\n",
        "        up_idx = np.triu_indices(basis.m//2, 1)\n",
        "        h_labels = [mat[up_idx] for mat in g_arr]\n",
        "\n",
        "    elif h_type == 'vect':\n",
        "        en_batch = u_energy_seed\n",
        "        symmetry = True # Necesario para la invesión de BCS\n",
        "        label_size = basis.m // 2 - 1\n",
        "        labels_gen = lambda x: np.sort(np.random.uniform(g_init, g_stop, basis.m // 2 - 1))[::-1]\n",
        "        #labels_gen = lambda x: np.random.uniform(g_init, g_stop, basis.m // 2 - 1)\n",
        "        h_labels = [np.insert(labels_gen(0), 0, 0) for _ in range(0, gpu_batch_size)] # OJO CON EL DIAGONAL!\n",
        "        indices = np.abs(np.arange(basis.m)[:, np.newaxis] - np.arange(basis.m))\n",
        "        g_arr = [np.repeat(x,2)[indices] if symmetry else x[indices] for x in h_labels]\n",
        "        h_labels = [x[1:] for x in h_labels] # removemos el 0 agregado por el termino diagonal\n",
        "\n",
        "    elif h_type == 'vectnosymm':\n",
        "        en_batch = u_energy_seed\n",
        "        label_size = basis.m - 1\n",
        "        labels_gen = lambda x: np.sort(np.random.uniform(g_init, g_stop, basis.m - 1))[::-1]\n",
        "        h_labels = [np.insert(labels_gen(0), 0, 0) for _ in range(0, gpu_batch_size)] # OJO CON EL DIAGONAL!\n",
        "        indices = np.abs(np.arange(basis.m)[:, np.newaxis] - np.arange(basis.m))\n",
        "        g_arr = [x[indices] for x in h_labels]\n",
        "        h_labels = [x[1:] for x in h_labels] # removemos el 0 agregado por el termino diagonal\n",
        "\n",
        "    ## Caso reducido\n",
        "    elif h_type == 'gaussian':\n",
        "        en_batch = u_energy_seed\n",
        "        label_size = 2\n",
        "        h_labels = np.array([[np.random.uniform(g_init, g_stop), np.random.random()*10 + 0.1] for _ in range(0, gpu_batch_size)])\n",
        "        g_arr = gen_gauss_mat_np(h_labels[:,0], h_labels[:,1], basis.m)\n",
        "        h_labels = tf.constant(h_labels, dtype=tf.float32)\n",
        "\n",
        "    elif h_type == 'gaussvect':\n",
        "        en_batch = u_energy_seed\n",
        "        label_size = 2\n",
        "        h_labels = np.array([[np.random.uniform(g_init, g_stop), np.random.random()*2 + 0.1] for _ in range(0, gpu_batch_size)])\n",
        "        vect_arr = gen_gauss_plus_vect(h_labels[:,0], h_labels[:,1], basis.m)\n",
        "        indices = np.abs(np.arange(basis.m)[:, np.newaxis] - np.arange(basis.m))\n",
        "        g_arr = [np.repeat(x,2)[indices] for x in vect_arr]\n",
        "        g_arr = [g_arr[k] - np.diag(np.diag(g_arr[k])) for k in range(gpu_batch_size)]\n",
        "        h_labels = tf.constant(h_labels, dtype=tf.float32)\n",
        "\n",
        "    # HAMILTONIANOS GENERALES\n",
        "    elif h_type == 'randomenerg':\n",
        "        # Energias\n",
        "        label_size_en = 2*basis.m\n",
        "        en_batch = np.random.uniform(g_init*ph, g_stop*ph,(gpu_batch_size, label_size_en))\n",
        "        h_labels_en = en_batch\n",
        "        # Interacción\n",
        "        label_size_int = t_basis.size * (t_basis.size + 1)//2\n",
        "        h_labels_int = np.random.uniform(g_init, g_stop,(gpu_batch_size, label_size_int))\n",
        "        h_labels_int = np.zeros((gpu_batch_size, label_size_int))\n",
        "        g_arr = h_labels_int\n",
        "        # Combinamos\n",
        "        #label_size = label_size_en + label_size_int\n",
        "        #h_labels = np.concatenate([h_labels_en, h_labels_int], axis=-1)\n",
        "        h_labels = h_labels_en\n",
        "        label_size = label_size_en\n",
        "\n",
        "    # HAMILTONIANOS REDUCIDOS\n",
        "    elif h_type == 'blockgen':\n",
        "        en_batch = u_energy_seed\n",
        "        label_size = basis.m**4-basis.m**2\n",
        "        h_labels = np.random.uniform(g_init, g_stop,(gpu_batch_size, label_size))\n",
        "        g_arr = h_labels\n",
        "\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "    # Que indices usamos?\n",
        "    if h_type == 'blockgen':\n",
        "        indices = block_indices\n",
        "    elif h_type == 'randomenerg':\n",
        "        indices = None\n",
        "    else:\n",
        "        indices = k_indices\n",
        "\n",
        "    # Construimos los hamiltonianos basados en g_arr\n",
        "    h_arr = two_body_hamiltonian_sp(en_batch, g_arr, rho_1_arrays, rho_2_arrays, indices = indices).todense()\n",
        "\n",
        "    # Calculamos los estados\n",
        "    if state_type == 'thermal':\n",
        "        state = thermal_state(h_arr*beta)\n",
        "    else:\n",
        "        state = pure_state(h_arr)\n",
        "\n",
        "    # Calculamos la feature\n",
        "    if input_type == 'rho2':\n",
        "        #rho_input = rho_2(state, rho_2_kkbar_arrays).todense()\n",
        "        rho_input = rho_2(state, rho_2_block_arrays).todense()\n",
        "        rho_input = tf.constant(rho_input, dtype=tf.float32)\n",
        "    elif input_type == 'rho1+rho2':\n",
        "        rho_2_input = rho_2(state, rho_2_kkbar_arrays).todense() # ! CAMBIAR POR rho_2_arrays_kkbar_tf SI ES RHO2KKBAR\n",
        "        rho_2_input = tf.constant(rho_2_input, dtype=tf.float32)\n",
        "        rho_1_input = rho_1(state, rho_1_arrays).todense()\n",
        "        #rho_1_input = np.linalg.eigvals(rho_1(state, rho_1_arrays).todense()) # damos los autovalores en su lugar\n",
        "        rho_1_input = tf.constant(rho_1_input, dtype=tf.float32)\n",
        "\n",
        "    else:\n",
        "        rho_input = rho_1(state, rho_1_arrays).todense()\n",
        "        rho_input = tf.constant(rho_input, dtype=tf.float32)\n",
        "\n",
        "\n",
        "    # Calculamos la enegia\n",
        "    if include_energy:\n",
        "        energy = state_energy(state, h_arr)\n",
        "\n",
        "    # Generación de dataset\n",
        "    # Tradicional (rho2 tipo matricial)\n",
        "    if input_type == 'rho1' or input_type == 'rho2':\n",
        "        if include_energy:\n",
        "            return ((rho_input, energy), h_labels)\n",
        "        else:\n",
        "            return ((rho_input), h_labels)\n",
        "    else:\n",
        "        return ((rho_1_input, rho_2_input, energy), h_labels)\n",
        "\n",
        "def gen_dataset(h_type: valid_h_type, g_init: float, g_stop: float, state_type: valid_state_type, input_type: valid_input_type, include_energy: bool, num_samples = 100000, ph = 1):\n",
        "    result_ids = []\n",
        "    datasets = []\n",
        "\n",
        "    for i in range(num_samples // gpu_batch_size + 1):\n",
        "        result_ids.append(gen_dataset_slice.remote(i, h_type, g_init, g_stop, state_type, input_type, include_energy, ph))\n",
        "\n",
        "    # Initialize tqdm progress bar\n",
        "    progress_bar = tqdm(total=len(result_ids), unit=\"batch\")\n",
        "\n",
        "    # Fetch results with ray.wait() to update the progress bar\n",
        "    num_completed = 0\n",
        "    while num_completed < len(result_ids):\n",
        "        # Wait for at least 1 task to complete\n",
        "        completed, result_ids = ray.wait(result_ids, num_returns=1)\n",
        "\n",
        "        # Get the completed result\n",
        "        results = ray.get(completed)\n",
        "\n",
        "        # Add the results to datasets\n",
        "        for result in results:\n",
        "            datasets.append(tf.data.Dataset.from_tensor_slices(result))\n",
        "\n",
        "        # Update progress bar\n",
        "        num_completed = len(completed)\n",
        "        progress_bar.update(len(completed))\n",
        "\n",
        "    # Close the progress bar when done\n",
        "    progress_bar.close()\n",
        "    print(len(datasets))\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices(datasets)\n",
        "    dataset = ds.interleave(\n",
        "        lambda x: x,\n",
        "        cycle_length=1,\n",
        "        num_parallel_calls=tf.data.AUTOTUNE,\n",
        "    )\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def label_size_calc(h_type):\n",
        "    if h_type == 'const':\n",
        "        return 1\n",
        "    elif h_type == 'vect':\n",
        "        return basis.m // 2 - 1\n",
        "    elif h_type == 'vectnosymm':\n",
        "        return basis.m - 1\n",
        "    elif h_type == 'random':\n",
        "        return basis.m*(basis.m-1)// 2\n",
        "    elif h_type == 'randomenerg':\n",
        "        # Energias\n",
        "        label_size_en = 2*basis.m\n",
        "        label_size_int = t_basis.size * (t_basis.size + 1)//2\n",
        "        label_size = label_size_en + label_size_int\n",
        "        label_size = label_size_en\n",
        "        return label_size\n",
        "    elif h_type == 'blockgen':\n",
        "        return basis.m**4 - basis.m**2\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9BguEdwl8Uy"
      },
      "source": [
        "## DNN y CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYEEjNB-7b8y"
      },
      "source": [
        "### Definición de modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "gGCKE4zEl8Uy"
      },
      "outputs": [],
      "source": [
        "def gen_dnn_model(label_size, input_type, include_energy: bool, res = 1):\n",
        "    if input_type == 'rho1':\n",
        "        rho_layer =  tf.keras.layers.Input(shape=(basis.d, basis.d, 1), name='rho')\n",
        "    elif input_type == 'rho2':\n",
        "        rho_layer =  tf.keras.layers.Input(shape=(basis.m, basis.m, 1), name='rho')\n",
        "    else:\n",
        "        rho_1_layer =  tf.keras.layers.Input(shape=(basis.d, basis.d, 1), name='rho1')\n",
        "        rho_2_layer =  tf.keras.layers.Input(shape=(t_basis.size, t_basis.size, 1), name='rho2')\n",
        "\n",
        "    if include_energy:\n",
        "        energy_layer = tf.keras.layers.Input(shape=(1, ), name='energy')\n",
        "\n",
        "    if input_type == 'rho1' or input_type == 'rho2':\n",
        "        flatten_rho = tf.keras.layers.Flatten()(rho_layer)\n",
        "        #flatten_rho2 = tf.keras.layers.BatchNormalization()(flatten_rho2)\n",
        "        if include_energy:\n",
        "            dense1 = tf.keras.layers.concatenate([flatten_rho, energy_layer])\n",
        "        else:\n",
        "            dense1 = tf.keras.layers.concatenate([flatten_rho])\n",
        "    else:\n",
        "        flatten_rho_1 = tf.keras.layers.Flatten()(rho_1_layer)\n",
        "        flatten_rho_2 = tf.keras.layers.Flatten()(rho_2_layer)\n",
        "        #flatten_rho2 = tf.keras.layers.BatchNormalization()(flatten_rho2)\n",
        "        dense1 = tf.keras.layers.concatenate([flatten_rho_1, flatten_rho_2, energy_layer])\n",
        "\n",
        "    local_size = label_size\n",
        "    l = 4 + (res-1)\n",
        "    layer_s = [128//i*2 * res for i in reversed(range(1,l))]\n",
        "    for i in range(0,l-1):\n",
        "        dense1 = tf.keras.layers.Dense(layer_s[i], activation='sigmoid')(dense1)\n",
        "        #dense1 = tf.keras.layers.Dense(layer_s[i], activation='sigmoid', kernel_regularizer=tf.keras.regularizers.L1(0.001))(dense1)\n",
        "        #dense1 = tf.keras.layers.Dropout(0.1)(dense1)\n",
        "\n",
        "    output = tf.keras.layers.Dense(local_size)(dense1)\n",
        "    if input_type == 'rho1' or input_type == 'rho2':\n",
        "        if include_energy:\n",
        "            model = tf.keras.models.Model(inputs=[rho_layer, energy_layer], outputs=output)\n",
        "        else:\n",
        "            model = tf.keras.models.Model(inputs=[rho_layer], outputs=output)\n",
        "    else:\n",
        "        model = tf.keras.models.Model(inputs=[rho_1_layer, rho_2_layer, energy_layer], outputs=output)\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "# NOT SUPPORTED FOR RHO1+RHO2\n",
        "def gen_cnn_model(label_size, input_type, include_energy: bool, res = 1):\n",
        "    if input_type == 'rho1':\n",
        "        rho_layer =  tf.keras.layers.Input(shape=(basis.d, basis.d, 1), name='rho')\n",
        "    elif input_type == 'rho2':\n",
        "        #rho_layer =  tf.keras.layers.Input(shape=(basis.m, basis.m, 1), name='rho')\n",
        "        rho_layer =  tf.keras.layers.Input(shape=(basis.m**2, basis.m**2, 1), name='rho')\n",
        "    else:\n",
        "        rho_1_layer =  tf.keras.layers.Input(shape=(basis.d, basis.d, 1), name='rho1')\n",
        "        rho_layer =  tf.keras.layers.Input(shape=(basis.m, basis.m, 1), name='rho2')\n",
        "        #rho_layer =  tf.keras.layers.Input(shape=(t_basis.size, t_basis.size, 1), name='rho2')\n",
        "\n",
        "    if include_energy:\n",
        "        energy_layer = tf.keras.layers.Input(shape=(1, ), name='energy')\n",
        "\n",
        "    # CNN\n",
        "    # Factor de cantidad de filtros\n",
        "    lf = 4 * res\n",
        "    conv_limit_2 = 2\n",
        "    conv_limit_1 = 1\n",
        "    fs = 1\n",
        "    conv_rho = tf.keras.layers.Conv2D(lf*2**conv_limit_2, (2*fs, 2*fs), activation='relu')(rho_layer)\n",
        "    #conv_rho = tf.keras.layers.BatchNormalization()(conv_rho)\n",
        "    for j in [(2**conv_limit_2 - 2**k) for k in range(1,conv_limit_2)]:\n",
        "        conv_rho = tf.keras.layers.Conv2D(lf*j, (2*fs, 2*fs), activation='relu')(conv_rho)\n",
        "        #conv_rho = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(conv_rho)\n",
        "        #conv_rho = tf.keras.layers.BatchNormalization()(conv_rho)\n",
        "\n",
        "    # A rho1, en el caso 1+2 solo le aplicamos un filtro, porque es muy chica. Luego concatenamos\n",
        "    if input_type == 'rho1+rho2':\n",
        "        conv_rho1 = tf.keras.layers.Conv2D(lf*2**conv_limit_1, (2*fs, 2*fs), activation='relu')(rho_1_layer)\n",
        "        for j in [(2**conv_limit_1 - 2**k) for k in range(1,conv_limit_1)]:\n",
        "            conv_rho1 = tf.keras.layers.Conv2D(lf*j, (2*fs, 2*fs), activation='relu')(conv_rho1)\n",
        "        conv_rho1 = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(conv_rho1)\n",
        "        flatten_rho = tf.keras.layers.Flatten()(conv_rho)\n",
        "        flatten_rho_1 = tf.keras.layers.Flatten()(conv_rho1)\n",
        "        flatten_rho = tf.keras.layers.concatenate([flatten_rho, flatten_rho_1])\n",
        "\n",
        "    else:\n",
        "        flatten_rho = tf.keras.layers.Flatten()(conv_rho)\n",
        "\n",
        "    # DNN\n",
        "    #flatten_rho = tf.keras.layers.BatchNormalization()(flatten_rho)\n",
        "    if include_energy:\n",
        "        dense1 = tf.keras.layers.concatenate([flatten_rho, energy_layer])\n",
        "    else:\n",
        "        dense1 = tf.keras.layers.concatenate([flatten_rho])\n",
        "\n",
        "    local_size = label_size\n",
        "    l = 2\n",
        "    layer_s = [32//i*2 * res for i in reversed(range(1,l))]\n",
        "    for i in range(0,l-1):\n",
        "        dense1 = tf.keras.layers.Dense(layer_s[i], activation='relu')(dense1)\n",
        "        #dense1 = tf.keras.layers.Dense(layer_s[i], activation='sigmoid', kernel_regularizer=tf.keras.regularizers.L1(0.001))(dense1)\n",
        "        #dense1 = tf.keras.layers.Dropout(0.1)(dense1)\n",
        "\n",
        "    output = tf.keras.layers.Dense(local_size)(dense1)\n",
        "\n",
        "    if include_energy and input_type != 'rho1+rho2':\n",
        "        model = tf.keras.models.Model(inputs=[rho_layer, energy_layer], outputs=output)\n",
        "    elif input_type == 'rho1+rho2' and include_energy:\n",
        "        model = tf.keras.models.Model(inputs=[rho_1_layer, rho_layer, energy_layer], outputs=output)\n",
        "    else:\n",
        "        model = tf.keras.models.Model(inputs=[rho_layer], outputs=output)\n",
        "    model.summary()\n",
        "\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "# Custom loss functions\n",
        "def base_vec_to_h(h_labels):\n",
        "    #h_labels = tf.stack(h_labels)\n",
        "    g_arr = tf.map_fn(gather_elements, h_labels) # Construyo la matrix g desde los labels\n",
        "    h_arr = two_body_hamiltonian_tf(t_basis, basis.m, en_batch, g_arr, rho_1_arrays, rho_2_arrays, k_indices_tf)\n",
        "\n",
        "    return h_arr\n",
        "\n",
        "def base_vect_loss(base_pred, base_true):\n",
        "    h_true = base_vec_to_h(base_true)\n",
        "    h_pred = base_vec_to_h(base_pred)\n",
        "    return tf.math.reduce_mean(tf.norm(h_pred - h_true, axis=(-2,-1), ord='fro'))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiWk9piJtNIZ"
      },
      "source": [
        "### Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "nhJCHf0fQdRl"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import RMSprop, Adam, Nadam, Lion\n",
        "\n",
        "def dnn_fit(dataset, label_size, input_type, include_energy, cnn = True, loss = 'MSE', num_epochs = 50, res = 1):\n",
        "    if cnn:\n",
        "        model = gen_cnn_model(label_size, input_type, include_energy, res = res)\n",
        "    else:\n",
        "        model = gen_dnn_model(label_size, input_type, include_energy, res = res)\n",
        "\n",
        "    # Dividimos los datasets\n",
        "    train_size = int(0.8 * num_samples)\n",
        "\n",
        "    train_dataset = dataset.take(train_size)\n",
        "    val_dataset = dataset.skip(train_size)\n",
        "\n",
        "    batch_size = 128 #gpu_batch_size\n",
        "    train_dataset = train_dataset.batch(batch_size)\n",
        "    val_dataset = val_dataset.batch(batch_size)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(),\n",
        "                loss=loss,\n",
        "                metrics=['accuracy', 'mean_squared_error'])\n",
        "\n",
        "    # Train the model\n",
        "    device_name = tf.test.gpu_device_name()\n",
        "\n",
        "    with tf.device('/gpu:0'):\n",
        "        history = model.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset)\n",
        "        #history = model.fit(train_dataset, epochs=num_epochs)\n",
        "\n",
        "    return model, val_dataset, history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_mBUIZVl8Uz"
      },
      "source": [
        "Diferencias en error RMSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "teM1e4QSl8Uz"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def dnn_error_coef(model, val_dataset):\n",
        "    # Assuming you have a validation dataset (val_dataset)\n",
        "    iterador = iter(val_dataset)\n",
        "    sample = next(iterador)\n",
        "    next_sample = next(iterador)\n",
        "    input_data = sample[0]  # Assuming your dataset provides input data as the first element\n",
        "    actual_values = sample[1]  # Assuming your dataset provides actual labels as the second element\n",
        "\n",
        "    # Predict using the model\n",
        "    predictions = model.predict(input_data)\n",
        "\n",
        "    #mean_squared_error(predictions, actual_values)\n",
        "\n",
        "    # Vemos algunos valores\n",
        "    for e in val_dataset:\n",
        "        for i in range(0, 4):\n",
        "            print(e[1][i])\n",
        "            print(predictions[i])\n",
        "        break\n",
        "\n",
        "    # Veamos el MSE de los valores de G\n",
        "    #RMSE_pred = mean_squared_error(actual_values, predictions, squared=False)\n",
        "    #RMSE_rand = mean_squared_error(actual_values, next_sample[1], squared=False)\n",
        "    #print(RMSE_pred, RMSE_rand)\n",
        "    #rint(RMSE_rand/RMSE_pred)\n",
        "    # Veamos los errores en términos de MSE\n",
        "    if predictions.shape[1] == 1:\n",
        "        norm_pred = np.mean(np.abs(predictions.T-actual_values))\n",
        "        norm_rand = np.mean(np.abs(next_sample[1]-actual_values))\n",
        "    else:\n",
        "        norm_pred = np.mean(np.linalg.norm(predictions-actual_values,ord=2, axis=1))\n",
        "        norm_rand = np.mean(np.linalg.norm(next_sample[1]-actual_values,ord=2, axis=1))\n",
        "    print(norm_pred, norm_rand)\n",
        "    print(norm_rand / norm_pred)\n",
        "    # Veamos los errores en norma 2\n",
        "    if predictions.shape[1] == basis.m: # caso vectorial\n",
        "        norm_pred = base_vect_loss(predictions, actual_values)\n",
        "        norm_rand = base_vect_loss(next_sample[1], actual_values)\n",
        "    print(norm_pred, norm_rand)\n",
        "    print(norm_rand / norm_pred)\n",
        "\n",
        "    return(norm_rand / norm_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGJfsO4ql8Uz"
      },
      "source": [
        "Análisis rho2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "mDD7MANrl8U0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reconstruye rho a partir de G\n",
        "# Codigo medio copiado de gen_dataset, not good\n",
        "def rho_reconstruction(h_labels, h_type, state_type):\n",
        "    en_batch = u_energy_seed\n",
        "    ## Caso G proporcional a ones\n",
        "    if h_type == 'const':\n",
        "        g_arr = [np.ones((basis.num, basis.num))*g_seed for g_seed in h_labels]\n",
        "\n",
        "    # WIP DE ACA EN ADELANTE\n",
        "    ## Caso generico\n",
        "    elif h_type == 'random':\n",
        "        # Construimos la mat G\n",
        "        g_arr = gen_random_arr(h_labels)\n",
        "        h_labels = tf.constant(h_labels, dtype=tf.float32)\n",
        "\n",
        "    elif h_type == 'vect':\n",
        "        indices = np.abs(np.arange(basis.m)[:, np.newaxis] - np.arange(basis.m))\n",
        "        g_arr = [np.repeat(x,2)[indices] if symmetry else x[indices] for x in h_labels]\n",
        "        h_labels = [x[1:] for x in h_labels] # removemos el 0 agregado por el termino diagonal\n",
        "        h_labels = tf.constant(h_labels, dtype=tf.float32)\n",
        "        g_arr = tf.constant(g_arr, dtype=tf.float32)\n",
        "\n",
        "    elif h_type == 'vectnosymm':\n",
        "        indices = np.abs(np.arange(basis.m)[:, np.newaxis] - np.arange(basis.m))\n",
        "        g_arr = [x[indices] for x in h_labels]\n",
        "        h_labels = [x[1:] for x in h_labels] # removemos el 0 agregado por el termino diagonal\n",
        "        h_labels = tf.constant(h_labels, dtype=tf.float32)\n",
        "        g_arr = tf.constant(g_arr, dtype=tf.float32)\n",
        "\n",
        "    ## Caso reducido\n",
        "    elif h_type == 'gaussian':\n",
        "        g_arr = gen_gauss_mat_np(h_labels[:,0], h_labels[:,1], basis.m)\n",
        "        h_labels = tf.constant(h_labels, dtype=tf.float32)\n",
        "        g_arr = tf.constant(g_arr, dtype=tf.float32)\n",
        "\n",
        "    elif h_type == 'gaussvect':\n",
        "        vect_arr = gen_gauss_plus_vect(h_labels[:,0], h_labels[:,1], basis.m)\n",
        "        indices = np.abs(np.arange(basis.m)[:, np.newaxis] - np.arange(basis.m))\n",
        "        g_arr = [np.repeat(x,2)[indices] for x in vect_arr]\n",
        "        g_arr = [g_arr[k] - np.diag(np.diag(g_arr[k])) for k in range(gpu_batch_size)]\n",
        "        h_labels = tf.constant(h_labels, dtype=tf.float32)\n",
        "        g_arr = tf.constant(g_arr, dtype=tf.float32)\n",
        "\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "    # Construimos los hamiltonianos basados en g_arr\n",
        "    r_indices = k_indices if not arb else None\n",
        "    h_arr = two_body_hamiltonian_sp(en_batch, g_arr, rho_1_arrays, rho_2_arrays, indices = k_indices).todense()\n",
        "\n",
        "    # Calculamos los estados\n",
        "    if state_type == 'thermal':\n",
        "        state = thermal_state(h_arr*beta)\n",
        "    else:\n",
        "        state = pure_state(h_arr)\n",
        "\n",
        "    # Calculamos la feature\n",
        "    if input_type == 'rho2':\n",
        "        rho_input = rho_2(state, rho_2_kkbar_arrays).todense()\n",
        "        rho_input = tf.constant(rho_input, dtype=tf.float32)\n",
        "    elif input_type == 'rho1+rho2':\n",
        "        rho_2_input = rho_2_tf(state, rho_2_arrays_tf) # ! CAMBIAR POR rho_2_arrays_kkbar_tf SI ES RHO2KKBAR\n",
        "        rho_1_input = rho_1_tf(state, rho_1_arrays_tf)\n",
        "    else:\n",
        "        rho_input = rho_1_tf(state, rho_1_arrays_tf)\n",
        "\n",
        "    return rho_input\n",
        "\n",
        "\n",
        "# Vemos algunos valores\n",
        "def dnn_rho_reconstruction_error(model, val_dataset, h_type, state_type):\n",
        "    iterador = iter(val_dataset)\n",
        "    sample = next(iterador)\n",
        "    next_sample = next(iterador)\n",
        "    input_data = sample[0]  # Assuming your dataset provides input data as the first element\n",
        "    actual_values = sample[1].numpy()\n",
        "    predictions = model.predict(input_data)\n",
        "\n",
        "    # Calculamos los rho\n",
        "    rho_pred = rho_reconstruction_tf(predictions, h_type, state_type)\n",
        "    rho_true = rho_reconstruction_tf(actual_values, h_type, state_type)\n",
        "    rho_rand = rho_reconstruction_tf(next_sample[1].numpy(), h_type, state_type)\n",
        "\n",
        "\n",
        "    rho_2_s = lambda x: np.sort(np.linalg.eigvals(x))\n",
        "\n",
        "    # Analisis RMSE\n",
        "    #RMSE_pred = mean_squared_error(rho_2_true, rho_2_pred, squared=False)\n",
        "    #RMSE_rand = mean_squared_error(rho_2_true, rho_2_rand, squared=False)\n",
        "    #print(RMSE_pred, RMSE_rand)\n",
        "    #print(RMSE_rand/RMSE_pred)\n",
        "    # Printeamos algunos valores\n",
        "    for i in range(0, 2):\n",
        "        print(\"true: \" + str(rho_2_s(rho_true[i])))\n",
        "        print(\"pred: \" + str(rho_2_s(rho_pred[i])))\n",
        "\n",
        "    print(rho_true, rho_pred)\n",
        "    norm_pred = np.mean(np.linalg.norm(rho_true-rho_pred, axis=(1,2)))\n",
        "    norm_rand = np.mean(np.linalg.norm(rho_true-rho_rand, axis=(1,2)))\n",
        "    print(norm_pred, norm_rand)\n",
        "    print(norm_rand / norm_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcu5-DK7l8U0"
      },
      "source": [
        "## Main exe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLHiL3Dtl8U0"
      },
      "outputs": [],
      "source": [
        "num_samples = 500000\n",
        "h_type = 'blockgen'\n",
        "g_init = 0.01\n",
        "g_stop = 2.5\n",
        "state_type = 'thermal'\n",
        "input_type = 'rho2'\n",
        "include_energy = True\n",
        "cnn = True\n",
        "loss = 'MSE'\n",
        "num_epochs = 5\n",
        "res = 2 # Resolución de la topología de la red (a mayor res, más parámetros)\n",
        "ph = 2 # Factor energía en randomenerg\n",
        "\n",
        "# Inicialización de Ray\n",
        "ray.shutdown()\n",
        "ray.init()\n",
        "rho_1_arrays_r = ray.put(rho_1_arrays)\n",
        "rho_2_arrays_r = ray.put(rho_2_arrays)\n",
        "rho_2_kkbar_arrays_r = ray.put(rho_2_kkbar_arrays)\n",
        "k_indices_r = ray.put(k_indices)\n",
        "\n",
        "dataset = gen_dataset(h_type, g_init, g_stop, state_type, input_type, include_energy, num_samples, ph)\n",
        "label_size = label_size_calc(h_type)\n",
        "# DNN\n",
        "model, val_dataset, history = dnn_fit(dataset, label_size, input_type, include_energy, cnn = cnn, loss = loss, num_epochs = num_epochs, res = res)\n",
        "print(dnn_error_coef(model, val_dataset))\n",
        "#print(dnn_rho_reconstruction_error(model, val_dataset, h_type, state_type))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "jmpvvQ5kH5Cg",
        "outputId": "205e73ac-d890-4f2f-d6cb-d27810e2d17f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-43-9e7ea838b812>, line 4)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-43-9e7ea838b812>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    input_data = sample[0][][idx]\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "idx = 1\n",
        "iterador = iter(val_dataset)\n",
        "sample = next(iterador)\n",
        "input_data = sample[0][][idx]\n",
        "rho_init = input_data\n",
        "actual_energy = sample[0][1][idx]\n",
        "input_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMftAalol8U1"
      },
      "source": [
        "#### Comparación con BCS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pj8jlomvl8U1"
      },
      "source": [
        "##### Caso G = G"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb-iUousl8U1"
      },
      "outputs": [],
      "source": [
        "import numba as nb\n",
        "\n",
        "en_batch = [np.arange(0, basis.m) - basis.m//2 + 1/2 for _ in range(0,gpu_batch_size)]  # ojo si lo cambie en H\n",
        "en_batch = tf.constant(en_batch, dtype=tf.float32)\n",
        "\n",
        "energ = np.array(en_batch[0])\n",
        "e_mean = np.mean(energ)\n",
        "\n",
        "# Calcula rho2 ha partir del delta dado, en formato [delta]. Es por el optimize, perdon\n",
        "@nb.jit(nopython=True)\n",
        "def bcs_delta(delta: np.ndarray, m = basis.m):\n",
        "    delta = delta[0]\n",
        "\n",
        "    lambda_k = lambda k: np.sqrt((energ[k])**2 + delta**2)\n",
        "    f_k = lambda k: 1/2 * (1 - (energ[k])/lambda_k(k))\n",
        "    r_k = lambda k: delta/(2*lambda_k(k))\n",
        "\n",
        "    rho = np.zeros((m, m))\n",
        "    for k in range(0, m):\n",
        "        for kp in range(0, m):\n",
        "            p = f_k(k)**2 if k == kp else 0.0\n",
        "            rho[k, kp] = r_k(k) * r_k(kp) + p\n",
        "\n",
        "    return rho\n",
        "\n",
        "\n",
        "# Calculamos g_BCS a partir de la rho2 calculada por BCS más cercana a rho dada\n",
        "def g_bcs(rho_init):\n",
        "    rho_dist = lambda x: np.linalg.norm(bcs_delta(x)-rho_init)\n",
        "    opti = scipy.optimize.minimize(rho_dist, 1, method='Nelder-Mead')\n",
        "    delta = opti.x\n",
        "    lambda_k = lambda k: np.sqrt((en_batch[0][k] - e_mean)**2 + delta**2)\n",
        "    G = 1/(np.sum([ 1/(2*lambda_k(x)) for x in range(0, basis.m)]))\n",
        "\n",
        "    return G\n",
        "\n",
        "# Cargamos elementos del conjunto de validación\n",
        "iterador = iter(val_dataset)\n",
        "sample = next(iterador)\n",
        "input_data = sample[0]\n",
        "actual_values = sample[1]\n",
        "predictions = model.predict(input_data)\n",
        "\n",
        "# Ordenamos los valores de G con el fin de plotear\n",
        "g_ids = actual_values.numpy().argsort()\n",
        "predictions_sort = predictions[g_ids]\n",
        "g_true_sort = actual_values.numpy()[g_ids]\n",
        "\n",
        "predictions_sort = predictions_sort.T[0]\n",
        "rho_pred = rho_reconstruction(predictions_sort, h_type, state_type)\n",
        "\n",
        "# Calculamos ahora G BCS\n",
        "rho_actual = rho_reconstruction(actual_values.numpy()[g_ids], h_type, state_type)\n",
        "g_bcs_sort = [g_bcs(x) for x in rho_actual.numpy()]\n",
        "rho_bcs = rho_reconstruction(g_bcs_sort, h_type, state_type)\n",
        "\n",
        "rho_error = lambda x: np.linalg.norm(rho_actual.numpy()-x, ord=2, axis=(1,2))\n",
        "\n",
        "plt.rcParams['axes.labelsize'] = 16\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "plt.rcParams['legend.fontsize'] = 16\n",
        "plt.rcParams['axes.linewidth'] = 1.5\n",
        "plt.rcParams['figure.dpi'] = 300\n",
        "\n",
        "plt.plot(g_true_sort, rho_error(rho_pred), label='CNN')\n",
        "plt.plot(g_true_sort, rho_error(rho_bcs), label='BCS')\n",
        "\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel(\"G\")\n",
        "plt.ylabel(r\"Error de reconstrucción $\\rho^{(2)}$\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sE1DPuNzl8U1"
      },
      "outputs": [],
      "source": [
        "bcs_deltak_rho(delta_r, state_type='thermal'), rho_init\n",
        "#print(actual_values)\n",
        "bcs_opti_cost(sample[1][idx], delta_r, h_type='gaussvect', state_type='thermal')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vaTmuvTl8U2"
      },
      "outputs": [],
      "source": [
        "G = 1\n",
        "sigma = 2\n",
        "G * np.exp(-np.arange(basis.m//2)**2/(2 * sigma))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPNOKBTjl8U2"
      },
      "outputs": [],
      "source": [
        "np.abs([x if x<0 else 0 for x in np.diff(term)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyNqhmeql8U2"
      },
      "outputs": [],
      "source": [
        "auxterm = np.zeros(basis.m)\n",
        "term = [1,2,3,6,5,6,7,2]\n",
        "for i in range(0,basis.m-1):\n",
        "    if term[i] > term[i+1]:\n",
        "        auxterm[i] = 1\n",
        "print(auxterm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmbZkBkxl8U2"
      },
      "source": [
        "##### Caso G = G(k-k')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3OzeUO1l8U2"
      },
      "outputs": [],
      "source": [
        "# Calcula rho_2 en función del delta_k dado\n",
        "import scipy.optimize\n",
        "from sympy import symbols, Function, diff, lambdify\n",
        "import sympy\n",
        "import numba as nb\n",
        "\n",
        "energ = np.array(en_batch[0])\n",
        "pfact = 2 if h_type == 'vect' else 1\n",
        "\n",
        "# Dado delta_k simétrico (basis.m//2) devuelve rho asociada\n",
        "@nb.jit(nopython=True)\n",
        "def bcs_deltak_rho(delta_k, m = basis.m, state_type = 'gs'):\n",
        "    delta_k = np.concatenate((delta_k, np.flip(delta_k))) # impongo simetría\n",
        "\n",
        "    sq = lambda k: np.sqrt(energ[k]**2+delta_k[k]**2)\n",
        "    vk = lambda k: np.sqrt(1/2 * (1 - energ[k]/sq(k)))\n",
        "    uk = lambda k: np.sqrt(1/2 * (1 + energ[k]/sq(k)))\n",
        "    fk = lambda k: 1/(1+np.exp(beta*sq(k))) if state_type == 'thermal' else 0\n",
        "    ukvk = lambda k: uk(k)*vk(k)*(1-2*fk(k))\n",
        "    vksq = lambda k: vk(k)**2 * (1-2*fk(k)) + fk(k)\n",
        "\n",
        "    rho = np.zeros((m, m))\n",
        "    for k in range(0, m):\n",
        "        for kp in range(0, m):\n",
        "            p = vksq(k)**2 if k == kp else 0\n",
        "            rho[k, kp] = ukvk(k)*ukvk(kp) + p\n",
        "\n",
        "    return rho\n",
        "\n",
        "# Pasemos a la inversion, es decir, obtener G(delta_k)\n",
        "\n",
        "## Metodo exacto (caso vectorial)\n",
        "### Generamos las funciones para escribir M_ij\n",
        "def gen_sympy_func(m):\n",
        "    uv_s = symbols(f'uv0:{m}')\n",
        "    g_s = symbols(f'g0:{m}')\n",
        "    d_s = np.zeros(m, dtype=object)\n",
        "    for i in range(m):\n",
        "        d_s[i] = np.sum([g_s[np.abs((i-j))//2] * uv_s[j] for j in range(m)])\n",
        "\n",
        "    funcarr = np.zeros((m//2, m//2), dtype=object)\n",
        "    for i in range(m//2):\n",
        "        for j in range(m//2):\n",
        "            funcarr[i, j] = lambdify(uv_s, diff(d_s[i], g_s[j]), 'numpy')\n",
        "\n",
        "    return funcarr\n",
        "\n",
        "M_funcarr = gen_sympy_func(basis.m)\n",
        "\n",
        "def bcs_build_M(delta_k, m=basis.m):\n",
        "    delta_k = np.abs(np.concatenate((delta_k, np.flip(delta_k))))\n",
        "    sq = lambda k: np.sqrt(energ[k]**2+delta_k[k]**2)\n",
        "    vk = lambda k: np.sqrt(1/2 * (1 - energ[k]/sq(k)))\n",
        "    uk = lambda k: np.sqrt(1/2 * (1 + energ[k]/sq(k)))\n",
        "    ukvk = lambda k: uk(k)*vk(k)\n",
        "\n",
        "    M = np.zeros((m//2, m//2))\n",
        "    uv_vals = [ukvk(k) for k in range(basis.m)]\n",
        "    for i in range(m//2):\n",
        "        for j in range(m//2):\n",
        "            M[i, j] = M_funcarr[i, j](*uv_vals)\n",
        "\n",
        "    M = np.linalg.inv(M)\n",
        "    return M\n",
        "\n",
        "## Inversion numérica, calculo de la función de costo a partir de autoconsistencia\n",
        "#@nb.jit(nopython=True)\n",
        "def bcs_opti_cost(g, delta_k, m=basis.m, state_type='gs', h_type='vect'):\n",
        "    if h_type == 'gaussvect':\n",
        "        G, sigma = g[0], g[1] # g = (G, sigma)\n",
        "        g_s = G * np.exp(-np.arange(m//2)**2/(2 * sigma))\n",
        "    else:\n",
        "        g_s = np.insert(g, 0, 0) # nada que hacer en el caso vectorial\n",
        "\n",
        "    # El choclo es por numba\n",
        "    delta_k = np.concatenate((delta_k, np.flip(delta_k)))\n",
        "    sq = lambda k: np.sqrt(energ[k]**2+delta_k[k]**2)\n",
        "    vk = lambda k: np.sqrt(1/2 * (1 - energ[k]/sq(k)))\n",
        "    uk = lambda k: np.sqrt(1/2 * (1 + energ[k]/sq(k)))\n",
        "    fk = lambda k: 1/(1+np.exp(beta*sq(k))) if state_type == 'thermal' else 0\n",
        "    ukvk = lambda k: uk(k)*vk(k)*(1-2*fk(k))\n",
        "    vksq = lambda k: vk(k)**2 * (1-2*fk(k)) + fk(k)\n",
        "\n",
        "    term = np.zeros(m)\n",
        "    for i in range(m):\n",
        "        if state_type == 'thermal':\n",
        "            #term[i] = delta_k[i] - sum([g_s[np.abs((i-j))//2] * delta_k[j] / (2 * sq(j)) * np.tanh(beta * sq(j)/2) for j in range(m)])\n",
        "            term[i] = delta_k[i] - sum([g_s[np.abs((i-j))//pfact] * ukvk(j) for j in range(m)])\n",
        "        else:\n",
        "            term[i] = delta_k[i] - sum([g_s[np.abs((i-j))//2] * ukvk(j) for j in range(m)]) # ojo con un factorcito por aca\n",
        "\n",
        "    #return term\n",
        "    return np.linalg.norm(term, ord=2)\n",
        "\n",
        "# Implementación de la función de inversión mediante ambas estrategias (exacto y numérico)\n",
        "def bcs_rho_g(rho_init, h_type = 'vect', state_type = 'gs', exact = False, energ_f=0, actual_energy=0, just_delta = False):\n",
        "    # Buscamos delta_k   TODO: CASO TERMICO ENERGIA\n",
        "    dist = lambda delta_k: np.linalg.norm(bcs_deltak_rho(delta_k, basis.m, state_type)-rho_init, ord=2) + energ_f * (delta_energ(delta_k, state_type)-actual_energy)**2 + 0 * np.linalg.norm([x if x<0 else 0 for x in np.diff(delta_k)])\n",
        "    bounds = [(0.01, 100) for _ in range(basis.m//2)] # Bounds de delta_k, TODO determinar o acotar\n",
        "    opti = scipy.optimize.dual_annealing(dist, bounds=bounds, maxiter=5000)\n",
        "    #print(opti)\n",
        "    delta_k = opti.x\n",
        "\n",
        "    if just_delta:\n",
        "        return 0, delta_k, opti\n",
        "\n",
        "    if exact and state_type == 'gs' and h_type == 'vect':\n",
        "        M = bcs_build_M(delta_k)\n",
        "        g_rebuild = M @ delta_k[:basis.m//2]\n",
        "        delta_k = delta_k[:basis.m//2]\n",
        "\n",
        "    else:\n",
        "        if h_type == 'gaussvect':\n",
        "            #g_dist = lambda g: bcs_opti_cost(g, delta_k, basis.m, state_type=state_type, h_type='gaussvect')[:2]\n",
        "            #optig = scipy.optimize.root(g_dist, np.random.rand(2), method='broyden1', options={'maxiter': 1000})\n",
        "            #print(optig)\n",
        "            g_dist = lambda g: bcs_opti_cost(g, delta_k, basis.m, state_type=state_type, h_type='gaussvect')\n",
        "            bounds = [(g_init, g_stop), (0.01, 2.1)]\n",
        "            optig = scipy.optimize.dual_annealing(g_dist, bounds=bounds, maxiter=1000)\n",
        "\n",
        "        else:\n",
        "            g_dist = lambda g: bcs_opti_cost(g, delta_k, basis.m, state_type=state_type, h_type='vect')\n",
        "            bounds = [(g_init*5, g_stop*5) for _ in range(basis.m-1)]\n",
        "            optig = scipy.optimize.dual_annealing(g_dist, bounds=bounds, maxiter=1000)\n",
        "\n",
        "        #print(optig)\n",
        "        g_rebuild = optig.x\n",
        "\n",
        "    return g_rebuild, delta_k, opti\n",
        "\n",
        "# Calculo de energía en funcion de delta\n",
        "def delta_energ(delta_k, state_type = 'gs'):\n",
        "    delta_k = np.concatenate((delta_k, np.flip(delta_k)))\n",
        "    sq = lambda k: np.sqrt(energ[k]**2+delta_k[k]**2)\n",
        "    vk = lambda k: np.sqrt(1/2 * (1 - energ[k]/sq(k)))\n",
        "    uk = lambda k: np.sqrt(1/2 * (1 + energ[k]/sq(k)))\n",
        "    fk = lambda k: 1/(1+np.exp(beta*sq(k))) if state_type == 'thermal' else 0\n",
        "    ukvk = lambda k: uk(k)*vk(k)*(1-2*fk(k))\n",
        "    vksq = lambda k: vk(k)**2 * (1-2*fk(k)) + fk(k)\n",
        "\n",
        "    h0 = np.sum([2*energ[k]*vksq(k) for k in range(basis.m)])\n",
        "    hi = np.sum([delta_k[k]**2/(2*sq(k))*(1-2*fk(k)) for k in range(basis.m)])\n",
        "\n",
        "    return h0 - hi\n",
        "\n",
        "\n",
        "# Reduce el constraint en energia hasta alcanzar una distancia tolerable en rho\n",
        "def opti_delta(rho_init, exact, actual_energy, energ_f=0.1):\n",
        "    max_iter = 1\n",
        "    tol = 0.2\n",
        "    dist = lambda delta_k: np.linalg.norm(bcs_deltak_rho(delta_k, basis.m)-rho_init)\n",
        "    gf, deltaf = np.zeros(basis.m//2), np.zeros(basis.m//2)\n",
        "    i = 1\n",
        "    while i < max_iter:\n",
        "        g, delta, opti = bcs_rho_g(rho_init, exact, energ_f/i, actual_energy=actual_energy) # vamos de a poco reduciendo el constrain en energia\n",
        "        if dist(delta) < tol:\n",
        "            gf, deltaf = g, delta\n",
        "            #print(opti)\n",
        "            break\n",
        "        if dist(delta) < dist(deltaf):\n",
        "            gf, deltaf = g, delta\n",
        "        i += 1\n",
        "\n",
        "    return gf, deltaf\n",
        "\n",
        "# Función auxiliar: calculemos rho2 para esos otros g de la inversion. Recuperamos rho??\n",
        "def rho_reconstruction_vect(gex):\n",
        "    return rho_reconstruction([gex], 'vect', state_type)[0]\n",
        "\n",
        "# Dado delta, obtengo G via la inversión y reconstruyo rho\n",
        "# delta -> G -> rho_reconstruction_tf\n",
        "def rho_reconstruction_from_delta(delta_k):\n",
        "    delta_k = np.abs(np.concatenate((delta_k, np.flip(delta_k))))\n",
        "    sq = lambda k: np.sqrt(energ[k]**2+delta_k[k]**2)\n",
        "    vk = lambda k: np.sqrt(1/2 * (1 - energ[k]/sq(k)))\n",
        "    uk = lambda k: np.sqrt(1/2 * (1 + energ[k]/sq(k)))\n",
        "\n",
        "    M = bcs_build_M(delta_k[:basis.m//2])\n",
        "    gex = M @ delta_k[:basis.m//2]\n",
        "\n",
        "    return rho_reconstruction_vect(gex)\n",
        "\n",
        "# Calculamos el delta por autoconsitencia\n",
        "def auto_delta(delta_k, g, m = basis.m, state_type = 'thermal'):\n",
        "    delta_k = np.concatenate((delta_k, np.flip(delta_k)))\n",
        "    sq = lambda k: np.sqrt(energ[k]**2+delta_k[k]**2)\n",
        "    vk = lambda k: np.sqrt(1/2 * (1 - energ[k]/sq(k)))\n",
        "    uk = lambda k: np.sqrt(1/2 * (1 + energ[k]/sq(k)))\n",
        "    fk = lambda k: 1/(1+np.exp(beta*sq(k))) if state_type == 'thermal' else 0\n",
        "    ukvk = lambda k: uk(k)*vk(k)*(1-2*fk(k))\n",
        "    vksq = lambda k: vk(k)**2 * (1-2*fk(k)) + fk(k)\n",
        "\n",
        "    gt = g\n",
        "    term = np.zeros(m//2)\n",
        "    for i in range(m//2):\n",
        "        if state_type == 'thermal':\n",
        "            term[i] = sum([gt[np.abs((i-j))//pfact] * delta_k[j] / (2 * sq(j)) * np.tanh(beta * sq(j)/2) for j in range(m)])\n",
        "        else:\n",
        "            term[i] = sum([gt[np.abs((i-j))//pfact] * ukvk(j) for j in range(m)]) # ojo con un factorcito por aca\n",
        "\n",
        "    return term"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsjuOYEPl8U3"
      },
      "outputs": [],
      "source": [
        "pfact"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_g2GpZgl8U3"
      },
      "source": [
        "Autoconsistencia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9W2k5Xrkl8U3"
      },
      "outputs": [],
      "source": [
        "idx = np.random.randint(0,gpu_batch_size)\n",
        "idx = 293\n",
        "\n",
        "# Cargamos los valores\n",
        "iterador = iter(val_dataset)\n",
        "sample = next(iterador)\n",
        "input_data = sample[0][0][idx]\n",
        "rho_init = input_data\n",
        "rand_idx = np.random.randint(0,64)\n",
        "rho_rand = sample[0][0][rand_idx]\n",
        "actual_energy = sample[0][1][idx]\n",
        "\n",
        "# Compatibilizamos los dos casos\n",
        "if h_type == 'gaussvect':\n",
        "    s = sample[1]\n",
        "    actual_values = gen_gauss_plus_vect(s[:,0], s[:,1], basis.m)[idx]\n",
        "    p = model.predict(sample[0])\n",
        "    prediction = gen_gauss_plus_vect(p[:,0], p[:,1], basis.m)[idx]\n",
        "elif h_type == 'vect':\n",
        "    conv = lambda x: np.repeat(np.insert(x,0,0),2)\n",
        "    actual_values = conv(sample[1][idx])\n",
        "else:\n",
        "    actual_values = sample[1][idx]\n",
        "    prediction = model.predict(sample[0])[idx]\n",
        "\n",
        "gex = actual_values\n",
        "delta_r = np.random.rand(basis.m//2)\n",
        "for k in range(0, 100):\n",
        "    delta_r = auto_delta(delta_r, gex, state_type=state_type)\n",
        "\n",
        "dist = lambda delta_k, rho: np.linalg.norm(bcs_deltak_rho(delta_k, basis.m, state_type=state_type)-rho, ord=2) # delta_energ(delta_k)-sample[0][1][0])**2+\n",
        "#rho_error_from_delta = lambda delta, rho: np.linalg.norm(rho_reconstruction_from_delta(delta) - rho, ord=2)\n",
        "rho_error_from_ge = lambda ge, rho: np.linalg.norm(rho_reconstruction_vect(ge) - rho, ord=2)\n",
        "\n",
        "print(np.linalg.norm(rho_init, ord=2)/np.linalg.norm(delta_r, ord=2))\n",
        "print('>>Autoconsitencia a partir de g true')\n",
        "print(f'Delta {delta_r}')\n",
        "print(f'Dist {dist(delta_r, rho_init)} de rho')\n",
        "print('\\n>>Resultados de la inversión')\n",
        "gex, dex, opti = bcs_rho_g(rho_init, h_type, state_type, exact = False, energ_f=0.01, actual_energy=actual_energy) #energ_f = 0.01\n",
        "print(gex, dex, opti)\n",
        "print(np.linalg.norm(rho_init, ord=2)/np.linalg.norm(dex, ord=2))\n",
        "\n",
        "print(f'Delta {dex}')\n",
        "print(f'Dist {dist(dex, rho_init)} de rho')\n",
        "print(f'Energia {delta_energ(dex, state_type=state_type)}')\n",
        "#print(f'Reconstruccion rho {rho_error_from_delta(dex, rho_init)}')\n",
        "print('\\n>>Valores de G')\n",
        "print(f'True {sample[1][idx].numpy()}')\n",
        "print(f'Pred {gex}')\n",
        "print(f'Energia real {actual_energy}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMjjGEJpl8U4"
      },
      "source": [
        "Dado delta, mejor G?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcrS5cL6l8U5"
      },
      "source": [
        "Version restringida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5QrwwCfl8U5"
      },
      "outputs": [],
      "source": [
        "def gen_sympy_func(m, label_size, h_type):\n",
        "    uv_s = symbols(f'uv0:{m}')\n",
        "    if h_type == 'randomsymm':\n",
        "        g_s = symbols(f'g0:{label_size}') # gs (semilla)\n",
        "    else:\n",
        "        g_s = symbols(f'g0:{label_size + 1}') # debido al 0 inicial\n",
        "\n",
        "    d_s = np.zeros(m, dtype=object)\n",
        "\n",
        "    if h_type == 'vectnosymm' or h_type == 'vect':\n",
        "        for i in range(m):\n",
        "            d_s[i] = np.sum([g_s[np.abs((i-j))//pfact] * uv_s[j] for j in range(m)])\n",
        "\n",
        "    elif h_type == 'randomsymm':\n",
        "        # Generamos la matriz G_kk'\n",
        "        g_mat = random_fermi_arr_inv(g_s, m, obj = True)\n",
        "        for i in range(m):\n",
        "            d_s[i] = np.sum([g_mat[i,j] * uv_s[j] for j in range(m)])\n",
        "\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "    for i in range(basis.m):\n",
        "        d_s[i] = d_s[i].subs({g_s[0]:0})\n",
        "\n",
        "    #return d_s\n",
        "    funcarr = np.zeros((m, label_size), dtype=object)\n",
        "    for i in range(m):\n",
        "        for j in range(label_size):\n",
        "            funcarr[i, j] = lambdify(uv_s, diff(d_s[i], g_s[j+1]), 'numpy')\n",
        "\n",
        "    return funcarr\n",
        "\n",
        "M_funcarr = gen_sympy_func(basis.m, label_size, h_type)\n",
        "\n",
        "def bcs_build_M_thermal(delta_k, label_size, m=basis.m, state_type = 'thermal'):\n",
        "    delta_k = np.concatenate((delta_k, np.flip(delta_k)))\n",
        "    sq = lambda k: np.sqrt(energ[k]**2+delta_k[k]**2)\n",
        "    vk = lambda k: np.sqrt(1/2 * (1 - energ[k]/sq(k)))\n",
        "    uk = lambda k: np.sqrt(1/2 * (1 + energ[k]/sq(k)))\n",
        "    fk = lambda k: 1/(1+np.exp(beta*sq(k))) if state_type == 'thermal' else 0\n",
        "    ukvk = lambda k: uk(k)*vk(k)*(1-2*fk(k))\n",
        "    vksq = lambda k: vk(k)**2 * (1-2*fk(k)) + fk(k)\n",
        "\n",
        "    M = np.zeros((m//2, label_size))\n",
        "    uv_vals = [ukvk(k) for k in range(m)]\n",
        "    for i in range(m//2):\n",
        "        for j in range(label_size):\n",
        "            M[i, j] = M_funcarr[i, j](*uv_vals)\n",
        "\n",
        "    return M\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io8SUvJ4l8U5"
      },
      "source": [
        "Versión libre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZu3BtoRl8U5"
      },
      "outputs": [],
      "source": [
        "def gen_sympy_func(m, label_size, h_type):\n",
        "    uv_s = symbols(f'uv0:{m}')\n",
        "    if h_type == 'randomsymm':\n",
        "        g_s = symbols(f'g0:{label_size}') # gs (semilla)\n",
        "    else:\n",
        "        g_s = symbols(f'g0:{m}') # debido al 0 inicial\n",
        "\n",
        "    d_s = np.zeros(m, dtype=object)\n",
        "\n",
        "    if h_type == 'vectnosymm' or h_type == 'vect':\n",
        "        for i in range(m):\n",
        "            d_s[i] = np.sum([g_s[np.abs((i-j))] * uv_s[j] for j in range(m)])\n",
        "\n",
        "    elif h_type == 'randomsymm':\n",
        "        # Generamos la matriz G_kk'\n",
        "        g_mat = random_fermi_arr_inv(g_s, m, obj = True)\n",
        "        for i in range(m):\n",
        "            d_s[i] = np.sum([g_mat[i,j] * uv_s[j] for j in range(m)])\n",
        "\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "    #return d_s\n",
        "    funcarr = np.zeros((m, m), dtype=object)\n",
        "    for i in range(m):\n",
        "        for j in range(m):\n",
        "            funcarr[i, j] = lambdify(uv_s, diff(d_s[i], g_s[j]), 'numpy')\n",
        "\n",
        "    return funcarr\n",
        "\n",
        "M_funcarr = gen_sympy_func(basis.m, basis.m, h_type)\n",
        "#print(M_funcarr)\n",
        "def bcs_build_M_thermal(delta_k, label_size, m=basis.m, state_type = 'thermal'):\n",
        "    delta_k = np.concatenate((delta_k, np.flip(delta_k)))\n",
        "    sq = lambda k: np.sqrt(energ[k]**2+delta_k[k]**2)\n",
        "    vk = lambda k: np.sqrt(1/2 * (1 - energ[k]/sq(k)))\n",
        "    uk = lambda k: np.sqrt(1/2 * (1 + energ[k]/sq(k)))\n",
        "    fk = lambda k: 1/(1+np.exp(beta*sq(k))) if state_type == 'thermal' else 0\n",
        "    ukvk = lambda k: uk(k)*vk(k)*(1-2*fk(k))\n",
        "    vksq = lambda k: vk(k)**2 * (1-2*fk(k)) + fk(k)\n",
        "\n",
        "    M = np.zeros((m//2, m))\n",
        "    uv_vals = [ukvk(k) for k in range(m)]\n",
        "    print(uv_vals)\n",
        "    for i in range(m//2):\n",
        "        for j in range(m):\n",
        "            M[i, j] = M_funcarr[i, j](*uv_vals)\n",
        "\n",
        "    return M\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZtsQjxel8U5"
      },
      "source": [
        "Ahora la idea es hacer todo lo anterior, de manera sistemática"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CstDoeBYl8U6"
      },
      "outputs": [],
      "source": [
        "# TODO Reducir repetición de código con lo anterior\n",
        "\n",
        "#label_size = 3\n",
        "idx = 10\n",
        "# Cargamos valores\n",
        "val_dataset = dataset.batch(gpu_batch_size*20)\n",
        "iterador = iter(val_dataset)\n",
        "sample = next(iterador)\n",
        "input_data_arr = sample[0][0].numpy()\n",
        "actual_energy_arr = sample[0][1].numpy()\n",
        "actual_values_arr = sample[1].numpy()\n",
        "prediction = model.predict(sample[0])\n",
        "conv = lambda x: np.repeat(np.insert(x, 0, 0),2) #caso simetrico libre\n",
        "#conv = lambda x: np.insert(x, 0, 0)\n",
        "#conv = lambda x: x\n",
        "conv2 = lambda x: np.insert(x, 0, 0)\n",
        "\n",
        "def min_opti_g(idx, state_type):\n",
        "    input_data = input_data_arr[idx]\n",
        "    rho_init = input_data\n",
        "    actual_energy = actual_energy_arr[idx]\n",
        "    #actual_values = conv(actual_values_arr[idx])\n",
        "    actual_values = actual_values_arr[idx]\n",
        "\n",
        "    # Calculamos el delta autoconsistente\n",
        "    gex = conv(actual_values)\n",
        "    delta_r = np.random.rand(basis.m//2)\n",
        "    for k in range(0, 100):\n",
        "        delta_r = auto_delta(delta_r, conv2(actual_values), state_type=state_type)\n",
        "    #print(delta_r, auto_delta(delta_r, gex))\n",
        "\n",
        "    # Optimizamos para hallar el delta\n",
        "    gex, dex, opti = bcs_rho_g(rho_init, h_type, state_type, exact = False, energ_f=0.01, actual_energy=actual_energy, just_delta=True)\n",
        "    #print(opti)\n",
        "\n",
        "    delta_o = dex\n",
        "    print(dex)\n",
        "    M = bcs_build_M_thermal(delta_o, label_size, state_type = state_type)\n",
        "    #print(M)\n",
        "    #print(M.shape, delta_o)\n",
        "    #print(M @ actual_values)\n",
        "\n",
        "\n",
        "    eps = 0.1\n",
        "    lin_const = scipy.optimize.LinearConstraint(M, delta_o-eps, delta_o+eps)\n",
        "    #cost = lambda g: np.linalg.norm(g-conv(actual_values)) # +np.abs(g[0])\n",
        "    cost = lambda g: np.linalg.norm(g-conv(actual_values))\n",
        "    bounds = [(0.01, 100) for _ in range(basis.m)]\n",
        "\n",
        "    # Exacto (GS)\n",
        "    #pfact = 2\n",
        "    #M_funcarr = gen_sympy_func(basis.m, label_size, h_type)\n",
        "    #M = bcs_build_M_thermal(dex, label_size, basis.m, 'gs')\n",
        "    #gg = np.linalg.pinv(M) @ dex\n",
        "\n",
        "    # Nunérico\n",
        "    opt = scipy.optimize.minimize(cost, np.random.rand(basis.m), constraints=lin_const, bounds=bounds, method='SLSQP')\n",
        "    gg = opt.x\n",
        "\n",
        "\n",
        "    #print(opt)\n",
        "    #print(actual_values)\n",
        "    #print(M @ opt.x - delta_o)\n",
        "    return gg, conv(actual_values), conv(prediction[idx]), delta_r\n",
        "\n",
        "\n",
        "x = min_opti_g(293, 'gs')\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "va3wQ8FRl8U6"
      },
      "outputs": [],
      "source": [
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "\n",
        "min_actual_g = []\n",
        "min_num_g = []\n",
        "min_pred_g = []\n",
        "delta_r_g = []\n",
        "\n",
        "def task(i):\n",
        "    return min_opti_g(i, state_type)\n",
        "\n",
        "with ProcessPoolExecutor() as executor:\n",
        "    # Submit all the tasks to the executor\n",
        "    futures = [executor.submit(task, i) for i in range(gpu_batch_size*20)]\n",
        "\n",
        "    # Use tqdm to display progress\n",
        "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
        "        num, actual, pred, delt = future.result()\n",
        "        min_actual_g.append(actual)\n",
        "        min_num_g.append(num)\n",
        "        min_pred_g.append(pred)\n",
        "        delta_r_g.append(delt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce2LJSjUl8U6"
      },
      "outputs": [],
      "source": [
        "plt.plot(x[200:], np.linalg.norm(delta_r_g, axis=-1)[sortids][200:])\n",
        "plt.xlabel(\"Norma G_true\")\n",
        "plt.ylabel(\"Norma delta_r\")\n",
        "plt.yscale(\"log\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enoYsGxjl8U7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtdvUclnl8U7"
      },
      "outputs": [],
      "source": [
        "op = lambda g: [np.linalg.norm(x) for x in g]\n",
        "\n",
        "for i in range(len(min_actual_g)):\n",
        "    min_actual_gm = op(min_actual_g)\n",
        "    min_num_gm = op(min_num_g)\n",
        "    min_pred_gm = np.array(op(min_pred_g))\n",
        "\n",
        "sortids = np.array(min_actual_gm).argsort().astype(int)\n",
        "x = np.array(min_actual_gm)[sortids]\n",
        "print(f'{beta, g_init, g_stop}, {basis.size, state_type}')\n",
        "plt.plot(x, np.array(min_num_gm)[sortids], label='BCS')\n",
        "plt.plot(x, np.array(min_actual_gm)[sortids])\n",
        "plt.plot(x, min_pred_gm[sortids], label='ML')\n",
        "\n",
        "#plt.yscale(\"log\")\n",
        "#plt.ylim(0,5)\n",
        "plt.legend()\n",
        "plt.xlabel(\"Norma G_true\")\n",
        "plt.ylabel(\"Norma G_BCS\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qD9tghvl8U7"
      },
      "outputs": [],
      "source": [
        "difop = lambda g: np.linalg.norm(np.array(g)[sortids] - np.array([x for x in min_actual_g])[sortids], axis=-1)\n",
        "difop2 = lambda g: np.linalg.norm(np.array(g)[sortids] - np.array(min_actual_g)[sortids], axis=-1)\n",
        "\n",
        "x = np.linalg.norm(min_actual_g, axis=-1)[sortids]\n",
        "plt.plot(x, difop(min_num_g), label='BCS')\n",
        "plt.plot(x, difop2(min_pred_g), label='ML')\n",
        "plt.legend()\n",
        "plt.xlabel('Norma G_true')\n",
        "plt.ylabel('Norma diferencia G-G_true')\n",
        "plt.yscale(\"log\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPudsn31l8VI"
      },
      "source": [
        "\n",
        "#### Caso h_type=randomenerg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNpiu5Wsl8VI"
      },
      "outputs": [],
      "source": [
        "iterador = iter(val_dataset)\n",
        "sample = next(iterador)\n",
        "rho_arr2 = sample[0][0]\n",
        "prediction2 = model.predict(sample[0])\n",
        "actual_values2 = sample[1]\n",
        "\n",
        "sortids2 = np.array(np.linalg.norm(actual_values2, axis=-1)).argsort().astype(int)\n",
        "x2 = np.array(np.linalg.norm(actual_values2, axis=-1))[sortids2]\n",
        "actual_values2 = actual_values2.numpy()[sortids2]\n",
        "ml_arr2 = prediction2[sortids2]\n",
        "\n",
        "err2 = lambda a: np.linalg.norm(a-actual_values2, axis=-1)\n",
        "err = lambda a: np.linalg.norm(a-actual_values, axis=-1)\n",
        "\n",
        "plt.rcParams['axes.labelsize'] = 16\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "plt.rcParams['legend.fontsize'] = 16\n",
        "plt.rcParams['axes.linewidth'] = 1.5\n",
        "plt.rcParams['figure.dpi'] = 300\n",
        "plt.xlabel('|h|')\n",
        "plt.ylabel('Error (MSE)')\n",
        "plt.yscale('log')\n",
        "plt.plot(x2, err2(ml_arr2), label='ML+int')\n",
        "plt.plot(x, err(inv_arr)[sortids], label='Inversion')\n",
        "\n",
        "plt.plot(x, err(ml_arr), label='ML')\n",
        "\n",
        "plt.legend()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rye3Sthnl8VJ"
      },
      "outputs": [],
      "source": [
        "# Definición de estimadores\n",
        "def rho1_to_h_gc(rho, beta, mu = 0):\n",
        "    inv = scipy.linalg.inv(rho)\n",
        "    mat = scipy.linalg.logm(inv-np.eye(inv.shape[0]))\n",
        "    return (mat+mu)/beta\n",
        "\n",
        "idx = np.random.randint(0,100)\n",
        "\n",
        "iterador = iter(val_dataset)\n",
        "sample = next(iterador)\n",
        "rho_arr = sample[0][0]\n",
        "prediction = model.predict(sample[0])\n",
        "actual_values = sample[1]\n",
        "\n",
        "# Ploteamos\n",
        "## Eje X\n",
        "sortids = np.array(np.linalg.norm(actual_values, axis=-1)).argsort().astype(int)\n",
        "x = np.array(np.linalg.norm(actual_values, axis=-1))[sortids]\n",
        "actual_values = actual_values.numpy()[sortids]\n",
        "\n",
        "# Gráficos\n",
        "inv_arr = np.array([np.diagonal(rho1_to_h_gc(r, beta, 0)) for r in rho_arr]).reshape(gpu_batch_size//2,basis.d)\n",
        "ml_arr = prediction[sortids]\n",
        "err = lambda a: np.linalg.norm(a-actual_values, axis=-1)\n",
        "\n",
        "#%matplotlib inline\n",
        "#plt.rcParams['text.usetex'] = True\n",
        "\n",
        "plt.plot(x, err(inv_arr)[sortids], label='Inv GC')\n",
        "plt.plot(x, err(ml_arr), label='ML')\n",
        "plt.rcParams['axes.labelsize'] = 16\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "plt.rcParams['legend.fontsize'] = 16\n",
        "plt.rcParams['axes.linewidth'] = 1.5\n",
        "plt.rcParams['figure.dpi'] = 300\n",
        "plt.legend()\n",
        "plt.xlabel('|h|')\n",
        "plt.ylabel('Error (MSE)')\n",
        "plt.yscale('log')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BgFg2Zel8VJ"
      },
      "outputs": [],
      "source": [
        "for i, x in enumerate(min_actual_gm):\n",
        "    if np.linalg.norm(x) < 10:\n",
        "        print(x, min_num_gm[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfpL5jC9l8VJ"
      },
      "outputs": [],
      "source": [
        "pre_const = lambda g: bcs_opti_cost_alt(g, dex, basis.m, state_type=state_type, h_type=h_type)\n",
        "nl_const = scipy.optimize.NonlinearConstraint(pre_const, -0.1, 0.1)\n",
        "cost = lambda g: np.linalg.norm(g-np.repeat(actual_values,2))\n",
        "bounds = [(0.1, g_stop+0.5) for _ in range(basis.m)]\n",
        "\n",
        "#opt = scipy.optimize.minimize(cost, np.random.rand(basis.m), constraints=nl_const, bounds=bounds, method='SLSQP')\n",
        "opt = scipy.optimize.differential_evolution(cost, bounds=bounds, constraints=nl_const)\n",
        "print(opt)\n",
        "print(pre_const(opt.x), actual_values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JY5NwXVl8VK"
      },
      "outputs": [],
      "source": [
        "# Buscamos delta_k   TODO: CASO TERMICO ENERGIA\n",
        "#dist = lambda delta_k: np.linalg.norm(bcs_deltak_rho(delta_k, basis.m, state_type)-rho_init, ord=2) + 0.01 * (delta_energ(delta_k, state_type)-actual_energy)**2\n",
        "#bounds = [(0, 50) for _ in range(basis.m//2)] # Bounds de delta_k, TODO determinar o acotar\n",
        "#opti = scipy.optimize.dual_annealing(dist, bounds=bounds, maxiter=1000)\n",
        "#delta_k = opti.x\n",
        "#print(delta_k, delta_r)\n",
        "\n",
        "g_dist = lambda g: bcs_opti_cost(g, delta_r, basis.m, state_type=state_type, h_type='vect')\n",
        "bounds = [(g_init, g_stop), (0.01, 2.1)]\n",
        "#optig = scipy.optimize.dual_annealing(g_dist, bounds=bounds, maxiter=1000) broyden1!!\n",
        "scipy.optimize.root(g_dist, (1,1), method='hybr', options={'maxiter': 10000})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uwC-u8ll8VK"
      },
      "outputs": [],
      "source": [
        "scipy.optimize.roots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FdOXzdPl8VK"
      },
      "outputs": [],
      "source": [
        "auto_delta(delta_r, state_type=state_type), delta_r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsvgVN6Fl8VL"
      },
      "outputs": [],
      "source": [
        "g_dist = lambda g: bcs_opti_cost(g, delta_r, basis.m, state_type=state_type, h_type='vect')\n",
        "bounds = [(g_init, g_stop) for _ in range(basis.m//2)]\n",
        "optig = scipy.optimize.dual_annealing(g_dist, bounds=bounds, maxiter=1000)\n",
        "print(optig)\n",
        "optig.x, sample[1][idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1ADJy7xl8VL"
      },
      "outputs": [],
      "source": [
        "dex, auto_delta(dex, state_type=state_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQqFCSNnl8VL"
      },
      "outputs": [],
      "source": [
        "rbcs = bcs_deltak_rho(dex, state_type=state_type)\n",
        "np.linalg.eigvals(rbcs), np.linalg.eigvals(rho_init)\n",
        "plt.plot(np.linalg.eigvals(rbcs))\n",
        "plt.plot(np.linalg.eigvals(rho_init))\n",
        "plt.yscale('log')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SN-vuPJCl8VL"
      },
      "outputs": [],
      "source": [
        "nn = [0,1,2,None,np.inf,-1,-2]\n",
        "for x in nn:\n",
        "    d = dex\n",
        "    print(np.linalg.norm(d, ord=x))\n",
        "\n",
        "print(np.linalg.norm(d, ord=2)*2-np.linalg.norm(d, ord=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiR1RxNVl8VM"
      },
      "outputs": [],
      "source": [
        "bounds = [(0, 50) for _ in range(basis.m//2)]\n",
        "scipy.optimize.dual_annealing(dist, bounds=bounds, maxiter=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_psBheVl8VM"
      },
      "outputs": [],
      "source": [
        "dist = lambda delta_k: (bcs_deltak_rho(delta_k, basis.m, state_type)-rho_init).numpy().flatten()\n",
        "op = scipy.optimize.root(dist, np.random.rand(4), method='lm', tol=1e-8, epsfcn = 0.1)\n",
        "op, delta_r\n",
        "#delta_k = op.x\n",
        "#type(dist(delta_r))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmOyUb7Wl8VM"
      },
      "outputs": [],
      "source": [
        "op = scipy.optimize.fsolve(dist, np.random.rand(basis.m//2))\n",
        "op"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvseXU36l8VM"
      },
      "outputs": [],
      "source": [
        "rho_init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUF6vUVsl8VM"
      },
      "outputs": [],
      "source": [
        "dist = lambda delta_k: np.linalg.norm(bcs_deltak_rho(delta_k, basis.m, state_type)-rho_init)+(np.linalg.norm(delta_k, ord=0)-basis.m//2)**2\n",
        "bounds = [(0,10) for _ in range(4)]\n",
        "opti = scipy.optimize.dual_annealing(dist, bounds=bounds, maxiter=10000)\n",
        "delta_k = opti.x\n",
        "print(dist(delta_r))\n",
        "\n",
        "opti, delta_r\n",
        "#g_dist = lambda g: bcs_opti_cost(g, delta_r, basis.m, state_type=state_type, h_type='gaussvect')\n",
        "#optig = scipy.optimize.dual_annealing(g_dist, bounds=[(0,10), (0,10)], maxiter=10000)\n",
        "#optig, sample[1][idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhlS6HOWl8VN"
      },
      "outputs": [],
      "source": [
        "np.linalg.norm(delta_k, ord=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sujAhDb2l8VN"
      },
      "outputs": [],
      "source": [
        "nn = [0,1,2,None,np.inf,-1,-2]\n",
        "for x in nn:\n",
        "    print(np.linalg.norm(delta_r, ord=x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3d0ykwBl8VN"
      },
      "outputs": [],
      "source": [
        "opti +"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTltRxBJl8VN"
      },
      "outputs": [],
      "source": [
        "#opti = scipy.optimize.differential_evolution(dist, bounds=bounds)\n",
        "#opti\n",
        "opti = scipy.optimize.direct(dist, bounds=bounds, maxiter=1000)\n",
        "print(dist(delta_r))\n",
        "opti"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utyvGjNRl8VO"
      },
      "outputs": [],
      "source": [
        "g_dist = lambda g: bcs_opti_cost(g, delta_r, basis.m, state_type=state_type, h_type='gaussvect')\n",
        "boundss = [(0.01,20) for _ in range(2)]\n",
        "optig = scipy.optimize.dual_annealing(g_dist, bounds=boundss, maxiter=1000)\n",
        "sample[1][idx], optig, delta_r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXxntCmxl8VO"
      },
      "outputs": [],
      "source": [
        "bcs_opti_cost(sample[1][idx], delta_r, state_type=state_type, h_type='gaussvect')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qq-jYX1cl8VO"
      },
      "outputs": [],
      "source": [
        "g_dist = lambda g: bcs_opti_cost(g, delta_r, basis.m, state_type=state_type, h_type='gaussvect')\n",
        "opti = scipy.optimize.minimize(g_dist, np.random.rand(2), method='Nelder-Mead', tol=1e-8)\n",
        "opti, sample[1][idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1uwSrXOl8VO"
      },
      "outputs": [],
      "source": [
        "g_dist(sample[1][rand_idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixkQldkwl8VO"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.linalg.eigvals(rho_init))\n",
        "\n",
        "plt.plot(np.linalg.eigvals(bcs_deltak_rho(dex, basis.m)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leq8yiZtl8VO"
      },
      "outputs": [],
      "source": [
        "delta_k = dex\n",
        "# Calculamos el sistema de ecs\n",
        "delta_k = np.abs(np.concatenate((delta_k, np.flip(delta_k)))) # pues el resultado son los delta indep\n",
        "sq = lambda k: np.sqrt(energ[k]**2+delta_k[k]**2)\n",
        "vk = lambda k: np.sqrt(1/2 * (1 - energ[k]/sq(k)))\n",
        "uk = lambda k: np.sqrt(1/2 * (1 + energ[k]/sq(k)))\n",
        "\n",
        "M = bcs_build_M(lambda k: uk(k) * vk(k))\n",
        "M @ delta_k[:basis.m//2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wa1HwXhgl8VP"
      },
      "outputs": [],
      "source": [
        "arr = []\n",
        "for i in range(0, 100):\n",
        "    arr.append(rho_error_from_ge(actual_values.numpy(), sample[0][0][i]))\n",
        "np.mean(arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-warz5-Bl8VP"
      },
      "outputs": [],
      "source": [
        "rho_init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAyBfO-bl8VP"
      },
      "outputs": [],
      "source": [
        "maxval = 100\n",
        "# Cargamos elementos del conjunto de validación\n",
        "iterador = iter(val_dataset)\n",
        "sample = next(iterador)\n",
        "\n",
        "input_rhos = sample[0][0].numpy()[:maxval]\n",
        "input_energies = sample[0][1].numpy()[:maxval]\n",
        "actual_values = sample[1].numpy()[:maxval]\n",
        "input_data = sample[0]\n",
        "predictions = model.predict(input_data)\n",
        "\n",
        "# Ordenamos los valores de G con el fin de plotear\n",
        "g_ids = actual_values[:,0].argsort()\n",
        "g_ids = np.mean(actual_values, axis=-1).argsort()\n",
        "predictions_sort = predictions[g_ids]\n",
        "g_true_sort = actual_values[g_ids]\n",
        "rho_pred = rho_reconstruction(predictions_sort)\n",
        "rho_actual = input_rhos[g_ids]\n",
        "\n",
        "# Calculamos ahora G BCS\n",
        "rho_bcs_arr = []\n",
        "for l in tqdm(range(actual_values.shape[0])): # equiv al batch_size\n",
        "    rho = input_rhos[l]\n",
        "    actual_energy = input_energies[l]\n",
        "    gex, dex = opti_delta(rho, actual_energy,0)\n",
        "    rho_bcs = rho_reconstruction(gex)\n",
        "    #print(rho_bcs)\n",
        "    rho_bcs_arr.append(rho_bcs)\n",
        "\n",
        "rho_bcs = np.array(rho_bcs_arr)[g_ids]\n",
        "\n",
        "rho_error = lambda x: np.linalg.norm(rho_actual-x, ord='fro', axis=(1,2))\n",
        "\n",
        "plt.plot(g_true_sort[:,0], rho_error(rho_pred), label='DNN prediction') # ploteamos segun el primero\n",
        "plt.plot(g_true_sort[:,0], rho_error(rho_bcs), label='BCS')\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel(\"g\")\n",
        "plt.ylabel(\"Rho2 reconstruction error\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_FO9XwKl8VP"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.sort(np.mean(actual_values, axis=-1)), rho_error(rho_pred), label='CNN prediction') # ploteamos segun el primero\n",
        "plt.plot(np.sort(np.mean(actual_values, axis=-1)), rho_error(rho_bcs), label='BCS')\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel(\"g\")\n",
        "plt.ylabel(\"Rho2 reconstruction error\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wP11lf1fl8VQ"
      },
      "outputs": [],
      "source": [
        "rho_error = lambda x: np.linalg.norm(rho_actual-x, ord=2, axis=(1,2))\n",
        "\n",
        "plt.plot(g_true_sort[:,0], rho_error(rho_pred), label='DNN prediction') # ploteamos segun el primero\n",
        "plt.plot(g_true_sort[:,0], rho_error(rho_bcs), label='BCS')\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel(\"g\")\n",
        "plt.ylabel(\"Rho2 reconstruction error\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYa8VRD0l8VQ"
      },
      "source": [
        "#### Análisis para G cte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGquuRl7l8VQ"
      },
      "outputs": [],
      "source": [
        "# Generacion de elementos, rho2 a partir de ellos, y comparación con la predicción\n",
        "# Nuevamente, el resultado depende pura y exclusivamente del modelo, y no de los ptos tomados\n",
        "h_labels = np.linspace(0.1,1,512)\n",
        "g_arr = [np.ones((basis.m, basis.m))*g_seed for g_seed in h_labels]\n",
        "g_arr = tf.constant(g_arr, dtype=tf.float32)\n",
        "h_arr = two_body_hamiltonian_tf(t_basis, basis.m, en_batch, g_arr.numpy(), rho_1_arrays, rho_2_arrays, k_indices_tf)\n",
        "\n",
        "# Estados térmicos\n",
        "state = thermal_state_tf(h_arr*beta)\n",
        "state = tf.cast(state, dtype=tf.float32)\n",
        "# Estados puros\n",
        "#state = pure_state(h_arr)\n",
        "\n",
        "rho_2_input = rho_2_tf(state, rho_2_arrays_kkbar_tf)\n",
        "predictions = model.predict(rho_2_input).T\n",
        "G_err = np.abs(predictions-h_labels).T\n",
        "plt.plot(h_labels, G_err)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dui35UOnl8VQ"
      },
      "outputs": [],
      "source": [
        "# Ploteo de varios elementos de val_dataset\n",
        "# No sirve de mucho, depende del modelo y no la muestra\n",
        "max_plt = 10\n",
        "idx = 0\n",
        "for e in val_dataset:\n",
        "    predictions = model.predict(e[0])\n",
        "    pred_ids = predictions.T.argsort()\n",
        "    predictions_sort = predictions[pred_ids][0]\n",
        "    G_true_sorted = e[1].numpy()[pred_ids].T\n",
        "    G_err = np.abs(predictions_sort-G_true_sorted)\n",
        "    plt.plot(predictions_sort,G_err)\n",
        "    idx += 1\n",
        "    if idx > max_plt:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzh2uhMxl8VR"
      },
      "source": [
        "## Modelos Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7X_Nfysl8VR"
      },
      "source": [
        "Tenemos que trabajar con DataFrames para trabajar con xgboost, por eso inicialmente desempaquetamos el dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1xDzN9Bl8VR"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow_decision_forests as tfdf\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def rf_fit(dataset):\n",
        "    # Generación de dataset\n",
        "    ds_f = {} # features\n",
        "    ds_l = {} # labels\n",
        "\n",
        "    # Generamos las etiquetas\n",
        "    for i in range(basis.m*basis.m):\n",
        "        ds_f[f'{i}'] = []\n",
        "    # Generacion de labels TODO: Escribir todo en función del label size y fue\n",
        "    if label_size == 1:\n",
        "        ds_l['g'] = []\n",
        "    elif label_size == 2:\n",
        "        ds_l['g'] = []\n",
        "        ds_l['sigma'] = []\n",
        "    else:\n",
        "        for i in range(0, label_size):\n",
        "            ds_l[f'l{i}'] = []\n",
        "\n",
        "    # Poblamos el DF\n",
        "    for e in list(dataset.as_numpy_iterator()):\n",
        "        # Elementos de rho2\n",
        "        for i in range(0,basis.m*basis.m):\n",
        "            ds_f[f'{i}'].append(np.ndarray.flatten(e[0])[i])\n",
        "        # Labels\n",
        "        if label_size == 1:\n",
        "            ds_l['g'].append(e[1])\n",
        "        elif label_size == 2:\n",
        "            ds_l['g'].append(e[1][0])\n",
        "            ds_l['sigma'].append(e[1][1])\n",
        "        else:\n",
        "            for i in range(0, label_size):\n",
        "                ds_l[f'l{i}'].append(e[1][i])\n",
        "\n",
        "    ds_l = pd.DataFrame(ds_l)\n",
        "    ds_f = pd.DataFrame(ds_f)\n",
        "\n",
        "    # Spliteamos los datasets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(ds_f, ds_l, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Entrenamos\n",
        "    regressor = xgb.XGBRegressor(objective='reg:squarederror', max_depth=20)\n",
        "    regressor.fit(X_train, y_train)\n",
        "    predictions = regressor.predict(X_test)\n",
        "\n",
        "    # Evaluamos\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    print(f'Mean Squared Error: {mse}')\n",
        "\n",
        "    return regressor, X_test, y_test, y_train\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbX9wZ25l8VR"
      },
      "source": [
        "Análicemos los resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amxulDOAl8VR"
      },
      "outputs": [],
      "source": [
        "def rf_error_coef(regressor, X_test, y_test, y_train):\n",
        "    predictions = regressor.predict(X_test)\n",
        "    # Printeamos algunos valores\n",
        "    for i in range(0, 10):\n",
        "        print(predictions[i], y_test.to_numpy()[i])\n",
        "\n",
        "    if label_size == 1:\n",
        "        actual_values = y_test.to_numpy()\n",
        "        norm_pred = np.mean(np.abs(predictions-actual_values.T))\n",
        "        norm_rand = np.mean(np.abs(y_train.to_numpy()[:len(actual_values)]-actual_values))\n",
        "    elif label_size > 1:\n",
        "        norm_pred = np.mean(np.linalg.norm(predictions-y_test.to_numpy(),ord=2, axis=1))\n",
        "        norm_rand = np.mean(np.linalg.norm(y_train.to_numpy()[:len(predictions)]-y_test.to_numpy(),ord=2, axis=1))\n",
        "\n",
        "    print(norm_pred, norm_rand)\n",
        "    print(norm_rand / norm_pred)\n",
        "    return norm_rand / norm_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGeVHvE7l8VR"
      },
      "source": [
        "# Análisis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0Rq4chyl8VR"
      },
      "source": [
        "Ejemplo de uso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KxJzgdbl8VR"
      },
      "outputs": [],
      "source": [
        "dataset, label_size = gen_dataset('const', 0.1, 5, 'thermal', 'rho1')\n",
        "# DNN\n",
        "#model, val_dataset = dnn_fit(dataset, label_size)\n",
        "#dnn_error_coef(model, val_dataset)\n",
        "# RF\n",
        "regressor, X_test, y_test, y_train = rf_fit(dataset)\n",
        "rf_error_coef(regressor, X_test, y_test, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TTBKm09l8VS"
      },
      "outputs": [],
      "source": [
        "# Barrido en intervalos de G para G cte\n",
        "g_init_range = np.linspace(0.01,10,20)\n",
        "err_arr = []\n",
        "for g_init in g_init_range:\n",
        "    print(g_init)\n",
        "    dataset, label_size, input_type = gen_dataset('const', g_init, g_init+0.5, 'gs', 'rho1')\n",
        "    # DNN\n",
        "    model, val_dataset, history = dnn_fit(dataset, label_size, input_type)\n",
        "    err = dnn_error_coef(model, val_dataset)\n",
        "    # RF\n",
        "    #regressor, X_test, y_test, y_train = rf_fit(dataset)\n",
        "    #err = rf_error_coef(regressor, X_test, y_test, y_train)\n",
        "    err_arr.append(err)\n",
        "\n",
        "plt.plot(g_init_range,err_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlOV-48sl8VS"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.xlabel('G init')\n",
        "plt.ylabel('Loss coef')\n",
        "plt.plot(g_init_range,err_arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-jkL5OFl8VS"
      },
      "source": [
        "# Misc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7aHerU4l8VS"
      },
      "outputs": [],
      "source": [
        "from xgboost import plot_tree\n",
        "import matplotlib\n",
        "xgb.plot_tree(regressor, num_trees=20)\n",
        "fig = matplotlib.pyplot.gcf()\n",
        "fig.set_size_inches(150, 100)\n",
        "fig.savefig('tree.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmZmjikGl8VS"
      },
      "outputs": [],
      "source": [
        "# Para G cte, error en función de G. Sí, es cualquier cosa\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pred_ids = predictions.T.argsort()\n",
        "predictions_sort = predictions[pred_ids]\n",
        "G_true_sorted = y_test.to_numpy()[pred_ids].T[0]\n",
        "G_err = np.abs(predictions_sort-G_true_sorted)\n",
        "plt.plot(predictions_sort,G_err)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3DIFdswl8VT"
      },
      "outputs": [],
      "source": [
        "# Spliteo de DataFrames y generacion de Datasets\n",
        "label = 'h_labels'\n",
        "\n",
        "def split_dataset(dataset, test_ratio=0.30):\n",
        "  \"\"\"Splits a panda dataframe in two.\"\"\"\n",
        "  test_indices = np.random.rand(len(dataset)) < test_ratio\n",
        "  return dataset[~test_indices], dataset[test_indices]\n",
        "\n",
        "\n",
        "train_ds_pd, test_ds_pd = split_dataset(df)\n",
        "print(\"{} examples in training, {} examples for testing.\".format(\n",
        "    len(train_ds_pd), len(test_ds_pd)))\n",
        "\n",
        "train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label=label, task=tfdf.keras.Task.REGRESSION)\n",
        "test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_ds_pd, label=label, task=tfdf.keras.Task.REGRESSION)\n",
        "\n",
        "# Entrenamiento\n",
        "model = tfdf.keras.RandomForestModel(task = tfdf.keras.Task.REGRESSION)\n",
        "model.compile(metrics=[\"mse\"])\n",
        "model.fit(x=train_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0Ifs73Jl8VT"
      },
      "outputs": [],
      "source": [
        "tfdf.model_plotter.plot_model_in_colab(model, tree_idx=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SK08v8NFl8VT"
      },
      "outputs": [],
      "source": [
        "model.compile(metrics=[\"mse\"])\n",
        "evaluation = model.evaluate(test_ds, return_dict=True)\n",
        "print()\n",
        "\n",
        "for name, value in evaluation.items():\n",
        "  print(f\"{name}: {value:.4f}\")\n",
        "\n",
        "predictions = model.predict(test_ds)\n",
        "\n",
        "for e in test_ds:\n",
        "    for i in range(0, 10):\n",
        "        print(e[1][i])\n",
        "        print(predictions[i])\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRkpNXlAl8VU"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "logs = model.make_inspector().training_logs()\n",
        "plt.plot([log.num_trees for log in logs], [log.evaluation.rmse for log in logs])\n",
        "plt.xlabel(\"Number of trees\")\n",
        "plt.ylabel(\"RMSE (out-of-bag)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keORGbzWl8VU"
      },
      "source": [
        "Testeo barrido en G código anterior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFGt_Z0ll8VU"
      },
      "outputs": [],
      "source": [
        "num = 100\n",
        "g_range = np.linspace(0.01,20,num)\n",
        "rho_range= {}\n",
        "gpu_batch_size = 2\n",
        "\n",
        "# Construccion de parametros y matrices auxiliares\n",
        "#rho1_size = m1_basis.size\n",
        "rho2_size = m2_basis.size\n",
        "rho2kkbar_size = basis.m\n",
        "fund_size = basis.size\n",
        "hamil_base_size = basis.d*(basis.d+1)//2\n",
        "rho_1_arrays = rho_1_gen(basis)\n",
        "rho_1_arrays_tf = tf.constant(rho_1_arrays, dtype=tf.float32)\n",
        "rho_2_arrays = rho_2_gen(basis, nm2_basis, m2_basis)\n",
        "rho_2_arrays_tf = tf.constant(rho_2_arrays, dtype=tf.float32)\n",
        "rho_2_arrays_kkbar = rho_2_kkbar_gen(t_basis, rho_2_arrays)\n",
        "rho_2_arrays_kkbar_tf = tf.constant(rho_2_arrays_kkbar, dtype=tf.float32)\n",
        "k_indices = get_kkbar_indices(t_basis)\n",
        "k_indices_tf = gen_update_indices(t_basis, gpu_batch_size)\n",
        "\n",
        "batch_size = 2\n",
        "indices = tf.constant(get_kkbar_indices(t_basis))\n",
        "indices_tf = gen_update_indices(t_basis, batch_size)\n",
        "en_batch = [np.arange(0, basis.m) for _ in range(0,batch_size)]\n",
        "en_batch = tf.cast(en_batch, dtype=tf.float32)\n",
        "G_batched = [np.ones((basis.m,basis.m)) for _ in range(0, batch_size)]\n",
        "\n",
        "#h_arr = two_body_hamiltonian_tf(t_basis, basis.m, en_batch, g_arr.numpy(), rho_1_arrays, rho_2_arrays, k_indices_tf)\n",
        "#(h0, hi) = (t[0][0].numpy(), t[1][0].numpy())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njnGqWrkl8VU"
      },
      "outputs": [],
      "source": [
        "fund = [np.linalg.eigh(h[k,:,:])[1][:,0] for k in range(gpu_batch_size)]\n",
        "for x in range(gpu_batch_size):\n",
        "    fund_arr = np.array([np.outer(v,v) for v in fund])\n",
        "\n",
        "fund_arr[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kO9AufIml8VV"
      },
      "outputs": [],
      "source": [
        "\n",
        "en_batch = [np.repeat(np.arange(0, basis.num) - basis.num//2 + 1/2, 2) for _ in range(0,gpu_batch_size)] # Semilla para H equiespaciado\n",
        "energy_seed = en_batch\n",
        "#print(g)\n",
        "## CONST\n",
        "G_batched = [g * np.ones((basis.num,basis.num)) for _ in range(0, gpu_batch_size)]\n",
        "\n",
        "h = two_body_hamiltonian_sp(energy_seed, G_batched, rho_1_arrays, rho_2_arrays, get_kkbar_indices(t_basis))\n",
        "h = h.todense()\n",
        "fund = [np.linalg.eigh(h[k,:,:])[1][:,0] for k in range(gpu_batch_size)]\n",
        "for x in range(gpu_batch_size):\n",
        "    fund_arr = np.array([np.outer(v,v) for v in fund])\n",
        "\n",
        "print(fund_arr[0])\n",
        "print(fund_arr.shape, rho_2_arrays.shape)\n",
        "np.einsum('bkl,ijkl->bij', fund_arr, rho_2_arrays)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hl4ScHXfl8VV"
      },
      "outputs": [],
      "source": [
        "gpu_batch_size = 2\n",
        "def compute_g(g):\n",
        "    en_batch = [np.repeat(np.arange(0, basis.num) - basis.num//2 + 1/2, 2) for _ in range(0,gpu_batch_size)] # Semilla para H equiespaciado\n",
        "    energy_seed = en_batch\n",
        "    #print(g)\n",
        "    ## CONST\n",
        "    G_batched = [g * np.ones((basis.num,basis.num)) for _ in range(0, gpu_batch_size)]\n",
        "    #G_batched = [g * np.ones(t_basis.size * (t_basis.size + 1)//2) for _ in range(0, gpu_batch_size)]\n",
        "    h = two_body_hamiltonian_sp(energy_seed, G_batched, rho_1_arrays, rho_2_arrays, get_kkbar_indices(t_basis))\n",
        "    #h = two_body_hamiltonian_sp(energy_seed, G_batched, rho_1_arrays, rho_2_arrays)\n",
        "    print(h.shape)\n",
        "    h = h.todense()\n",
        "    fund = [np.linalg.eigh(h[k,:,:])[1][:,0] for k in range(gpu_batch_size)]\n",
        "    for x in range(gpu_batch_size):\n",
        "        fund_arr = np.array([np.outer(v,v) for v in fund])\n",
        "    print(fund_arr.shape)\n",
        "    #print(fund)\n",
        "    #print('rho')\n",
        "    #Toda la matriz\n",
        "    rho = rho_2(fund_arr, rho_2_arrays)[0]\n",
        "    #print(rho.todense())\n",
        "    #Solo el bloque kkbar\n",
        "    #rho = rho_2_kkbar(basis, fund, ml_basis, mll_basis, t_basis)\n",
        "    #Rho1\n",
        "    #rho = rho_1(fund_arr, rho_1_arrays)[0]\n",
        "    r = np.sort(np.linalg.eigvals(rho.todense()).real)\n",
        "    print(r)\n",
        "    return (g, r)\n",
        "\n",
        "# Version sincrónica\n",
        "rho_range = {}\n",
        "g_range = np.linspace(0,5,10)\n",
        "\n",
        "for g in g_range:\n",
        "    print(g)\n",
        "    rho_range[g] = compute_g(g)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jx_FwJ27l8VV"
      },
      "outputs": [],
      "source": [
        "# Ploteamos\n",
        "rho_range = dict(rho_range)\n",
        "rho_range = dict(sorted(rho_range.items()))\n",
        "x_axis = list(g_range)\n",
        "values = list(rho_range.items())\n",
        "size = len(values[0][1])\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "num = 10\n",
        "# Plot using matplotlib\n",
        "# Use LaTeX to format all text\n",
        "\n",
        "plt.rcParams['text.usetex'] = False #True\n",
        "plt.rcParams['axes.labelsize'] = 30\n",
        "plt.rcParams['xtick.labelsize'] = 20\n",
        "plt.rcParams['ytick.labelsize'] = 20\n",
        "plt.rcParams['legend.fontsize'] = 20\n",
        "plt.rcParams['axes.linewidth'] = 1.5\n",
        "\n",
        "plt.cla()\n",
        "plt.figure(figsize=(8, 5))\n",
        "#%matplotlib qt\n",
        "%matplotlib inline\n",
        "for k in range(1,size):\n",
        "    plt.plot(x_axis, [values[j][1][k] for j in range(0,num)], linewidth=2)\n",
        "\n",
        "#plt.xlabel(r'$G/\\epsilon$', fontsize=18)\n",
        "#plt.ylabel(r'$\\lambda^{(2)}$', fontsize=18)\n",
        "plt.xlim(0, 5)  # Set x-axis limits from 0 to 6\n",
        "plt.ylim(0, 5)  # Set y-axis limits from 5 to 12\n",
        "\n",
        "#matplotlib.use('Agg')\n",
        "#matplotlib.use('GTK3Agg')\n",
        "\n",
        "plt.tick_params(axis='x', which='both', bottom=True, top=True, labelbottom=True)\n",
        "\n",
        "# Enable minor ticks on the x-axis\n",
        "plt.minorticks_on()\n",
        "\n",
        "# Customize the appearance of minor ticks on the x-axis\n",
        "plt.tick_params(axis='x', which='minor', width=1.5)\n",
        "plt.tick_params(axis='x', which='major', width=1.5)\n",
        "plt.tick_params(axis='y', which='major', width=1.5)\n",
        "\n",
        "plt.show()\n",
        "#matplotlib.pyplot.savefig('filename.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqD0ZzcTl8VW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}