{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aguschanchu/FermionicML/blob/main/FermionicML_thermal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXz5cOlVwrzZ"
      },
      "source": [
        "# FermionicML:\n",
        "\n",
        "Code based on aguschanchu/Bosonic.py\n",
        "\n",
        "A diferencia del código anterior, este modelo trabaja sobre estados térmicos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLD2Yai55rMm"
      },
      "source": [
        "## Código base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgf9ExZN4jA7"
      },
      "source": [
        "Cargamos el código de Bosonic.py básico, branch fermionic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Gydz4kCH4l5w"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-03 12:47:06.680307: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-03 12:47:07.333442: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.special import binom\n",
        "from scipy.sparse import dok_matrix, linalg\n",
        "from scipy import linalg as linalg_d\n",
        "from joblib import Memory\n",
        "import random\n",
        "import plotly.graph_objects as go\n",
        "from joblib import Parallel, delayed\n",
        "from numba import jit, prange, njit\n",
        "import numba as nb\n",
        "import pickle\n",
        "import math\n",
        "import tensorflow_probability as tfp\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Funciones auxiliares optimiadas\n",
        "@nb.jit(nopython=True, parallel=True)\n",
        "def int_to_tuple_arr(ni,nf, b, digits=None):\n",
        "    sol = np.zeros((nf-ni, digits), dtype=np.int64)\n",
        "    for n in prange(ni, nf):\n",
        "        r = np.zeros(digits, dtype=np.int64)\n",
        "        ncop = n\n",
        "        idx = 0\n",
        "        while n != 0:\n",
        "            r[idx] = n % b\n",
        "            n = n // b\n",
        "            idx += 1\n",
        "        if digits is not None:\n",
        "            if idx < digits:\n",
        "                for i in range(idx, digits):\n",
        "                    r[i] = 0\n",
        "                idx = digits\n",
        "        sol[ncop-ni,:] = r[:idx]\n",
        "    return sol\n",
        "\n",
        "def tuple_to_int(t, d):\n",
        "    b = d-1\n",
        "    l = len(t)\n",
        "    s = [t[k]*b**(l-k-1) for k in range(0,l)]\n",
        "    return sum(s)\n",
        "\n",
        "def create_basis_(m, d, size):\n",
        "    base = []\n",
        "    index = 0\n",
        "    chunk_size = 1000000\n",
        "    for x in range(0,(m+1)**d, chunk_size):\n",
        "        start_index = x\n",
        "        end_index = min(x + chunk_size, (m+1)**d)\n",
        "        arr = int_to_tuple_arr(start_index, end_index, m+1, d)\n",
        "        sums = np.sum(arr, axis=1)\n",
        "        rows = np.where(sums == m)[0]\n",
        "        for row in [arr[i] for i in rows]:\n",
        "            if np.all(np.logical_or(row == 0, row == 1)):\n",
        "                base.append(row)\n",
        "\n",
        "    # Como consecuencia de la paralelizacion, es necesario reordenar la base\n",
        "    sorted_base = sorted(base, key=lambda x: tuple_to_int(x, d), reverse=True)\n",
        "    assert len(base) == size\n",
        "\n",
        "    return sorted_base\n",
        "\n",
        "def custom_base_representation_tf(n_min, n_max, base, num_digits):\n",
        "    # Generate a range of numbers from n_min to n_max\n",
        "    numbers = tf.range(n_min, n_max + 1, dtype=tf.int64)\n",
        "    \n",
        "    # Calculate the digits in the custom base using broadcasting\n",
        "    digits = tf.pow(tf.cast(base, dtype=tf.float64), tf.cast(tf.range(num_digits), dtype=tf.float64))\n",
        "    \n",
        "    # Reshape the digits to [1, num_digits] for broadcasting\n",
        "    digits = tf.reshape(digits, [1, -1])\n",
        "    \n",
        "    # Reshape numbers to [batch_size, 1]\n",
        "    numbers = tf.reshape(tf.cast(numbers, dtype=tf.float64), [-1, 1])\n",
        "    \n",
        "    # Calculate the digits in the custom base for each number using broadcasting\n",
        "    result = tf.cast(tf.math.floormod(tf.math.floordiv(numbers, digits), base), dtype=tf.int32)\n",
        "    \n",
        "    # Pad the result to have exactly num_digits columns\n",
        "    result = tf.pad(result, paddings=[[0, 0], [0, num_digits - tf.shape(result)[1]]], constant_values=0)\n",
        "    \n",
        "    # Reverse the order of columns\n",
        "    #result = tf.reverse(result, axis=[1])\n",
        "\n",
        "    return result\n",
        "\n",
        "def select_rows_with_sum(arr, m):\n",
        "    # Create a mask based on the criteria\n",
        "    mask = tf.reduce_all(tf.math.logical_or(tf.equal(arr, 0), tf.equal(arr, 1)), axis=1) & (tf.reduce_sum(arr, axis=1) == m)\n",
        "    \n",
        "    # Use the mask to select the rows\n",
        "    result = tf.boolean_mask(arr, mask, axis=0)\n",
        "    \n",
        "    return result\n",
        "\n",
        "def create_basis_tf_(m, d):\n",
        "    base = []\n",
        "    index = 0\n",
        "    chunk_size = 10000000\n",
        "    for x in tqdm(range(0,(m+1)**d, chunk_size)):\n",
        "        start_index = x\n",
        "        end_index = min(x + chunk_size, (m+1)**d)\n",
        "        res = custom_base_representation_tf(start_index, end_index, m+1, d)\n",
        "        arr = select_rows_with_sum(res, m)\n",
        "        base.append(arr.numpy())\n",
        "\n",
        "    return np.concatenate(base)\n",
        "\n",
        "class fixed_basis:\n",
        "\n",
        "    # Convierte a un enterno n a su escritura en base b\n",
        "    def _int_to_tuple(self, n, b, digits = None):\n",
        "        rep = np.base_repr(n, b)\n",
        "        rep_int = [int(x,b) for x in rep]\n",
        "        if digits is not None:\n",
        "            zeros = [0 for i in range(0,digits-len(rep))]\n",
        "            return zeros + rep_int\n",
        "        else:\n",
        "            return rep_int\n",
        "\n",
        "    # Revierte la transformacion anterior\n",
        "    def tuple_to_int(self, t):\n",
        "        b = self.d-1\n",
        "        l = len(t)\n",
        "        s = [t[k]*b**(l-k-1) for k in range(0,l)]\n",
        "        return sum(s)\n",
        "\n",
        "    # Convierte el vector en su representacion\n",
        "    def vect_to_repr(self, vect):\n",
        "        for i, k in enumerate(vect):\n",
        "            if k == 1. or k == 1:\n",
        "                break\n",
        "        else:\n",
        "            return 0\n",
        "        return self.base[i,:]\n",
        "\n",
        "    def rep_to_vect(self, rep):\n",
        "        rep = list(rep)\n",
        "        for i, r in [(j, self.base[j,:]) for j in range(0,self.size)]:\n",
        "            if list(r) == rep:\n",
        "                return self.canonicals[:,i]\n",
        "        else:\n",
        "            None\n",
        "\n",
        "    def rep_to_index(self, rep):\n",
        "        return self.base.tolist().index(list(rep))\n",
        "\n",
        "    @staticmethod\n",
        "    def rep_to_exi(rep):\n",
        "        r = []\n",
        "        for i, k in enumerate(rep):\n",
        "            r += [i for x in range(0,k)]\n",
        "        return r\n",
        "\n",
        "    # Crea base de M particulas en D estados (repr y base canonica)\n",
        "    def create_basis(self, m, d):\n",
        "        #print(\"Creating basis: \", m, d)\n",
        "        length = int(binom(d,m))\n",
        "        base = np.array(create_basis_tf_(m, d))\n",
        "        # Asignamos a cada uno de ellos un canónico\n",
        "        canonicals = np.eye(length)\n",
        "        return base, canonicals\n",
        "\n",
        "    def __init__(self, m, d):\n",
        "        self.m = m\n",
        "        self.d = d\n",
        "        self.size = int(binom(d,m))\n",
        "        (self.base, self.canonicals) = self.create_basis(m, d)\n",
        "\n",
        "\n",
        "# Matrices de aniquilación y creación endomórficas. Estan fuera de la clase para poder ser cacheadas\n",
        "#@memory.cache\n",
        "def bdb(basis, i, j):\n",
        "    mat = dok_matrix((basis.size, basis.size), dtype=np.float32)\n",
        "    if i != j:\n",
        "        for k, v in enumerate(basis.base):\n",
        "            if v[j] != 0 and v[i] != 1:\n",
        "                dest = list(v.copy())\n",
        "                dest[j] -= 1\n",
        "                dest[i] += 1\n",
        "                tar = basis.rep_to_index(dest)\n",
        "                mat[tar, k] = np.sqrt(v[i]+1)*np.sqrt(v[j])\n",
        "    else:\n",
        "        for k, v in enumerate(basis.base):\n",
        "            if v[j] != 0:\n",
        "                mat[k, k] = v[i]\n",
        "    return mat\n",
        "\n",
        "#@memory.cache\n",
        "def bbd(basis, i, j):\n",
        "    mat = dok_matrix((basis.size, basis.size), dtype=np.float32)\n",
        "    if i != j:\n",
        "        for k, v in enumerate(basis.base):\n",
        "            if v[i] != 0 and v[j] != 1:\n",
        "                dest = list(v.copy())\n",
        "                dest[i] -= 1\n",
        "                dest[j] += 1\n",
        "                tar = basis.rep_to_index(dest)\n",
        "                mat[tar, k] = np.sqrt(v[j]+1)*np.sqrt(v[i])\n",
        "    else:\n",
        "        for k, v in enumerate(basis.base):\n",
        "            if v[i] != 1:\n",
        "                mat[k, k] = v[i]+1\n",
        "    return mat\n",
        "\n",
        "# Matrices de aniquilación y creación.Toman la base de origen y destino (basis_o, basis_d) resp\n",
        "#@nb.jit(nopython=True, parallel=True)\n",
        "@nb.jit(nopython=True)\n",
        "def b_aux(basis_o, basis_d, i):\n",
        "    mat = np.zeros((len(basis_d), len(basis_o)), dtype=np.float32)\n",
        "    for k in prange(len(basis_o)):\n",
        "        if basis_o[k][i] != 0:\n",
        "            dest = list(basis_o[k].copy())\n",
        "            dest[i] -= 1\n",
        "            for j in prange(len(basis_d)):\n",
        "                if list(basis_d[j]) == dest:\n",
        "                    tar = j\n",
        "                    mat[tar, k] = np.sqrt(basis_o[k][i])\n",
        "    return mat\n",
        "\n",
        "def b(basis_o, basis_d, i):\n",
        "    return b_aux(basis_o.base, basis_d.base, i)\n",
        "\n",
        "#@nb.jit(nopython=True, parallel=True)\n",
        "@nb.jit(nopython=True)\n",
        "def bd_aux(basis_o, basis_d, i):\n",
        "    mat = np.zeros((len(basis_d), len(basis_o)), dtype=np.float32)\n",
        "    for k in prange(len(basis_o)):\n",
        "        if basis_o[k][i] != 1:\n",
        "            dest = list(basis_o[k].copy())\n",
        "            dest[i] += 1\n",
        "            for j in prange(len(basis_d)):\n",
        "                if list(basis_d[j]) == dest:\n",
        "                    tar = j\n",
        "                    mat[tar, k] = np.sqrt(basis_o[k][i]+1)\n",
        "    return mat\n",
        "\n",
        "def bd(basis_o, basis_d, i):\n",
        "    return bd_aux(basis_o.base, basis_d.base, i)\n",
        "\n",
        "\n",
        "# Acepta una lista de indices a crear\n",
        "@nb.jit(nopython=True, parallel=True)\n",
        "def bd_gen_aux(basis_o, basis_d, gen_list):\n",
        "    mat = np.zeros((len(basis_d), len(basis_o)), dtype=np.float32)\n",
        "    for k in prange(len(basis_o)):\n",
        "        conds = np.zeros(len(gen_list), dtype=np.int64)\n",
        "        for i in range(len(gen_list)):\n",
        "            if basis_o[k][gen_list[i]] != 1:\n",
        "                conds[i] = 1\n",
        "        if np.all(conds):\n",
        "            dest = list(basis_o[k].copy())\n",
        "            for i in gen_list:\n",
        "                dest[i] += 1\n",
        "            for j in prange(len(basis_d)):\n",
        "                if list(basis_d[j]) == dest:\n",
        "                    tar = j\n",
        "                    mat[tar, k] = np.sqrt(basis_o[k][i]+1)\n",
        "    return mat\n",
        "\n",
        "def bd_gen(basis_o, basis_d, i):\n",
        "    return bd_gen_aux(basis_o.base, basis_d.base, np.array(i))\n",
        "\n",
        "def b_gen(basis_o, basis_d, i):\n",
        "    return np.transpose(bd_gen(basis_d, basis_o, i))\n",
        "\n",
        "# Volvemos a definir la función para compilarla\n",
        "@nb.jit(forceobj=True)\n",
        "def _rep_to_index(base, rep):\n",
        "    return base.tolist().index(list(rep))\n",
        "\n",
        "# Funciones auxiliares para calcular rho2kkbar y gamma_p\n",
        "@nb.jit(nopython=True)\n",
        "def rep_to_exi(rep):\n",
        "    r = []\n",
        "    for i in range(len(rep)):\n",
        "        for j in range(rep[i]):\n",
        "            r.append(i)\n",
        "    return r\n",
        "\n",
        "@nb.njit\n",
        "def factorial(n):\n",
        "    result = 1\n",
        "    for i in range(1, n + 1):\n",
        "        result *= i\n",
        "    return result\n",
        "\n",
        "@nb.njit\n",
        "def gamma_lamba(x):\n",
        "    res = 1.0\n",
        "    for o in x:\n",
        "        res *= math.sqrt(factorial(o))\n",
        "    return res\n",
        "\n",
        "@nb.jit\n",
        "def gamma_lamba_inv(x):\n",
        "    res = 1.0\n",
        "    for o in x:\n",
        "        res *= 1.0 / np.sqrt(factorial(o))\n",
        "    return res\n",
        "\n",
        "@nb.njit\n",
        "def rep_to_index_np(base, rep):\n",
        "    for i in range(len(base)):\n",
        "        if np.all(base[i] == rep):\n",
        "            return i\n",
        "    return -1\n",
        "\n",
        "\n",
        "def gamma_p(basis, m, vect, m_basis = None, nm_basis = None):\n",
        "    d = basis.d\n",
        "    if not m_basis or not nm_basis:\n",
        "        m_basis = fixed_basis(m, d)\n",
        "        nm_basis = fixed_basis(basis.m-m,d)\n",
        "    return gamma_p_aux(basis.base, vect, m_basis.base, nm_basis.base)\n",
        "\n",
        "@nb.njit()\n",
        "def gamma_p_aux(basis, vect, m_basis, nm_basis):\n",
        "    mat = np.zeros((len(m_basis), len(nm_basis)), dtype=np.float32)\n",
        "    for i in prange(len(m_basis)):\n",
        "        v = m_basis[i]\n",
        "        for j in prange(len(nm_basis)):\n",
        "            w = nm_basis[j]\n",
        "            targ = v + w\n",
        "            index = rep_to_index_np(basis, targ)\n",
        "            if index != -1:\n",
        "                coef = vect[index]\n",
        "                if coef != 0:\n",
        "                    coef = coef * gamma_lamba_inv(v) * gamma_lamba_inv(w) * gamma_lamba(targ)\n",
        "                mat[i, j] = coef\n",
        "    return mat\n",
        "# Devuelve la matriz rho M asociada al vector\n",
        "def rho_m(basis, m, vect, m_basis = None, nm_basis = None):\n",
        "    g = gamma_p(basis, m, vect, m_basis, nm_basis)\n",
        "    return np.dot(g,np.transpose(g))\n",
        "\n",
        "# Devuelve la matriz gamma asociada a la descomposición (M,N-M) del vector\n",
        "@jit(forceobj=True)\n",
        "def gamma(basis, m, vect, m_basis = None, nm_basis = None):\n",
        "    d = basis.d\n",
        "    if not m_basis or not nm_basis:\n",
        "        m_basis = fixed_basis(m, d)\n",
        "        nm_basis = fixed_basis(basis.m-m,d)\n",
        "    mat = dok_matrix((m_basis.size, nm_basis.size), dtype=np.float32)\n",
        "    for i, v in enumerate(m_basis.base):\n",
        "        for j, w in enumerate(nm_basis.base):\n",
        "            targ = v+w\n",
        "            # Revisamos que sea un estado fermionico valido\n",
        "            arr = np.asarray(targ)\n",
        "            if not np.all(np.logical_or(arr == 0, arr == 1)):\n",
        "                continue\n",
        "            index = _rep_to_index(basis.base, targ)\n",
        "            coef = vect[index]\n",
        "            if coef != 0:\n",
        "                aux = lambda x: np.prod(np.reciprocal(np.sqrt([np.math.factorial(o) for o in x])))\n",
        "                aux_inv = lambda x: np.prod(np.sqrt([np.math.factorial(o) for o in x]))\n",
        "                coef = coef * aux(v) * aux(w) * aux_inv(targ)\n",
        "                #coef = coef\n",
        "                #print(v,w,coef)\n",
        "            mat[i,j] = coef\n",
        "    return mat\n",
        "\n",
        "# Genera las matrices de rho1\n",
        "def rho_1_gen(basis):\n",
        "    d = basis.d\n",
        "    s = basis.size\n",
        "    mat = np.empty((d,d,s,s), dtype=np.float32)\n",
        "    for i in range(0, d):\n",
        "        for j in range(0, d):\n",
        "            mat[i,j,:,:] = np.array(bdb(basis,j, i).todense())\n",
        "    return mat\n",
        "\n",
        "#@jit(parallel=True, nopython=True)\n",
        "def rho_1(d, state, rho_1_arrays):\n",
        "    state_expanded = state[np.newaxis, np.newaxis, :, :]\n",
        "    product = state_expanded * rho_1_arrays\n",
        "    mat = np.sum(product, axis=(-2, -1))\n",
        "\n",
        "    return mat\n",
        "\n",
        "\n",
        "# Genera las matrices de rho2\n",
        "def rho_2_gen(basis, mll_basis, t_basis):\n",
        "    size = t_basis.size\n",
        "    s = basis.size\n",
        "    # La entrada i, j contiene C_j^\\dag C_i    i, j \\in t_basis\n",
        "    mat = np.empty((size, size, s, s), dtype=np.float32)\n",
        "    for i, v in enumerate(t_basis.base):\n",
        "        for j, w in enumerate(t_basis.base):\n",
        "            c_i = b_gen(basis, mll_basis, rep_to_exi(v))\n",
        "            cdag_j = bd_gen(mll_basis, basis, rep_to_exi(w))\n",
        "            mat[i, j, :, :] = np.dot(cdag_j, c_i)\n",
        "\n",
        "    return mat\n",
        "\n",
        "def rho_2(size, state, rho_2_arrays):\n",
        "    state_expanded = np.expand_dims(state, axis=1)\n",
        "    state_expanded = np.expand_dims(state_expanded, axis=1)\n",
        "    rho_2_arrays = rho_2_arrays[np.newaxis, :, :, :, :]\n",
        "    print(state_expanded.shape, rho_2_arrays.shape)\n",
        "    product = state_expanded * rho_2_arrays\n",
        "    mat = np.sum(product, axis=(-2, -1))\n",
        "    return mat\n",
        "\n",
        "def rho_2_kkbar_gen(m, rho_2_arrays):\n",
        "    offset = 4*m+1\n",
        "    indices = [-(k-1)*(2*k-offset) for k in range(1,m+1)] # Lo saque de Mathmatica, vos confia\n",
        "    i, j = np.meshgrid(indices, indices, indexing='ij') # Lo usamos para rellenar la mat deseada\n",
        "\n",
        "    rho_2_arrays_kkbar = rho_2_arrays[i, j, :, :]\n",
        "\n",
        "    return rho_2_arrays_kkbar\n",
        "\n",
        "# Devuelve la matriz rho 2 asociada al bloque kkbar\n",
        "def rho_2_kkbar(basis, vect, ml_basis = None, mll_basis = None, t_basis = None):\n",
        "    d = basis.d\n",
        "    # Creo las bases si no están dadas\n",
        "    if ml_basis == None or mll_basis == None or t_basis == None:\n",
        "        ml_basis = fixed_basis(m-1,d)\n",
        "        mll_basis = fixed_basis(m-2,d)\n",
        "        t_basis = fixed_basis(2,d)\n",
        "    diag = []\n",
        "    for v in t_basis.base:\n",
        "        for j in range(0, d, 2):\n",
        "            if v[j] == v[j+1]:\n",
        "                continue\n",
        "            else:\n",
        "                break\n",
        "        else:\n",
        "            diag.append(v)\n",
        "    diag = np.array(diag)\n",
        "    return rho_2_kkbar_aux(diag, vect, basis.base, ml_basis.base, mll_basis.base, t_basis.base)\n",
        "\n",
        "@nb.njit\n",
        "def rho_2_kkbar_lambda(x):\n",
        "    res = 1.0\n",
        "    for o in x:\n",
        "        res *= 1.0 / math.sqrt(factorial(o))\n",
        "    return res\n",
        "\n",
        "#@nb.njit(parallel=True)\n",
        "def rho_2_kkbar_aux(diag, vect, basis, ml_basis, mll_basis, t_basis):\n",
        "    mat = np.zeros((len(diag), len(diag)), dtype=np.float32)\n",
        "    for i in prange(len(diag)):\n",
        "        for j in prange(len(diag)):\n",
        "            v = diag[i]\n",
        "            w = diag[j]\n",
        "            # Creacion de los a\n",
        "            i_set = rep_to_exi(v)\n",
        "            b_m = b_aux(ml_basis, mll_basis, i_set[1]) @ b_aux(basis, ml_basis, i_set[0])\n",
        "            # Creacion de los ad\n",
        "            i_set = rep_to_exi(w)\n",
        "            bd_m = bd_aux(ml_basis, basis, i_set[1]) @ bd_aux(mll_basis, ml_basis, i_set[0])\n",
        "            # v1 = vect @ bd_m @ b_m @ vect Para estados puros\n",
        "            # Mult de b's y filleo de mat\n",
        "            coef = np.trace(vect @ bd_m @ b_m)\n",
        "            mat[i,j] = coef * rho_2_kkbar_lambda(v) * rho_2_kkbar_lambda(w)\n",
        "    return mat\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dga5Xx_5vDf"
      },
      "source": [
        "## Definicion de Hamiltoniano"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myiTq53L5E1U"
      },
      "source": [
        "Cargamos el código de creación y resolución de Hamiltonianos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h5FXWv849Mq",
        "outputId": "49dd47b5-8c16-4ad4-92e7-e172462229b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-03 12:49:04.334431: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-03 12:49:04.361656: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-03 12:49:04.361881: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-03 12:49:04.363414: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-03 12:49:04.363644: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-03 12:49:04.363771: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-03 12:49:04.760442: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-03 12:49:04.760614: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-03 12:49:04.760745: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-03 12:49:04.760835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6365 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
            "2024-01-03 12:49:04.922120: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:543] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\n",
            "Searched for CUDA in the following directories:\n",
            "  ./cuda_sdk_lib\n",
            "  /usr/local/cuda-11.8\n",
            "  /usr/local/cuda\n",
            "  .\n",
            "You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00, 138.69it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 131.39it/s]\n"
          ]
        }
      ],
      "source": [
        "m = 4\n",
        "d = 8\n",
        "# Creo las bases para no tener que recrearlas luego\n",
        "basis = fixed_basis(m, d)\n",
        "basis_m1 = fixed_basis(m-1, d)\n",
        "basis_m2 = fixed_basis(m-2, d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PToiSs915TXw"
      },
      "outputs": [],
      "source": [
        "## Usamos este approach si queremos guardar los generadores\n",
        "# Dados 1/2 (d^2+d) elementos, genera una mat de dxd:\n",
        "eps = 0.00001\n",
        "\n",
        "def sym_mat_gen(vect, d):\n",
        "    matrix = fill_matrix(vect, d)\n",
        "    return matrix + matrix.T - np.diag(matrix.diagonal())\n",
        "\n",
        "@jit(nopython=True)\n",
        "def fill_matrix(vect, d):\n",
        "    matrix = np.zeros((d, d))\n",
        "    idx = 0\n",
        "    for i in prange(d):\n",
        "        for j in prange(i, d):\n",
        "            matrix[i, j] = vect[idx]\n",
        "            idx += 1\n",
        "    return matrix\n",
        "\n",
        "# Generamos una matrix aleatoria. Cuidado con la distribución, ver https://stackoverflow.com/questions/56605189/is-there-an-efficient-way-to-generate-a-symmetric-random-matrix\n",
        "def hamil_base_gen(d):\n",
        "    U = np.random.uniform(low=0, high=1.0, size=(d, d))\n",
        "    hamil_base = np.tril(U) + np.tril(U, -1).T\n",
        "    return hamil_base\n",
        "\n",
        "# Dada un a mat dxd simetrica, contruye el hamiltoniano de un cuerpo a_{ij} c^{dag}_i c_j\n",
        "# Alternativamente podemos construirlo a partir de rho_1_gen\n",
        "def base_hamiltonian_aux(mat, size, d, rho_1_gen):\n",
        "    # Construccion de H\n",
        "    rho_1_gen_transposed = rho_1_gen.transpose(1, 0, 2, 3)\n",
        "    mat_expanded = mat[:, :, np.newaxis, np.newaxis]\n",
        "    h = np.sum(mat_expanded * rho_1_gen_transposed[:, :, :, :], axis=(0, 1))\n",
        "    return h.astype(np.float32)\n",
        "\n",
        "def base_hamiltonian(mat, basis, rho_1_gen):\n",
        "    return base_hamiltonian_aux(mat, basis.size, basis.d, rho_1_gen)\n",
        "\n",
        "def get_kkbar_indices(t_basis):\n",
        "    indices = []\n",
        "    for i, v in enumerate(t_basis.base):\n",
        "        if np.all(v[::2] == v[1::2]):\n",
        "            indices.append(i)\n",
        "    return indices\n",
        "\n",
        "def two_body_hamiltonian(t_basis_size, m, energy, G, rho_1_arrays, rho_2_arrays, indices):\n",
        "    # Creamos la mat diagonal de d*d con los elementos de energy\n",
        "    # cada uno de estos, se contraen con los elementos de rho_1_arrays\n",
        "    # la mat energy contiene las energias de cada termino c^\\dag_k c_k para k kbar (iguales)\n",
        "    # por ello los elementos se repiten \n",
        "    energy_matrix = np.diagflat(np.kron(energy, np.ones(2))) + eps * np.random.random((2*m,2*m))\n",
        "    \n",
        "    # Construimos la mat de energía\n",
        "    rho_1_arrays_t = tf.transpose(rho_1_arrays,perm=[1, 0, 2, 3])\n",
        "    h0 = np.sum(energy_matrix[:, :, np.newaxis, np.newaxis] * rho_1_arrays_t[:, :, :, :], axis=(0, 1))\n",
        "\n",
        "    # Pasamos ahora a la matrix de interacción con la misma estrategia\n",
        "    # dada G que indica la interacción entre los pares k' k'bar k kbar \n",
        "    # (que son elementos particulares de t_basis)\n",
        "    # transladamos estos coeficientes a una matriz en t_basis\n",
        "    # y multiplicamos por rho_2_arrays\n",
        "    \n",
        "    # Primero determinamos, dada t_basis, cuales son los indices de pares kkbar\n",
        "    i, j = np.meshgrid(indices, indices, indexing='ij') # Lo usamos para rellenar la mat deseada\n",
        "    rho_2_arrays_t = tf.transpose(rho_2_arrays,perm=[1, 0, 2, 3])\n",
        "\n",
        "    # Contruimos la mat que contraeremos con rho_2_arrays\n",
        "    mat = np.zeros((t_basis_size, t_basis_size))\n",
        "    mat[i, j] = G\n",
        "    hi = np.sum(mat[:, :, np.newaxis, np.newaxis] * rho_2_arrays_t[:, :, :, :], axis=(0, 1))\n",
        "    return (h0, hi)\n",
        "\n",
        "def solve(h, last_step = None):\n",
        "    sol = linalg.eigsh(h, which='SA',k=19)\n",
        "    eigenspace_tol = 0.0001\n",
        "    if type(last_step) != type(None):\n",
        "        # Seleccionamos todos los autovects que difieren sus autovalores menos que tol (mismo autoespacio)\n",
        "        # y tomamos la proyección en el autoespacio de la solución del paso anterior (last_step)\n",
        "        eig = sol[0].real\n",
        "        eigv = sol[1]\n",
        "        cand = [eigv[:,i].real  for (i, x) in enumerate(eig) if abs(x-min(eig)) < eigenspace_tol]\n",
        "        cand_norm = [x/np.linalg.norm(x) for x in cand]\n",
        "        fund = np.zeros(len(cand[0]))\n",
        "        for x in cand_norm:\n",
        "            fund += np.dot(last_step,x) * x\n",
        "    else:\n",
        "        argmin = np.argmin(sol[0].real)\n",
        "        fund = sol[1][:,argmin]\n",
        "    fund = fund.real / np.linalg.norm(fund)\n",
        "    return fund\n",
        "\n",
        "# Generacion de H basada en TF\n",
        "\n",
        "# Funciones auxiliares de gen de H basado en TF\n",
        "## Dada matrix de indices, genera los indices de updates de TF\n",
        "def gen_update_indices(t_basis, batch_size):\n",
        "    # Calculamos los indices de kkbar en t_basis\n",
        "    indices = tf.constant(get_kkbar_indices(t_basis))\n",
        "    # Creamos el array de indices x indices\n",
        "    i, j = tf.meshgrid(indices, indices, indexing='ij')\n",
        "    matrix = tf.reshape(tf.stack([i, j], axis=-1), (-1, 2))\n",
        "\n",
        "    # Repeat the matrix along the first axis (axis=0) 'b' times\n",
        "    repeated_matrix = tf.repeat(tf.expand_dims(matrix, axis=0), repeats=batch_size, axis=0)\n",
        "\n",
        "    # Create an index array from 0 to b-1\n",
        "    indices = tf.range(batch_size, dtype=tf.int32)\n",
        "\n",
        "    # Expand the index array to have the same shape as the repeated matrix\n",
        "    indices = tf.expand_dims(indices, axis=-1)\n",
        "    indices = tf.expand_dims(indices, axis=-1)\n",
        "    indices = tf.tile(indices, multiples=[1,matrix.shape[0],1]) \n",
        "\n",
        "    # Concatenate the index array to the repeated matrix along a new axis\n",
        "    tiled_matrix = tf.concat([indices, repeated_matrix], axis=-1)\n",
        "    tiled_matrix = tf.reshape(tiled_matrix, [-1,3])\n",
        "    return tiled_matrix\n",
        "\n",
        "\n",
        "def two_body_hamiltonian_tf(t_basis, m, energy_batch, G_batched, rho_1_arrays, rho_2_arrays, indices):\n",
        "    # SECCIÓN ENERGIAS\n",
        "    ## Dado un batch de niveles, lo pasamos a TF\n",
        "    energy_matrix = tf.constant(energy_batch, dtype=tf.float32)\n",
        "    ## Repetimos los niveles para cada uno de los pares (por el nivel k y kbar)\n",
        "    energy_matrix = tf.repeat(energy_matrix, repeats=2, axis=1)\n",
        "    ## Generamos la matrix diagonal y expandimos\n",
        "    energy_matrix_expanded = tf.linalg.diag(energy_matrix)\n",
        "    energy_matrix_expanded = energy_matrix_expanded[:, :, :, np.newaxis, np.newaxis]\n",
        "    rho_1_gen_transposed = tf.transpose(rho_1_arrays, perm=[1, 0, 2, 3])\n",
        "    # Multiplicamos por los operadores C^dag C\n",
        "    h0_arr = tf.reduce_sum(energy_matrix_expanded * rho_1_gen_transposed[np.newaxis,:,:,:,:], axis=[1,2])\n",
        "\n",
        "    # SECCIÓN INTERACCIÓN\n",
        "    # Ya tenemos los indices de updates, ahora tomamos la mat en t_basis (una de zeros)\n",
        "    # y updateamos de acuerdo a la lista de G's cada uno flatteneados\n",
        "    G_flatten = np.ndarray.flatten(np.array([np.ndarray.flatten(G) for G in G_batched]))\n",
        "    # Creamos la mat de t_basis y updateamos a partir de los indices de kkbar\n",
        "    mat = tf.zeros((len(energy_batch), t_basis.size, t_basis.size), dtype=tf.float32)\n",
        "    mat = tf.tensor_scatter_nd_update(mat, indices, G_flatten)\n",
        "    # Preparamos las dimensiones y multiplicamos\n",
        "    mat_expanded = mat[:, :, :, np.newaxis, np.newaxis]\n",
        "    rho_2_gen_transposed = tf.transpose(rho_2_arrays, perm=[1, 0, 2, 3])\n",
        "    hi_arr = tf.reduce_sum(mat_expanded * rho_2_gen_transposed[np.newaxis,:,:,:,:], axis=[1,2])\n",
        "\n",
        "    return h0_arr - hi_arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emVBTg2QD-Fg"
      },
      "source": [
        "## Modelo de ML\n",
        "Basado en matrices densidad de 1 y 2 cuerpos como input, con hamiltoniano como salida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aF_Ec_mCGX96"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-02 15:30:19.417062: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /device:GPU:0 with 20609 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tf.test.gpu_device_name()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJDoa6LUJJ8O",
        "outputId": "73481454-fbcb-469f-d72f-cd0f8d534808"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 56.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "28\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 182.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1 1 0 0 0 0 0 0]\n",
            " [1 0 1 0 0 0 0 0]\n",
            " [0 1 1 0 0 0 0 0]\n",
            " [1 0 0 1 0 0 0 0]\n",
            " [0 1 0 1 0 0 0 0]\n",
            " [0 0 1 1 0 0 0 0]\n",
            " [1 0 0 0 1 0 0 0]\n",
            " [0 1 0 0 1 0 0 0]\n",
            " [0 0 1 0 1 0 0 0]\n",
            " [0 0 0 1 1 0 0 0]\n",
            " [1 0 0 0 0 1 0 0]\n",
            " [0 1 0 0 0 1 0 0]\n",
            " [0 0 1 0 0 1 0 0]\n",
            " [0 0 0 1 0 1 0 0]\n",
            " [0 0 0 0 1 1 0 0]\n",
            " [1 0 0 0 0 0 1 0]\n",
            " [0 1 0 0 0 0 1 0]\n",
            " [0 0 1 0 0 0 1 0]\n",
            " [0 0 0 1 0 0 1 0]\n",
            " [0 0 0 0 1 0 1 0]\n",
            " [0 0 0 0 0 1 1 0]\n",
            " [1 0 0 0 0 0 0 1]\n",
            " [0 1 0 0 0 0 0 1]\n",
            " [0 0 1 0 0 0 0 1]\n",
            " [0 0 0 1 0 0 0 1]\n",
            " [0 0 0 0 1 0 0 1]\n",
            " [0 0 0 0 0 1 0 1]\n",
            " [0 0 0 0 0 0 1 1]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 170.87it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 149.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n",
            "[[1 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 1]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 142.67it/s]\n"
          ]
        }
      ],
      "source": [
        "# Construccion de bases para calculo de rho1 y rho2\n",
        "# rho2\n",
        "m = 2\n",
        "m2_basis = fixed_basis(m, d)\n",
        "print(m2_basis.size)\n",
        "nm2_basis = fixed_basis(basis.m-m, d)\n",
        "print(nm2_basis.base)\n",
        "t_basis = fixed_basis(2, basis.d)\n",
        "# rho1\n",
        "m = 1\n",
        "m1_basis = fixed_basis(m, d)\n",
        "print(m1_basis.size)\n",
        "print(m1_basis.base)\n",
        "nm1_basis = fixed_basis(basis.m-m, d)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oapxWkD16fHg"
      },
      "source": [
        "### Algunos benchmarks y funciones auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "umCIrxCZKXQd"
      },
      "outputs": [],
      "source": [
        "# Given h calculo en rho2 y rho1 máximo\n",
        "def rho1_rho2(h, beta):\n",
        "    fund = thermal_state(h, beta)\n",
        "    rho2 = np.array(rho_2(basis, m2_basis.size, state, rho_2_arrays))\n",
        "    r = np.sort(linalg_d.eigvals(rho2).real)\n",
        "    rho_2_max = r[0]\n",
        "    rho1 = np.array(rho_1(basis, state, rho_1_arrays))\n",
        "    r = np.sort(linalg_d.eigvals(rho1).real)\n",
        "    rho_1_max = r[0]\n",
        "\n",
        "    return (rho_1_max, rho_2_max)\n",
        "\n",
        "def fill_triangular_np(x):\n",
        "    m = x.shape[0]\n",
        "    n = np.int32(np.sqrt(.25 + 2 * m) - .5)\n",
        "    x_tail = x[(m - (n**2 - m)):]\n",
        "    return np.triu(np.concatenate([x, x_tail[::-1]], 0).reshape(n, n))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QaNnIIc5bZux"
      },
      "outputs": [],
      "source": [
        "# TEST: Las funciones de TF y comunes coinciden\n",
        "\n",
        "# Dado h, \\beta, construyo el estado térmico\n",
        "from scipy.linalg import expm\n",
        "\n",
        "def thermal_state(h, beta):\n",
        "    quotient = expm(-beta*h)\n",
        "    return quotient / np.trace(quotient)\n",
        "\n",
        "## NO usar para mat no hermiticas\n",
        "@nb.jit(nopython=True)\n",
        "def thermal_state_eig(h, beta):\n",
        "    w, v = np.linalg.eigh(-beta*h)\n",
        "    D = np.diag(np.exp(w))\n",
        "    mat = v @ D @ v.T\n",
        "    mat = mat / np.trace(mat)\n",
        "    return mat\n",
        "    \n",
        "def gen_to_h(base, rho_1_arrays):\n",
        "    triag = fill_triangular_np(base)\n",
        "    body_gen = triag + np.transpose(triag)-np.diag(np.diag(triag))\n",
        "    h = np.array(base_hamiltonian(body_gen, basis, rho_1_arrays))  \n",
        "    return h \n",
        "\n",
        "def gen_to_h_1b(hamil_base):\n",
        "    triag = tfp.math.fill_triangular(hamil_base, upper=True)\n",
        "    body_gen = triag + tf.transpose(triag, perm=[0,2,1])-tf.linalg.diag(tf.linalg.diag_part(triag))\n",
        "    return body_gen\n",
        "\n",
        "def gen_to_h_tf(hamil_base, rho_1_arrays):\n",
        "    triag = tfp.math.fill_triangular(hamil_base, upper=True)\n",
        "    body_gen = triag + tf.transpose(triag, perm=[0,2,1])-tf.linalg.diag(tf.linalg.diag_part(triag)) # Simetrizamos y generamos la matriz de h\n",
        "    hamil_expanded = body_gen[:, :, :, np.newaxis, np.newaxis]\n",
        "    rho_1_gen_transposed = tf.transpose(rho_1_arrays, perm=[1, 0, 2, 3])\n",
        "    h_arr = tf.reduce_sum(hamil_expanded * rho_1_gen_transposed[np.newaxis,:,:,:,:], axis=[1,2])\n",
        "    return h_arr\n",
        "\n",
        "def thermal_state_tf(h):\n",
        "    # Assume beta=1\n",
        "    exp_hamiltonian = tf.linalg.expm(-h)\n",
        "    partition_function = tf.linalg.trace(exp_hamiltonian)\n",
        "    partition_function = tf.expand_dims(partition_function, axis=1)\n",
        "    partition_function = tf.expand_dims(partition_function, axis=1)\n",
        "    \n",
        "    rho = exp_hamiltonian / partition_function\n",
        "\n",
        "    return rho\n",
        "\n",
        "def rho_1_tf(state, rho_1_arrays):\n",
        "    state = tf.expand_dims(state, axis=1)  # Shape: (5120, 10, 1, 10)\n",
        "    state_expanded = tf.expand_dims(state, axis=1)\n",
        "    rho_1_arrays_expanded = tf.expand_dims(rho_1_arrays, axis=0)  # Shape: (1, 5, 5, 10, 10)\n",
        "    product = state_expanded * rho_1_arrays_expanded  # Shape: (5120, 10, 5, 10, 10)\n",
        "    mat = tf.reduce_sum(product, axis=[-2, -1])  # Shape: (5120, 5, 5)\n",
        "    \n",
        "    return mat\n",
        "\n",
        "def rho_2_tf(state, rho_2_arrays):\n",
        "    state = tf.expand_dims(state, axis=1)  # Shape: (5120, 10, 1, 10)\n",
        "    state_expanded = tf.expand_dims(state, axis=1)\n",
        "    rho_2_arrays_expanded = tf.expand_dims(rho_2_arrays, axis=0)  # Shape: (1, 5, 5, 10, 10)\n",
        "    product = state_expanded * rho_2_arrays_expanded  # Shape: (5120, 10, 5, 10, 10)\n",
        "    mat = tf.reduce_sum(product, axis=[-2, -1])  # Shape: (5120, 5, 5)\n",
        "    \n",
        "    return mat\n",
        "\n",
        "# NOTA: para calcular el bloque rho2kkbar, utilizar en lugar\n",
        "\n",
        "def rho_1_gc_tf(hamil_base):\n",
        "    e, v = tf.linalg.eigh(gen_to_h_1b(hamil_base))\n",
        "    result = 1 / (1 + tf.exp(e))\n",
        "    result = tf.linalg.diag(result)\n",
        "    res = tf.linalg.matmul(v,result)\n",
        "    res = tf.linalg.matmul(res,v,adjoint_b=True)\n",
        "    \n",
        "    return tf.cast(res, tf.float32)\n",
        "\n",
        "# Aux function\n",
        "def outer_product(vector):\n",
        "    return tf.einsum('i,j->ij', vector, vector)\n",
        "\n",
        "def pure_state(h):\n",
        "    e, v = tf.linalg.eigh(h)\n",
        "    fund = v[:,:,0]\n",
        "    d = tf.map_fn(outer_product, fund)\n",
        "    return d\n",
        "\n",
        "# Casos de entrenamiento tipo mat gaussianas\n",
        "def gen_gauss_mat(G, sigma_sq, size):\n",
        "    mat = np.zeros((size, size))\n",
        "    for i in range(0, size):\n",
        "        for j in range(0, size):\n",
        "                mat[i,j] = G * np.exp(-(i-j)**2/(2*sigma_sq))\n",
        "    return mat\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylpy_BCw6jxF"
      },
      "source": [
        "### Construccion de dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Version sincrónica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2is_Eo_qGpEz",
        "outputId": "9a968190-59f2-4695-ef18-b99ff5b4a212"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-03 12:50:19.897371: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-03 12:50:19.897653: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-03 12:50:19.897818: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-03 12:50:19.898011: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-03 12:50:19.898172: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-03 12:50:19.898417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /device:GPU:0 with 6365 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/device:GPU:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/12 [00:00<?, ?it/s]2024-01-03 12:50:21.857551: I tensorflow/core/util/cuda_solvers.cc:179] Creating GpuSolver handles for stream 0x1b4e3040\n",
            "100%|██████████| 12/12 [00:29<00:00,  2.43s/it]\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "# Config\n",
        "num_samples = 3000\n",
        "use_gpu = True\n",
        "gpu_batch_size = 256\n",
        "\n",
        "# Beta\n",
        "beta = 1\n",
        "\n",
        "# Construccion de parametros y matrices auxiliares\n",
        "rho1_size = m1_basis.size\n",
        "rho2_size = m2_basis.size\n",
        "rho2kkbar_size = basis.m\n",
        "fund_size = basis.size\n",
        "hamil_base_size = basis.d*(basis.d+1)//2\n",
        "rho_1_arrays = rho_1_gen(basis)\n",
        "rho_1_arrays_tf = tf.constant(rho_1_arrays, dtype=tf.float32)\n",
        "rho_2_arrays = rho_2_gen(basis, nm2_basis, m2_basis)\n",
        "rho_2_arrays_tf = tf.constant(rho_2_arrays, dtype=tf.float32)\n",
        "rho_2_arrays_kkbar = rho_2_kkbar_gen(basis.m, rho_2_arrays)\n",
        "rho_2_arrays_kkbar_tf = tf.constant(rho_2_arrays_kkbar, dtype=tf.float32)\n",
        "k_indices = get_kkbar_indices(t_basis)\n",
        "k_indices_tf = gen_update_indices(t_basis, gpu_batch_size)\n",
        "\n",
        "# Generacion de hamiltoniano\n",
        "# (h0, hi) = two_body_hamiltonian(t_basis.size, basis.m, np.arange(0, basis.m), np.ones((basis.m,basis.m)), rho_1_arrays_tf, rho_2_arrays_tf) esto es para g cte\n",
        "\n",
        "\n",
        "if use_gpu:\n",
        "    print(tf.test.gpu_device_name())\n",
        "    datasets = []\n",
        "    for i in tqdm(range(num_samples//gpu_batch_size+1)):\n",
        "        size = basis.m*(basis.m+1)//2\n",
        "        # En una primera versión vamos a pasar una mat proporcional a range(0,m) para energias\n",
        "        en_batch = [np.arange(0, basis.m) for _ in range(0,gpu_batch_size)] \n",
        "        # Como interacción una matriz G semidefinida positiva\n",
        "        # Primero creamos las semillas, es decir, la diagonal superior de la matrix g\n",
        "        # Caso generico\n",
        "        #label_size = basis.m*(basis.m+1)// 2 # CASO GENERICO elementos independientes de una mat de m x m\n",
        "        #h_labels = [np.random.random()*np.ones(label_size) for _ in range(0,gpu_batch_size)] # TODO: Aumentar la amplitud de la interacción\n",
        "        # Construimos la mat G\n",
        "        #triag = tfp.math.fill_triangular(h_labels, upper=True)\n",
        "        #g_arr = triag + tf.transpose(triag, perm=[0,2,1])-tf.linalg.diag(tf.linalg.diag_part(triag))\n",
        "        # Caso reducido\n",
        "        label_size = 2\n",
        "        h_labels = [[np.random.random(), np.random.random()] for _ in range(0, gpu_batch_size)]\n",
        "        h_labels = tf.constant(h_labels, dtype=tf.float32)\n",
        "        g_arr = [gen_gauss_mat(*l, basis.m) for l in h_labels]\n",
        "        g_arr = tf.constant(g_arr, dtype=tf.float32)\n",
        "        # Construimos los hamiltonianos basados en g_arr\n",
        "        h_arr = two_body_hamiltonian_tf(t_basis, basis.m, en_batch, g_arr.numpy(), rho_1_arrays, rho_2_arrays, k_indices_tf)\n",
        "        # Estados térmicos\n",
        "        #state = thermal_state_tf(h_arr*beta) \n",
        "        #state = tf.cast(state, dtype=tf.float32)\n",
        "        # Estados puros\n",
        "        state = pure_state(h_arr)\n",
        "        #rho_2_input = rho_2_tf(state, rho_2_arrays_tf)\n",
        "        rho_2_input = rho_2_tf(state, rho_2_arrays_kkbar_tf)\n",
        "\n",
        "        datasets.append(tf.data.Dataset.from_tensor_slices(((rho_2_input), h_labels)))\n",
        "        #datasets.append(tf.data.Dataset.from_tensor_slices(((rho_1_input, rho_2_input), h_labels)))\n",
        "        #datasets.append(tf.data.Dataset.from_tensor_slices(((rho_1_input, rho_2_input, state), h_labels)))\n",
        "    ds = tf.data.Dataset.from_tensor_slices(datasets)\n",
        "    dataset = ds.interleave(\n",
        "        lambda x: x,\n",
        "        cycle_length=1,\n",
        "        num_parallel_calls=tf.data.AUTOTUNE,\n",
        "    )\n",
        "\n",
        "\n",
        "#batch_size = 32\n",
        "#dataset = dataset.shuffle(buffer_size=num_samples).batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[9.9996418e-01 3.5462839e-05 3.5462766e-05 3.5462795e-05]\n",
            "[1.3978813 2.703863  2.703863  2.703863  2.703863  3.3984344 3.7038124\n",
            " 3.7038124 3.7038124 3.7038124 3.7038124 3.7038124 3.7038124 3.7038124\n",
            " 4.694301  4.694301  4.694301  4.694301  4.694301  4.694301  4.694301\n",
            " 4.694301  4.694301  4.694301  4.694301  4.694301  5.407458  5.407726\n",
            " 6.        6.        6.        6.        6.        6.        6.\n",
            " 6.        6.        6.        6.        6.        6.        6.\n",
            " 6.        6.        6.713425  6.713425  6.713425  6.713425  6.713425\n",
            " 6.713425  6.713425  6.713425  6.713425  6.713425  6.713425  6.713425\n",
            " 7.4175572 7.7039137 7.7039137 7.7039137 7.7039137 7.7039137 7.7039137\n",
            " 7.7039137 7.7039137 8.703863  8.703863  8.703863  8.703863  9.417299 ]\n",
            "0.005955061\n",
            "[0, 5, 14, 27]\n"
          ]
        }
      ],
      "source": [
        "print(np.linalg.eigvals(rho_2_input[0]))\n",
        "p_state = np.linalg.eigh(h_arr[0])[1][:,0]\n",
        "print(np.linalg.eigh(h_arr[0])[0])\n",
        "print(p_state[14])\n",
        "print(k_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Filleo de dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "# Save and load dataset\n",
        "save_dataset = False\n",
        "load_dataset = False\n",
        "path = \"/home/agus/TF\"\n",
        "#num_samples = 5000000\n",
        "if save_dataset:\n",
        "    tf.data.Dataset.save(dataset, path)\n",
        "    with open(\"/home/agus/\"+'/file.pkl', 'wb') as file:\n",
        "        pickle.dump(beta_input, file)\n",
        "if load_dataset:\n",
        "    dataset = tf.data.Dataset.load(path)\n",
        "    with open(\"/home/agus/\"+'file.pkl', 'rb') as file:\n",
        "        beta_input = pickle.load(file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "8moZIlfabZuy"
      },
      "outputs": [],
      "source": [
        "# Dividimos los datasets\n",
        "train_size = int(0.8 * num_samples)\n",
        "\n",
        "train_dataset = dataset.take(train_size)\n",
        "val_dataset = dataset.skip(train_size)\n",
        "\n",
        "\n",
        "batch_size = 256\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "val_dataset = val_dataset.batch(batch_size)\n",
        "\n",
        "#beta_val = beta_input[train_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Dataset Size: -2\n"
          ]
        }
      ],
      "source": [
        "# Cardinality no funciona con los datasets generados por GPU\n",
        "val_size = tf.data.experimental.cardinality(val_dataset).numpy()\n",
        "print(\"Validation Dataset Size:\", val_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYEEjNB-7b8y"
      },
      "source": [
        "### Definición de modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8kkhJr5K0ZQ",
        "outputId": "f1b731f1-6a02-4181-f0b5-5677a2a85784"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rho2 (InputLayer)           [(None, 4, 4, 1)]         0         \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 3, 3, 16)          80        \n",
            "                                                                 \n",
            " batch_normalization_7 (Bat  (None, 3, 3, 16)          64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 144)               0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 32)                4640      \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 10)                330       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5114 (19.98 KB)\n",
            "Trainable params: 5082 (19.85 KB)\n",
            "Non-trainable params: 32 (128.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Definicion de layers basado en Conv 2D\n",
        "\n",
        "# Factor de cantidad de filtros\n",
        "lf = 16 \n",
        "conv_limit = (rho2kkbar_size - 4)\n",
        "initial_dense = (lf*2**(conv_limit-1)*((rho2kkbar_size-(conv_limit-1))//2)**2)\n",
        "## rho 1\n",
        "rho2_layer =  tf.keras.layers.Input(shape=(rho2kkbar_size,rho2kkbar_size, 1), name='rho2')\n",
        "\n",
        "# Procesamos el primer input\n",
        "conv_rho2 = tf.keras.layers.Conv2D(lf*2**conv_limit, (2, 2), activation='relu')(rho2_layer)\n",
        "conv_rho2 = tf.keras.layers.BatchNormalization()(conv_rho2)\n",
        "for j in [(2**conv_limit - 2**k) for k in range(1,conv_limit)]:\n",
        "    conv_rho2 = tf.keras.layers.Conv2D(lf*j, (2, 2), activation='relu')(conv_rho2 if 2**j != 1 else rho1_layer)\n",
        "    conv_rho2 = tf.keras.layers.BatchNormalization()(conv_rho2)\n",
        "\n",
        "#conv_rho2 = tf.keras.layers.MaxPooling2D((2, 2))(conv_rho2)\n",
        "\n",
        "flatten_rho2 = tf.keras.layers.Flatten()(conv_rho2)\n",
        "#flatten_rho1 = tf.keras.layers.Dense(rho1_size*rho1_size, activation='relu')(flatten_rho1)\n",
        "\n",
        "#local_size = basis.size*basis.size\n",
        "local_size = label_size\n",
        "\n",
        "#dense1 = tf.keras.layers.Dense(8*8*4*4, activation='relu')(dense1)\n",
        "#dense1 = tf.keras.layers.Dense(512, activation='relu')(flatten_rho1)\n",
        "#dense1 = tf.keras.layers.Dense(128, activation='relu')(flatten_rho1)\n",
        "dense1 = tf.keras.layers.Dense(initial_dense, activation='relu')(flatten_rho2)\n",
        "#dense1 = tf.keras.layers.Dense(initial_dense//2, activation='relu')(dense1)\n",
        "\n",
        "output = tf.keras.layers.Dense(local_size)(dense1)\n",
        "\n",
        "\n",
        "# Creamos el modelo y compulamos\n",
        "#model = tf.keras.models.Model(inputs=[rho1_layer, rho2_layer, fund_layer], outputs=output)\n",
        "#model = tf.keras.models.Model(inputs=[rho1_layer, rho2_layer], outputs=output)\n",
        "model = tf.keras.models.Model(inputs=[rho2_layer], outputs=output)\n",
        "\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZBtonvGbZuz",
        "outputId": "f197277e-a84b-4ffd-c81f-c81581707fb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rho2 (InputLayer)           [(None, 4, 4, 1)]         0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 16)                0         \n",
            "                                                                 \n",
            " concatenate_1 (Concatenate  (None, 16)                0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 32)                544       \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 64)                2112      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2)                 130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2786 (10.88 KB)\n",
            "Trainable params: 2786 (10.88 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Modelo denso + fundamental\n",
        "rho2_layer =  tf.keras.layers.Input(shape=(basis.m,basis.m, 1), name='rho2')\n",
        "flatten_rho2 = tf.keras.layers.Flatten()(rho2_layer)\n",
        "#rho2_layer =  tf.keras.layers.Input(shape=(rho2_size,rho2_size, 1), name='rho2')\n",
        "#flatten_rho2 = tf.keras.layers.Flatten()(rho2_layer)\n",
        "#fund_layer =  tf.keras.layers.Input(shape=(fund_size, fund_size, 1 ), name='fund')\n",
        "#flatten_fund = tf.keras.layers.Flatten()(fund_layer)\n",
        "\n",
        "dense1 = tf.keras.layers.concatenate([flatten_rho2])\n",
        "#dense1 = tf.keras.layers.concatenate([dense1, flatten_fund])\n",
        "#dense1 = tf.keras.layers.Dense(rho1_size*rho1_size, activation='relu')(dense1)\n",
        "\n",
        "local_size = label_size\n",
        "l=3\n",
        "layer_s = [32//i*2 for i in reversed(range(1,l))]\n",
        "for i in range(0,l-1):\n",
        "    dense1 = tf.keras.layers.Dense(layer_s[i], activation='relu')(dense1)\n",
        "\n",
        "output = tf.keras.layers.Dense(local_size)(dense1)\n",
        "# Creamos el modelo y compulamos\n",
        "model = tf.keras.models.Model(inputs=[rho2_layer], outputs=output)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RgoMlCyyfBe-"
      },
      "outputs": [],
      "source": [
        "# LOSS FUNCTIONS\n",
        "r_size = basis.size\n",
        "\n",
        "# Custom loss function based on GS MSE\n",
        "def gs_loss(h_pred, h_true):\n",
        "    h_pred = tf.reshape(h_pred, shape=(-1,r_size,r_size))\n",
        "    e, v = tf.linalg.eigh(h_pred)\n",
        "    gs_pred = v[:, 0]\n",
        "\n",
        "    h_true = tf.reshape(h_true, shape=(-1,r_size,r_size))\n",
        "    e, v = tf.linalg.eigh(h_true)\n",
        "    gs_true = v[:, 0]\n",
        "\n",
        "    gs_diff = tf.norm(gs_true - gs_pred)\n",
        "\n",
        "    return gs_diff + tf.reduce_mean(tf.square(h_true - h_pred)) * 100\n",
        "\n",
        "def distance_to_hermitian(matrix):\n",
        "    hermitian_part = 0.5 * (matrix + tf.linalg.adjoint(matrix))\n",
        "    distance = tf.norm(matrix - hermitian_part, ord='euclidean')\n",
        "    return distance\n",
        "\n",
        "# Custom loss function based on MSE + non-hermitian penalization\n",
        "def herm_loss(h_pred, h_true):\n",
        "    h_pred_arr = tf.reshape(h_pred, shape=(-1,r_size,r_size))\n",
        "    return tf.reduce_mean(tf.square(h_true - h_pred)) + distance_to_hermitian(h_pred_arr)\n",
        "\n",
        "# Custom loss function based on h eigenvalues\n",
        "def eig_loss(h_pred, h_true):\n",
        "    eig_true = tf.sort(tf.math.real(tf.linalg.eigvals(tf.reshape(h_true, (-1, fund_size, fund_size)))))\n",
        "    eig_pred = tf.sort(tf.math.real(tf.linalg.eigvals(tf.reshape(h_pred, (-1, fund_size, fund_size)))))\n",
        "    return tf.reduce_mean(tf.square(eig_true - eig_pred))\n",
        "\n",
        "# MSE with a factor\n",
        "def mse_f(h_pred, h_true):\n",
        "    f = 1000\n",
        "    return tf.reduce_mean(tf.square(h_true - h_pred))*f\n",
        "\n",
        "# Spectral radius loss\n",
        "def spectral_loss(h_pred, h_true):\n",
        "    eig = tf.math.real(tf.linalg.eigvals(tf.reshape(h_true-h_pred, (-1, fund_size, fund_size))))\n",
        "    return tf.math.reduce_max(tf.abs(eig))\n",
        "\n",
        "# Hamiltonian MSE loss (using generators)\n",
        "def base_mse_loss(base_pred, base_true):\n",
        "    h_pred = gen_to_h_tf(base_pred, rho_1_arrays_tf)\n",
        "    h_true = gen_to_h_tf(base_true, rho_1_arrays_tf)\n",
        "    mat = tf.reshape(h_pred-h_true, (-1, fund_size, fund_size))\n",
        "    return tf.norm(mat, ord='fro', axis=[-1, -2])\n",
        "\n",
        "# Custom loss function based on h eigenvalues (using generators)\n",
        "def base_eig_loss(base_pred, base_true):\n",
        "    h_pred = gen_to_h_tf(base_pred, rho_1_arrays_tf)\n",
        "    h_true = gen_to_h_tf(base_true, rho_1_arrays_tf)\n",
        "    eig_true = tf.sort(tf.math.real(tf.linalg.eigvals(tf.reshape(h_true, (-1, fund_size, fund_size)))))\n",
        "    eig_pred = tf.sort(tf.math.real(tf.linalg.eigvals(tf.reshape(h_pred, (-1, fund_size, fund_size)))))\n",
        "    return tf.reduce_mean(tf.square(eig_true - eig_pred))\n",
        "\n",
        "# Custom loss function based on rho1 eigenvals\n",
        "## Auxiliary function\n",
        "def base_to_rho_1_tf(base_pred):\n",
        "    h = gen_to_h_tf(base_pred, rho_1_arrays_tf)\n",
        "    h = tf.reshape(h, (-1, fund_size, fund_size))\n",
        "    state = thermal_state_tf(h)\n",
        "    rho1 = rho_1_tf(state, rho_1_arrays_tf)\n",
        "    return rho1\n",
        "    \n",
        "def rho1_loss(base_pred, base_true):\n",
        "    mat = base_to_rho_1_tf(base_pred) - base_to_rho_1_tf(base_true)\n",
        "    return tf.norm(mat, ord='fro', axis=[-1, -2])\n",
        "\n",
        "# Custom loss function based on rho1 eigenvals (using generators)\n",
        "def base_rho1_loss(base_pred, base_true):\n",
        "    h_pred = gen_to_h_tf(base_pred, rho_1_arrays_tf)\n",
        "    h_true = gen_to_h_tf(base_true, rho_1_arrays_tf)\n",
        "    return tf.reduce_mean(tf.square(rho_1_eig_tf(h_pred) - rho_1_eig_tf(h_true)))*1000\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiWk9piJtNIZ"
      },
      "source": [
        "## Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhJCHf0fQdRl",
        "outputId": "1821cf27-9ff5-4d67-e9f5-956d20eda5e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-02 16:16:28.738149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /device:GPU:0 with 20609 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     84/Unknown - 1s 2ms/step - loss: 0.7543 - accuracy: 0.5102 - mean_squared_error: 0.7543"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-02 16:16:30.030827: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2907355234671359745\n",
            "2024-01-02 16:16:30.030868: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 13306386533350685134\n",
            "2024-01-02 16:16:30.030874: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 16762392026046786804\n",
            "2024-01-02 16:16:30.030894: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2706308901136465856\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "94/94 [==============================] - 2s 8ms/step - loss: 0.6846 - accuracy: 0.5130 - mean_squared_error: 0.6846 - val_loss: 0.0772 - val_accuracy: 0.5417 - val_mean_squared_error: 0.0772\n",
            "Epoch 2/50\n",
            "60/94 [==================>...........] - ETA: 0s - loss: 0.0618 - accuracy: 0.5536 - mean_squared_error: 0.0618"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-02 16:16:30.567903: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2907355234671359745\n",
            "2024-01-02 16:16:30.567946: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 659348077575952846\n",
            "2024-01-02 16:16:30.567964: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2706308901136465856\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0581 - accuracy: 0.5578 - mean_squared_error: 0.0581 - val_loss: 0.0498 - val_accuracy: 0.5804 - val_mean_squared_error: 0.0498\n",
            "Epoch 3/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0496 - accuracy: 0.5730 - mean_squared_error: 0.0496 - val_loss: 0.0479 - val_accuracy: 0.5812 - val_mean_squared_error: 0.0479\n",
            "Epoch 4/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0485 - accuracy: 0.5747 - mean_squared_error: 0.0485 - val_loss: 0.0472 - val_accuracy: 0.5805 - val_mean_squared_error: 0.0472\n",
            "Epoch 5/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0479 - accuracy: 0.5745 - mean_squared_error: 0.0479 - val_loss: 0.0468 - val_accuracy: 0.5833 - val_mean_squared_error: 0.0468\n",
            "Epoch 6/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0475 - accuracy: 0.5754 - mean_squared_error: 0.0475 - val_loss: 0.0465 - val_accuracy: 0.5852 - val_mean_squared_error: 0.0465\n",
            "Epoch 7/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0472 - accuracy: 0.5764 - mean_squared_error: 0.0472 - val_loss: 0.0464 - val_accuracy: 0.5863 - val_mean_squared_error: 0.0464\n",
            "Epoch 8/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0471 - accuracy: 0.5765 - mean_squared_error: 0.0471 - val_loss: 0.0463 - val_accuracy: 0.5871 - val_mean_squared_error: 0.0463\n",
            "Epoch 9/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0470 - accuracy: 0.5780 - mean_squared_error: 0.0470 - val_loss: 0.0462 - val_accuracy: 0.5870 - val_mean_squared_error: 0.0462\n",
            "Epoch 10/50\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.0469 - accuracy: 0.5781 - mean_squared_error: 0.0469 - val_loss: 0.0462 - val_accuracy: 0.5867 - val_mean_squared_error: 0.0462\n",
            "Epoch 11/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0469 - accuracy: 0.5780 - mean_squared_error: 0.0469 - val_loss: 0.0461 - val_accuracy: 0.5878 - val_mean_squared_error: 0.0461\n",
            "Epoch 12/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0469 - accuracy: 0.5774 - mean_squared_error: 0.0469 - val_loss: 0.0461 - val_accuracy: 0.5876 - val_mean_squared_error: 0.0461\n",
            "Epoch 13/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0468 - accuracy: 0.5775 - mean_squared_error: 0.0468 - val_loss: 0.0461 - val_accuracy: 0.5886 - val_mean_squared_error: 0.0461\n",
            "Epoch 14/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0468 - accuracy: 0.5773 - mean_squared_error: 0.0468 - val_loss: 0.0461 - val_accuracy: 0.5886 - val_mean_squared_error: 0.0461\n",
            "Epoch 15/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0468 - accuracy: 0.5772 - mean_squared_error: 0.0468 - val_loss: 0.0461 - val_accuracy: 0.5888 - val_mean_squared_error: 0.0461\n",
            "Epoch 16/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0468 - accuracy: 0.5772 - mean_squared_error: 0.0468 - val_loss: 0.0460 - val_accuracy: 0.5889 - val_mean_squared_error: 0.0460\n",
            "Epoch 17/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0468 - accuracy: 0.5778 - mean_squared_error: 0.0468 - val_loss: 0.0460 - val_accuracy: 0.5886 - val_mean_squared_error: 0.0460\n",
            "Epoch 18/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0468 - accuracy: 0.5770 - mean_squared_error: 0.0468 - val_loss: 0.0460 - val_accuracy: 0.5878 - val_mean_squared_error: 0.0460\n",
            "Epoch 19/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0468 - accuracy: 0.5773 - mean_squared_error: 0.0468 - val_loss: 0.0459 - val_accuracy: 0.5889 - val_mean_squared_error: 0.0459\n",
            "Epoch 20/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0467 - accuracy: 0.5773 - mean_squared_error: 0.0467 - val_loss: 0.0459 - val_accuracy: 0.5891 - val_mean_squared_error: 0.0459\n",
            "Epoch 21/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0467 - accuracy: 0.5774 - mean_squared_error: 0.0467 - val_loss: 0.0459 - val_accuracy: 0.5899 - val_mean_squared_error: 0.0459\n",
            "Epoch 22/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0467 - accuracy: 0.5768 - mean_squared_error: 0.0467 - val_loss: 0.0458 - val_accuracy: 0.5889 - val_mean_squared_error: 0.0458\n",
            "Epoch 23/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0467 - accuracy: 0.5763 - mean_squared_error: 0.0467 - val_loss: 0.0458 - val_accuracy: 0.5880 - val_mean_squared_error: 0.0458\n",
            "Epoch 24/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0467 - accuracy: 0.5765 - mean_squared_error: 0.0467 - val_loss: 0.0458 - val_accuracy: 0.5862 - val_mean_squared_error: 0.0458\n",
            "Epoch 25/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0467 - accuracy: 0.5758 - mean_squared_error: 0.0467 - val_loss: 0.0458 - val_accuracy: 0.5860 - val_mean_squared_error: 0.0458\n",
            "Epoch 26/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0467 - accuracy: 0.5764 - mean_squared_error: 0.0467 - val_loss: 0.0457 - val_accuracy: 0.5849 - val_mean_squared_error: 0.0457\n",
            "Epoch 27/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0466 - accuracy: 0.5758 - mean_squared_error: 0.0466 - val_loss: 0.0457 - val_accuracy: 0.5826 - val_mean_squared_error: 0.0457\n",
            "Epoch 28/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0466 - accuracy: 0.5767 - mean_squared_error: 0.0466 - val_loss: 0.0457 - val_accuracy: 0.5846 - val_mean_squared_error: 0.0457\n",
            "Epoch 29/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0466 - accuracy: 0.5772 - mean_squared_error: 0.0466 - val_loss: 0.0457 - val_accuracy: 0.5847 - val_mean_squared_error: 0.0457\n",
            "Epoch 30/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0466 - accuracy: 0.5773 - mean_squared_error: 0.0466 - val_loss: 0.0457 - val_accuracy: 0.5838 - val_mean_squared_error: 0.0457\n",
            "Epoch 31/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0466 - accuracy: 0.5775 - mean_squared_error: 0.0466 - val_loss: 0.0456 - val_accuracy: 0.5833 - val_mean_squared_error: 0.0456\n",
            "Epoch 32/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0465 - accuracy: 0.5780 - mean_squared_error: 0.0465 - val_loss: 0.0456 - val_accuracy: 0.5825 - val_mean_squared_error: 0.0456\n",
            "Epoch 33/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0465 - accuracy: 0.5779 - mean_squared_error: 0.0465 - val_loss: 0.0456 - val_accuracy: 0.5820 - val_mean_squared_error: 0.0456\n",
            "Epoch 34/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0465 - accuracy: 0.5777 - mean_squared_error: 0.0465 - val_loss: 0.0456 - val_accuracy: 0.5826 - val_mean_squared_error: 0.0456\n",
            "Epoch 35/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0465 - accuracy: 0.5781 - mean_squared_error: 0.0465 - val_loss: 0.0456 - val_accuracy: 0.5810 - val_mean_squared_error: 0.0456\n",
            "Epoch 36/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0464 - accuracy: 0.5781 - mean_squared_error: 0.0464 - val_loss: 0.0456 - val_accuracy: 0.5813 - val_mean_squared_error: 0.0456\n",
            "Epoch 37/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0464 - accuracy: 0.5787 - mean_squared_error: 0.0464 - val_loss: 0.0456 - val_accuracy: 0.5810 - val_mean_squared_error: 0.0456\n",
            "Epoch 38/50\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.0464 - accuracy: 0.5789 - mean_squared_error: 0.0464 - val_loss: 0.0455 - val_accuracy: 0.5810 - val_mean_squared_error: 0.0455\n",
            "Epoch 39/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0464 - accuracy: 0.5789 - mean_squared_error: 0.0464 - val_loss: 0.0455 - val_accuracy: 0.5810 - val_mean_squared_error: 0.0455\n",
            "Epoch 40/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0463 - accuracy: 0.5789 - mean_squared_error: 0.0463 - val_loss: 0.0455 - val_accuracy: 0.5820 - val_mean_squared_error: 0.0455\n",
            "Epoch 41/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0463 - accuracy: 0.5791 - mean_squared_error: 0.0463 - val_loss: 0.0455 - val_accuracy: 0.5815 - val_mean_squared_error: 0.0455\n",
            "Epoch 42/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0463 - accuracy: 0.5790 - mean_squared_error: 0.0463 - val_loss: 0.0454 - val_accuracy: 0.5818 - val_mean_squared_error: 0.0454\n",
            "Epoch 43/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0463 - accuracy: 0.5792 - mean_squared_error: 0.0463 - val_loss: 0.0454 - val_accuracy: 0.5820 - val_mean_squared_error: 0.0454\n",
            "Epoch 44/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0462 - accuracy: 0.5795 - mean_squared_error: 0.0462 - val_loss: 0.0454 - val_accuracy: 0.5820 - val_mean_squared_error: 0.0454\n",
            "Epoch 45/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0462 - accuracy: 0.5791 - mean_squared_error: 0.0462 - val_loss: 0.0454 - val_accuracy: 0.5815 - val_mean_squared_error: 0.0454\n",
            "Epoch 46/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0462 - accuracy: 0.5787 - mean_squared_error: 0.0462 - val_loss: 0.0454 - val_accuracy: 0.5810 - val_mean_squared_error: 0.0454\n",
            "Epoch 47/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0462 - accuracy: 0.5790 - mean_squared_error: 0.0462 - val_loss: 0.0454 - val_accuracy: 0.5804 - val_mean_squared_error: 0.0454\n",
            "Epoch 48/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0461 - accuracy: 0.5794 - mean_squared_error: 0.0461 - val_loss: 0.0454 - val_accuracy: 0.5809 - val_mean_squared_error: 0.0454\n",
            "Epoch 49/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0461 - accuracy: 0.5791 - mean_squared_error: 0.0461 - val_loss: 0.0454 - val_accuracy: 0.5804 - val_mean_squared_error: 0.0454\n",
            "Epoch 50/50\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0461 - accuracy: 0.5790 - mean_squared_error: 0.0461 - val_loss: 0.0453 - val_accuracy: 0.5804 - val_mean_squared_error: 0.0453\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.optimizers import RMSprop, Adam, Nadam, Lion\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(),\n",
        "              loss='MSE',  \n",
        "              metrics=['accuracy', 'mean_squared_error'])\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 50\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "    history = model.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset)\n",
        "\n",
        "# Dense: 1.3\n",
        "# CNN: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "cvpE_X1iTXcB",
        "outputId": "eff0e5f5-5b26-46ea-ec6b-491d1de9944c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "#plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.plot(history.history['mean_squared_error'], label='Training MSE')\n",
        "plt.plot(history.history['val_mean_squared_error'], label='Validation MSE')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE')\n",
        "plt.title('MSE vs. Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# MIN LOSS = 0.0128 c/fund 50epochs MSE\n",
        "##         = 0.0118 s/fund 50epochs MSE\n",
        "##         = 0.0039 s/fund 50epochs MSE m=4 d=6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRlZuRUNa6Yb",
        "outputId": "85850559-311b-4cf4-ea5b-465a9ee8a7af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8/8 [==============================] - 0s 1ms/step\n"
          ]
        }
      ],
      "source": [
        "# Assuming you have a validation dataset (val_dataset)\n",
        "iterador = iter(val_dataset)\n",
        "sample = next(iterador)\n",
        "next_sample = next(iterador)\n",
        "input_data = sample[0]  # Assuming your dataset provides input data as the first element\n",
        "actual_values = sample[1]  # Assuming your dataset provides actual labels as the second element\n",
        "\n",
        "# Predict using the model\n",
        "predictions = model.predict(input_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([0.07476098 0.7980103 ], shape=(2,), dtype=float32)\n",
            "[0.39398882 0.33004683]\n",
            "tf.Tensor([0.9964156 0.8925601], shape=(2,), dtype=float32)\n",
            "[0.9809724  0.92489624]\n",
            "tf.Tensor([0.4073432  0.42250717], shape=(2,), dtype=float32)\n",
            "[0.40022734 0.39078894]\n",
            "tf.Tensor([0.08898959 0.07408614], shape=(2,), dtype=float32)\n",
            "[0.38596258 0.25198147]\n",
            "0.21560432 0.408812\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Vemos algunos valores\n",
        "for e in val_dataset:\n",
        "    for i in range(0, 4):\n",
        "        print(e[1][i])\n",
        "        print(predictions[i])\n",
        "    break\n",
        "    \n",
        "\n",
        "RMSE_pred = mean_squared_error(actual_values, predictions, squared=False)\n",
        "RMSE_rand = mean_squared_error(actual_values, next_sample[1], squared=False)\n",
        "print(RMSE_pred, RMSE_rand)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6.666666666666667"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "0.4/0.06"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "ds5iD1OMbZu3"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "cannot reshape array of size 2 into shape (1,1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb Cell 33\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb#Y146sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m val_dataset:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb#Y146sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m \u001b[39mif\u001b[39;00m printear \u001b[39melse\u001b[39;00m batch_size):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb#Y146sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         \u001b[39m# Valores actuales\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb#Y146sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39m#h = e[1][i].numpy().reshape(basis.size,basis.size)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb#Y146sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         h_true \u001b[39m=\u001b[39m gen_to_h(e[\u001b[39m1\u001b[39;49m][i], rho_1_arrays)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb#Y146sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         \u001b[39m#print(h) if printear else 0\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb#Y146sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         r \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39meigvals(e[\u001b[39m0\u001b[39m][i]))\n",
            "\u001b[1;32m/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb Cell 33\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb#Y146sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgen_to_h\u001b[39m(base, rho_1_arrays):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb#Y146sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     triag \u001b[39m=\u001b[39m fill_triangular_np(base)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb#Y146sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     body_gen \u001b[39m=\u001b[39m triag \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39mtranspose(triag)\u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39mdiag(np\u001b[39m.\u001b[39mdiag(triag))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb#Y146sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     h \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(base_hamiltonian(body_gen, basis, rho_1_arrays))  \n",
            "\u001b[1;32m/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb Cell 33\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb#Y146sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m n \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mint32(np\u001b[39m.\u001b[39msqrt(\u001b[39m.25\u001b[39m \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m m) \u001b[39m-\u001b[39m \u001b[39m.5\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb#Y146sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m x_tail \u001b[39m=\u001b[39m x[(m \u001b[39m-\u001b[39m (n\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m-\u001b[39m m)):]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb#Y146sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mtriu(np\u001b[39m.\u001b[39;49mconcatenate([x, x_tail[::\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]], \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mreshape(n, n))\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 2 into shape (1,1)"
          ]
        }
      ],
      "source": [
        "m_size = basis.size\n",
        "rho_1_pred = []\n",
        "rho_1_actual = []\n",
        "norm = []\n",
        "norm_rand = []\n",
        "printear =  False\n",
        "\n",
        "for e in val_dataset:\n",
        "    for i in range(0, 3 if printear else batch_size):\n",
        "        # Valores actuales\n",
        "        #h = e[1][i].numpy().reshape(basis.size,basis.size)\n",
        "        h_true = gen_to_h(e[1][i], rho_1_arrays)\n",
        "        #print(h) if printear else 0\n",
        "        r = max(np.linalg.eigvals(e[0][i]))\n",
        "        rho_1_actual.append(r)\n",
        "\n",
        "        print(h_true) if printear else 0\n",
        "        print(r) if printear else 0\n",
        "\n",
        "        # Valores predichos\n",
        "        #h = predictions[i].reshape(basis.size,basis.size)\n",
        "        h_pred = gen_to_h(predictions[i], rho_1_arrays)\n",
        "        beta = 1\n",
        "        # Estado térmico\n",
        "        state = thermal_state(h_pred, beta)\n",
        "        # Estado puro\n",
        "        #state = pure_state(h_pred)\n",
        "        rho1 = np.array(rho_1(basis.d, state, rho_1_arrays))\n",
        "        r = max(np.sort(linalg_d.eigvals(rho1).real))\n",
        "        rho_1_pred.append(r)\n",
        "\n",
        "        print(h_pred) if printear else 0\n",
        "        print(r) if printear else 0\n",
        "        \n",
        "\n",
        "        # Normas\n",
        "        norm.append(np.linalg.norm(h_true-h_pred, ord='fro'))\n",
        "        print(f'Norma {norm[-1]}') if printear else 0\n",
        "        ## Vamos a comparar con un h aleatorio\n",
        "        size = basis.d*(basis.d+1)//2\n",
        "        base = np.random.uniform(low=0, high=1.0, size=(size,))\n",
        "        h_rand = gen_to_h(base, rho_1_arrays)\n",
        "        norm_rand.append(np.linalg.norm(h_true-h_rand, ord='fro'))\n",
        "        #print(f'Norma random {norm_rand[-1]}') if printear else 0\n",
        "        print('') if printear else 0\n",
        "        \n",
        "\n",
        "\n",
        "    # e contiene todo el batch y nos basta con uno\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(e[1][10])\n",
        "predictions[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "AL2EC9Ci-0HG",
        "outputId": "545ebe57-d3de-490f-f076-709d5c47b5f2"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "f=1\n",
        "rho_1_actual = np.array(rho_1_actual)\n",
        "rho_1_pred = np.array(rho_1_pred)\n",
        "#print(mean_squared_error(rho_1_pred, rho_1_actual))\n",
        "\n",
        "print('Rho1 based statistics')\n",
        "print(np.mean(np.abs(rho_1_actual-rho_1_pred)))\n",
        "print(np.mean(rho_1_actual)*f)\n",
        "print('std')\n",
        "print(np.std(rho_1_actual-rho_1_pred)*f)\n",
        "print(np.std(rho_1_actual)*f)\n",
        "print(np.std(rho_1_pred)*f)\n",
        "plt.hist(np.array(rho_1_pred-rho_1_actual), bins=50)\n",
        "plt.show()\n",
        "print('H based statistics')\n",
        "print(np.mean(norm), np.mean(norm_rand))\n",
        "print(np.mean(norm_rand)/np.mean(norm))\n",
        "\n",
        "\n",
        "# BEST: FACTOR 1/8 c/fund\n",
        "## 500 epochs, 10M dataset\n",
        "# BEST: FACTOR 1/9 s/fund\n",
        "## 50 epochs, 5M dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "6.25/1.30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 25 epochs d = m*2\n",
        "res = {}\n",
        "res[5] = 35/8.19 \n",
        "res[4] = 15/2.47\n",
        "res[3] = 6.2/1.73"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YioVllOX3M1N",
        "outputId": "b7715c37-1400-4c04-8be3-dd247b4b9db9"
      },
      "outputs": [],
      "source": [
        "# Get the weights of all dense layers in the model\n",
        "dense_weights = []\n",
        "for layer in model.layers:\n",
        "    if isinstance(layer, tf.keras.layers.Dense):\n",
        "        weights = layer.get_weights()\n",
        "        if len(weights) > 0:\n",
        "            dense_weights.append(weights[0])\n",
        "\n",
        "# Visualize the weights of each dense layer\n",
        "for i, weights in enumerate(dense_weights):\n",
        "    plt.figure()\n",
        "    plt.imshow(weights, cmap='viridis', aspect='auto')\n",
        "    plt.colorbar()\n",
        "    plt.title(f\"Dense Layer {i+1} Weights Visualization\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rho_2_arrays[9,0,0,9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 0 0 0 1 1] [0 1 1 0 0 0]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Genera las matrices de rho2\n",
        "def rho_2_gen(basis, mll_basis, t_basis):\n",
        "    size = t_basis.size\n",
        "    s = basis.size\n",
        "    # La entrada i, j contiene C_j^\\dag C_i    i, j \\in t_basis\n",
        "    mat = np.empty((size, size, s, s), dtype=np.float32)\n",
        "    for i, v in enumerate(t_basis.base):\n",
        "        for j, w in enumerate(t_basis.base):\n",
        "            c_i = b_gen(basis, mll_basis, rep_to_exi(v))\n",
        "            cdag_j = bd_gen(mll_basis, basis, rep_to_exi(w))\n",
        "            mat[i, j, :, :] = np.dot(cdag_j, c_i)\n",
        "            if mat[i,j,0,9] != 0:\n",
        "                print(v,w)\n",
        "\n",
        "    return mat\n",
        "\n",
        "r = rho_2_gen(basis, basis_m2, t_basis)\n",
        "r[9,0,0,9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "print(b_gen(basis, mll_basis, rep_to_exi([0,0,0,1,1,0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 1, 1, 0, 0, 0],\n",
              "       [1, 1, 0, 1, 0, 0],\n",
              "       [1, 1, 0, 0, 1, 0],\n",
              "       [1, 1, 0, 0, 0, 1],\n",
              "       [1, 0, 1, 1, 0, 0],\n",
              "       [1, 0, 1, 0, 1, 0],\n",
              "       [1, 0, 1, 0, 0, 1],\n",
              "       [1, 0, 0, 1, 1, 0],\n",
              "       [1, 0, 0, 1, 0, 1],\n",
              "       [1, 0, 0, 0, 1, 1],\n",
              "       [0, 1, 1, 1, 0, 0],\n",
              "       [0, 1, 1, 0, 1, 0],\n",
              "       [0, 1, 1, 0, 0, 1],\n",
              "       [0, 1, 0, 1, 1, 0],\n",
              "       [0, 1, 0, 1, 0, 1],\n",
              "       [0, 1, 0, 0, 1, 1],\n",
              "       [0, 0, 1, 1, 1, 0],\n",
              "       [0, 0, 1, 1, 0, 1],\n",
              "       [0, 0, 1, 0, 1, 1],\n",
              "       [0, 0, 0, 1, 1, 1]])"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "basis.base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(6, 20)\n",
            "[array([0, 1, 0, 1, 1, 0])] [0 1 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "col = 1\n",
        "b = b_gen(basis, mll_basis, rep_to_exi([0,0,0,1,1,0]))\n",
        "print(b.shape)\n",
        "for x in range(0,b.shape[1]):\n",
        "    if b[col,x] != 0:\n",
        "        ind = x\n",
        "        break\n",
        "else:\n",
        "    ind = NaN\n",
        "\n",
        "print([basis.base[ind]], mll_basis.base[col])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "m = 3\n",
        "d = 2*m\n",
        "basis = fixed_basis(m, d)\n",
        "t_basis = fixed_basis(2, d)\n",
        "basis_m1 = fixed_basis(m-1, d)\n",
        "basis_m2 = fixed_basis(m-2, d)\n",
        "ml_basis = basis_m1\n",
        "mll_basis = basis_m2\n",
        "rho_1_arrays = rho_1_gen(basis)\n",
        "rho_2_arrays = rho_2_gen(basis, mll_basis, t_basis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t_basis = fixed_basis(2, d)\n",
        "mll_basis = fixed_basis(basis.m-2, d)\n",
        "rho_1_arrays = rho_1_gen(basis)\n",
        "rho_2_arrays = rho_2_gen(basis, mll_basis, t_basis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def two_body_hamiltonian(t_basis_size, m, energy, G, rho_1_arrays, rho_2_arrays):\n",
        "    # Creamos la mat diagonal de d*d con los elementos de energy\n",
        "    # cada uno de estos, se contraen con los elementos de rho_1_arrays\n",
        "    # la mat energy contiene las energias de cada termino c^\\dag_k c_k para k kbar (iguales)\n",
        "    # por ello los elementos se repiten \n",
        "    energy_matrix = np.diagflat(np.kron(energy, np.ones(2)))\n",
        "    \n",
        "    # Construimos la mat de energía\n",
        "    h0 = np.sum(energy_matrix[:, :, np.newaxis, np.newaxis] * rho_1_arrays[:, :, :, :], axis=(0, 1))\n",
        "\n",
        "    # Pasamos ahora a la matrix de interacción con la misma estrategia\n",
        "    # dada G que indica la interacción entre los pares k' k'bar k kbar \n",
        "    # (que son elementos particulares de t_basis)\n",
        "    # transladamos estos coeficientes a una matriz en t_basis\n",
        "    # y multiplicamos por rho_2_arrays\n",
        "    \n",
        "    # Primero determinamos, dada t_basis, cuales son los indices de pares kkbar\n",
        "    offset = 4*m+1\n",
        "    indices = [-(k-1)*(2*k-offset) for k in range(1,m+1)] # Lo saque de Mathmatica, vos confia\n",
        "    i, j = np.meshgrid(indices, indices, indexing='ij') # Lo usamos para rellenar la mat deseada\n",
        "    \n",
        "    # Contruimos la mat que contraeremos con rho_2_arrays\n",
        "    mat = np.zeros((t_basis_size, t_basis_size))\n",
        "    mat[i, j] = G\n",
        "\n",
        "    hi = -np.sum(mat[:, :, np.newaxis, np.newaxis] * rho_2_arrays[:, :, :, :], axis=(0, 1))\n",
        "\n",
        "    return (h0, hi)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(h02,hi2) = two_body_hamiltonian(t_basis.size, m, [0,1,2], np.ones((3,3)), rho_1_arrays, rho_2_arrays)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[[[1., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 1., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 1., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [1., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 1., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [1., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 1., 0., ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[1., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [1., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 1., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 1., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "       ...,\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 1.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., ..., 1., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 1.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 1., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 1.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 1., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., ..., 0., 1., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 1.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 1., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 1., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 1.]]]], dtype=float32)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rho_2_arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0\n"
          ]
        }
      ],
      "source": [
        "print(rho_2_arrays[9,0,0,9])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "rho_1_arrays = rho_1_gen(basis)\n",
        "\n",
        "A = np.array([0, 1, 2])  # Your list with d elements\n",
        "\n",
        "# Create a diagonal matrix with each element repeated twice\n",
        "result_matrix = np.diagflat(np.kron(A, np.ones(2)))\n",
        "\n",
        "print(result_matrix)\n",
        "np.kron(A, np.ones(2))\n",
        "\n",
        "mat = np.zeros((basis.size, basis.size))\n",
        "for i in range(0,2*d):\n",
        "    for j in range(0, 2*d):\n",
        "        mat += result_matrix[i,j] * rho_1_arrays[i,j,:,:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mat = np.sum(result_matrix[:, :, np.newaxis, np.newaxis] * rho_1_arrays[:, :, :, :], axis=(0, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "h0 == mat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1 1 0 0 0 0]\n",
            "[0 0 1 1 0 0]\n",
            "[0 0 0 0 1 1]\n",
            "[0, 9, 14]\n",
            "[0, 9, 14]\n"
          ]
        }
      ],
      "source": [
        "d = 3\n",
        "t_basis = fixed_basis(2, 2*d)\n",
        "basis = fixed_basis(d, 2*d)\n",
        "size = t_basis.size\n",
        "#basis = fixed_basis(d, 2*d)\n",
        "diag_elem = []\n",
        "for x in t_basis.base:\n",
        "    if all([x[i] == x[i+1] for i in range(0, 2*d, 2)]):\n",
        "        print(x)\n",
        "        diag_elem.append(t_basis.rep_to_index(x))\n",
        "\n",
        "print(diag_elem)\n",
        "# Veamos el GALERAZO de Wolfram\n",
        "n = 4*d+1\n",
        "print([-(k-1)*(2*k-n) for k in range(1,d+1)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "m2_basis = fixed_basis(2, d)\n",
        "print(m2_basis.size)\n",
        "nm2_basis = fixed_basis(basis.m-2, d)\n",
        "print(nm2_basis.base)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "W = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
        "index = [0,9,14]\n",
        "mat = np.zeros((size,size))\n",
        "for i in range(0,3):\n",
        "    for j in range(0,3):\n",
        "        mat[index[i], index[j]] = W[i,j]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "rho_2_arrays = rho_2_gen(basis, nm2_basis, m2_basis)\n",
        "#rho_2_arrays_tf = tf.constant(rho_2_arrays, dtype=tf.float32)\n",
        "\n",
        "W = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "W = np.ones((3,3))\n",
        "index = [0, 9, 14]\n",
        "size = 15  # Assuming size is the size of the matrix\n",
        "\n",
        "# Create a meshgrid of indices\n",
        "i, j = np.meshgrid(index, index, indexing='ij')\n",
        "\n",
        "# Use the meshgrid indices to assign values from W to the specified positions in mat\n",
        "mat = np.zeros((size, size))\n",
        "mat[i, j] = W\n",
        "\n",
        "# La mat... mat corresponde a los coeficientes en t_basis\n",
        "inte = np.zeros((basis.size, basis.size))\n",
        "for i in range(0, t_basis.size):\n",
        "    for j in range(0, t_basis.size):\n",
        "        inte += - mat[i, j] * rho_2_arrays[i,j,:,:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inte == hi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "m = 3\n",
        "basis = fixed_basis(m, d)\n",
        "basis_m1 = fixed_basis(m-1, d)\n",
        "basis_m2 = fixed_basis(m-2, d)\n",
        "\n",
        "from numba import njit\n",
        "\n",
        "# Parametros hamiltoniano\n",
        "e = 1\n",
        "eps = 0\n",
        "e0 = np.zeros(2*d)\n",
        "eigenspace_tol = 0.0001\n",
        "for k in range(0, d):\n",
        "    r = random.random() * eps * 0\n",
        "    e0[2*k] = k*e+r\n",
        "    e0[2*k+1] = k*e+r\n",
        "\n",
        "@njit(parallel=True)\n",
        "def base_hamiltonian_aux(basis, size, d, basis_m1, basis_m2):\n",
        "    # Construccion de H\n",
        "    d = d//2\n",
        "    h0 = np.zeros((size,size), dtype=np.float32)\n",
        "    for k in prange(0,2*d):\n",
        "        h0 += e0[k] * np.dot(bd_aux(basis_m1, basis, k),b_aux(basis, basis_m1, k))\n",
        "    hi = np.zeros((size, size), dtype=np.float32)\n",
        "    for k in prange(0,d):\n",
        "        for kb in prange(0,d):\n",
        "            bd_terms = np.dot(bd_aux(basis_m1, basis, 2*k),bd_aux(basis_m2, basis_m1, 2*k+1))\n",
        "            b_terms = np.dot(b_aux(basis_m1, basis_m2, 2*kb+1),b_aux(basis, basis_m1, 2*kb))\n",
        "            hi += -1*np.dot(bd_terms,b_terms)\n",
        "\n",
        "    return (h0, hi)\n",
        "\n",
        "def base_hamiltonian(basis, basis_m1, basis_m2):\n",
        "    return base_hamiltonian_aux(basis.base, basis.size, basis.d, basis_m1.base, basis_m2.base)\n",
        "\n",
        "h0, hi = base_hamiltonian(basis, basis_m1, basis_m2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "oapxWkD16fHg"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
