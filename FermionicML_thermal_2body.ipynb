{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aguschanchu/FermionicML/blob/main/FermionicML_thermal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXz5cOlVwrzZ"
      },
      "source": [
        "# FermionicML:\n",
        "\n",
        "Code based on aguschanchu/Bosonic.py\n",
        "\n",
        "A diferencia del código anterior, este modelo trabaja sobre estados térmicos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLD2Yai55rMm"
      },
      "source": [
        "## Código base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgf9ExZN4jA7"
      },
      "source": [
        "Cargamos el código de Bosonic.py básico, branch fermionic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Gydz4kCH4l5w"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-20 15:20:33.500985: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-12-20 15:20:34.241077: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-20 15:20:34.241118: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-20 15:20:34.245418: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-20 15:20:34.618924: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/var/tmp/ipykernel_3373/4156838298.py:296: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
            "  def gamma_lamba_inv(x):\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.special import binom\n",
        "from scipy.sparse import dok_matrix, linalg\n",
        "from scipy import linalg as linalg_d\n",
        "from joblib import Memory\n",
        "import random\n",
        "import plotly.graph_objects as go\n",
        "from joblib import Parallel, delayed\n",
        "from numba import jit, prange, njit\n",
        "import numba as nb\n",
        "import pickle\n",
        "import math\n",
        "import tensorflow_probability as tfp\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Funciones auxiliares optimiadas\n",
        "@nb.jit(nopython=True, parallel=True)\n",
        "def int_to_tuple_arr(ni,nf, b, digits=None):\n",
        "    sol = np.zeros((nf-ni, digits), dtype=np.int64)\n",
        "    for n in prange(ni, nf):\n",
        "        r = np.zeros(digits, dtype=np.int64)\n",
        "        ncop = n\n",
        "        idx = 0\n",
        "        while n != 0:\n",
        "            r[idx] = n % b\n",
        "            n = n // b\n",
        "            idx += 1\n",
        "        if digits is not None:\n",
        "            if idx < digits:\n",
        "                for i in range(idx, digits):\n",
        "                    r[i] = 0\n",
        "                idx = digits\n",
        "        sol[ncop-ni,:] = r[:idx]\n",
        "    return sol\n",
        "\n",
        "def tuple_to_int(t, d):\n",
        "    b = d-1\n",
        "    l = len(t)\n",
        "    s = [t[k]*b**(l-k-1) for k in range(0,l)]\n",
        "    return sum(s)\n",
        "\n",
        "def create_basis_(m, d, size):\n",
        "    base = []\n",
        "    index = 0\n",
        "    chunk_size = 1000000\n",
        "    for x in range(0,(m+1)**d, chunk_size):\n",
        "        start_index = x\n",
        "        end_index = min(x + chunk_size, (m+1)**d)\n",
        "        arr = int_to_tuple_arr(start_index, end_index, m+1, d)\n",
        "        sums = np.sum(arr, axis=1)\n",
        "        rows = np.where(sums == m)[0]\n",
        "        for row in [arr[i] for i in rows]:\n",
        "            if np.all(np.logical_or(row == 0, row == 1)):\n",
        "                base.append(row)\n",
        "\n",
        "    # Como consecuencia de la paralelizacion, es necesario reordenar la base\n",
        "    sorted_base = sorted(base, key=lambda x: tuple_to_int(x, d), reverse=True)\n",
        "    assert len(base) == size\n",
        "\n",
        "    return sorted_base\n",
        "\n",
        "def custom_base_representation_tf(n_min, n_max, base, num_digits):\n",
        "    # Generate a range of numbers from n_min to n_max\n",
        "    numbers = tf.range(n_min, n_max + 1, dtype=tf.int64)\n",
        "    \n",
        "    # Calculate the digits in the custom base using broadcasting\n",
        "    digits = tf.pow(tf.cast(base, dtype=tf.float64), tf.cast(tf.range(num_digits), dtype=tf.float64))\n",
        "    \n",
        "    # Reshape the digits to [1, num_digits] for broadcasting\n",
        "    digits = tf.reshape(digits, [1, -1])\n",
        "    \n",
        "    # Reshape numbers to [batch_size, 1]\n",
        "    numbers = tf.reshape(tf.cast(numbers, dtype=tf.float64), [-1, 1])\n",
        "    \n",
        "    # Calculate the digits in the custom base for each number using broadcasting\n",
        "    result = tf.cast(tf.math.floormod(tf.math.floordiv(numbers, digits), base), dtype=tf.int32)\n",
        "    \n",
        "    # Pad the result to have exactly num_digits columns\n",
        "    result = tf.pad(result, paddings=[[0, 0], [0, num_digits - tf.shape(result)[1]]], constant_values=0)\n",
        "    \n",
        "    # Reverse the order of columns\n",
        "    #result = tf.reverse(result, axis=[1])\n",
        "\n",
        "    return result\n",
        "\n",
        "def select_rows_with_sum(arr, m):\n",
        "    # Create a mask based on the criteria\n",
        "    mask = tf.reduce_all(tf.math.logical_or(tf.equal(arr, 0), tf.equal(arr, 1)), axis=1) & (tf.reduce_sum(arr, axis=1) == m)\n",
        "    \n",
        "    # Use the mask to select the rows\n",
        "    result = tf.boolean_mask(arr, mask, axis=0)\n",
        "    \n",
        "    return result\n",
        "\n",
        "def create_basis_tf_(m, d):\n",
        "    base = []\n",
        "    index = 0\n",
        "    chunk_size = 10000000\n",
        "    for x in tqdm(range(0,(m+1)**d, chunk_size)):\n",
        "        start_index = x\n",
        "        end_index = min(x + chunk_size, (m+1)**d)\n",
        "        res = custom_base_representation_tf(start_index, end_index, m+1, d)\n",
        "        arr = select_rows_with_sum(res, m)\n",
        "        base.append(arr.numpy())\n",
        "\n",
        "    return np.concatenate(base)\n",
        "\n",
        "class fixed_basis:\n",
        "\n",
        "    # Convierte a un enterno n a su escritura en base b\n",
        "    def _int_to_tuple(self, n, b, digits = None):\n",
        "        rep = np.base_repr(n, b)\n",
        "        rep_int = [int(x,b) for x in rep]\n",
        "        if digits is not None:\n",
        "            zeros = [0 for i in range(0,digits-len(rep))]\n",
        "            return zeros + rep_int\n",
        "        else:\n",
        "            return rep_int\n",
        "\n",
        "    # Revierte la transformacion anterior\n",
        "    def tuple_to_int(self, t):\n",
        "        b = self.d-1\n",
        "        l = len(t)\n",
        "        s = [t[k]*b**(l-k-1) for k in range(0,l)]\n",
        "        return sum(s)\n",
        "\n",
        "    # Convierte el vector en su representacion\n",
        "    def vect_to_repr(self, vect):\n",
        "        for i, k in enumerate(vect):\n",
        "            if k == 1. or k == 1:\n",
        "                break\n",
        "        else:\n",
        "            return 0\n",
        "        return self.base[i,:]\n",
        "\n",
        "    def rep_to_vect(self, rep):\n",
        "        rep = list(rep)\n",
        "        for i, r in [(j, self.base[j,:]) for j in range(0,self.size)]:\n",
        "            if list(r) == rep:\n",
        "                return self.canonicals[:,i]\n",
        "        else:\n",
        "            None\n",
        "\n",
        "    def rep_to_index(self, rep):\n",
        "        return self.base.tolist().index(list(rep))\n",
        "\n",
        "    @staticmethod\n",
        "    def rep_to_exi(rep):\n",
        "        r = []\n",
        "        for i, k in enumerate(rep):\n",
        "            r += [i for x in range(0,k)]\n",
        "        return r\n",
        "\n",
        "    # Crea base de M particulas en D estados (repr y base canonica)\n",
        "    def create_basis(self, m, d):\n",
        "        #print(\"Creating basis: \", m, d)\n",
        "        length = int(binom(d,m))\n",
        "        base = np.array(create_basis_tf_(m, d))\n",
        "        # Asignamos a cada uno de ellos un canónico\n",
        "        canonicals = np.eye(length)\n",
        "        return base, canonicals\n",
        "\n",
        "    def __init__(self, m, d):\n",
        "        self.m = m\n",
        "        self.d = d\n",
        "        self.size = int(binom(d,m))\n",
        "        (self.base, self.canonicals) = self.create_basis(m, d)\n",
        "\n",
        "\n",
        "# Matrices de aniquilación y creación endomórficas. Estan fuera de la clase para poder ser cacheadas\n",
        "#@memory.cache\n",
        "def bdb(basis, i, j):\n",
        "    mat = dok_matrix((basis.size, basis.size), dtype=np.float32)\n",
        "    if i != j:\n",
        "        for k, v in enumerate(basis.base):\n",
        "            if v[j] != 0 and v[i] != 1:\n",
        "                dest = list(v.copy())\n",
        "                dest[j] -= 1\n",
        "                dest[i] += 1\n",
        "                tar = basis.rep_to_index(dest)\n",
        "                mat[tar, k] = np.sqrt(v[i]+1)*np.sqrt(v[j])\n",
        "    else:\n",
        "        for k, v in enumerate(basis.base):\n",
        "            if v[j] != 0:\n",
        "                mat[k, k] = v[i]\n",
        "    return mat\n",
        "\n",
        "#@memory.cache\n",
        "def bbd(basis, i, j):\n",
        "    mat = dok_matrix((basis.size, basis.size), dtype=np.float32)\n",
        "    if i != j:\n",
        "        for k, v in enumerate(basis.base):\n",
        "            if v[i] != 0 and v[j] != 1:\n",
        "                dest = list(v.copy())\n",
        "                dest[i] -= 1\n",
        "                dest[j] += 1\n",
        "                tar = basis.rep_to_index(dest)\n",
        "                mat[tar, k] = np.sqrt(v[j]+1)*np.sqrt(v[i])\n",
        "    else:\n",
        "        for k, v in enumerate(basis.base):\n",
        "            if v[i] != 1:\n",
        "                mat[k, k] = v[i]+1\n",
        "    return mat\n",
        "\n",
        "# Matrices de aniquilación y creación.Toman la base de origen y destino (basis_o, basis_d) resp\n",
        "#@nb.jit(nopython=True, parallel=True)\n",
        "@nb.jit(nopython=True)\n",
        "def b_aux(basis_o, basis_d, i):\n",
        "    mat = np.zeros((len(basis_d), len(basis_o)), dtype=np.float32)\n",
        "    for k in prange(len(basis_o)):\n",
        "        if basis_o[k][i] != 0:\n",
        "            dest = list(basis_o[k].copy())\n",
        "            dest[i] -= 1\n",
        "            for j in prange(len(basis_d)):\n",
        "                if list(basis_d[j]) == dest:\n",
        "                    tar = j\n",
        "                    mat[tar, k] = np.sqrt(basis_o[k][i])\n",
        "    return mat\n",
        "\n",
        "def b(basis_o, basis_d, i):\n",
        "    return b_aux(basis_o.base, basis_d.base, i)\n",
        "\n",
        "#@nb.jit(nopython=True, parallel=True)\n",
        "@nb.jit(nopython=True)\n",
        "def bd_aux(basis_o, basis_d, i):\n",
        "    mat = np.zeros((len(basis_d), len(basis_o)), dtype=np.float32)\n",
        "    for k in prange(len(basis_o)):\n",
        "        if basis_o[k][i] != 1:\n",
        "            dest = list(basis_o[k].copy())\n",
        "            dest[i] += 1\n",
        "            for j in prange(len(basis_d)):\n",
        "                if list(basis_d[j]) == dest:\n",
        "                    tar = j\n",
        "                    mat[tar, k] = np.sqrt(basis_o[k][i]+1)\n",
        "    return mat\n",
        "\n",
        "def bd(basis_o, basis_d, i):\n",
        "    return bd_aux(basis_o.base, basis_d.base, i)\n",
        "\n",
        "\n",
        "# Acepta una lista de indices a crear\n",
        "@nb.jit(nopython=True, parallel=True)\n",
        "def bd_gen_aux(basis_o, basis_d, gen_list):\n",
        "    mat = np.zeros((len(basis_d), len(basis_o)), dtype=np.float32)\n",
        "    for k in prange(len(basis_o)):\n",
        "        conds = np.zeros(len(gen_list), dtype=np.int64)\n",
        "        for i in range(len(gen_list)):\n",
        "            if basis_o[k][gen_list[i]] != 1:\n",
        "                conds[i] = 1\n",
        "        if np.all(conds):\n",
        "            dest = list(basis_o[k].copy())\n",
        "            for i in gen_list:\n",
        "                dest[i] += 1\n",
        "            for j in prange(len(basis_d)):\n",
        "                if list(basis_d[j]) == dest:\n",
        "                    tar = j\n",
        "                    mat[tar, k] = np.sqrt(basis_o[k][i]+1)\n",
        "    return mat\n",
        "\n",
        "def bd_gen(basis_o, basis_d, i):\n",
        "    return bd_gen_aux(basis_o.base, basis_d.base, np.array(i))\n",
        "\n",
        "def b_gen(basis_o, basis_d, i):\n",
        "    return np.transpose(bd_gen(basis_d, basis_o, i))\n",
        "\n",
        "# Volvemos a definir la función para compilarla\n",
        "@nb.jit(forceobj=True)\n",
        "def _rep_to_index(base, rep):\n",
        "    return base.tolist().index(list(rep))\n",
        "\n",
        "# Funciones auxiliares para calcular rho2kkbar y gamma_p\n",
        "@nb.jit(nopython=True)\n",
        "def rep_to_exi(rep):\n",
        "    r = []\n",
        "    for i in range(len(rep)):\n",
        "        for j in range(rep[i]):\n",
        "            r.append(i)\n",
        "    return r\n",
        "\n",
        "@nb.njit\n",
        "def factorial(n):\n",
        "    result = 1\n",
        "    for i in range(1, n + 1):\n",
        "        result *= i\n",
        "    return result\n",
        "\n",
        "@nb.njit\n",
        "def gamma_lamba(x):\n",
        "    res = 1.0\n",
        "    for o in x:\n",
        "        res *= math.sqrt(factorial(o))\n",
        "    return res\n",
        "\n",
        "@nb.jit\n",
        "def gamma_lamba_inv(x):\n",
        "    res = 1.0\n",
        "    for o in x:\n",
        "        res *= 1.0 / np.sqrt(factorial(o))\n",
        "    return res\n",
        "\n",
        "@nb.njit\n",
        "def rep_to_index_np(base, rep):\n",
        "    for i in range(len(base)):\n",
        "        if np.all(base[i] == rep):\n",
        "            return i\n",
        "    return -1\n",
        "\n",
        "\n",
        "def gamma_p(basis, m, vect, m_basis = None, nm_basis = None):\n",
        "    d = basis.d\n",
        "    if not m_basis or not nm_basis:\n",
        "        m_basis = fixed_basis(m, d)\n",
        "        nm_basis = fixed_basis(basis.m-m,d)\n",
        "    return gamma_p_aux(basis.base, vect, m_basis.base, nm_basis.base)\n",
        "\n",
        "@nb.njit()\n",
        "def gamma_p_aux(basis, vect, m_basis, nm_basis):\n",
        "    mat = np.zeros((len(m_basis), len(nm_basis)), dtype=np.float32)\n",
        "    for i in prange(len(m_basis)):\n",
        "        v = m_basis[i]\n",
        "        for j in prange(len(nm_basis)):\n",
        "            w = nm_basis[j]\n",
        "            targ = v + w\n",
        "            index = rep_to_index_np(basis, targ)\n",
        "            if index != -1:\n",
        "                coef = vect[index]\n",
        "                if coef != 0:\n",
        "                    coef = coef * gamma_lamba_inv(v) * gamma_lamba_inv(w) * gamma_lamba(targ)\n",
        "                mat[i, j] = coef\n",
        "    return mat\n",
        "# Devuelve la matriz rho M asociada al vector\n",
        "def rho_m(basis, m, vect, m_basis = None, nm_basis = None):\n",
        "    g = gamma_p(basis, m, vect, m_basis, nm_basis)\n",
        "    return np.dot(g,np.transpose(g))\n",
        "\n",
        "# Devuelve la matriz gamma asociada a la descomposición (M,N-M) del vector\n",
        "@jit(forceobj=True)\n",
        "def gamma(basis, m, vect, m_basis = None, nm_basis = None):\n",
        "    d = basis.d\n",
        "    if not m_basis or not nm_basis:\n",
        "        m_basis = fixed_basis(m, d)\n",
        "        nm_basis = fixed_basis(basis.m-m,d)\n",
        "    mat = dok_matrix((m_basis.size, nm_basis.size), dtype=np.float32)\n",
        "    for i, v in enumerate(m_basis.base):\n",
        "        for j, w in enumerate(nm_basis.base):\n",
        "            targ = v+w\n",
        "            # Revisamos que sea un estado fermionico valido\n",
        "            arr = np.asarray(targ)\n",
        "            if not np.all(np.logical_or(arr == 0, arr == 1)):\n",
        "                continue\n",
        "            index = _rep_to_index(basis.base, targ)\n",
        "            coef = vect[index]\n",
        "            if coef != 0:\n",
        "                aux = lambda x: np.prod(np.reciprocal(np.sqrt([np.math.factorial(o) for o in x])))\n",
        "                aux_inv = lambda x: np.prod(np.sqrt([np.math.factorial(o) for o in x]))\n",
        "                coef = coef * aux(v) * aux(w) * aux_inv(targ)\n",
        "                #coef = coef\n",
        "                #print(v,w,coef)\n",
        "            mat[i,j] = coef\n",
        "    return mat\n",
        "\n",
        "# Genera las matrices de rho1\n",
        "def rho_1_gen(basis):\n",
        "    d = basis.d\n",
        "    s = basis.size\n",
        "    mat = np.empty((d,d,s,s), dtype=np.float32)\n",
        "    for i in range(0, d):\n",
        "        for j in range(0, d):\n",
        "            mat[i,j,:,:] = np.array(bdb(basis,j, i).todense())\n",
        "    return mat\n",
        "\n",
        "#@jit(parallel=True, nopython=True)\n",
        "def rho_1(d, state, rho_1_arrays):\n",
        "    state_expanded = state[np.newaxis, np.newaxis, :, :]\n",
        "    product = state_expanded * rho_1_arrays\n",
        "    mat = np.sum(product, axis=(-2, -1))\n",
        "\n",
        "    return mat\n",
        "\n",
        "\n",
        "# Genera las matrices de rho2\n",
        "def rho_2_gen(basis, mll_basis, t_basis):\n",
        "    size = t_basis.size\n",
        "    s = basis.size\n",
        "    # La entrada i, j contiene C_j^\\dag C_i    i, j \\in t_basis\n",
        "    mat = np.empty((size, size, s, s), dtype=np.float32)\n",
        "    for i, v in enumerate(t_basis.base):\n",
        "        for j, w in enumerate(t_basis.base):\n",
        "            c_i = b_gen(basis, mll_basis, rep_to_exi(v))\n",
        "            cdag_j = bd_gen(mll_basis, basis, rep_to_exi(w))\n",
        "            mat[i, j, :, :] = np.dot(cdag_j, c_i)\n",
        "\n",
        "    return mat\n",
        "\n",
        "def rho_2(size, state, rho_2_arrays):\n",
        "    state_expanded = np.expand_dims(state, axis=1)\n",
        "    state_expanded = np.expand_dims(state_expanded, axis=1)\n",
        "    rho_2_arrays = rho_2_arrays[np.newaxis, :, :, :, :]\n",
        "    print(state_expanded.shape, rho_2_arrays.shape)\n",
        "    product = state_expanded * rho_2_arrays\n",
        "    mat = np.sum(product, axis=(-2, -1))\n",
        "    return mat\n",
        "\n",
        "def rho_2_kkbar_gen(m, rho_2_arrays):\n",
        "    offset = 4*m+1\n",
        "    indices = [-(k-1)*(2*k-offset) for k in range(1,m+1)] # Lo saque de Mathmatica, vos confia\n",
        "    i, j = np.meshgrid(indices, indices, indexing='ij') # Lo usamos para rellenar la mat deseada\n",
        "\n",
        "    rho_2_arrays_kkbar = rho_2_arrays[i, j, :, :]\n",
        "\n",
        "    return rho_2_arrays_kkbar\n",
        "\n",
        "# Devuelve la matriz rho 2 asociada al bloque kkbar\n",
        "def rho_2_kkbar(basis, vect, ml_basis = None, mll_basis = None, t_basis = None):\n",
        "    d = basis.d\n",
        "    # Creo las bases si no están dadas\n",
        "    if ml_basis == None or mll_basis == None or t_basis == None:\n",
        "        ml_basis = fixed_basis(m-1,d)\n",
        "        mll_basis = fixed_basis(m-2,d)\n",
        "        t_basis = fixed_basis(2,d)\n",
        "    diag = []\n",
        "    for v in t_basis.base:\n",
        "        for j in range(0, d, 2):\n",
        "            if v[j] == v[j+1]:\n",
        "                continue\n",
        "            else:\n",
        "                break\n",
        "        else:\n",
        "            diag.append(v)\n",
        "    diag = np.array(diag)\n",
        "    return rho_2_kkbar_aux(diag, vect, basis.base, ml_basis.base, mll_basis.base, t_basis.base)\n",
        "\n",
        "@nb.njit\n",
        "def rho_2_kkbar_lambda(x):\n",
        "    res = 1.0\n",
        "    for o in x:\n",
        "        res *= 1.0 / math.sqrt(factorial(o))\n",
        "    return res\n",
        "\n",
        "#@nb.njit(parallel=True)\n",
        "def rho_2_kkbar_aux(diag, vect, basis, ml_basis, mll_basis, t_basis):\n",
        "    mat = np.zeros((len(diag), len(diag)), dtype=np.float32)\n",
        "    for i in prange(len(diag)):\n",
        "        for j in prange(len(diag)):\n",
        "            v = diag[i]\n",
        "            w = diag[j]\n",
        "            # Creacion de los a\n",
        "            i_set = rep_to_exi(v)\n",
        "            b_m = b_aux(ml_basis, mll_basis, i_set[1]) @ b_aux(basis, ml_basis, i_set[0])\n",
        "            # Creacion de los ad\n",
        "            i_set = rep_to_exi(w)\n",
        "            bd_m = bd_aux(ml_basis, basis, i_set[1]) @ bd_aux(mll_basis, ml_basis, i_set[0])\n",
        "            # v1 = vect @ bd_m @ b_m @ vect Para estados puros\n",
        "            # Mult de b's y filleo de mat\n",
        "            coef = np.trace(vect @ bd_m @ b_m)\n",
        "            mat[i,j] = coef * rho_2_kkbar_lambda(v) * rho_2_kkbar_lambda(w)\n",
        "    return mat\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dga5Xx_5vDf"
      },
      "source": [
        "## Definicion de Hamiltoniano"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myiTq53L5E1U"
      },
      "source": [
        "Cargamos el código de creación y resolución de Hamiltonianos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h5FXWv849Mq",
        "outputId": "49dd47b5-8c16-4ad4-92e7-e172462229b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|                                                                                                                                                                                                      | 0/1 [00:00<?, ?it/s]2023-12-20 15:20:41.068106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20609 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
            "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.46s/it]\n",
            "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 89.16it/s]\n",
            "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 140.16it/s]\n"
          ]
        }
      ],
      "source": [
        "m = 4\n",
        "d = 8\n",
        "# Creo las bases para no tener que recrearlas luego\n",
        "basis = fixed_basis(m, d)\n",
        "basis_m1 = fixed_basis(m-1, d)\n",
        "basis_m2 = fixed_basis(m-2, d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PToiSs915TXw"
      },
      "outputs": [],
      "source": [
        "## Usamos este approach si queremos guardar los generadores\n",
        "# Dados 1/2 (d^2+d) elementos, genera una mat de dxd:\n",
        "eps = 0.00001\n",
        "\n",
        "def sym_mat_gen(vect, d):\n",
        "    matrix = fill_matrix(vect, d)\n",
        "    return matrix + matrix.T - np.diag(matrix.diagonal())\n",
        "\n",
        "@jit(nopython=True)\n",
        "def fill_matrix(vect, d):\n",
        "    matrix = np.zeros((d, d))\n",
        "    idx = 0\n",
        "    for i in prange(d):\n",
        "        for j in prange(i, d):\n",
        "            matrix[i, j] = vect[idx]\n",
        "            idx += 1\n",
        "    return matrix\n",
        "\n",
        "# Generamos una matrix aleatoria. Cuidado con la distribución, ver https://stackoverflow.com/questions/56605189/is-there-an-efficient-way-to-generate-a-symmetric-random-matrix\n",
        "def hamil_base_gen(d):\n",
        "    U = np.random.uniform(low=0, high=1.0, size=(d, d))\n",
        "    hamil_base = np.tril(U) + np.tril(U, -1).T\n",
        "    return hamil_base\n",
        "\n",
        "# Dada un a mat dxd simetrica, contruye el hamiltoniano de un cuerpo a_{ij} c^{dag}_i c_j\n",
        "# Alternativamente podemos construirlo a partir de rho_1_gen\n",
        "def base_hamiltonian_aux(mat, size, d, rho_1_gen):\n",
        "    # Construccion de H\n",
        "    rho_1_gen_transposed = rho_1_gen.transpose(1, 0, 2, 3)\n",
        "    mat_expanded = mat[:, :, np.newaxis, np.newaxis]\n",
        "    h = np.sum(mat_expanded * rho_1_gen_transposed[:, :, :, :], axis=(0, 1))\n",
        "    return h.astype(np.float32)\n",
        "\n",
        "def base_hamiltonian(mat, basis, rho_1_gen):\n",
        "    return base_hamiltonian_aux(mat, basis.size, basis.d, rho_1_gen)\n",
        "\n",
        "def get_kkbar_indices(t_basis):\n",
        "    indices = []\n",
        "    for i, v in enumerate(t_basis.base):\n",
        "        if np.all(v[::2] == v[1::2]):\n",
        "            indices.append(i)\n",
        "    return indices\n",
        "\n",
        "def two_body_hamiltonian(t_basis_size, m, energy, G, rho_1_arrays, rho_2_arrays, indices):\n",
        "    # Creamos la mat diagonal de d*d con los elementos de energy\n",
        "    # cada uno de estos, se contraen con los elementos de rho_1_arrays\n",
        "    # la mat energy contiene las energias de cada termino c^\\dag_k c_k para k kbar (iguales)\n",
        "    # por ello los elementos se repiten \n",
        "    energy_matrix = np.diagflat(np.kron(energy, np.ones(2))) + eps * np.random.random((2*m,2*m))\n",
        "    \n",
        "    # Construimos la mat de energía\n",
        "    rho_1_arrays_t = tf.transpose(rho_1_arrays,perm=[1, 0, 2, 3])\n",
        "    h0 = np.sum(energy_matrix[:, :, np.newaxis, np.newaxis] * rho_1_arrays_t[:, :, :, :], axis=(0, 1))\n",
        "\n",
        "    # Pasamos ahora a la matrix de interacción con la misma estrategia\n",
        "    # dada G que indica la interacción entre los pares k' k'bar k kbar \n",
        "    # (que son elementos particulares de t_basis)\n",
        "    # transladamos estos coeficientes a una matriz en t_basis\n",
        "    # y multiplicamos por rho_2_arrays\n",
        "    \n",
        "    # Primero determinamos, dada t_basis, cuales son los indices de pares kkbar\n",
        "    i, j = np.meshgrid(indices, indices, indexing='ij') # Lo usamos para rellenar la mat deseada\n",
        "    rho_2_arrays_t = tf.transpose(rho_2_arrays,perm=[1, 0, 2, 3])\n",
        "\n",
        "    # Contruimos la mat que contraeremos con rho_2_arrays\n",
        "    mat = np.zeros((t_basis_size, t_basis_size))\n",
        "    mat[i, j] = G\n",
        "    hi = np.sum(mat[:, :, np.newaxis, np.newaxis] * rho_2_arrays_t[:, :, :, :], axis=(0, 1))\n",
        "    return (h0, hi)\n",
        "\n",
        "def solve(h, last_step = None):\n",
        "    sol = linalg.eigsh(h, which='SA',k=19)\n",
        "    eigenspace_tol = 0.0001\n",
        "    if type(last_step) != type(None):\n",
        "        # Seleccionamos todos los autovects que difieren sus autovalores menos que tol (mismo autoespacio)\n",
        "        # y tomamos la proyección en el autoespacio de la solución del paso anterior (last_step)\n",
        "        eig = sol[0].real\n",
        "        eigv = sol[1]\n",
        "        cand = [eigv[:,i].real  for (i, x) in enumerate(eig) if abs(x-min(eig)) < eigenspace_tol]\n",
        "        cand_norm = [x/np.linalg.norm(x) for x in cand]\n",
        "        fund = np.zeros(len(cand[0]))\n",
        "        for x in cand_norm:\n",
        "            fund += np.dot(last_step,x) * x\n",
        "    else:\n",
        "        argmin = np.argmin(sol[0].real)\n",
        "        fund = sol[1][:,argmin]\n",
        "    fund = fund.real / np.linalg.norm(fund)\n",
        "    return fund\n",
        "\n",
        "# Generacion de H basada en TF\n",
        "\n",
        "# Funciones auxiliares de gen de H basado en TF\n",
        "## Dada matrix de indices, genera los indices de updates de TF\n",
        "def gen_update_indices(t_basis, batch_size):\n",
        "    # Calculamos los indices de kkbar en t_basis\n",
        "    indices = tf.constant(get_kkbar_indices(t_basis))\n",
        "    # Creamos el array de indices x indices\n",
        "    i, j = tf.meshgrid(indices, indices, indexing='ij')\n",
        "    matrix = tf.reshape(tf.stack([i, j], axis=-1), (-1, 2))\n",
        "\n",
        "    # Repeat the matrix along the first axis (axis=0) 'b' times\n",
        "    repeated_matrix = tf.repeat(tf.expand_dims(matrix, axis=0), repeats=batch_size, axis=0)\n",
        "\n",
        "    # Create an index array from 0 to b-1\n",
        "    indices = tf.range(batch_size, dtype=tf.int32)\n",
        "\n",
        "    # Expand the index array to have the same shape as the repeated matrix\n",
        "    indices = tf.expand_dims(indices, axis=-1)\n",
        "    indices = tf.expand_dims(indices, axis=-1)\n",
        "    indices = tf.tile(indices, multiples=[1,matrix.shape[0],1]) \n",
        "\n",
        "    # Concatenate the index array to the repeated matrix along a new axis\n",
        "    tiled_matrix = tf.concat([indices, repeated_matrix], axis=-1)\n",
        "    tiled_matrix = tf.reshape(tiled_matrix, [-1,3])\n",
        "    return tiled_matrix\n",
        "\n",
        "\n",
        "def two_body_hamiltonian_tf(t_basis, m, energy_batch, G_batched, rho_1_arrays, rho_2_arrays, indices):\n",
        "    # SECCIÓN ENERGIAS\n",
        "    ## Dado un batch de niveles, lo pasamos a TF\n",
        "    energy_matrix = tf.constant(energy_batch, dtype=tf.float32)\n",
        "    ## Repetimos los niveles para cada uno de los pares (por el nivel k y kbar)\n",
        "    energy_matrix = tf.repeat(energy_matrix, repeats=2, axis=1)\n",
        "    ## Generamos la matrix diagonal y expandimos\n",
        "    energy_matrix_expanded = tf.linalg.diag(energy_matrix)\n",
        "    energy_matrix_expanded = energy_matrix_expanded[:, :, :, np.newaxis, np.newaxis]\n",
        "    rho_1_gen_transposed = tf.transpose(rho_1_arrays, perm=[1, 0, 2, 3])\n",
        "    # Multiplicamos por los operadores C^dag C\n",
        "    h0_arr = tf.reduce_sum(energy_matrix_expanded * rho_1_gen_transposed[np.newaxis,:,:,:,:], axis=[1,2])\n",
        "\n",
        "    # SECCIÓN INTERACCIÓN\n",
        "    # Ya tenemos los indices de updates, ahora tomamos la mat en t_basis (una de zeros)\n",
        "    # y updateamos de acuerdo a la lista de G's cada uno flatteneados\n",
        "    G_flatten = np.ndarray.flatten(np.array([np.ndarray.flatten(G) for G in G_batched]))\n",
        "    # Creamos la mat de t_basis y updateamos a partir de los indices de kkbar\n",
        "    mat = tf.zeros((len(energy_batch), t_basis.size, t_basis.size), dtype=tf.float32)\n",
        "    mat = tf.tensor_scatter_nd_update(mat, indices, G_flatten)\n",
        "    # Preparamos las dimensiones y multiplicamos\n",
        "    mat_expanded = mat[:, :, :, np.newaxis, np.newaxis]\n",
        "    rho_2_gen_transposed = tf.transpose(rho_2_arrays, perm=[1, 0, 2, 3])\n",
        "    hi_arr = tf.reduce_sum(mat_expanded * rho_2_gen_transposed[np.newaxis,:,:,:,:], axis=[1,2])\n",
        "\n",
        "    return (h0_arr, hi_arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emVBTg2QD-Fg"
      },
      "source": [
        "## Modelo de ML\n",
        "Basado en matrices densidad de 1 y 2 cuerpos como input, con hamiltoniano como salida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aF_Ec_mCGX96"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-20 15:20:44.106320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /device:GPU:0 with 20609 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tf.test.gpu_device_name()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJDoa6LUJJ8O",
        "outputId": "73481454-fbcb-469f-d72f-cd0f8d534808"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 123.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "28\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 148.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1 1 0 0 0 0 0 0]\n",
            " [1 0 1 0 0 0 0 0]\n",
            " [0 1 1 0 0 0 0 0]\n",
            " [1 0 0 1 0 0 0 0]\n",
            " [0 1 0 1 0 0 0 0]\n",
            " [0 0 1 1 0 0 0 0]\n",
            " [1 0 0 0 1 0 0 0]\n",
            " [0 1 0 0 1 0 0 0]\n",
            " [0 0 1 0 1 0 0 0]\n",
            " [0 0 0 1 1 0 0 0]\n",
            " [1 0 0 0 0 1 0 0]\n",
            " [0 1 0 0 0 1 0 0]\n",
            " [0 0 1 0 0 1 0 0]\n",
            " [0 0 0 1 0 1 0 0]\n",
            " [0 0 0 0 1 1 0 0]\n",
            " [1 0 0 0 0 0 1 0]\n",
            " [0 1 0 0 0 0 1 0]\n",
            " [0 0 1 0 0 0 1 0]\n",
            " [0 0 0 1 0 0 1 0]\n",
            " [0 0 0 0 1 0 1 0]\n",
            " [0 0 0 0 0 1 1 0]\n",
            " [1 0 0 0 0 0 0 1]\n",
            " [0 1 0 0 0 0 0 1]\n",
            " [0 0 1 0 0 0 0 1]\n",
            " [0 0 0 1 0 0 0 1]\n",
            " [0 0 0 0 1 0 0 1]\n",
            " [0 0 0 0 0 1 0 1]\n",
            " [0 0 0 0 0 0 1 1]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 152.56it/s]\n",
            "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 159.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n",
            "[[1 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 1]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 121.20it/s]\n"
          ]
        }
      ],
      "source": [
        "# Construccion de bases para calculo de rho1 y rho2\n",
        "# rho2\n",
        "m = 2\n",
        "m2_basis = fixed_basis(m, d)\n",
        "print(m2_basis.size)\n",
        "nm2_basis = fixed_basis(basis.m-m, d)\n",
        "print(nm2_basis.base)\n",
        "t_basis = fixed_basis(2, basis.d)\n",
        "# rho1\n",
        "m = 1\n",
        "m1_basis = fixed_basis(m, d)\n",
        "print(m1_basis.size)\n",
        "print(m1_basis.base)\n",
        "nm1_basis = fixed_basis(basis.m-m, d)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oapxWkD16fHg"
      },
      "source": [
        "### Algunos benchmarks y funciones auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "umCIrxCZKXQd"
      },
      "outputs": [],
      "source": [
        "# Given h calculo en rho2 y rho1 máximo\n",
        "def rho1_rho2(h, beta):\n",
        "    fund = thermal_state(h, beta)\n",
        "    rho2 = np.array(rho_2(basis, m2_basis.size, state, rho_2_arrays))\n",
        "    r = np.sort(linalg_d.eigvals(rho2).real)\n",
        "    rho_2_max = r[0]\n",
        "    rho1 = np.array(rho_1(basis, state, rho_1_arrays))\n",
        "    r = np.sort(linalg_d.eigvals(rho1).real)\n",
        "    rho_1_max = r[0]\n",
        "\n",
        "    return (rho_1_max, rho_2_max)\n",
        "\n",
        "def fill_triangular_np(x):\n",
        "    m = x.shape[0]\n",
        "    n = np.int32(np.sqrt(.25 + 2 * m) - .5)\n",
        "    x_tail = x[(m - (n**2 - m)):]\n",
        "    return np.triu(np.concatenate([x, x_tail[::-1]], 0).reshape(n, n))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QaNnIIc5bZux"
      },
      "outputs": [],
      "source": [
        "# TEST: Las funciones de TF y comunes coinciden\n",
        "\n",
        "# Dado h, \\beta, construyo el estado térmico\n",
        "from scipy.linalg import expm\n",
        "\n",
        "def thermal_state(h, beta):\n",
        "    quotient = expm(-beta*h)\n",
        "    return quotient / np.trace(quotient)\n",
        "\n",
        "## NO usar para mat no hermiticas\n",
        "@nb.jit(nopython=True)\n",
        "def thermal_state_eig(h, beta):\n",
        "    w, v = np.linalg.eigh(-beta*h)\n",
        "    D = np.diag(np.exp(w))\n",
        "    mat = v @ D @ v.T\n",
        "    mat = mat / np.trace(mat)\n",
        "    return mat\n",
        "    \n",
        "def gen_to_h(base, rho_1_arrays):\n",
        "    triag = fill_triangular_np(base)\n",
        "    body_gen = triag + np.transpose(triag)-np.diag(np.diag(triag))\n",
        "    h = np.array(base_hamiltonian(body_gen, basis, rho_1_arrays))  \n",
        "    return h \n",
        "\n",
        "def gen_to_h_1b(hamil_base):\n",
        "    triag = tfp.math.fill_triangular(hamil_base, upper=True)\n",
        "    body_gen = triag + tf.transpose(triag, perm=[0,2,1])-tf.linalg.diag(tf.linalg.diag_part(triag))\n",
        "    return body_gen\n",
        "\n",
        "def gen_to_h_tf(hamil_base, rho_1_arrays):\n",
        "    triag = tfp.math.fill_triangular(hamil_base, upper=True)\n",
        "    body_gen = triag + tf.transpose(triag, perm=[0,2,1])-tf.linalg.diag(tf.linalg.diag_part(triag)) # Simetrizamos y generamos la matriz de h\n",
        "    hamil_expanded = body_gen[:, :, :, np.newaxis, np.newaxis]\n",
        "    rho_1_gen_transposed = tf.transpose(rho_1_arrays, perm=[1, 0, 2, 3])\n",
        "    h_arr = tf.reduce_sum(hamil_expanded * rho_1_gen_transposed[np.newaxis,:,:,:,:], axis=[1,2])\n",
        "    return h_arr\n",
        "\n",
        "def thermal_state_tf(h):\n",
        "    # Assume beta=1\n",
        "    exp_hamiltonian = tf.linalg.expm(-h)\n",
        "    partition_function = tf.linalg.trace(exp_hamiltonian)\n",
        "    partition_function = tf.expand_dims(partition_function, axis=1)\n",
        "    partition_function = tf.expand_dims(partition_function, axis=1)\n",
        "    \n",
        "    rho = exp_hamiltonian / partition_function\n",
        "\n",
        "    return rho\n",
        "\n",
        "def rho_1_tf(state, rho_1_arrays):\n",
        "    state = tf.expand_dims(state, axis=1)  # Shape: (5120, 10, 1, 10)\n",
        "    state_expanded = tf.expand_dims(state, axis=1)\n",
        "    rho_1_arrays_expanded = tf.expand_dims(rho_1_arrays, axis=0)  # Shape: (1, 5, 5, 10, 10)\n",
        "    product = state_expanded * rho_1_arrays_expanded  # Shape: (5120, 10, 5, 10, 10)\n",
        "    mat = tf.reduce_sum(product, axis=[-2, -1])  # Shape: (5120, 5, 5)\n",
        "    \n",
        "    return mat\n",
        "\n",
        "def rho_2_tf(state, rho_2_arrays):\n",
        "    state = tf.expand_dims(state, axis=1)  # Shape: (5120, 10, 1, 10)\n",
        "    state_expanded = tf.expand_dims(state, axis=1)\n",
        "    rho_2_arrays_expanded = tf.expand_dims(rho_2_arrays, axis=0)  # Shape: (1, 5, 5, 10, 10)\n",
        "    product = state_expanded * rho_2_arrays_expanded  # Shape: (5120, 10, 5, 10, 10)\n",
        "    mat = tf.reduce_sum(product, axis=[-2, -1])  # Shape: (5120, 5, 5)\n",
        "    \n",
        "    return mat\n",
        "\n",
        "# NOTA: para calcular el bloque rho2kkbar, utilizar en lugar\n",
        "\n",
        "def rho_1_gc_tf(hamil_base):\n",
        "    e, v = tf.linalg.eigh(gen_to_h_1b(hamil_base))\n",
        "    result = 1 / (1 + tf.exp(e))\n",
        "    result = tf.linalg.diag(result)\n",
        "    res = tf.linalg.matmul(v,result)\n",
        "    res = tf.linalg.matmul(res,v,adjoint_b=True)\n",
        "    \n",
        "    return tf.cast(res, tf.float32)\n",
        "\n",
        "# Aux function\n",
        "def outer_product(vector):\n",
        "    return tf.einsum('i,j->ij', vector, vector)\n",
        "\n",
        "def pure_state(h):\n",
        "    e, v = tf.linalg.eigh(h)\n",
        "    fund = v[:,:,0]\n",
        "    d = tf.map_fn(outer_product, fund)\n",
        "    return d\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylpy_BCw6jxF"
      },
      "source": [
        "### Construccion de dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Version sincrónica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2is_Eo_qGpEz",
        "outputId": "9a968190-59f2-4695-ef18-b99ff5b4a212"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-20 16:10:52.386739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /device:GPU:0 with 20609 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/device:GPU:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 293/293 [34:54<00:00,  7.15s/it]\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "# Config\n",
        "num_samples = 300000\n",
        "use_gpu = True\n",
        "gpu_batch_size = 1024\n",
        "\n",
        "# Beta\n",
        "beta = 1\n",
        "\n",
        "# Construccion de parametros y matrices auxiliares\n",
        "rho1_size = m1_basis.size\n",
        "rho2_size = m2_basis.size\n",
        "rho2kkbar_size = basis.m\n",
        "fund_size = basis.size\n",
        "hamil_base_size = basis.d*(basis.d+1)//2\n",
        "rho_1_arrays = rho_1_gen(basis)\n",
        "rho_1_arrays_tf = tf.constant(rho_1_arrays, dtype=tf.float32)\n",
        "rho_2_arrays = rho_2_gen(basis, nm2_basis, m2_basis)\n",
        "rho_2_arrays_tf = tf.constant(rho_2_arrays, dtype=tf.float32)\n",
        "rho_2_arrays_kkbar = rho_2_kkbar_gen(basis.m, rho_2_arrays)\n",
        "rho_2_arrays_kkbar_tf = tf.constant(rho_2_arrays_kkbar, dtype=tf.float32)\n",
        "k_indices = get_kkbar_indices(t_basis)\n",
        "k_indices_tf = gen_update_indices(t_basis, batch_size)\n",
        "\n",
        "# Generacion de hamiltoniano\n",
        "# (h0, hi) = two_body_hamiltonian(t_basis.size, basis.m, np.arange(0, basis.m), np.ones((basis.m,basis.m)), rho_1_arrays_tf, rho_2_arrays_tf) esto es para g cte\n",
        "\n",
        "\n",
        "if use_gpu:\n",
        "    print(tf.test.gpu_device_name())\n",
        "    datasets = []\n",
        "    for i in tqdm(range(num_samples//gpu_batch_size+1)):\n",
        "        size = basis.m*(basis.m+1)//2\n",
        "        # En una primera versión vamos a pasar una mat proporcional a range(0,m) para energias\n",
        "        # y como interacción una matriz G semidefinida positiva\n",
        "        # Primero creamos las semillas, es decir, la diagonal superior de la matrix g\n",
        "        label_size = basis.m*(basis.m+1)//2 # elementos independientes de una mat de m x m\n",
        "        #h_labels = [np.random.uniform(low=0, high=5.0, size=(size,)) for _ in range(0,gpu_batch_size)] # Generamos los generadores\n",
        "        h_labels = [np.random.random()*np.ones(label_size) for _ in range(0,gpu_batch_size)]\n",
        "        h_labels = tf.constant(h_labels, dtype=tf.float32)\n",
        "        # Construimos la mat G\n",
        "        triag = tfp.math.fill_triangular(h_labels, upper=True)\n",
        "        g_arr = triag + tf.transpose(triag, perm=[0,2,1])-tf.linalg.diag(tf.linalg.diag_part(triag))\n",
        "        # Construimos los hamiltonianos basados en g_arr\n",
        "        # TODO: IMPLEMENTAR FUNCION NUEVA\n",
        "        h_arr = two_body_hamiltonian_tf(t_basis, basis.m, en_batch, G_batched, rho_1_arrays, rho_2_arrays, indices_tf)\n",
        "        for i, g in enumerate(g_arr):\n",
        "            (h0, hi) = two_body_hamiltonian(t_basis.size, basis.m, np.arange(0, basis.m), g, rho_1_arrays_tf, rho_2_arrays_tf, k_indices)\n",
        "            h_arr[i,:,:] = h0 - hi\n",
        "        # Estados térmicos\n",
        "        state = thermal_state_tf(h_arr*beta) \n",
        "        state = tf.cast(state, dtype=tf.float32)\n",
        "        # Estados puros\n",
        "        #state = pure_state(h_arr)\n",
        "        #rho_2_input = rho_2_tf(state, rho_2_arrays_tf)\n",
        "        rho_2_input = rho_2_tf(state, rho_2_arrays_kkbar_tf)\n",
        "\n",
        "        datasets.append(tf.data.Dataset.from_tensor_slices(((rho_2_input), h_labels)))\n",
        "        #datasets.append(tf.data.Dataset.from_tensor_slices(((rho_1_input, rho_2_input), h_labels)))\n",
        "        #datasets.append(tf.data.Dataset.from_tensor_slices(((rho_1_input, rho_2_input, state), h_labels)))\n",
        "    ds = tf.data.Dataset.from_tensor_slices(datasets)\n",
        "    dataset = ds.interleave(\n",
        "        lambda x: x,\n",
        "        cycle_length=1,\n",
        "        num_parallel_calls=tf.data.AUTOTUNE,\n",
        "    )\n",
        "\n",
        "\n",
        "#batch_size = 32\n",
        "#dataset = dataset.shuffle(buffer_size=num_samples).batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.ones(size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Filleo de dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "# Save and load dataset\n",
        "save_dataset = False\n",
        "load_dataset = False\n",
        "path = \"/home/agus/TF\"\n",
        "#num_samples = 5000000\n",
        "if save_dataset:\n",
        "    tf.data.Dataset.save(dataset, path)\n",
        "    with open(\"/home/agus/\"+'/file.pkl', 'wb') as file:\n",
        "        pickle.dump(beta_input, file)\n",
        "if load_dataset:\n",
        "    dataset = tf.data.Dataset.load(path)\n",
        "    with open(\"/home/agus/\"+'file.pkl', 'rb') as file:\n",
        "        beta_input = pickle.load(file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "8moZIlfabZuy"
      },
      "outputs": [],
      "source": [
        "# Dividimos los datasets\n",
        "train_size = int(0.8 * num_samples)\n",
        "\n",
        "train_dataset = dataset.take(train_size)\n",
        "val_dataset = dataset.skip(train_size)\n",
        "\n",
        "\n",
        "batch_size = 1024\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "val_dataset = val_dataset.batch(batch_size)\n",
        "\n",
        "#beta_val = beta_input[train_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Dataset Size: -2\n"
          ]
        }
      ],
      "source": [
        "# Cardinality no funciona con los datasets generados por GPU\n",
        "val_size = tf.data.experimental.cardinality(val_dataset).numpy()\n",
        "print(\"Validation Dataset Size:\", val_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYEEjNB-7b8y"
      },
      "source": [
        "### Definición de modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8kkhJr5K0ZQ",
        "outputId": "f1b731f1-6a02-4181-f0b5-5677a2a85784"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rho2 (InputLayer)           [(None, 4, 4, 1)]         0         \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 3, 3, 16)          80        \n",
            "                                                                 \n",
            " batch_normalization_7 (Bat  (None, 3, 3, 16)          64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 144)               0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 32)                4640      \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 10)                330       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5114 (19.98 KB)\n",
            "Trainable params: 5082 (19.85 KB)\n",
            "Non-trainable params: 32 (128.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Definicion de layers basado en Conv 2D\n",
        "\n",
        "# Factor de cantidad de filtros\n",
        "lf = 16 \n",
        "conv_limit = (rho2kkbar_size - 4)\n",
        "initial_dense = (lf*2**(conv_limit-1)*((rho2kkbar_size-(conv_limit-1))//2)**2)\n",
        "## rho 1\n",
        "rho2_layer =  tf.keras.layers.Input(shape=(rho2kkbar_size,rho2kkbar_size, 1), name='rho2')\n",
        "\n",
        "# Procesamos el primer input\n",
        "conv_rho2 = tf.keras.layers.Conv2D(lf*2**conv_limit, (2, 2), activation='relu')(rho2_layer)\n",
        "conv_rho2 = tf.keras.layers.BatchNormalization()(conv_rho2)\n",
        "for j in [(2**conv_limit - 2**k) for k in range(1,conv_limit)]:\n",
        "    conv_rho2 = tf.keras.layers.Conv2D(lf*j, (2, 2), activation='relu')(conv_rho2 if 2**j != 1 else rho1_layer)\n",
        "    conv_rho2 = tf.keras.layers.BatchNormalization()(conv_rho2)\n",
        "\n",
        "#conv_rho2 = tf.keras.layers.MaxPooling2D((2, 2))(conv_rho2)\n",
        "\n",
        "flatten_rho2 = tf.keras.layers.Flatten()(conv_rho2)\n",
        "#flatten_rho1 = tf.keras.layers.Dense(rho1_size*rho1_size, activation='relu')(flatten_rho1)\n",
        "\n",
        "#local_size = basis.size*basis.size\n",
        "local_size = label_size\n",
        "\n",
        "#dense1 = tf.keras.layers.Dense(8*8*4*4, activation='relu')(dense1)\n",
        "#dense1 = tf.keras.layers.Dense(512, activation='relu')(flatten_rho1)\n",
        "#dense1 = tf.keras.layers.Dense(128, activation='relu')(flatten_rho1)\n",
        "dense1 = tf.keras.layers.Dense(initial_dense, activation='relu')(flatten_rho2)\n",
        "#dense1 = tf.keras.layers.Dense(initial_dense//2, activation='relu')(dense1)\n",
        "\n",
        "output = tf.keras.layers.Dense(local_size)(dense1)\n",
        "\n",
        "\n",
        "# Creamos el modelo y compulamos\n",
        "#model = tf.keras.models.Model(inputs=[rho1_layer, rho2_layer, fund_layer], outputs=output)\n",
        "#model = tf.keras.models.Model(inputs=[rho1_layer, rho2_layer], outputs=output)\n",
        "model = tf.keras.models.Model(inputs=[rho2_layer], outputs=output)\n",
        "\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZBtonvGbZuz",
        "outputId": "f197277e-a84b-4ffd-c81f-c81581707fb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_18\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rho2 (InputLayer)           [(None, 4, 4, 1)]         0         \n",
            "                                                                 \n",
            " flatten_18 (Flatten)        (None, 16)                0         \n",
            "                                                                 \n",
            " concatenate_10 (Concatenat  (None, 16)                0         \n",
            " e)                                                              \n",
            "                                                                 \n",
            " dense_57 (Dense)            (None, 128)               2176      \n",
            "                                                                 \n",
            " dense_58 (Dense)            (None, 256)               33024     \n",
            "                                                                 \n",
            " dense_59 (Dense)            (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 37770 (147.54 KB)\n",
            "Trainable params: 37770 (147.54 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Modelo denso + fundamental\n",
        "rho2_layer =  tf.keras.layers.Input(shape=(basis.m,basis.m, 1), name='rho2')\n",
        "flatten_rho2 = tf.keras.layers.Flatten()(rho2_layer)\n",
        "#rho2_layer =  tf.keras.layers.Input(shape=(rho2_size,rho2_size, 1), name='rho2')\n",
        "#flatten_rho2 = tf.keras.layers.Flatten()(rho2_layer)\n",
        "#fund_layer =  tf.keras.layers.Input(shape=(fund_size, fund_size, 1 ), name='fund')\n",
        "#flatten_fund = tf.keras.layers.Flatten()(fund_layer)\n",
        "\n",
        "dense1 = tf.keras.layers.concatenate([flatten_rho2])\n",
        "#dense1 = tf.keras.layers.concatenate([dense1, flatten_fund])\n",
        "#dense1 = tf.keras.layers.Dense(rho1_size*rho1_size, activation='relu')(dense1)\n",
        "\n",
        "local_size = label_size\n",
        "l=3\n",
        "layer_s = [128//i*2 for i in reversed(range(1,l))]\n",
        "for i in range(0,l-1):\n",
        "    dense1 = tf.keras.layers.Dense(layer_s[i], activation='relu')(dense1)\n",
        "\n",
        "output = tf.keras.layers.Dense(local_size)(dense1)\n",
        "# Creamos el modelo y compulamos\n",
        "model = tf.keras.models.Model(inputs=[rho2_layer], outputs=output)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RgoMlCyyfBe-"
      },
      "outputs": [],
      "source": [
        "# LOSS FUNCTIONS\n",
        "r_size = basis.size\n",
        "\n",
        "# Custom loss function based on GS MSE\n",
        "def gs_loss(h_pred, h_true):\n",
        "    h_pred = tf.reshape(h_pred, shape=(-1,r_size,r_size))\n",
        "    e, v = tf.linalg.eigh(h_pred)\n",
        "    gs_pred = v[:, 0]\n",
        "\n",
        "    h_true = tf.reshape(h_true, shape=(-1,r_size,r_size))\n",
        "    e, v = tf.linalg.eigh(h_true)\n",
        "    gs_true = v[:, 0]\n",
        "\n",
        "    gs_diff = tf.norm(gs_true - gs_pred)\n",
        "\n",
        "    return gs_diff + tf.reduce_mean(tf.square(h_true - h_pred)) * 100\n",
        "\n",
        "def distance_to_hermitian(matrix):\n",
        "    hermitian_part = 0.5 * (matrix + tf.linalg.adjoint(matrix))\n",
        "    distance = tf.norm(matrix - hermitian_part, ord='euclidean')\n",
        "    return distance\n",
        "\n",
        "# Custom loss function based on MSE + non-hermitian penalization\n",
        "def herm_loss(h_pred, h_true):\n",
        "    h_pred_arr = tf.reshape(h_pred, shape=(-1,r_size,r_size))\n",
        "    return tf.reduce_mean(tf.square(h_true - h_pred)) + distance_to_hermitian(h_pred_arr)\n",
        "\n",
        "# Custom loss function based on h eigenvalues\n",
        "def eig_loss(h_pred, h_true):\n",
        "    eig_true = tf.sort(tf.math.real(tf.linalg.eigvals(tf.reshape(h_true, (-1, fund_size, fund_size)))))\n",
        "    eig_pred = tf.sort(tf.math.real(tf.linalg.eigvals(tf.reshape(h_pred, (-1, fund_size, fund_size)))))\n",
        "    return tf.reduce_mean(tf.square(eig_true - eig_pred))\n",
        "\n",
        "# MSE with a factor\n",
        "def mse_f(h_pred, h_true):\n",
        "    f = 1000\n",
        "    return tf.reduce_mean(tf.square(h_true - h_pred))*f\n",
        "\n",
        "# Spectral radius loss\n",
        "def spectral_loss(h_pred, h_true):\n",
        "    eig = tf.math.real(tf.linalg.eigvals(tf.reshape(h_true-h_pred, (-1, fund_size, fund_size))))\n",
        "    return tf.math.reduce_max(tf.abs(eig))\n",
        "\n",
        "# Hamiltonian MSE loss (using generators)\n",
        "def base_mse_loss(base_pred, base_true):\n",
        "    h_pred = gen_to_h_tf(base_pred, rho_1_arrays_tf)\n",
        "    h_true = gen_to_h_tf(base_true, rho_1_arrays_tf)\n",
        "    mat = tf.reshape(h_pred-h_true, (-1, fund_size, fund_size))\n",
        "    return tf.norm(mat, ord='fro', axis=[-1, -2])\n",
        "\n",
        "# Custom loss function based on h eigenvalues (using generators)\n",
        "def base_eig_loss(base_pred, base_true):\n",
        "    h_pred = gen_to_h_tf(base_pred, rho_1_arrays_tf)\n",
        "    h_true = gen_to_h_tf(base_true, rho_1_arrays_tf)\n",
        "    eig_true = tf.sort(tf.math.real(tf.linalg.eigvals(tf.reshape(h_true, (-1, fund_size, fund_size)))))\n",
        "    eig_pred = tf.sort(tf.math.real(tf.linalg.eigvals(tf.reshape(h_pred, (-1, fund_size, fund_size)))))\n",
        "    return tf.reduce_mean(tf.square(eig_true - eig_pred))\n",
        "\n",
        "# Custom loss function based on rho1 eigenvals\n",
        "## Auxiliary function\n",
        "def base_to_rho_1_tf(base_pred):\n",
        "    h = gen_to_h_tf(base_pred, rho_1_arrays_tf)\n",
        "    h = tf.reshape(h, (-1, fund_size, fund_size))\n",
        "    state = thermal_state_tf(h)\n",
        "    rho1 = rho_1_tf(state, rho_1_arrays_tf)\n",
        "    return rho1\n",
        "    \n",
        "def rho1_loss(base_pred, base_true):\n",
        "    mat = base_to_rho_1_tf(base_pred) - base_to_rho_1_tf(base_true)\n",
        "    return tf.norm(mat, ord='fro', axis=[-1, -2])\n",
        "\n",
        "# Custom loss function based on rho1 eigenvals (using generators)\n",
        "def base_rho1_loss(base_pred, base_true):\n",
        "    h_pred = gen_to_h_tf(base_pred, rho_1_arrays_tf)\n",
        "    h_true = gen_to_h_tf(base_true, rho_1_arrays_tf)\n",
        "    return tf.reduce_mean(tf.square(rho_1_eig_tf(h_pred) - rho_1_eig_tf(h_true)))*1000\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiWk9piJtNIZ"
      },
      "source": [
        "## Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhJCHf0fQdRl",
        "outputId": "1821cf27-9ff5-4d67-e9f5-956d20eda5e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-20 16:46:40.383224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /device:GPU:0 with 20609 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    231/Unknown - 2s 5ms/step - loss: 0.2246 - accuracy: 0.3441 - mean_squared_error: 0.2246"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-20 16:46:42.630667: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8521845697087371733\n",
            "2023-12-20 16:46:42.630708: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 1103777055225571639\n",
            "2023-12-20 16:46:42.630717: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6787227761116863464\n",
            "2023-12-20 16:46:42.630735: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 16335097799725485596\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "235/235 [==============================] - 4s 13ms/step - loss: 0.2240 - accuracy: 0.3454 - mean_squared_error: 0.2240 - val_loss: 0.1823 - val_accuracy: 0.4082 - val_mean_squared_error: 0.1823\n",
            "Epoch 2/100\n",
            " 31/235 [==>...........................] - ETA: 1s - loss: 0.1792 - accuracy: 0.4095 - mean_squared_error: 0.1792"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-20 16:46:44.334906: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8521845697087371733\n",
            "2023-12-20 16:46:44.334962: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12839889363195875512\n",
            "2023-12-20 16:46:44.334980: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6787227761116863464\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "235/235 [==============================] - 3s 12ms/step - loss: 0.1696 - accuracy: 0.3856 - mean_squared_error: 0.1696 - val_loss: 0.1553 - val_accuracy: 0.3685 - val_mean_squared_error: 0.1553\n",
            "Epoch 3/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.1502 - accuracy: 0.3243 - mean_squared_error: 0.1502 - val_loss: 0.1472 - val_accuracy: 0.3255 - val_mean_squared_error: 0.1472\n",
            "Epoch 4/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.1423 - accuracy: 0.3272 - mean_squared_error: 0.1423 - val_loss: 0.1365 - val_accuracy: 0.3114 - val_mean_squared_error: 0.1365\n",
            "Epoch 5/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.1309 - accuracy: 0.3308 - mean_squared_error: 0.1309 - val_loss: 0.1251 - val_accuracy: 0.3282 - val_mean_squared_error: 0.1251\n",
            "Epoch 6/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.1216 - accuracy: 0.3325 - mean_squared_error: 0.1216 - val_loss: 0.1187 - val_accuracy: 0.3197 - val_mean_squared_error: 0.1187\n",
            "Epoch 7/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.1169 - accuracy: 0.3397 - mean_squared_error: 0.1169 - val_loss: 0.1150 - val_accuracy: 0.3248 - val_mean_squared_error: 0.1150\n",
            "Epoch 8/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.1139 - accuracy: 0.3409 - mean_squared_error: 0.1139 - val_loss: 0.1122 - val_accuracy: 0.3141 - val_mean_squared_error: 0.1122\n",
            "Epoch 9/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.1114 - accuracy: 0.3340 - mean_squared_error: 0.1114 - val_loss: 0.1100 - val_accuracy: 0.3097 - val_mean_squared_error: 0.1100\n",
            "Epoch 10/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.1094 - accuracy: 0.3268 - mean_squared_error: 0.1094 - val_loss: 0.1082 - val_accuracy: 0.3061 - val_mean_squared_error: 0.1082\n",
            "Epoch 11/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.1077 - accuracy: 0.3171 - mean_squared_error: 0.1077 - val_loss: 0.1065 - val_accuracy: 0.2924 - val_mean_squared_error: 0.1065\n",
            "Epoch 12/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.1061 - accuracy: 0.3104 - mean_squared_error: 0.1061 - val_loss: 0.1050 - val_accuracy: 0.2850 - val_mean_squared_error: 0.1050\n",
            "Epoch 13/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.1046 - accuracy: 0.3054 - mean_squared_error: 0.1046 - val_loss: 0.1033 - val_accuracy: 0.2812 - val_mean_squared_error: 0.1033\n",
            "Epoch 14/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.1028 - accuracy: 0.3007 - mean_squared_error: 0.1028 - val_loss: 0.1018 - val_accuracy: 0.2836 - val_mean_squared_error: 0.1018\n",
            "Epoch 15/100\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.1012 - accuracy: 0.2986 - mean_squared_error: 0.1012 - val_loss: 0.1002 - val_accuracy: 0.2856 - val_mean_squared_error: 0.1002\n",
            "Epoch 16/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0997 - accuracy: 0.2964 - mean_squared_error: 0.0997 - val_loss: 0.0987 - val_accuracy: 0.2819 - val_mean_squared_error: 0.0987\n",
            "Epoch 17/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0983 - accuracy: 0.2942 - mean_squared_error: 0.0983 - val_loss: 0.0970 - val_accuracy: 0.2830 - val_mean_squared_error: 0.0970\n",
            "Epoch 18/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0961 - accuracy: 0.2987 - mean_squared_error: 0.0961 - val_loss: 0.0938 - val_accuracy: 0.2852 - val_mean_squared_error: 0.0938\n",
            "Epoch 19/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0926 - accuracy: 0.2919 - mean_squared_error: 0.0926 - val_loss: 0.0911 - val_accuracy: 0.2853 - val_mean_squared_error: 0.0911\n",
            "Epoch 20/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0902 - accuracy: 0.2861 - mean_squared_error: 0.0902 - val_loss: 0.0889 - val_accuracy: 0.2829 - val_mean_squared_error: 0.0889\n",
            "Epoch 21/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0882 - accuracy: 0.2799 - mean_squared_error: 0.0882 - val_loss: 0.0871 - val_accuracy: 0.2656 - val_mean_squared_error: 0.0871\n",
            "Epoch 22/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0865 - accuracy: 0.2706 - mean_squared_error: 0.0865 - val_loss: 0.0856 - val_accuracy: 0.2614 - val_mean_squared_error: 0.0856\n",
            "Epoch 23/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0851 - accuracy: 0.2611 - mean_squared_error: 0.0851 - val_loss: 0.0844 - val_accuracy: 0.2515 - val_mean_squared_error: 0.0844\n",
            "Epoch 24/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0839 - accuracy: 0.2527 - mean_squared_error: 0.0839 - val_loss: 0.0832 - val_accuracy: 0.2462 - val_mean_squared_error: 0.0832\n",
            "Epoch 25/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0829 - accuracy: 0.2465 - mean_squared_error: 0.0829 - val_loss: 0.0823 - val_accuracy: 0.2418 - val_mean_squared_error: 0.0823\n",
            "Epoch 26/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0820 - accuracy: 0.2416 - mean_squared_error: 0.0820 - val_loss: 0.0814 - val_accuracy: 0.2333 - val_mean_squared_error: 0.0814\n",
            "Epoch 27/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0813 - accuracy: 0.2367 - mean_squared_error: 0.0813 - val_loss: 0.0806 - val_accuracy: 0.2243 - val_mean_squared_error: 0.0806\n",
            "Epoch 28/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0806 - accuracy: 0.2321 - mean_squared_error: 0.0806 - val_loss: 0.0800 - val_accuracy: 0.2187 - val_mean_squared_error: 0.0800\n",
            "Epoch 29/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0800 - accuracy: 0.2271 - mean_squared_error: 0.0800 - val_loss: 0.0794 - val_accuracy: 0.2193 - val_mean_squared_error: 0.0794\n",
            "Epoch 30/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0794 - accuracy: 0.2248 - mean_squared_error: 0.0794 - val_loss: 0.0789 - val_accuracy: 0.2134 - val_mean_squared_error: 0.0789\n",
            "Epoch 31/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0789 - accuracy: 0.2232 - mean_squared_error: 0.0789 - val_loss: 0.0783 - val_accuracy: 0.2152 - val_mean_squared_error: 0.0783\n",
            "Epoch 32/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0784 - accuracy: 0.2221 - mean_squared_error: 0.0784 - val_loss: 0.0778 - val_accuracy: 0.2136 - val_mean_squared_error: 0.0778\n",
            "Epoch 33/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0779 - accuracy: 0.2212 - mean_squared_error: 0.0779 - val_loss: 0.0774 - val_accuracy: 0.2161 - val_mean_squared_error: 0.0774\n",
            "Epoch 34/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0775 - accuracy: 0.2209 - mean_squared_error: 0.0775 - val_loss: 0.0770 - val_accuracy: 0.2211 - val_mean_squared_error: 0.0770\n",
            "Epoch 35/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0771 - accuracy: 0.2214 - mean_squared_error: 0.0771 - val_loss: 0.0766 - val_accuracy: 0.2314 - val_mean_squared_error: 0.0766\n",
            "Epoch 36/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0767 - accuracy: 0.2220 - mean_squared_error: 0.0767 - val_loss: 0.0762 - val_accuracy: 0.2293 - val_mean_squared_error: 0.0762\n",
            "Epoch 37/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0763 - accuracy: 0.2229 - mean_squared_error: 0.0763 - val_loss: 0.0758 - val_accuracy: 0.2293 - val_mean_squared_error: 0.0758\n",
            "Epoch 38/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0760 - accuracy: 0.2241 - mean_squared_error: 0.0760 - val_loss: 0.0754 - val_accuracy: 0.2364 - val_mean_squared_error: 0.0754\n",
            "Epoch 39/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0756 - accuracy: 0.2249 - mean_squared_error: 0.0756 - val_loss: 0.0751 - val_accuracy: 0.2451 - val_mean_squared_error: 0.0751\n",
            "Epoch 40/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0753 - accuracy: 0.2256 - mean_squared_error: 0.0753 - val_loss: 0.0747 - val_accuracy: 0.2509 - val_mean_squared_error: 0.0747\n",
            "Epoch 41/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0750 - accuracy: 0.2263 - mean_squared_error: 0.0750 - val_loss: 0.0744 - val_accuracy: 0.2570 - val_mean_squared_error: 0.0744\n",
            "Epoch 42/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0747 - accuracy: 0.2263 - mean_squared_error: 0.0747 - val_loss: 0.0741 - val_accuracy: 0.2631 - val_mean_squared_error: 0.0741\n",
            "Epoch 43/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0744 - accuracy: 0.2271 - mean_squared_error: 0.0744 - val_loss: 0.0738 - val_accuracy: 0.2689 - val_mean_squared_error: 0.0738\n",
            "Epoch 44/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0741 - accuracy: 0.2279 - mean_squared_error: 0.0741 - val_loss: 0.0735 - val_accuracy: 0.2679 - val_mean_squared_error: 0.0735\n",
            "Epoch 45/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0738 - accuracy: 0.2278 - mean_squared_error: 0.0738 - val_loss: 0.0732 - val_accuracy: 0.2825 - val_mean_squared_error: 0.0732\n",
            "Epoch 46/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0735 - accuracy: 0.2269 - mean_squared_error: 0.0735 - val_loss: 0.0729 - val_accuracy: 0.2756 - val_mean_squared_error: 0.0729\n",
            "Epoch 47/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0732 - accuracy: 0.2266 - mean_squared_error: 0.0732 - val_loss: 0.0726 - val_accuracy: 0.2807 - val_mean_squared_error: 0.0726\n",
            "Epoch 48/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0729 - accuracy: 0.2265 - mean_squared_error: 0.0729 - val_loss: 0.0724 - val_accuracy: 0.2769 - val_mean_squared_error: 0.0724\n",
            "Epoch 49/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0726 - accuracy: 0.2269 - mean_squared_error: 0.0726 - val_loss: 0.0721 - val_accuracy: 0.2714 - val_mean_squared_error: 0.0721\n",
            "Epoch 50/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0723 - accuracy: 0.2275 - mean_squared_error: 0.0723 - val_loss: 0.0718 - val_accuracy: 0.2671 - val_mean_squared_error: 0.0718\n",
            "Epoch 51/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0720 - accuracy: 0.2282 - mean_squared_error: 0.0720 - val_loss: 0.0715 - val_accuracy: 0.2608 - val_mean_squared_error: 0.0715\n",
            "Epoch 52/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0718 - accuracy: 0.2286 - mean_squared_error: 0.0718 - val_loss: 0.0713 - val_accuracy: 0.2487 - val_mean_squared_error: 0.0713\n",
            "Epoch 53/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0715 - accuracy: 0.2289 - mean_squared_error: 0.0715 - val_loss: 0.0711 - val_accuracy: 0.2368 - val_mean_squared_error: 0.0711\n",
            "Epoch 54/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0713 - accuracy: 0.2291 - mean_squared_error: 0.0713 - val_loss: 0.0709 - val_accuracy: 0.2255 - val_mean_squared_error: 0.0709\n",
            "Epoch 55/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0710 - accuracy: 0.2295 - mean_squared_error: 0.0710 - val_loss: 0.0707 - val_accuracy: 0.2237 - val_mean_squared_error: 0.0707\n",
            "Epoch 56/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0708 - accuracy: 0.2302 - mean_squared_error: 0.0708 - val_loss: 0.0705 - val_accuracy: 0.2269 - val_mean_squared_error: 0.0705\n",
            "Epoch 57/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0706 - accuracy: 0.2303 - mean_squared_error: 0.0706 - val_loss: 0.0704 - val_accuracy: 0.2307 - val_mean_squared_error: 0.0704\n",
            "Epoch 58/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0703 - accuracy: 0.2309 - mean_squared_error: 0.0703 - val_loss: 0.0702 - val_accuracy: 0.2360 - val_mean_squared_error: 0.0702\n",
            "Epoch 59/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0701 - accuracy: 0.2307 - mean_squared_error: 0.0701 - val_loss: 0.0701 - val_accuracy: 0.2372 - val_mean_squared_error: 0.0701\n",
            "Epoch 60/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0699 - accuracy: 0.2298 - mean_squared_error: 0.0699 - val_loss: 0.0699 - val_accuracy: 0.2427 - val_mean_squared_error: 0.0699\n",
            "Epoch 61/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0697 - accuracy: 0.2294 - mean_squared_error: 0.0697 - val_loss: 0.0698 - val_accuracy: 0.2421 - val_mean_squared_error: 0.0698\n",
            "Epoch 62/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0695 - accuracy: 0.2286 - mean_squared_error: 0.0695 - val_loss: 0.0697 - val_accuracy: 0.2402 - val_mean_squared_error: 0.0697\n",
            "Epoch 63/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0693 - accuracy: 0.2280 - mean_squared_error: 0.0693 - val_loss: 0.0696 - val_accuracy: 0.2446 - val_mean_squared_error: 0.0696\n",
            "Epoch 64/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0692 - accuracy: 0.2270 - mean_squared_error: 0.0692 - val_loss: 0.0694 - val_accuracy: 0.2385 - val_mean_squared_error: 0.0694\n",
            "Epoch 65/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0690 - accuracy: 0.2263 - mean_squared_error: 0.0690 - val_loss: 0.0693 - val_accuracy: 0.2336 - val_mean_squared_error: 0.0693\n",
            "Epoch 66/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0688 - accuracy: 0.2253 - mean_squared_error: 0.0688 - val_loss: 0.0692 - val_accuracy: 0.2241 - val_mean_squared_error: 0.0692\n",
            "Epoch 67/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0686 - accuracy: 0.2243 - mean_squared_error: 0.0686 - val_loss: 0.0690 - val_accuracy: 0.2193 - val_mean_squared_error: 0.0690\n",
            "Epoch 68/100\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0685 - accuracy: 0.2233 - mean_squared_error: 0.0685 - val_loss: 0.0689 - val_accuracy: 0.2179 - val_mean_squared_error: 0.0689\n",
            "Epoch 69/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0683 - accuracy: 0.2226 - mean_squared_error: 0.0683 - val_loss: 0.0687 - val_accuracy: 0.2112 - val_mean_squared_error: 0.0687\n",
            "Epoch 70/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0681 - accuracy: 0.2221 - mean_squared_error: 0.0681 - val_loss: 0.0684 - val_accuracy: 0.2047 - val_mean_squared_error: 0.0684\n",
            "Epoch 71/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0680 - accuracy: 0.2215 - mean_squared_error: 0.0680 - val_loss: 0.0682 - val_accuracy: 0.2039 - val_mean_squared_error: 0.0682\n",
            "Epoch 72/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0678 - accuracy: 0.2201 - mean_squared_error: 0.0678 - val_loss: 0.0680 - val_accuracy: 0.1997 - val_mean_squared_error: 0.0680\n",
            "Epoch 73/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0677 - accuracy: 0.2194 - mean_squared_error: 0.0677 - val_loss: 0.0678 - val_accuracy: 0.1998 - val_mean_squared_error: 0.0678\n",
            "Epoch 74/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0675 - accuracy: 0.2187 - mean_squared_error: 0.0675 - val_loss: 0.0676 - val_accuracy: 0.1959 - val_mean_squared_error: 0.0676\n",
            "Epoch 75/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0674 - accuracy: 0.2178 - mean_squared_error: 0.0674 - val_loss: 0.0674 - val_accuracy: 0.1913 - val_mean_squared_error: 0.0674\n",
            "Epoch 76/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0672 - accuracy: 0.2172 - mean_squared_error: 0.0672 - val_loss: 0.0671 - val_accuracy: 0.1844 - val_mean_squared_error: 0.0671\n",
            "Epoch 77/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0670 - accuracy: 0.2168 - mean_squared_error: 0.0670 - val_loss: 0.0669 - val_accuracy: 0.1783 - val_mean_squared_error: 0.0669\n",
            "Epoch 78/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0669 - accuracy: 0.2161 - mean_squared_error: 0.0669 - val_loss: 0.0667 - val_accuracy: 0.1774 - val_mean_squared_error: 0.0667\n",
            "Epoch 79/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0667 - accuracy: 0.2156 - mean_squared_error: 0.0667 - val_loss: 0.0665 - val_accuracy: 0.1728 - val_mean_squared_error: 0.0665\n",
            "Epoch 80/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0666 - accuracy: 0.2150 - mean_squared_error: 0.0666 - val_loss: 0.0662 - val_accuracy: 0.1688 - val_mean_squared_error: 0.0662\n",
            "Epoch 81/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0664 - accuracy: 0.2140 - mean_squared_error: 0.0664 - val_loss: 0.0661 - val_accuracy: 0.1686 - val_mean_squared_error: 0.0661\n",
            "Epoch 82/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0663 - accuracy: 0.2131 - mean_squared_error: 0.0663 - val_loss: 0.0659 - val_accuracy: 0.1675 - val_mean_squared_error: 0.0659\n",
            "Epoch 83/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0661 - accuracy: 0.2122 - mean_squared_error: 0.0661 - val_loss: 0.0657 - val_accuracy: 0.1626 - val_mean_squared_error: 0.0657\n",
            "Epoch 84/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0660 - accuracy: 0.2110 - mean_squared_error: 0.0660 - val_loss: 0.0655 - val_accuracy: 0.1618 - val_mean_squared_error: 0.0655\n",
            "Epoch 85/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0658 - accuracy: 0.2100 - mean_squared_error: 0.0658 - val_loss: 0.0654 - val_accuracy: 0.1628 - val_mean_squared_error: 0.0654\n",
            "Epoch 86/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0656 - accuracy: 0.2091 - mean_squared_error: 0.0656 - val_loss: 0.0652 - val_accuracy: 0.1610 - val_mean_squared_error: 0.0652\n",
            "Epoch 87/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0655 - accuracy: 0.2080 - mean_squared_error: 0.0655 - val_loss: 0.0650 - val_accuracy: 0.1571 - val_mean_squared_error: 0.0650\n",
            "Epoch 88/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0653 - accuracy: 0.2071 - mean_squared_error: 0.0653 - val_loss: 0.0649 - val_accuracy: 0.1553 - val_mean_squared_error: 0.0649\n",
            "Epoch 89/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0652 - accuracy: 0.2060 - mean_squared_error: 0.0652 - val_loss: 0.0647 - val_accuracy: 0.1470 - val_mean_squared_error: 0.0647\n",
            "Epoch 90/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0650 - accuracy: 0.2052 - mean_squared_error: 0.0650 - val_loss: 0.0646 - val_accuracy: 0.1458 - val_mean_squared_error: 0.0646\n",
            "Epoch 91/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0648 - accuracy: 0.2047 - mean_squared_error: 0.0648 - val_loss: 0.0644 - val_accuracy: 0.1440 - val_mean_squared_error: 0.0644\n",
            "Epoch 92/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0647 - accuracy: 0.2040 - mean_squared_error: 0.0647 - val_loss: 0.0643 - val_accuracy: 0.1397 - val_mean_squared_error: 0.0643\n",
            "Epoch 93/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0645 - accuracy: 0.2030 - mean_squared_error: 0.0645 - val_loss: 0.0641 - val_accuracy: 0.1385 - val_mean_squared_error: 0.0641\n",
            "Epoch 94/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0643 - accuracy: 0.2026 - mean_squared_error: 0.0643 - val_loss: 0.0639 - val_accuracy: 0.1423 - val_mean_squared_error: 0.0639\n",
            "Epoch 95/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0642 - accuracy: 0.2022 - mean_squared_error: 0.0642 - val_loss: 0.0638 - val_accuracy: 0.1417 - val_mean_squared_error: 0.0638\n",
            "Epoch 96/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0640 - accuracy: 0.2020 - mean_squared_error: 0.0640 - val_loss: 0.0636 - val_accuracy: 0.1372 - val_mean_squared_error: 0.0636\n",
            "Epoch 97/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0638 - accuracy: 0.2014 - mean_squared_error: 0.0638 - val_loss: 0.0635 - val_accuracy: 0.1340 - val_mean_squared_error: 0.0635\n",
            "Epoch 98/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0636 - accuracy: 0.2001 - mean_squared_error: 0.0636 - val_loss: 0.0633 - val_accuracy: 0.1340 - val_mean_squared_error: 0.0633\n",
            "Epoch 99/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0635 - accuracy: 0.1994 - mean_squared_error: 0.0635 - val_loss: 0.0631 - val_accuracy: 0.1340 - val_mean_squared_error: 0.0631\n",
            "Epoch 100/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0633 - accuracy: 0.1988 - mean_squared_error: 0.0633 - val_loss: 0.0630 - val_accuracy: 0.1323 - val_mean_squared_error: 0.0630\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.optimizers import RMSprop, Adam, Nadam, Lion\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(),\n",
        "              loss='MSE',  \n",
        "              metrics=['accuracy', 'mean_squared_error'])\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 100\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "    history = model.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset)\n",
        "\n",
        "# Dense: 1.3\n",
        "# CNN: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "cvpE_X1iTXcB",
        "outputId": "eff0e5f5-5b26-46ea-ec6b-491d1de9944c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "#plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.plot(history.history['mean_squared_error'], label='Training MSE')\n",
        "plt.plot(history.history['val_mean_squared_error'], label='Validation MSE')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE')\n",
        "plt.title('MSE vs. Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# MIN LOSS = 0.0128 c/fund 50epochs MSE\n",
        "##         = 0.0118 s/fund 50epochs MSE\n",
        "##         = 0.0039 s/fund 50epochs MSE m=4 d=6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRlZuRUNa6Yb",
        "outputId": "85850559-311b-4cf4-ea5b-465a9ee8a7af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 0s 1ms/step\n"
          ]
        }
      ],
      "source": [
        "# Assuming you have a validation dataset (val_dataset)\n",
        "iterador = iter(val_dataset)\n",
        "sample = next(iterador)\n",
        "next_sample = next(iterador)\n",
        "input_data = sample[0]  # Assuming your dataset provides input data as the first element\n",
        "actual_values = sample[1]  # Assuming your dataset provides actual labels as the second element\n",
        "\n",
        "# Predict using the model\n",
        "predictions = model.predict(input_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([0. 0. 1. 0. 1. 1. 0. 1. 1. 1.], shape=(10,), dtype=float32)\n",
            "[-1.0790788e-02  4.7974849e-01  8.6542016e-01  7.4158609e-04\n",
            "  1.0478625e+00  1.0147773e+00  6.1628558e-02  1.2347273e+00\n",
            "  4.0650189e-01  1.1814189e+00]\n",
            "tf.Tensor([1. 0. 1. 0. 0. 0. 1. 0. 0. 0.], shape=(10,), dtype=float32)\n",
            "[ 0.98520124  0.30480784  0.4500829  -0.01747341  0.0344529  -0.00542959\n",
            "  0.86083186 -0.07224789  0.08322321 -0.03209463]\n",
            "tf.Tensor([1. 0. 1. 0. 1. 0. 0. 1. 0. 0.], shape=(10,), dtype=float32)\n",
            "[ 0.97317886  0.17325012  0.29977566  0.00215195  0.9480612   0.00985862\n",
            "  0.00584213  0.69356465  0.5739334  -0.15534937]\n",
            "tf.Tensor([1. 1. 1. 1. 0. 0. 0. 1. 0. 1.], shape=(10,), dtype=float32)\n",
            "[ 0.98055416  0.5345487   0.57909846  1.015525    0.2701311   0.01956952\n",
            " -0.05438101  0.6424998   0.42673802  0.98851514]\n",
            "0.2024261 0.7038722\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Vemos algunos valores\n",
        "for e in val_dataset:\n",
        "    for i in range(0, 4):\n",
        "        print(e[1][i])\n",
        "        print(predictions[i])\n",
        "    break\n",
        "    \n",
        "\n",
        "RMSE_pred = mean_squared_error(actual_values, predictions, squared=False)\n",
        "RMSE_rand = mean_squared_error(actual_values, next_sample[1], squared=False)\n",
        "print(RMSE_pred, RMSE_rand)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "ds5iD1OMbZu3"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "cannot reshape array of size 2 into shape (1,1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb Cell 33\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb#Y146sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m val_dataset:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb#Y146sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m \u001b[39mif\u001b[39;00m printear \u001b[39melse\u001b[39;00m batch_size):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb#Y146sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         \u001b[39m# Valores actuales\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb#Y146sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39m#h = e[1][i].numpy().reshape(basis.size,basis.size)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb#Y146sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         h_true \u001b[39m=\u001b[39m gen_to_h(e[\u001b[39m1\u001b[39;49m][i], rho_1_arrays)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb#Y146sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         \u001b[39m#print(h) if printear else 0\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb#Y146sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         r \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39meigvals(e[\u001b[39m0\u001b[39m][i]))\n",
            "\u001b[1;32m/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb Cell 33\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb#Y146sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgen_to_h\u001b[39m(base, rho_1_arrays):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb#Y146sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     triag \u001b[39m=\u001b[39m fill_triangular_np(base)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb#Y146sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     body_gen \u001b[39m=\u001b[39m triag \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39mtranspose(triag)\u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39mdiag(np\u001b[39m.\u001b[39mdiag(triag))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb#Y146sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     h \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(base_hamiltonian(body_gen, basis, rho_1_arrays))  \n",
            "\u001b[1;32m/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb Cell 33\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb#Y146sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m n \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mint32(np\u001b[39m.\u001b[39msqrt(\u001b[39m.25\u001b[39m \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m m) \u001b[39m-\u001b[39m \u001b[39m.5\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb#Y146sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m x_tail \u001b[39m=\u001b[39m x[(m \u001b[39m-\u001b[39m (n\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m-\u001b[39m m)):]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/agus/Escritorio/UNLP/FermionicML/FermionicML_thermal_2body.ipynb#Y146sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mtriu(np\u001b[39m.\u001b[39;49mconcatenate([x, x_tail[::\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]], \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mreshape(n, n))\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 2 into shape (1,1)"
          ]
        }
      ],
      "source": [
        "m_size = basis.size\n",
        "rho_1_pred = []\n",
        "rho_1_actual = []\n",
        "norm = []\n",
        "norm_rand = []\n",
        "printear =  False\n",
        "\n",
        "for e in val_dataset:\n",
        "    for i in range(0, 3 if printear else batch_size):\n",
        "        # Valores actuales\n",
        "        #h = e[1][i].numpy().reshape(basis.size,basis.size)\n",
        "        h_true = gen_to_h(e[1][i], rho_1_arrays)\n",
        "        #print(h) if printear else 0\n",
        "        r = max(np.linalg.eigvals(e[0][i]))\n",
        "        rho_1_actual.append(r)\n",
        "\n",
        "        print(h_true) if printear else 0\n",
        "        print(r) if printear else 0\n",
        "\n",
        "        # Valores predichos\n",
        "        #h = predictions[i].reshape(basis.size,basis.size)\n",
        "        h_pred = gen_to_h(predictions[i], rho_1_arrays)\n",
        "        beta = 1\n",
        "        # Estado térmico\n",
        "        state = thermal_state(h_pred, beta)\n",
        "        # Estado puro\n",
        "        #state = pure_state(h_pred)\n",
        "        rho1 = np.array(rho_1(basis.d, state, rho_1_arrays))\n",
        "        r = max(np.sort(linalg_d.eigvals(rho1).real))\n",
        "        rho_1_pred.append(r)\n",
        "\n",
        "        print(h_pred) if printear else 0\n",
        "        print(r) if printear else 0\n",
        "        \n",
        "\n",
        "        # Normas\n",
        "        norm.append(np.linalg.norm(h_true-h_pred, ord='fro'))\n",
        "        print(f'Norma {norm[-1]}') if printear else 0\n",
        "        ## Vamos a comparar con un h aleatorio\n",
        "        size = basis.d*(basis.d+1)//2\n",
        "        base = np.random.uniform(low=0, high=1.0, size=(size,))\n",
        "        h_rand = gen_to_h(base, rho_1_arrays)\n",
        "        norm_rand.append(np.linalg.norm(h_true-h_rand, ord='fro'))\n",
        "        #print(f'Norma random {norm_rand[-1]}') if printear else 0\n",
        "        print('') if printear else 0\n",
        "        \n",
        "\n",
        "\n",
        "    # e contiene todo el batch y nos basta con uno\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(e[1][10])\n",
        "predictions[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "AL2EC9Ci-0HG",
        "outputId": "545ebe57-d3de-490f-f076-709d5c47b5f2"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "f=1\n",
        "rho_1_actual = np.array(rho_1_actual)\n",
        "rho_1_pred = np.array(rho_1_pred)\n",
        "#print(mean_squared_error(rho_1_pred, rho_1_actual))\n",
        "\n",
        "print('Rho1 based statistics')\n",
        "print(np.mean(np.abs(rho_1_actual-rho_1_pred)))\n",
        "print(np.mean(rho_1_actual)*f)\n",
        "print('std')\n",
        "print(np.std(rho_1_actual-rho_1_pred)*f)\n",
        "print(np.std(rho_1_actual)*f)\n",
        "print(np.std(rho_1_pred)*f)\n",
        "plt.hist(np.array(rho_1_pred-rho_1_actual), bins=50)\n",
        "plt.show()\n",
        "print('H based statistics')\n",
        "print(np.mean(norm), np.mean(norm_rand))\n",
        "print(np.mean(norm_rand)/np.mean(norm))\n",
        "\n",
        "\n",
        "# BEST: FACTOR 1/8 c/fund\n",
        "## 500 epochs, 10M dataset\n",
        "# BEST: FACTOR 1/9 s/fund\n",
        "## 50 epochs, 5M dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "6.25/1.30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 25 epochs d = m*2\n",
        "res = {}\n",
        "res[5] = 35/8.19 \n",
        "res[4] = 15/2.47\n",
        "res[3] = 6.2/1.73"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YioVllOX3M1N",
        "outputId": "b7715c37-1400-4c04-8be3-dd247b4b9db9"
      },
      "outputs": [],
      "source": [
        "# Get the weights of all dense layers in the model\n",
        "dense_weights = []\n",
        "for layer in model.layers:\n",
        "    if isinstance(layer, tf.keras.layers.Dense):\n",
        "        weights = layer.get_weights()\n",
        "        if len(weights) > 0:\n",
        "            dense_weights.append(weights[0])\n",
        "\n",
        "# Visualize the weights of each dense layer\n",
        "for i, weights in enumerate(dense_weights):\n",
        "    plt.figure()\n",
        "    plt.imshow(weights, cmap='viridis', aspect='auto')\n",
        "    plt.colorbar()\n",
        "    plt.title(f\"Dense Layer {i+1} Weights Visualization\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rho_2_arrays[9,0,0,9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 0 0 0 1 1] [0 1 1 0 0 0]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Genera las matrices de rho2\n",
        "def rho_2_gen(basis, mll_basis, t_basis):\n",
        "    size = t_basis.size\n",
        "    s = basis.size\n",
        "    # La entrada i, j contiene C_j^\\dag C_i    i, j \\in t_basis\n",
        "    mat = np.empty((size, size, s, s), dtype=np.float32)\n",
        "    for i, v in enumerate(t_basis.base):\n",
        "        for j, w in enumerate(t_basis.base):\n",
        "            c_i = b_gen(basis, mll_basis, rep_to_exi(v))\n",
        "            cdag_j = bd_gen(mll_basis, basis, rep_to_exi(w))\n",
        "            mat[i, j, :, :] = np.dot(cdag_j, c_i)\n",
        "            if mat[i,j,0,9] != 0:\n",
        "                print(v,w)\n",
        "\n",
        "    return mat\n",
        "\n",
        "r = rho_2_gen(basis, basis_m2, t_basis)\n",
        "r[9,0,0,9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "print(b_gen(basis, mll_basis, rep_to_exi([0,0,0,1,1,0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 1, 1, 0, 0, 0],\n",
              "       [1, 1, 0, 1, 0, 0],\n",
              "       [1, 1, 0, 0, 1, 0],\n",
              "       [1, 1, 0, 0, 0, 1],\n",
              "       [1, 0, 1, 1, 0, 0],\n",
              "       [1, 0, 1, 0, 1, 0],\n",
              "       [1, 0, 1, 0, 0, 1],\n",
              "       [1, 0, 0, 1, 1, 0],\n",
              "       [1, 0, 0, 1, 0, 1],\n",
              "       [1, 0, 0, 0, 1, 1],\n",
              "       [0, 1, 1, 1, 0, 0],\n",
              "       [0, 1, 1, 0, 1, 0],\n",
              "       [0, 1, 1, 0, 0, 1],\n",
              "       [0, 1, 0, 1, 1, 0],\n",
              "       [0, 1, 0, 1, 0, 1],\n",
              "       [0, 1, 0, 0, 1, 1],\n",
              "       [0, 0, 1, 1, 1, 0],\n",
              "       [0, 0, 1, 1, 0, 1],\n",
              "       [0, 0, 1, 0, 1, 1],\n",
              "       [0, 0, 0, 1, 1, 1]])"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "basis.base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(6, 20)\n",
            "[array([0, 1, 0, 1, 1, 0])] [0 1 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "col = 1\n",
        "b = b_gen(basis, mll_basis, rep_to_exi([0,0,0,1,1,0]))\n",
        "print(b.shape)\n",
        "for x in range(0,b.shape[1]):\n",
        "    if b[col,x] != 0:\n",
        "        ind = x\n",
        "        break\n",
        "else:\n",
        "    ind = NaN\n",
        "\n",
        "print([basis.base[ind]], mll_basis.base[col])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "m = 3\n",
        "d = 2*m\n",
        "basis = fixed_basis(m, d)\n",
        "t_basis = fixed_basis(2, d)\n",
        "basis_m1 = fixed_basis(m-1, d)\n",
        "basis_m2 = fixed_basis(m-2, d)\n",
        "ml_basis = basis_m1\n",
        "mll_basis = basis_m2\n",
        "rho_1_arrays = rho_1_gen(basis)\n",
        "rho_2_arrays = rho_2_gen(basis, mll_basis, t_basis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t_basis = fixed_basis(2, d)\n",
        "mll_basis = fixed_basis(basis.m-2, d)\n",
        "rho_1_arrays = rho_1_gen(basis)\n",
        "rho_2_arrays = rho_2_gen(basis, mll_basis, t_basis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def two_body_hamiltonian(t_basis_size, m, energy, G, rho_1_arrays, rho_2_arrays):\n",
        "    # Creamos la mat diagonal de d*d con los elementos de energy\n",
        "    # cada uno de estos, se contraen con los elementos de rho_1_arrays\n",
        "    # la mat energy contiene las energias de cada termino c^\\dag_k c_k para k kbar (iguales)\n",
        "    # por ello los elementos se repiten \n",
        "    energy_matrix = np.diagflat(np.kron(energy, np.ones(2)))\n",
        "    \n",
        "    # Construimos la mat de energía\n",
        "    h0 = np.sum(energy_matrix[:, :, np.newaxis, np.newaxis] * rho_1_arrays[:, :, :, :], axis=(0, 1))\n",
        "\n",
        "    # Pasamos ahora a la matrix de interacción con la misma estrategia\n",
        "    # dada G que indica la interacción entre los pares k' k'bar k kbar \n",
        "    # (que son elementos particulares de t_basis)\n",
        "    # transladamos estos coeficientes a una matriz en t_basis\n",
        "    # y multiplicamos por rho_2_arrays\n",
        "    \n",
        "    # Primero determinamos, dada t_basis, cuales son los indices de pares kkbar\n",
        "    offset = 4*m+1\n",
        "    indices = [-(k-1)*(2*k-offset) for k in range(1,m+1)] # Lo saque de Mathmatica, vos confia\n",
        "    i, j = np.meshgrid(indices, indices, indexing='ij') # Lo usamos para rellenar la mat deseada\n",
        "    \n",
        "    # Contruimos la mat que contraeremos con rho_2_arrays\n",
        "    mat = np.zeros((t_basis_size, t_basis_size))\n",
        "    mat[i, j] = G\n",
        "\n",
        "    hi = -np.sum(mat[:, :, np.newaxis, np.newaxis] * rho_2_arrays[:, :, :, :], axis=(0, 1))\n",
        "\n",
        "    return (h0, hi)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(h02,hi2) = two_body_hamiltonian(t_basis.size, m, [0,1,2], np.ones((3,3)), rho_1_arrays, rho_2_arrays)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[[[1., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 1., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 1., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [1., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 1., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [1., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 1., 0., ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[1., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [1., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 1., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 1., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "       ...,\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 1.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., ..., 1., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 1.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 1., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 1.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 1., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., ..., 0., 1., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 1.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 1., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 1., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 1.]]]], dtype=float32)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rho_2_arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0\n"
          ]
        }
      ],
      "source": [
        "print(rho_2_arrays[9,0,0,9])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "rho_1_arrays = rho_1_gen(basis)\n",
        "\n",
        "A = np.array([0, 1, 2])  # Your list with d elements\n",
        "\n",
        "# Create a diagonal matrix with each element repeated twice\n",
        "result_matrix = np.diagflat(np.kron(A, np.ones(2)))\n",
        "\n",
        "print(result_matrix)\n",
        "np.kron(A, np.ones(2))\n",
        "\n",
        "mat = np.zeros((basis.size, basis.size))\n",
        "for i in range(0,2*d):\n",
        "    for j in range(0, 2*d):\n",
        "        mat += result_matrix[i,j] * rho_1_arrays[i,j,:,:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mat = np.sum(result_matrix[:, :, np.newaxis, np.newaxis] * rho_1_arrays[:, :, :, :], axis=(0, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "h0 == mat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1 1 0 0 0 0]\n",
            "[0 0 1 1 0 0]\n",
            "[0 0 0 0 1 1]\n",
            "[0, 9, 14]\n",
            "[0, 9, 14]\n"
          ]
        }
      ],
      "source": [
        "d = 3\n",
        "t_basis = fixed_basis(2, 2*d)\n",
        "basis = fixed_basis(d, 2*d)\n",
        "size = t_basis.size\n",
        "#basis = fixed_basis(d, 2*d)\n",
        "diag_elem = []\n",
        "for x in t_basis.base:\n",
        "    if all([x[i] == x[i+1] for i in range(0, 2*d, 2)]):\n",
        "        print(x)\n",
        "        diag_elem.append(t_basis.rep_to_index(x))\n",
        "\n",
        "print(diag_elem)\n",
        "# Veamos el GALERAZO de Wolfram\n",
        "n = 4*d+1\n",
        "print([-(k-1)*(2*k-n) for k in range(1,d+1)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "m2_basis = fixed_basis(2, d)\n",
        "print(m2_basis.size)\n",
        "nm2_basis = fixed_basis(basis.m-2, d)\n",
        "print(nm2_basis.base)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "W = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
        "index = [0,9,14]\n",
        "mat = np.zeros((size,size))\n",
        "for i in range(0,3):\n",
        "    for j in range(0,3):\n",
        "        mat[index[i], index[j]] = W[i,j]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "rho_2_arrays = rho_2_gen(basis, nm2_basis, m2_basis)\n",
        "#rho_2_arrays_tf = tf.constant(rho_2_arrays, dtype=tf.float32)\n",
        "\n",
        "W = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "W = np.ones((3,3))\n",
        "index = [0, 9, 14]\n",
        "size = 15  # Assuming size is the size of the matrix\n",
        "\n",
        "# Create a meshgrid of indices\n",
        "i, j = np.meshgrid(index, index, indexing='ij')\n",
        "\n",
        "# Use the meshgrid indices to assign values from W to the specified positions in mat\n",
        "mat = np.zeros((size, size))\n",
        "mat[i, j] = W\n",
        "\n",
        "# La mat... mat corresponde a los coeficientes en t_basis\n",
        "inte = np.zeros((basis.size, basis.size))\n",
        "for i in range(0, t_basis.size):\n",
        "    for j in range(0, t_basis.size):\n",
        "        inte += - mat[i, j] * rho_2_arrays[i,j,:,:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inte == hi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "m = 3\n",
        "basis = fixed_basis(m, d)\n",
        "basis_m1 = fixed_basis(m-1, d)\n",
        "basis_m2 = fixed_basis(m-2, d)\n",
        "\n",
        "from numba import njit\n",
        "\n",
        "# Parametros hamiltoniano\n",
        "e = 1\n",
        "eps = 0\n",
        "e0 = np.zeros(2*d)\n",
        "eigenspace_tol = 0.0001\n",
        "for k in range(0, d):\n",
        "    r = random.random() * eps * 0\n",
        "    e0[2*k] = k*e+r\n",
        "    e0[2*k+1] = k*e+r\n",
        "\n",
        "@njit(parallel=True)\n",
        "def base_hamiltonian_aux(basis, size, d, basis_m1, basis_m2):\n",
        "    # Construccion de H\n",
        "    d = d//2\n",
        "    h0 = np.zeros((size,size), dtype=np.float32)\n",
        "    for k in prange(0,2*d):\n",
        "        h0 += e0[k] * np.dot(bd_aux(basis_m1, basis, k),b_aux(basis, basis_m1, k))\n",
        "    hi = np.zeros((size, size), dtype=np.float32)\n",
        "    for k in prange(0,d):\n",
        "        for kb in prange(0,d):\n",
        "            bd_terms = np.dot(bd_aux(basis_m1, basis, 2*k),bd_aux(basis_m2, basis_m1, 2*k+1))\n",
        "            b_terms = np.dot(b_aux(basis_m1, basis_m2, 2*kb+1),b_aux(basis, basis_m1, 2*kb))\n",
        "            hi += -1*np.dot(bd_terms,b_terms)\n",
        "\n",
        "    return (h0, hi)\n",
        "\n",
        "def base_hamiltonian(basis, basis_m1, basis_m2):\n",
        "    return base_hamiltonian_aux(basis.base, basis.size, basis.d, basis_m1.base, basis_m2.base)\n",
        "\n",
        "h0, hi = base_hamiltonian(basis, basis_m1, basis_m2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "oapxWkD16fHg"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
