{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aguschanchu/FermionicML/blob/main/FermionicML_thermal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXz5cOlVwrzZ"
      },
      "source": [
        "# FermionicML:\n",
        "\n",
        "Code based on aguschanchu/Bosonic.py\n",
        "\n",
        "A diferencia del código anterior, este modelo trabaja sobre estados térmicos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLD2Yai55rMm"
      },
      "source": [
        "## Código base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgf9ExZN4jA7"
      },
      "source": [
        "Cargamos el código de Bosonic.py básico, branch fermionic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Gydz4kCH4l5w"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/tmp/ipykernel_2942/2785245815.py:319: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
            "  def gamma_lamba_inv(x):\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.special import binom\n",
        "from scipy.sparse import dok_matrix, linalg\n",
        "from scipy import linalg as linalg_d\n",
        "from joblib import Memory\n",
        "import random\n",
        "import plotly.graph_objects as go\n",
        "from joblib import Parallel, delayed\n",
        "from numba import jit, prange, njit\n",
        "import numba as nb\n",
        "import pickle\n",
        "import math\n",
        "import tensorflow_probability as tfp\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from itertools import combinations\n",
        "\n",
        "\n",
        "# Funciones auxiliares optimiadas\n",
        "@nb.jit(nopython=True, parallel=True)\n",
        "def int_to_tuple_arr(ni,nf, b, digits=None):\n",
        "    sol = np.zeros((nf-ni, digits), dtype=np.int64)\n",
        "    for n in prange(ni, nf):\n",
        "        r = np.zeros(digits, dtype=np.int64)\n",
        "        ncop = n\n",
        "        idx = 0\n",
        "        while n != 0:\n",
        "            r[idx] = n % b\n",
        "            n = n // b\n",
        "            idx += 1\n",
        "        if digits is not None:\n",
        "            if idx < digits:\n",
        "                for i in range(idx, digits):\n",
        "                    r[i] = 0\n",
        "                idx = digits\n",
        "        sol[ncop-ni,:] = r[:idx]\n",
        "    return sol\n",
        "\n",
        "def tuple_to_int(t, d):\n",
        "    b = d-1\n",
        "    l = len(t)\n",
        "    s = [t[k]*b**(l-k-1) for k in range(0,l)]\n",
        "    return sum(s)\n",
        "\n",
        "def create_basis_(m, d, size):\n",
        "    base = []\n",
        "    index = 0\n",
        "    chunk_size = 1000000\n",
        "    for x in range(0,(m+1)**d, chunk_size):\n",
        "        start_index = x\n",
        "        end_index = min(x + chunk_size, (m+1)**d)\n",
        "        arr = int_to_tuple_arr(start_index, end_index, m+1, d)\n",
        "        sums = np.sum(arr, axis=1)\n",
        "        rows = np.where(sums == m)[0]\n",
        "        for row in [arr[i] for i in rows]:\n",
        "            if np.all(np.logical_or(row == 0, row == 1)):\n",
        "                base.append(row)\n",
        "\n",
        "    # Como consecuencia de la paralelizacion, es necesario reordenar la base\n",
        "    sorted_base = sorted(base, key=lambda x: tuple_to_int(x, d), reverse=True)\n",
        "    assert len(base) == size\n",
        "\n",
        "    return sorted_base\n",
        "\n",
        "def custom_base_representation_tf(n_min, n_max, base, num_digits):\n",
        "    # Generate a range of numbers from n_min to n_max\n",
        "    numbers = tf.range(n_min, n_max + 1, dtype=tf.int64)\n",
        "    \n",
        "    # Calculate the digits in the custom base using broadcasting\n",
        "    digits = tf.pow(tf.cast(base, dtype=tf.float64), tf.cast(tf.range(num_digits), dtype=tf.float64))\n",
        "    \n",
        "    # Reshape the digits to [1, num_digits] for broadcasting\n",
        "    digits = tf.reshape(digits, [1, -1])\n",
        "    \n",
        "    # Reshape numbers to [batch_size, 1]\n",
        "    numbers = tf.reshape(tf.cast(numbers, dtype=tf.float64), [-1, 1])\n",
        "    \n",
        "    # Calculate the digits in the custom base for each number using broadcasting\n",
        "    result = tf.cast(tf.math.floormod(tf.math.floordiv(numbers, digits), base), dtype=tf.int32)\n",
        "    \n",
        "    # Pad the result to have exactly num_digits columns\n",
        "    result = tf.pad(result, paddings=[[0, 0], [0, num_digits - tf.shape(result)[1]]], constant_values=0)\n",
        "    \n",
        "    # Reverse the order of columns\n",
        "    #result = tf.reverse(result, axis=[1])\n",
        "\n",
        "    return result\n",
        "\n",
        "def select_rows_with_sum(arr, m):\n",
        "    # Create a mask based on the criteria\n",
        "    mask = tf.reduce_all(tf.math.logical_or(tf.equal(arr, 0), tf.equal(arr, 1)), axis=1) & (tf.reduce_sum(arr, axis=1) == m)\n",
        "    \n",
        "    # Use the mask to select the rows\n",
        "    result = tf.boolean_mask(arr, mask, axis=0)\n",
        "    \n",
        "    return result\n",
        "\n",
        "def create_basis_tf_(m, d):\n",
        "    base = []\n",
        "    index = 0\n",
        "    chunk_size = 10000000\n",
        "    for x in tqdm(range(0,(m+1)**d, chunk_size)):\n",
        "        start_index = x\n",
        "        end_index = min(x + chunk_size, (m+1)**d)\n",
        "        res = custom_base_representation_tf(start_index, end_index, m+1, d)\n",
        "        arr = select_rows_with_sum(res, m)\n",
        "        base.append(arr.numpy())\n",
        "\n",
        "    return np.concatenate(base)\n",
        "\n",
        "def create_fermionic_base_(m, d):\n",
        "    indices = list(range(d))\n",
        "    combinations_list = list(combinations(indices, m))\n",
        "    \n",
        "    vectors = []\n",
        "    for combo in combinations_list:\n",
        "        vector = [1 if i in combo else 0 for i in indices]\n",
        "        vectors.append(vector)\n",
        "    \n",
        "    return vectors\n",
        "\n",
        "# Dada una base, devuelve los vectores que estan dados de a pares\n",
        "def get_kkbar_indices_(base):\n",
        "    indices = []\n",
        "    for i, v in enumerate(base):\n",
        "        if np.all(v[::2] == v[1::2]):\n",
        "            indices.append(i)\n",
        "    return indices\n",
        "\n",
        "class fixed_basis:\n",
        "\n",
        "    # Convierte a un enterno n a su escritura en base b\n",
        "    def _int_to_tuple(self, n, b, digits = None):\n",
        "        rep = np.base_repr(n, b)\n",
        "        rep_int = [int(x,b) for x in rep]\n",
        "        if digits is not None:\n",
        "            zeros = [0 for i in range(0,digits-len(rep))]\n",
        "            return zeros + rep_int\n",
        "        else:\n",
        "            return rep_int\n",
        "\n",
        "    # Revierte la transformacion anterior\n",
        "    def tuple_to_int(self, t):\n",
        "        b = self.d-1\n",
        "        l = len(t)\n",
        "        s = [t[k]*b**(l-k-1) for k in range(0,l)]\n",
        "        return sum(s)\n",
        "\n",
        "    # Convierte el vector en su representacion\n",
        "    def vect_to_repr(self, vect):\n",
        "        for i, k in enumerate(vect):\n",
        "            if k == 1. or k == 1:\n",
        "                break\n",
        "        else:\n",
        "            return 0\n",
        "        return self.base[i,:]\n",
        "\n",
        "    def rep_to_vect(self, rep):\n",
        "        rep = list(rep)\n",
        "        for i, r in [(j, self.base[j,:]) for j in range(0,self.size)]:\n",
        "            if list(r) == rep:\n",
        "                return self.canonicals[:,i]\n",
        "        else:\n",
        "            None\n",
        "\n",
        "    def rep_to_index(self, rep):\n",
        "        try:\n",
        "            return self.base.tolist().index(list(rep))\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    @staticmethod\n",
        "    def rep_to_exi(rep):\n",
        "        r = []\n",
        "        for i, k in enumerate(rep):\n",
        "            r += [i for x in range(0,k)]\n",
        "        return r\n",
        "\n",
        "    # Crea base de M particulas en D estados (repr y base canonica)\n",
        "    def create_basis(self, m, d, pairs = False):\n",
        "        #print(\"Creating basis: \", m, d)\n",
        "        #base = np.array(create_basis_tf_(m, d)) CASO GENERICO\n",
        "        base = np.array(create_fermionic_base_(m,d)) # UNICAMENTE FERMIONICO\n",
        "        if pairs:\n",
        "            base = base[get_kkbar_indices_(base)]\n",
        "        length = base.shape[0]\n",
        "        # Asignamos a cada uno de ellos un canónico\n",
        "        canonicals = np.eye(length)\n",
        "        return base, canonicals\n",
        "    \n",
        "    def __init__(self, m, d, pairs = False):\n",
        "        self.m = m\n",
        "        self.d = d\n",
        "        (self.base, self.canonicals) = self.create_basis(m, d, pairs)\n",
        "        self.size = self.base.shape[0]\n",
        "\n",
        "# Matrices de aniquilación y creación endomórficas. Estan fuera de la clase para poder ser cacheadas\n",
        "#@memory.cache\n",
        "def bdb(basis, i, j):\n",
        "    mat = dok_matrix((basis.size, basis.size), dtype=np.float32)\n",
        "    if i != j:\n",
        "        for k, v in enumerate(basis.base):\n",
        "            if v[j] != 0 and v[i] != 1:\n",
        "                print(v)\n",
        "                dest = list(v.copy())\n",
        "                dest[j] -= 1\n",
        "                dest[i] += 1\n",
        "                tar = basis.rep_to_index(dest)\n",
        "                if tar is None:\n",
        "                    pass\n",
        "                else:\n",
        "                    mat[tar, k] = np.sqrt(v[i]+1)*np.sqrt(v[j])\n",
        "    else:\n",
        "        for k, v in enumerate(basis.base):\n",
        "            if v[j] != 0:\n",
        "                mat[k, k] = v[i] \n",
        "    return mat\n",
        "\n",
        "#@memory.cache\n",
        "def bbd(basis, i, j):\n",
        "    mat = dok_matrix((basis.size, basis.size), dtype=np.float32)\n",
        "    if i != j:\n",
        "        for k, v in enumerate(basis.base):\n",
        "            if v[i] != 0 and v[j] != 1:\n",
        "                dest = list(v.copy())\n",
        "                dest[i] -= 1\n",
        "                dest[j] += 1\n",
        "                tar = basis.rep_to_index(dest)\n",
        "                mat[tar, k] = np.sqrt(v[j]+1)*np.sqrt(v[i])\n",
        "    else:\n",
        "        for k, v in enumerate(basis.base):\n",
        "            if v[i] != 1:\n",
        "                mat[k, k] = v[i]+1\n",
        "    return mat\n",
        "\n",
        "# Matrices de aniquilación y creación.Toman la base de origen y destino (basis_o, basis_d) resp\n",
        "#@nb.jit(nopython=True, parallel=True)\n",
        "@nb.jit(nopython=True)\n",
        "def b_aux(basis_o, basis_d, i):\n",
        "    mat = np.zeros((len(basis_d), len(basis_o)), dtype=np.float32)\n",
        "    for k in prange(len(basis_o)):\n",
        "        if basis_o[k][i] != 0:\n",
        "            dest = list(basis_o[k].copy())\n",
        "            dest[i] -= 1\n",
        "            for j in prange(len(basis_d)):\n",
        "                if list(basis_d[j]) == dest:\n",
        "                    tar = j\n",
        "                    mat[tar, k] = np.sqrt(basis_o[k][i])\n",
        "    return mat\n",
        "\n",
        "def b(basis_o, basis_d, i):\n",
        "    return b_aux(basis_o.base, basis_d.base, i)\n",
        "\n",
        "#@nb.jit(nopython=True, parallel=True)\n",
        "@nb.jit(nopython=True)\n",
        "def bd_aux(basis_o, basis_d, i):\n",
        "    mat = np.zeros((len(basis_d), len(basis_o)), dtype=np.float32)\n",
        "    for k in prange(len(basis_o)):\n",
        "        if basis_o[k][i] != 1:\n",
        "            dest = list(basis_o[k].copy())\n",
        "            dest[i] += 1\n",
        "            for j in prange(len(basis_d)):\n",
        "                if list(basis_d[j]) == dest:\n",
        "                    tar = j\n",
        "                    mat[tar, k] = np.sqrt(basis_o[k][i]+1)\n",
        "    return mat\n",
        "\n",
        "def bd(basis_o, basis_d, i):\n",
        "    return bd_aux(basis_o.base, basis_d.base, i)\n",
        "\n",
        "\n",
        "# Acepta una lista de indices a crear\n",
        "@nb.jit(nopython=True, parallel=True)\n",
        "def bd_gen_aux(basis_o, basis_d, gen_list):\n",
        "    mat = np.zeros((len(basis_d), len(basis_o)), dtype=np.float32)\n",
        "    for k in prange(len(basis_o)):\n",
        "        conds = np.zeros(len(gen_list), dtype=np.int64)\n",
        "        for i in range(len(gen_list)):\n",
        "            if basis_o[k][gen_list[i]] != 1:\n",
        "                conds[i] = 1\n",
        "        if np.all(conds):\n",
        "            dest = list(basis_o[k].copy())\n",
        "            for i in gen_list:\n",
        "                dest[i] += 1\n",
        "            for j in prange(len(basis_d)):\n",
        "                if list(basis_d[j]) == dest:\n",
        "                    tar = j\n",
        "                    mat[tar, k] = np.sqrt(basis_o[k][i]+1)\n",
        "    return mat\n",
        "\n",
        "def bd_gen(basis_o, basis_d, i):\n",
        "    return bd_gen_aux(basis_o.base, basis_d.base, np.array(i))\n",
        "\n",
        "def b_gen(basis_o, basis_d, i):\n",
        "    return np.transpose(bd_gen(basis_d, basis_o, i))\n",
        "\n",
        "# Volvemos a definir la función para compilarla\n",
        "@nb.jit(forceobj=True)\n",
        "def _rep_to_index(base, rep):\n",
        "    return base.tolist().index(list(rep))\n",
        "\n",
        "# Funciones auxiliares para calcular rho2kkbar y gamma_p\n",
        "@nb.jit(nopython=True)\n",
        "def rep_to_exi(rep):\n",
        "    r = []\n",
        "    for i in range(len(rep)):\n",
        "        for j in range(rep[i]):\n",
        "            r.append(i)\n",
        "    return r\n",
        "\n",
        "@nb.njit\n",
        "def factorial(n):\n",
        "    result = 1\n",
        "    for i in range(1, n + 1):\n",
        "        result *= i\n",
        "    return result\n",
        "\n",
        "@nb.njit\n",
        "def gamma_lamba(x):\n",
        "    res = 1.0\n",
        "    for o in x:\n",
        "        res *= math.sqrt(factorial(o))\n",
        "    return res\n",
        "\n",
        "@nb.jit\n",
        "def gamma_lamba_inv(x):\n",
        "    res = 1.0\n",
        "    for o in x:\n",
        "        res *= 1.0 / np.sqrt(factorial(o))\n",
        "    return res\n",
        "\n",
        "@nb.njit\n",
        "def rep_to_index_np(base, rep):\n",
        "    for i in range(len(base)):\n",
        "        if np.all(base[i] == rep):\n",
        "            return i\n",
        "    return -1\n",
        "\n",
        "\n",
        "def gamma_p(basis, m, vect, m_basis = None, nm_basis = None):\n",
        "    d = basis.d\n",
        "    if not m_basis or not nm_basis:\n",
        "        m_basis = fixed_basis(m, d)\n",
        "        nm_basis = fixed_basis(basis.m-m,d)\n",
        "    return gamma_p_aux(basis.base, vect, m_basis.base, nm_basis.base)\n",
        "\n",
        "@nb.njit()\n",
        "def gamma_p_aux(basis, vect, m_basis, nm_basis):\n",
        "    mat = np.zeros((len(m_basis), len(nm_basis)), dtype=np.float32)\n",
        "    for i in prange(len(m_basis)):\n",
        "        v = m_basis[i]\n",
        "        for j in prange(len(nm_basis)):\n",
        "            w = nm_basis[j]\n",
        "            targ = v + w\n",
        "            index = rep_to_index_np(basis, targ)\n",
        "            if index != -1:\n",
        "                coef = vect[index]\n",
        "                if coef != 0:\n",
        "                    coef = coef * gamma_lamba_inv(v) * gamma_lamba_inv(w) * gamma_lamba(targ)\n",
        "                mat[i, j] = coef\n",
        "    return mat\n",
        "# Devuelve la matriz rho M asociada al vector\n",
        "def rho_m(basis, m, vect, m_basis = None, nm_basis = None):\n",
        "    g = gamma_p(basis, m, vect, m_basis, nm_basis)\n",
        "    return np.dot(g,np.transpose(g))\n",
        "\n",
        "# Devuelve la matriz gamma asociada a la descomposición (M,N-M) del vector\n",
        "@jit(forceobj=True)\n",
        "def gamma(basis, m, vect, m_basis = None, nm_basis = None):\n",
        "    d = basis.d\n",
        "    if not m_basis or not nm_basis:\n",
        "        m_basis = fixed_basis(m, d)\n",
        "        nm_basis = fixed_basis(basis.m-m,d)\n",
        "    mat = dok_matrix((m_basis.size, nm_basis.size), dtype=np.float32)\n",
        "    for i, v in enumerate(m_basis.base):\n",
        "        for j, w in enumerate(nm_basis.base):\n",
        "            targ = v+w\n",
        "            # Revisamos que sea un estado fermionico valido\n",
        "            arr = np.asarray(targ)\n",
        "            if not np.all(np.logical_or(arr == 0, arr == 1)):\n",
        "                continue\n",
        "            index = _rep_to_index(basis.base, targ)\n",
        "            coef = vect[index]\n",
        "            if coef != 0:\n",
        "                aux = lambda x: np.prod(np.reciprocal(np.sqrt([np.math.factorial(o) for o in x])))\n",
        "                aux_inv = lambda x: np.prod(np.sqrt([np.math.factorial(o) for o in x]))\n",
        "                coef = coef * aux(v) * aux(w) * aux_inv(targ)\n",
        "                #coef = coef\n",
        "                #print(v,w,coef)\n",
        "            mat[i,j] = coef\n",
        "    return mat\n",
        "\n",
        "# Genera las matrices de rho1\n",
        "def rho_1_gen(basis):\n",
        "    d = basis.d\n",
        "    s = basis.size\n",
        "    mat = np.empty((d,d,s,s), dtype=np.float32)\n",
        "    for i in range(0, d):\n",
        "        for j in range(0, d):\n",
        "            mat[i,j,:,:] = np.array(bdb(basis,j, i).todense())\n",
        "    return mat\n",
        "\n",
        "#@jit(parallel=True, nopython=True)\n",
        "def rho_1(d, state, rho_1_arrays):\n",
        "    state_expanded = state[np.newaxis, np.newaxis, :, :]\n",
        "    product = state_expanded * rho_1_arrays\n",
        "    mat = np.sum(product, axis=(-2, -1))\n",
        "\n",
        "    return mat\n",
        "\n",
        "\n",
        "# Genera las matrices de rho2\n",
        "def rho_2_gen(basis, mll_basis, t_basis):\n",
        "    size = t_basis.size\n",
        "    s = basis.size\n",
        "    # La entrada i, j contiene C_j^\\dag C_i    i, j \\in t_basis\n",
        "    mat = np.empty((size, size, s, s), dtype=np.float32)\n",
        "    for i, v in enumerate(t_basis.base):\n",
        "        for j, w in enumerate(t_basis.base):\n",
        "            c_i = b_gen(basis, mll_basis, rep_to_exi(v))\n",
        "            cdag_j = bd_gen(mll_basis, basis, rep_to_exi(w))\n",
        "            mat[i, j, :, :] = np.dot(cdag_j, c_i)\n",
        "\n",
        "    return mat\n",
        "\n",
        "def rho_2(size, state, rho_2_arrays):\n",
        "    state_expanded = np.expand_dims(state, axis=1)\n",
        "    state_expanded = np.expand_dims(state_expanded, axis=1)\n",
        "    rho_2_arrays = rho_2_arrays[np.newaxis, :, :, :, :]\n",
        "    print(state_expanded.shape, rho_2_arrays.shape)\n",
        "    product = state_expanded * rho_2_arrays\n",
        "    mat = np.sum(product, axis=(-2, -1))\n",
        "    return mat\n",
        "\n",
        "def get_kkbar_indices(t_basis):\n",
        "    indices = []\n",
        "    for i, v in enumerate(t_basis.base):\n",
        "        if np.all(v[::2] == v[1::2]):\n",
        "            indices.append(i)\n",
        "    return indices\n",
        "\n",
        "def rho_2_kkbar_gen(t_basis, rho_2_arrays):\n",
        "    indices = get_kkbar_indices(t_basis)\n",
        "    i, j = np.meshgrid(indices, indices, indexing='ij') # Lo usamos para rellenar la mat deseada\n",
        "\n",
        "    rho_2_arrays_kkbar = rho_2_arrays[i, j, :, :]\n",
        "\n",
        "    return rho_2_arrays_kkbar\n",
        "\n",
        "# Devuelve la matriz rho 2 asociada al bloque kkbar\n",
        "def rho_2_kkbar(basis, vect, ml_basis = None, mll_basis = None, t_basis = None):\n",
        "    d = basis.d\n",
        "    # Creo las bases si no están dadas\n",
        "    if ml_basis == None or mll_basis == None or t_basis == None:\n",
        "        ml_basis = fixed_basis(m-1,d)\n",
        "        mll_basis = fixed_basis(m-2,d)\n",
        "        t_basis = fixed_basis(2,d)\n",
        "    diag = []\n",
        "    for v in t_basis.base:\n",
        "        for j in range(0, d, 2):\n",
        "            if v[j] == v[j+1]:\n",
        "                continue\n",
        "            else:\n",
        "                break\n",
        "        else:\n",
        "            diag.append(v)\n",
        "    diag = np.array(diag)\n",
        "    return rho_2_kkbar_aux(diag, vect, basis.base, ml_basis.base, mll_basis.base, t_basis.base)\n",
        "\n",
        "@nb.njit\n",
        "def rho_2_kkbar_lambda(x):\n",
        "    res = 1.0\n",
        "    for o in x:\n",
        "        res *= 1.0 / math.sqrt(factorial(o))\n",
        "    return res\n",
        "\n",
        "#@nb.njit(parallel=True)\n",
        "def rho_2_kkbar_aux(diag, vect, basis, ml_basis, mll_basis, t_basis):\n",
        "    mat = np.zeros((len(diag), len(diag)), dtype=np.float32)\n",
        "    for i in prange(len(diag)):\n",
        "        for j in prange(len(diag)):\n",
        "            v = diag[i]\n",
        "            w = diag[j]\n",
        "            # Creacion de los a\n",
        "            i_set = rep_to_exi(v)\n",
        "            b_m = b_aux(ml_basis, mll_basis, i_set[1]) @ b_aux(basis, ml_basis, i_set[0])\n",
        "            # Creacion de los ad\n",
        "            i_set = rep_to_exi(w)\n",
        "            bd_m = bd_aux(ml_basis, basis, i_set[1]) @ bd_aux(mll_basis, ml_basis, i_set[0])\n",
        "            # v1 = vect @ bd_m @ b_m @ vect Para estados puros\n",
        "            # Mult de b's y filleo de mat\n",
        "            coef = np.trace(vect @ bd_m @ b_m)\n",
        "            mat[i,j] = coef * rho_2_kkbar_lambda(v) * rho_2_kkbar_lambda(w)\n",
        "    return mat\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dga5Xx_5vDf"
      },
      "source": [
        "## Definicion de Hamiltoniano"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myiTq53L5E1U"
      },
      "source": [
        "Cargamos el código de creación y resolución de Hamiltonianos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h5FXWv849Mq",
        "outputId": "49dd47b5-8c16-4ad4-92e7-e172462229b3"
      },
      "outputs": [],
      "source": [
        "m = 6\n",
        "d = 12\n",
        "# Creo las bases para no tener que recrearlas luego\n",
        "basis = fixed_basis(m, d)\n",
        "basis_m1 = fixed_basis(m-1, d)\n",
        "basis_m2 = fixed_basis(m-2, d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(924, 12)"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_kkbar_indices(t_basis):\n",
        "    indices = []\n",
        "    for i, v in enumerate(t_basis.base):\n",
        "        if np.all(v[::2] == v[1::2]):\n",
        "            indices.append(i)\n",
        "    return indices\n",
        "\n",
        "i = get_kkbar_indices(basis)\n",
        "print(len(i))\n",
        "basis.base.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PToiSs915TXw"
      },
      "outputs": [],
      "source": [
        "## Usamos este approach si queremos guardar los generadores\n",
        "# Dados 1/2 (d^2+d) elementos, genera una mat de dxd:\n",
        "eps = 0.00001\n",
        "\n",
        "def sym_mat_gen(vect, d):\n",
        "    matrix = fill_matrix(vect, d)\n",
        "    return matrix + matrix.T - np.diag(matrix.diagonal())\n",
        "\n",
        "@jit(nopython=True)\n",
        "def fill_matrix(vect, d):\n",
        "    matrix = np.zeros((d, d))\n",
        "    idx = 0\n",
        "    for i in prange(d):\n",
        "        for j in prange(i, d):\n",
        "            matrix[i, j] = vect[idx]\n",
        "            idx += 1\n",
        "    return matrix\n",
        "\n",
        "# Generamos una matrix aleatoria. Cuidado con la distribución, ver https://stackoverflow.com/questions/56605189/is-there-an-efficient-way-to-generate-a-symmetric-random-matrix\n",
        "def hamil_base_gen(d):\n",
        "    U = np.random.uniform(low=0, high=1.0, size=(d, d))\n",
        "    hamil_base = np.tril(U) + np.tril(U, -1).T\n",
        "    return hamil_base\n",
        "\n",
        "# Dada un a mat dxd simetrica, contruye el hamiltoniano de un cuerpo a_{ij} c^{dag}_i c_j\n",
        "# Alternativamente podemos construirlo a partir de rho_1_gen\n",
        "def base_hamiltonian_aux(mat, size, d, rho_1_gen):\n",
        "    # Construccion de H\n",
        "    rho_1_gen_transposed = rho_1_gen.transpose(1, 0, 2, 3)\n",
        "    mat_expanded = mat[:, :, np.newaxis, np.newaxis]\n",
        "    h = np.sum(mat_expanded * rho_1_gen_transposed[:, :, :, :], axis=(0, 1))\n",
        "    return h.astype(np.float32)\n",
        "\n",
        "def base_hamiltonian(mat, basis, rho_1_gen):\n",
        "    return base_hamiltonian_aux(mat, basis.size, basis.d, rho_1_gen)\n",
        "\n",
        "def two_body_hamiltonian(t_basis_size, m, energy, G, rho_1_arrays, rho_2_arrays, indices):\n",
        "    # Creamos la mat diagonal de d*d con los elementos de energy\n",
        "    # cada uno de estos, se contraen con los elementos de rho_1_arrays\n",
        "    # la mat energy contiene las energias de cada termino c^\\dag_k c_k para k kbar (iguales)\n",
        "    # por ello los elementos se repiten \n",
        "    energy_matrix = np.diagflat(np.kron(energy, np.ones(2))) + eps * np.random.random((2*m,2*m))\n",
        "    \n",
        "    # Construimos la mat de energía\n",
        "    rho_1_arrays_t = tf.transpose(rho_1_arrays,perm=[1, 0, 2, 3])\n",
        "    h0 = np.sum(energy_matrix[:, :, np.newaxis, np.newaxis] * rho_1_arrays_t[:, :, :, :], axis=(0, 1))\n",
        "\n",
        "    # Pasamos ahora a la matrix de interacción con la misma estrategia\n",
        "    # dada G que indica la interacción entre los pares k' k'bar k kbar \n",
        "    # (que son elementos particulares de t_basis)\n",
        "    # transladamos estos coeficientes a una matriz en t_basis\n",
        "    # y multiplicamos por rho_2_arrays\n",
        "    \n",
        "    # Primero determinamos, dada t_basis, cuales son los indices de pares kkbar\n",
        "    i, j = np.meshgrid(indices, indices, indexing='ij') # Lo usamos para rellenar la mat deseada\n",
        "    rho_2_arrays_t = tf.transpose(rho_2_arrays,perm=[1, 0, 2, 3])\n",
        "\n",
        "    # Contruimos la mat que contraeremos con rho_2_arrays\n",
        "    mat = np.zeros((t_basis_size, t_basis_size))\n",
        "    mat[i, j] = G\n",
        "    hi = np.sum(mat[:, :, np.newaxis, np.newaxis] * rho_2_arrays_t[:, :, :, :], axis=(0, 1))\n",
        "    return (h0, hi)\n",
        "\n",
        "def solve(h, last_step = None):\n",
        "    sol = linalg.eigsh(h, which='SA',k=19)\n",
        "    eigenspace_tol = 0.0001\n",
        "    if type(last_step) != type(None):\n",
        "        # Seleccionamos todos los autovects que difieren sus autovalores menos que tol (mismo autoespacio)\n",
        "        # y tomamos la proyección en el autoespacio de la solución del paso anterior (last_step)\n",
        "        eig = sol[0].real\n",
        "        eigv = sol[1]\n",
        "        cand = [eigv[:,i].real  for (i, x) in enumerate(eig) if abs(x-min(eig)) < eigenspace_tol]\n",
        "        cand_norm = [x/np.linalg.norm(x) for x in cand]\n",
        "        fund = np.zeros(len(cand[0]))\n",
        "        for x in cand_norm:\n",
        "            fund += np.dot(last_step,x) * x\n",
        "    else:\n",
        "        argmin = np.argmin(sol[0].real)\n",
        "        fund = sol[1][:,argmin]\n",
        "    fund = fund.real / np.linalg.norm(fund)\n",
        "    return fund\n",
        "\n",
        "# Generacion de H basada en TF\n",
        "\n",
        "# Funciones auxiliares de gen de H basado en TF\n",
        "## Dada matrix de indices, genera los indices de updates de TF\n",
        "def gen_update_indices(t_basis, batch_size):\n",
        "    # Calculamos los indices de kkbar en t_basis\n",
        "    indices = tf.constant(get_kkbar_indices(t_basis))\n",
        "    # Creamos el array de indices x indices\n",
        "    i, j = tf.meshgrid(indices, indices, indexing='ij')\n",
        "    matrix = tf.reshape(tf.stack([i, j], axis=-1), (-1, 2))\n",
        "\n",
        "    # Repeat the matrix along the first axis (axis=0) 'b' times\n",
        "    repeated_matrix = tf.repeat(tf.expand_dims(matrix, axis=0), repeats=batch_size, axis=0)\n",
        "\n",
        "    # Create an index array from 0 to b-1\n",
        "    indices = tf.range(batch_size, dtype=tf.int32)\n",
        "\n",
        "    # Expand the index array to have the same shape as the repeated matrix\n",
        "    indices = tf.expand_dims(indices, axis=-1)\n",
        "    indices = tf.expand_dims(indices, axis=-1)\n",
        "    indices = tf.tile(indices, multiples=[1,matrix.shape[0],1]) \n",
        "\n",
        "    # Concatenate the index array to the repeated matrix along a new axis\n",
        "    tiled_matrix = tf.concat([indices, repeated_matrix], axis=-1)\n",
        "    tiled_matrix = tf.reshape(tiled_matrix, [-1,3])\n",
        "    return tiled_matrix\n",
        "\n",
        "\n",
        "def two_body_hamiltonian_tf(t_basis, m, energy_batch, G_batched, rho_1_arrays, rho_2_arrays, indices):\n",
        "    # SECCIÓN ENERGIAS\n",
        "    ## Dado un batch de niveles, lo pasamos a TF\n",
        "    energy_matrix = tf.constant(energy_batch, dtype=tf.float32)\n",
        "    ## Repetimos los niveles para cada uno de los pares (por el nivel k y kbar)\n",
        "    energy_matrix = tf.repeat(energy_matrix, repeats=2, axis=1)\n",
        "    ## Generamos la matrix diagonal y expandimos\n",
        "    energy_matrix_expanded = tf.linalg.diag(energy_matrix)\n",
        "    energy_matrix_expanded = energy_matrix_expanded[:, :, :, np.newaxis, np.newaxis]\n",
        "    rho_1_gen_transposed = tf.transpose(rho_1_arrays, perm=[1, 0, 2, 3])\n",
        "    # Multiplicamos por los operadores C^dag C\n",
        "    h0_arr = tf.reduce_sum(energy_matrix_expanded * rho_1_gen_transposed[np.newaxis,:,:,:,:], axis=[1,2])\n",
        "\n",
        "    # SECCIÓN INTERACCIÓN\n",
        "    # Ya tenemos los indices de updates, ahora tomamos la mat en t_basis (una de zeros)\n",
        "    # y updateamos de acuerdo a la lista de G's cada uno flatteneados\n",
        "    G_flatten = np.ndarray.flatten(np.array([np.ndarray.flatten(G) for G in G_batched]))\n",
        "    # Creamos la mat de t_basis y updateamos a partir de los indices de kkbar\n",
        "    mat = tf.zeros((len(energy_batch), t_basis.size, t_basis.size), dtype=tf.float32)\n",
        "    mat = tf.tensor_scatter_nd_update(mat, indices, G_flatten)\n",
        "    # Preparamos las dimensiones y multiplicamos\n",
        "    mat_expanded = mat[:, :, :, np.newaxis, np.newaxis]\n",
        "    rho_2_gen_transposed = tf.transpose(rho_2_arrays, perm=[1, 0, 2, 3])\n",
        "    hi_arr = tf.reduce_sum(mat_expanded * rho_2_gen_transposed[np.newaxis,:,:,:,:], axis=[1,2])\n",
        "\n",
        "    return h0_arr - hi_arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emVBTg2QD-Fg"
      },
      "source": [
        "## Modelo de ML\n",
        "Basado en matrices densidad de 1 y 2 cuerpos como input, con hamiltoniano como salida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aF_Ec_mCGX96"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-03 16:24:40.407227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /device:GPU:0 with 20609 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tf.test.gpu_device_name()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJDoa6LUJJ8O",
        "outputId": "73481454-fbcb-469f-d72f-cd0f8d534808"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 134.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "28\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 148.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1 1 0 0 0 0 0 0]\n",
            " [1 0 1 0 0 0 0 0]\n",
            " [0 1 1 0 0 0 0 0]\n",
            " [1 0 0 1 0 0 0 0]\n",
            " [0 1 0 1 0 0 0 0]\n",
            " [0 0 1 1 0 0 0 0]\n",
            " [1 0 0 0 1 0 0 0]\n",
            " [0 1 0 0 1 0 0 0]\n",
            " [0 0 1 0 1 0 0 0]\n",
            " [0 0 0 1 1 0 0 0]\n",
            " [1 0 0 0 0 1 0 0]\n",
            " [0 1 0 0 0 1 0 0]\n",
            " [0 0 1 0 0 1 0 0]\n",
            " [0 0 0 1 0 1 0 0]\n",
            " [0 0 0 0 1 1 0 0]\n",
            " [1 0 0 0 0 0 1 0]\n",
            " [0 1 0 0 0 0 1 0]\n",
            " [0 0 1 0 0 0 1 0]\n",
            " [0 0 0 1 0 0 1 0]\n",
            " [0 0 0 0 1 0 1 0]\n",
            " [0 0 0 0 0 1 1 0]\n",
            " [1 0 0 0 0 0 0 1]\n",
            " [0 1 0 0 0 0 0 1]\n",
            " [0 0 1 0 0 0 0 1]\n",
            " [0 0 0 1 0 0 0 1]\n",
            " [0 0 0 0 1 0 0 1]\n",
            " [0 0 0 0 0 1 0 1]\n",
            " [0 0 0 0 0 0 1 1]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 148.78it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 150.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n",
            "[[1 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 1]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 122.73it/s]\n"
          ]
        }
      ],
      "source": [
        "# Construccion de bases para calculo de rho1 y rho2\n",
        "# rho2\n",
        "m = 2\n",
        "m2_basis = fixed_basis(m, d)\n",
        "print(m2_basis.size)\n",
        "nm2_basis = fixed_basis(basis.m-m, d)\n",
        "print(nm2_basis.base)\n",
        "t_basis = fixed_basis(2, basis.d)\n",
        "# rho1\n",
        "m = 1\n",
        "m1_basis = fixed_basis(m, d)\n",
        "print(m1_basis.size)\n",
        "print(m1_basis.base)\n",
        "nm1_basis = fixed_basis(basis.m-m, d)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oapxWkD16fHg"
      },
      "source": [
        "### Algunos benchmarks y funciones auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "umCIrxCZKXQd"
      },
      "outputs": [],
      "source": [
        "# Given h calculo en rho2 y rho1 máximo\n",
        "def rho1_rho2(h, beta):\n",
        "    fund = thermal_state(h, beta)\n",
        "    rho2 = np.array(rho_2(basis, m2_basis.size, state, rho_2_arrays))\n",
        "    r = np.sort(linalg_d.eigvals(rho2).real)\n",
        "    rho_2_max = r[0]\n",
        "    rho1 = np.array(rho_1(basis, state, rho_1_arrays))\n",
        "    r = np.sort(linalg_d.eigvals(rho1).real)\n",
        "    rho_1_max = r[0]\n",
        "\n",
        "    return (rho_1_max, rho_2_max)\n",
        "\n",
        "def fill_triangular_np(x):\n",
        "    m = x.shape[0]\n",
        "    n = np.int32(np.sqrt(.25 + 2 * m) - .5)\n",
        "    x_tail = x[(m - (n**2 - m)):]\n",
        "    return np.triu(np.concatenate([x, x_tail[::-1]], 0).reshape(n, n))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "QaNnIIc5bZux"
      },
      "outputs": [],
      "source": [
        "# TEST: Las funciones de TF y comunes coinciden\n",
        "\n",
        "# Dado h, \\beta, construyo el estado térmico\n",
        "from scipy.linalg import expm\n",
        "\n",
        "def thermal_state(h, beta):\n",
        "    quotient = expm(-beta*h)\n",
        "    return quotient / np.trace(quotient)\n",
        "\n",
        "## NO usar para mat no hermiticas\n",
        "@nb.jit(nopython=True)\n",
        "def thermal_state_eig(h, beta):\n",
        "    w, v = np.linalg.eigh(-beta*h)\n",
        "    D = np.diag(np.exp(w))\n",
        "    mat = v @ D @ v.T\n",
        "    mat = mat / np.trace(mat)\n",
        "    return mat\n",
        "    \n",
        "def gen_to_h(base, rho_1_arrays):\n",
        "    triag = fill_triangular_np(base)\n",
        "    body_gen = triag + np.transpose(triag)-np.diag(np.diag(triag))\n",
        "    h = np.array(base_hamiltonian(body_gen, basis, rho_1_arrays))  \n",
        "    return h \n",
        "\n",
        "def gen_to_h_1b(hamil_base):\n",
        "    triag = tfp.math.fill_triangular(hamil_base, upper=True)\n",
        "    body_gen = triag + tf.transpose(triag, perm=[0,2,1])-tf.linalg.diag(tf.linalg.diag_part(triag))\n",
        "    return body_gen\n",
        "\n",
        "def gen_to_h_tf(hamil_base, rho_1_arrays):\n",
        "    triag = tfp.math.fill_triangular(hamil_base, upper=True)\n",
        "    body_gen = triag + tf.transpose(triag, perm=[0,2,1])-tf.linalg.diag(tf.linalg.diag_part(triag)) # Simetrizamos y generamos la matriz de h\n",
        "    hamil_expanded = body_gen[:, :, :, np.newaxis, np.newaxis]\n",
        "    rho_1_gen_transposed = tf.transpose(rho_1_arrays, perm=[1, 0, 2, 3])\n",
        "    h_arr = tf.reduce_sum(hamil_expanded * rho_1_gen_transposed[np.newaxis,:,:,:,:], axis=[1,2])\n",
        "    return h_arr\n",
        "\n",
        "def thermal_state_tf(h):\n",
        "    # Assume beta=1\n",
        "    exp_hamiltonian = tf.linalg.expm(-h)\n",
        "    partition_function = tf.linalg.trace(exp_hamiltonian)\n",
        "    partition_function = tf.expand_dims(partition_function, axis=1)\n",
        "    partition_function = tf.expand_dims(partition_function, axis=1)\n",
        "    \n",
        "    rho = exp_hamiltonian / partition_function\n",
        "\n",
        "    return rho\n",
        "\n",
        "def rho_1_tf(state, rho_1_arrays):\n",
        "    state = tf.expand_dims(state, axis=1)  # Shape: (5120, 10, 1, 10)\n",
        "    state_expanded = tf.expand_dims(state, axis=1)\n",
        "    rho_1_arrays_expanded = tf.expand_dims(rho_1_arrays, axis=0)  # Shape: (1, 5, 5, 10, 10)\n",
        "    product = state_expanded * rho_1_arrays_expanded  # Shape: (5120, 10, 5, 10, 10)\n",
        "    mat = tf.reduce_sum(product, axis=[-2, -1])  # Shape: (5120, 5, 5)\n",
        "    \n",
        "    return mat\n",
        "\n",
        "def rho_2_tf(state, rho_2_arrays):\n",
        "    state = tf.expand_dims(state, axis=1)  # Shape: (5120, 10, 1, 10)\n",
        "    state_expanded = tf.expand_dims(state, axis=1)\n",
        "    rho_2_arrays_expanded = tf.expand_dims(rho_2_arrays, axis=0)  # Shape: (1, 5, 5, 10, 10)\n",
        "    product = state_expanded * rho_2_arrays_expanded  # Shape: (5120, 10, 5, 10, 10)\n",
        "    mat = tf.reduce_sum(product, axis=[-2, -1])  # Shape: (5120, 5, 5)\n",
        "    \n",
        "    return mat\n",
        "\n",
        "# NOTA: para calcular el bloque rho2kkbar, utilizar en lugar\n",
        "\n",
        "def rho_1_gc_tf(hamil_base):\n",
        "    e, v = tf.linalg.eigh(gen_to_h_1b(hamil_base))\n",
        "    result = 1 / (1 + tf.exp(e))\n",
        "    result = tf.linalg.diag(result)\n",
        "    res = tf.linalg.matmul(v,result)\n",
        "    res = tf.linalg.matmul(res,v,adjoint_b=True)\n",
        "    \n",
        "    return tf.cast(res, tf.float32)\n",
        "\n",
        "# Aux function\n",
        "def outer_product(vector):\n",
        "    return tf.einsum('i,j->ij', vector, vector)\n",
        "\n",
        "def pure_state(h):\n",
        "    e, v = tf.linalg.eigh(h)\n",
        "    fund = v[:,:,0]\n",
        "    d = tf.map_fn(outer_product, fund)\n",
        "    return d\n",
        "\n",
        "# Casos de entrenamiento tipo mat gaussianas\n",
        "def gen_gauss_mat(G, sigma_sq, size):\n",
        "    indices = np.arange(size)\n",
        "    mat = G * np.exp(-((indices - indices[:, np.newaxis])**2) / (2 * sigma_sq))\n",
        "    return mat\n",
        "\n",
        "def gen_gauss_mat_np(G_values, sigma_sq_values, size):\n",
        "    indices = np.arange(size, dtype=np.float32)\n",
        "    indices_diff = indices - indices[:, np.newaxis]\n",
        "\n",
        "    mat = G_values[:, np.newaxis, np.newaxis] * np.exp(-np.square(indices_diff) / (2 * sigma_sq_values[:, np.newaxis, np.newaxis]))\n",
        "\n",
        "    return mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylpy_BCw6jxF"
      },
      "source": [
        "### Construccion de dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Version sincrónica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2is_Eo_qGpEz",
        "outputId": "9a968190-59f2-4695-ef18-b99ff5b4a212"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-03 20:04:16.242117: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /device:GPU:0 with 20609 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/device:GPU:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 977/977 [08:36<00:00,  1.89it/s]\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "# Config\n",
        "num_samples = 250000\n",
        "use_gpu = True\n",
        "gpu_batch_size = 256\n",
        "\n",
        "# Beta\n",
        "beta = 1\n",
        "\n",
        "# Construccion de parametros y matrices auxiliares\n",
        "rho1_size = m1_basis.size\n",
        "rho2_size = m2_basis.size\n",
        "rho2kkbar_size = basis.m\n",
        "fund_size = basis.size\n",
        "hamil_base_size = basis.d*(basis.d+1)//2\n",
        "rho_1_arrays = rho_1_gen(basis)\n",
        "rho_1_arrays_tf = tf.constant(rho_1_arrays, dtype=tf.float32)\n",
        "rho_2_arrays = rho_2_gen(basis, nm2_basis, m2_basis)\n",
        "rho_2_arrays_tf = tf.constant(rho_2_arrays, dtype=tf.float32)\n",
        "rho_2_arrays_kkbar = rho_2_kkbar_gen(t_basis, rho_2_arrays)\n",
        "rho_2_arrays_kkbar_tf = tf.constant(rho_2_arrays_kkbar, dtype=tf.float32)\n",
        "k_indices = get_kkbar_indices(t_basis)\n",
        "k_indices_tf = gen_update_indices(t_basis, gpu_batch_size)\n",
        "\n",
        "# Generacion de hamiltoniano\n",
        "# (h0, hi) = two_body_hamiltonian(t_basis.size, basis.m, np.arange(0, basis.m), np.ones((basis.m,basis.m)), rho_1_arrays_tf, rho_2_arrays_tf) esto es para g cte\n",
        "\n",
        "\n",
        "if use_gpu:\n",
        "    print(tf.test.gpu_device_name())\n",
        "    datasets = []\n",
        "    for i in tqdm(range(num_samples//gpu_batch_size+1)):\n",
        "        size = basis.m*(basis.m+1)//2\n",
        "        # En una primera versión vamos a pasar una mat proporcional a range(0,m) para energias\n",
        "        en_batch = [np.arange(0, basis.m) for _ in range(0,gpu_batch_size)] \n",
        "        # Como interacción una matriz G semidefinida positiva\n",
        "        # Primero creamos las semillas, es decir, la diagonal superior de la matrix g\n",
        "        # Caso generico\n",
        "        #label_size = basis.m*(basis.m+1)// 2 # CASO GENERICO elementos independientes de una mat de m x m\n",
        "        #h_labels = [np.random.random(label_size) for _ in range(0,gpu_batch_size)] # TODO: Aumentar la amplitud de la interacción\n",
        "        # Construimos la mat G\n",
        "        #triag = tfp.math.fill_triangular(h_labels, upper=True)\n",
        "        #g_arr = triag + tf.transpose(triag, perm=[0,2,1])-tf.linalg.diag(tf.linalg.diag_part(triag))\n",
        "        # Caso reducido\n",
        "        label_size = 2\n",
        "        h_labels = np.array([[np.random.random(), np.random.random()] for _ in range(0, gpu_batch_size)])\n",
        "        g_arr = gen_gauss_mat_np(h_labels[:,0], h_labels[:,1], basis.m)\n",
        "        h_labels = tf.constant(h_labels, dtype=tf.float32)\n",
        "        g_arr = tf.constant(g_arr, dtype=tf.float32)\n",
        "        # Construimos los hamiltonianos basados en g_arr\n",
        "        h_arr = two_body_hamiltonian_tf(t_basis, basis.m, en_batch, g_arr.numpy(), rho_1_arrays, rho_2_arrays, k_indices_tf)\n",
        "        # Estados térmicos\n",
        "        #state = thermal_state_tf(h_arr*beta) \n",
        "        #state = tf.cast(state, dtype=tf.float32)\n",
        "        # Estados puros\n",
        "        state = pure_state(h_arr)\n",
        "        #rho_2_input = rho_2_tf(state, rho_2_arrays_tf)\n",
        "        rho_2_input = rho_2_tf(state, rho_2_arrays_kkbar_tf)\n",
        "\n",
        "        datasets.append(tf.data.Dataset.from_tensor_slices(((rho_2_input), h_labels)))\n",
        "        #datasets.append(tf.data.Dataset.from_tensor_slices(((rho_1_input, rho_2_input), h_labels)))\n",
        "        #datasets.append(tf.data.Dataset.from_tensor_slices(((rho_1_input, rho_2_input, state), h_labels)))\n",
        "    ds = tf.data.Dataset.from_tensor_slices(datasets)\n",
        "    dataset = ds.interleave(\n",
        "        lambda x: x,\n",
        "        cycle_length=1,\n",
        "        num_parallel_calls=tf.data.AUTOTUNE,\n",
        "    )\n",
        "\n",
        "\n",
        "#batch_size = 32\n",
        "#dataset = dataset.shuffle(buffer_size=num_samples).batch(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Filleo de dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "# Save and load dataset\n",
        "save_dataset = False\n",
        "load_dataset = False\n",
        "path = \"/home/agus/TF\"\n",
        "#num_samples = 5000000\n",
        "if save_dataset:\n",
        "    tf.data.Dataset.save(dataset, path)\n",
        "    with open(\"/home/agus/\"+'/file.pkl', 'wb') as file:\n",
        "        pickle.dump(beta_input, file)\n",
        "if load_dataset:\n",
        "    dataset = tf.data.Dataset.load(path)\n",
        "    with open(\"/home/agus/\"+'file.pkl', 'rb') as file:\n",
        "        beta_input = pickle.load(file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "8moZIlfabZuy"
      },
      "outputs": [],
      "source": [
        "# Dividimos los datasets\n",
        "train_size = int(0.8 * num_samples)\n",
        "\n",
        "train_dataset = dataset.take(train_size)\n",
        "val_dataset = dataset.skip(train_size)\n",
        "\n",
        "\n",
        "batch_size = 256\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "val_dataset = val_dataset.batch(batch_size)\n",
        "\n",
        "#beta_val = beta_input[train_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Dataset Size: -2\n"
          ]
        }
      ],
      "source": [
        "# Cardinality no funciona con los datasets generados por GPU\n",
        "val_size = tf.data.experimental.cardinality(val_dataset).numpy()\n",
        "print(\"Validation Dataset Size:\", val_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYEEjNB-7b8y"
      },
      "source": [
        "### Definición de modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8kkhJr5K0ZQ",
        "outputId": "f1b731f1-6a02-4181-f0b5-5677a2a85784"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_15\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rho2 (InputLayer)           [(None, 4, 4, 1)]         0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 3, 3, 16)          80        \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 3, 3, 16)          64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " flatten_15 (Flatten)        (None, 144)               0         \n",
            "                                                                 \n",
            " dense_56 (Dense)            (None, 32)                4640      \n",
            "                                                                 \n",
            " dense_57 (Dense)            (None, 10)                330       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5114 (19.98 KB)\n",
            "Trainable params: 5082 (19.85 KB)\n",
            "Non-trainable params: 32 (128.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Definicion de layers basado en Conv 2D\n",
        "\n",
        "# Factor de cantidad de filtros\n",
        "lf = 16 \n",
        "conv_limit = (rho2kkbar_size - 4)\n",
        "initial_dense = (lf*2**(conv_limit-1)*((rho2kkbar_size-(conv_limit-1))//2)**2)\n",
        "## rho 1\n",
        "rho2_layer =  tf.keras.layers.Input(shape=(rho2kkbar_size,rho2kkbar_size, 1), name='rho2')\n",
        "\n",
        "# Procesamos el primer input\n",
        "conv_rho2 = tf.keras.layers.Conv2D(lf*2**conv_limit, (2, 2), activation='relu')(rho2_layer)\n",
        "conv_rho2 = tf.keras.layers.BatchNormalization()(conv_rho2)\n",
        "for j in [(2**conv_limit - 2**k) for k in range(1,conv_limit)]:\n",
        "    conv_rho2 = tf.keras.layers.Conv2D(lf*j, (2, 2), activation='relu')(conv_rho2 if 2**j != 1 else rho1_layer)\n",
        "    conv_rho2 = tf.keras.layers.BatchNormalization()(conv_rho2)\n",
        "\n",
        "#conv_rho2 = tf.keras.layers.MaxPooling2D((2, 2))(conv_rho2)\n",
        "\n",
        "flatten_rho2 = tf.keras.layers.Flatten()(conv_rho2)\n",
        "#flatten_rho1 = tf.keras.layers.Dense(rho1_size*rho1_size, activation='relu')(flatten_rho1)\n",
        "\n",
        "#local_size = basis.size*basis.size\n",
        "local_size = label_size\n",
        "\n",
        "#dense1 = tf.keras.layers.Dense(8*8*4*4, activation='relu')(dense1)\n",
        "#dense1 = tf.keras.layers.Dense(512, activation='relu')(flatten_rho1)\n",
        "#dense1 = tf.keras.layers.Dense(128, activation='relu')(flatten_rho1)\n",
        "dense1 = tf.keras.layers.Dense(initial_dense, activation='relu')(flatten_rho2)\n",
        "#dense1 = tf.keras.layers.Dense(initial_dense//2, activation='relu')(dense1)\n",
        "\n",
        "output = tf.keras.layers.Dense(local_size)(dense1)\n",
        "\n",
        "\n",
        "# Creamos el modelo y compulamos\n",
        "#model = tf.keras.models.Model(inputs=[rho1_layer, rho2_layer, fund_layer], outputs=output)\n",
        "#model = tf.keras.models.Model(inputs=[rho1_layer, rho2_layer], outputs=output)\n",
        "model = tf.keras.models.Model(inputs=[rho2_layer], outputs=output)\n",
        "\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZBtonvGbZuz",
        "outputId": "f197277e-a84b-4ffd-c81f-c81581707fb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            " rho2 (InputLayer)           [(None, 4, 4, 1)]         0         \n",
            "                                                                 \n",
            " flatten_16 (Flatten)        (None, 16)                0         \n",
            "                                                                 \n",
            " concatenate_12 (Concatenat  (None, 16)                0         \n",
            " e)                                                              \n",
            "                                                                 \n",
            " dense_58 (Dense)            (None, 20)                340       \n",
            "                                                                 \n",
            " dense_59 (Dense)            (None, 32)                672       \n",
            "                                                                 \n",
            " dense_60 (Dense)            (None, 64)                2112      \n",
            "                                                                 \n",
            " dense_61 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3774 (14.74 KB)\n",
            "Trainable params: 3774 (14.74 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Modelo denso + fundamental\n",
        "rho2_layer =  tf.keras.layers.Input(shape=(basis.m,basis.m, 1), name='rho2')\n",
        "flatten_rho2 = tf.keras.layers.Flatten()(rho2_layer)\n",
        "#rho2_layer =  tf.keras.layers.Input(shape=(rho2_size,rho2_size, 1), name='rho2')\n",
        "#flatten_rho2 = tf.keras.layers.Flatten()(rho2_layer)\n",
        "#fund_layer =  tf.keras.layers.Input(shape=(fund_size, fund_size, 1 ), name='fund')\n",
        "#flatten_fund = tf.keras.layers.Flatten()(fund_layer)\n",
        "\n",
        "dense1 = tf.keras.layers.concatenate([flatten_rho2])\n",
        "#dense1 = tf.keras.layers.concatenate([dense1, flatten_fund])\n",
        "#dense1 = tf.keras.layers.Dense(rho1_size*rho1_size, activation='relu')(dense1)\n",
        "\n",
        "local_size = label_size\n",
        "l=4\n",
        "layer_s = [32//i*2 for i in reversed(range(1,l))]\n",
        "for i in range(0,l-1):\n",
        "    dense1 = tf.keras.layers.Dense(layer_s[i], activation='relu')(dense1)\n",
        "\n",
        "output = tf.keras.layers.Dense(local_size)(dense1)\n",
        "# Creamos el modelo y compulamos\n",
        "model = tf.keras.models.Model(inputs=[rho2_layer], outputs=output)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "RgoMlCyyfBe-"
      },
      "outputs": [],
      "source": [
        "# LOSS FUNCTIONS\n",
        "r_size = basis.size\n",
        "\n",
        "# Custom loss function based on GS MSE\n",
        "def gs_loss(h_pred, h_true):\n",
        "    h_pred = tf.reshape(h_pred, shape=(-1,r_size,r_size))\n",
        "    e, v = tf.linalg.eigh(h_pred)\n",
        "    gs_pred = v[:, 0]\n",
        "\n",
        "    h_true = tf.reshape(h_true, shape=(-1,r_size,r_size))\n",
        "    e, v = tf.linalg.eigh(h_true)\n",
        "    gs_true = v[:, 0]\n",
        "\n",
        "    gs_diff = tf.norm(gs_true - gs_pred)\n",
        "\n",
        "    return gs_diff + tf.reduce_mean(tf.square(h_true - h_pred)) * 100\n",
        "\n",
        "def distance_to_hermitian(matrix):\n",
        "    hermitian_part = 0.5 * (matrix + tf.linalg.adjoint(matrix))\n",
        "    distance = tf.norm(matrix - hermitian_part, ord='euclidean')\n",
        "    return distance\n",
        "\n",
        "# Custom loss function based on MSE + non-hermitian penalization\n",
        "def herm_loss(h_pred, h_true):\n",
        "    h_pred_arr = tf.reshape(h_pred, shape=(-1,r_size,r_size))\n",
        "    return tf.reduce_mean(tf.square(h_true - h_pred)) + distance_to_hermitian(h_pred_arr)\n",
        "\n",
        "# Custom loss function based on h eigenvalues\n",
        "def eig_loss(h_pred, h_true):\n",
        "    eig_true = tf.sort(tf.math.real(tf.linalg.eigvals(tf.reshape(h_true, (-1, fund_size, fund_size)))))\n",
        "    eig_pred = tf.sort(tf.math.real(tf.linalg.eigvals(tf.reshape(h_pred, (-1, fund_size, fund_size)))))\n",
        "    return tf.reduce_mean(tf.square(eig_true - eig_pred))\n",
        "\n",
        "# MSE with a factor\n",
        "def mse_f(h_pred, h_true):\n",
        "    f = 1000\n",
        "    return tf.reduce_mean(tf.square(h_true - h_pred))*f\n",
        "\n",
        "# Spectral radius loss\n",
        "def spectral_loss(h_pred, h_true):\n",
        "    eig = tf.math.real(tf.linalg.eigvals(tf.reshape(h_true-h_pred, (-1, fund_size, fund_size))))\n",
        "    return tf.math.reduce_max(tf.abs(eig))\n",
        "\n",
        "# Hamiltonian MSE loss (using generators)\n",
        "def base_mse_loss(base_pred, base_true):\n",
        "    h_pred = gen_to_h_tf(base_pred, rho_1_arrays_tf)\n",
        "    h_true = gen_to_h_tf(base_true, rho_1_arrays_tf)\n",
        "    mat = tf.reshape(h_pred-h_true, (-1, fund_size, fund_size))\n",
        "    return tf.norm(mat, ord='fro', axis=[-1, -2])\n",
        "\n",
        "# Custom loss function based on h eigenvalues (using generators)\n",
        "def base_eig_loss(base_pred, base_true):\n",
        "    h_pred = gen_to_h_tf(base_pred, rho_1_arrays_tf)\n",
        "    h_true = gen_to_h_tf(base_true, rho_1_arrays_tf)\n",
        "    eig_true = tf.sort(tf.math.real(tf.linalg.eigvals(tf.reshape(h_true, (-1, fund_size, fund_size)))))\n",
        "    eig_pred = tf.sort(tf.math.real(tf.linalg.eigvals(tf.reshape(h_pred, (-1, fund_size, fund_size)))))\n",
        "    return tf.reduce_mean(tf.square(eig_true - eig_pred))\n",
        "\n",
        "# Custom loss function based on rho1 eigenvals\n",
        "## Auxiliary function\n",
        "def base_to_rho_1_tf(base_pred):\n",
        "    h = gen_to_h_tf(base_pred, rho_1_arrays_tf)\n",
        "    h = tf.reshape(h, (-1, fund_size, fund_size))\n",
        "    state = thermal_state_tf(h)\n",
        "    rho1 = rho_1_tf(state, rho_1_arrays_tf)\n",
        "    return rho1\n",
        "    \n",
        "def rho1_loss(base_pred, base_true):\n",
        "    mat = base_to_rho_1_tf(base_pred) - base_to_rho_1_tf(base_true)\n",
        "    return tf.norm(mat, ord='fro', axis=[-1, -2])\n",
        "\n",
        "# Custom loss function based on rho1 eigenvals (using generators)\n",
        "def base_rho1_loss(base_pred, base_true):\n",
        "    h_pred = gen_to_h_tf(base_pred, rho_1_arrays_tf)\n",
        "    h_true = gen_to_h_tf(base_true, rho_1_arrays_tf)\n",
        "    return tf.reduce_mean(tf.square(rho_1_eig_tf(h_pred) - rho_1_eig_tf(h_true)))*1000\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiWk9piJtNIZ"
      },
      "source": [
        "## Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhJCHf0fQdRl",
        "outputId": "1821cf27-9ff5-4d67-e9f5-956d20eda5e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-03 19:57:39.938896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /device:GPU:0 with 20609 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 5s 4ms/step - loss: 0.0660 - accuracy: 0.3001 - mean_squared_error: 0.0660 - val_loss: 0.0501 - val_accuracy: 0.3496 - val_mean_squared_error: 0.0501\n",
            "Epoch 2/100\n",
            " 61/782 [=>............................] - ETA: 1s - loss: 0.0502 - accuracy: 0.3557 - mean_squared_error: 0.0502"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-03 19:57:44.501002: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2602375272984826185\n",
            "2024-01-03 19:57:44.501054: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 981021811506365581\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0498 - accuracy: 0.3574 - mean_squared_error: 0.0498 - val_loss: 0.0494 - val_accuracy: 0.3512 - val_mean_squared_error: 0.0494\n",
            "Epoch 3/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0491 - accuracy: 0.3580 - mean_squared_error: 0.0491 - val_loss: 0.0486 - val_accuracy: 0.3515 - val_mean_squared_error: 0.0486\n",
            "Epoch 4/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0485 - accuracy: 0.3584 - mean_squared_error: 0.0485 - val_loss: 0.0482 - val_accuracy: 0.3511 - val_mean_squared_error: 0.0482\n",
            "Epoch 5/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0482 - accuracy: 0.3579 - mean_squared_error: 0.0482 - val_loss: 0.0480 - val_accuracy: 0.3508 - val_mean_squared_error: 0.0480\n",
            "Epoch 6/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0481 - accuracy: 0.3578 - mean_squared_error: 0.0481 - val_loss: 0.0479 - val_accuracy: 0.3500 - val_mean_squared_error: 0.0479\n",
            "Epoch 7/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0480 - accuracy: 0.3582 - mean_squared_error: 0.0480 - val_loss: 0.0478 - val_accuracy: 0.3504 - val_mean_squared_error: 0.0478\n",
            "Epoch 8/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0480 - accuracy: 0.3585 - mean_squared_error: 0.0480 - val_loss: 0.0478 - val_accuracy: 0.3506 - val_mean_squared_error: 0.0478\n",
            "Epoch 9/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0479 - accuracy: 0.3584 - mean_squared_error: 0.0479 - val_loss: 0.0477 - val_accuracy: 0.3510 - val_mean_squared_error: 0.0477\n",
            "Epoch 10/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0478 - accuracy: 0.3584 - mean_squared_error: 0.0478 - val_loss: 0.0476 - val_accuracy: 0.3503 - val_mean_squared_error: 0.0476\n",
            "Epoch 11/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0477 - accuracy: 0.3583 - mean_squared_error: 0.0477 - val_loss: 0.0474 - val_accuracy: 0.3503 - val_mean_squared_error: 0.0474\n",
            "Epoch 12/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0474 - accuracy: 0.3581 - mean_squared_error: 0.0474 - val_loss: 0.0471 - val_accuracy: 0.3501 - val_mean_squared_error: 0.0471\n",
            "Epoch 13/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0472 - accuracy: 0.3584 - mean_squared_error: 0.0472 - val_loss: 0.0469 - val_accuracy: 0.3509 - val_mean_squared_error: 0.0469\n",
            "Epoch 14/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0469 - accuracy: 0.3583 - mean_squared_error: 0.0469 - val_loss: 0.0466 - val_accuracy: 0.3521 - val_mean_squared_error: 0.0466\n",
            "Epoch 15/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0466 - accuracy: 0.3585 - mean_squared_error: 0.0466 - val_loss: 0.0464 - val_accuracy: 0.3528 - val_mean_squared_error: 0.0464\n",
            "Epoch 16/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0463 - accuracy: 0.3584 - mean_squared_error: 0.0463 - val_loss: 0.0460 - val_accuracy: 0.3530 - val_mean_squared_error: 0.0460\n",
            "Epoch 17/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0461 - accuracy: 0.3590 - mean_squared_error: 0.0461 - val_loss: 0.0457 - val_accuracy: 0.3545 - val_mean_squared_error: 0.0457\n",
            "Epoch 18/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0458 - accuracy: 0.3599 - mean_squared_error: 0.0458 - val_loss: 0.0454 - val_accuracy: 0.3548 - val_mean_squared_error: 0.0454\n",
            "Epoch 19/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0455 - accuracy: 0.3608 - mean_squared_error: 0.0455 - val_loss: 0.0451 - val_accuracy: 0.3554 - val_mean_squared_error: 0.0451\n",
            "Epoch 20/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0453 - accuracy: 0.3616 - mean_squared_error: 0.0453 - val_loss: 0.0449 - val_accuracy: 0.3588 - val_mean_squared_error: 0.0449\n",
            "Epoch 21/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0450 - accuracy: 0.3631 - mean_squared_error: 0.0450 - val_loss: 0.0446 - val_accuracy: 0.3583 - val_mean_squared_error: 0.0446\n",
            "Epoch 22/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0448 - accuracy: 0.3634 - mean_squared_error: 0.0448 - val_loss: 0.0445 - val_accuracy: 0.3581 - val_mean_squared_error: 0.0445\n",
            "Epoch 23/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0446 - accuracy: 0.3645 - mean_squared_error: 0.0446 - val_loss: 0.0443 - val_accuracy: 0.3607 - val_mean_squared_error: 0.0443\n",
            "Epoch 24/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0444 - accuracy: 0.3652 - mean_squared_error: 0.0444 - val_loss: 0.0441 - val_accuracy: 0.3621 - val_mean_squared_error: 0.0441\n",
            "Epoch 25/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0443 - accuracy: 0.3657 - mean_squared_error: 0.0443 - val_loss: 0.0440 - val_accuracy: 0.3627 - val_mean_squared_error: 0.0440\n",
            "Epoch 26/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0442 - accuracy: 0.3663 - mean_squared_error: 0.0442 - val_loss: 0.0438 - val_accuracy: 0.3622 - val_mean_squared_error: 0.0438\n",
            "Epoch 27/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0440 - accuracy: 0.3666 - mean_squared_error: 0.0440 - val_loss: 0.0437 - val_accuracy: 0.3628 - val_mean_squared_error: 0.0437\n",
            "Epoch 28/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0439 - accuracy: 0.3674 - mean_squared_error: 0.0439 - val_loss: 0.0436 - val_accuracy: 0.3629 - val_mean_squared_error: 0.0436\n",
            "Epoch 29/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0438 - accuracy: 0.3676 - mean_squared_error: 0.0438 - val_loss: 0.0435 - val_accuracy: 0.3640 - val_mean_squared_error: 0.0435\n",
            "Epoch 30/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0437 - accuracy: 0.3681 - mean_squared_error: 0.0437 - val_loss: 0.0434 - val_accuracy: 0.3650 - val_mean_squared_error: 0.0434\n",
            "Epoch 31/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0436 - accuracy: 0.3687 - mean_squared_error: 0.0436 - val_loss: 0.0433 - val_accuracy: 0.3658 - val_mean_squared_error: 0.0433\n",
            "Epoch 32/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0435 - accuracy: 0.3688 - mean_squared_error: 0.0435 - val_loss: 0.0433 - val_accuracy: 0.3674 - val_mean_squared_error: 0.0433\n",
            "Epoch 33/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0434 - accuracy: 0.3691 - mean_squared_error: 0.0434 - val_loss: 0.0434 - val_accuracy: 0.3683 - val_mean_squared_error: 0.0434\n",
            "Epoch 34/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0434 - accuracy: 0.3697 - mean_squared_error: 0.0434 - val_loss: 0.0434 - val_accuracy: 0.3686 - val_mean_squared_error: 0.0434\n",
            "Epoch 35/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0433 - accuracy: 0.3702 - mean_squared_error: 0.0433 - val_loss: 0.0434 - val_accuracy: 0.3697 - val_mean_squared_error: 0.0434\n",
            "Epoch 36/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0432 - accuracy: 0.3705 - mean_squared_error: 0.0432 - val_loss: 0.0435 - val_accuracy: 0.3705 - val_mean_squared_error: 0.0435\n",
            "Epoch 37/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0432 - accuracy: 0.3708 - mean_squared_error: 0.0432 - val_loss: 0.0436 - val_accuracy: 0.3707 - val_mean_squared_error: 0.0436\n",
            "Epoch 38/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0431 - accuracy: 0.3711 - mean_squared_error: 0.0431 - val_loss: 0.0433 - val_accuracy: 0.3708 - val_mean_squared_error: 0.0433\n",
            "Epoch 39/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0430 - accuracy: 0.3712 - mean_squared_error: 0.0430 - val_loss: 0.0430 - val_accuracy: 0.3705 - val_mean_squared_error: 0.0430\n",
            "Epoch 40/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0430 - accuracy: 0.3714 - mean_squared_error: 0.0430 - val_loss: 0.0429 - val_accuracy: 0.3701 - val_mean_squared_error: 0.0429\n",
            "Epoch 41/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0429 - accuracy: 0.3714 - mean_squared_error: 0.0429 - val_loss: 0.0427 - val_accuracy: 0.3702 - val_mean_squared_error: 0.0427\n",
            "Epoch 42/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0429 - accuracy: 0.3715 - mean_squared_error: 0.0429 - val_loss: 0.0425 - val_accuracy: 0.3694 - val_mean_squared_error: 0.0425\n",
            "Epoch 43/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0428 - accuracy: 0.3719 - mean_squared_error: 0.0428 - val_loss: 0.0424 - val_accuracy: 0.3679 - val_mean_squared_error: 0.0424\n",
            "Epoch 44/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0428 - accuracy: 0.3724 - mean_squared_error: 0.0428 - val_loss: 0.0424 - val_accuracy: 0.3680 - val_mean_squared_error: 0.0424\n",
            "Epoch 45/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0427 - accuracy: 0.3728 - mean_squared_error: 0.0427 - val_loss: 0.0424 - val_accuracy: 0.3679 - val_mean_squared_error: 0.0424\n",
            "Epoch 46/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0427 - accuracy: 0.3731 - mean_squared_error: 0.0427 - val_loss: 0.0423 - val_accuracy: 0.3681 - val_mean_squared_error: 0.0423\n",
            "Epoch 47/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0426 - accuracy: 0.3732 - mean_squared_error: 0.0426 - val_loss: 0.0423 - val_accuracy: 0.3680 - val_mean_squared_error: 0.0423\n",
            "Epoch 48/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0426 - accuracy: 0.3735 - mean_squared_error: 0.0426 - val_loss: 0.0422 - val_accuracy: 0.3683 - val_mean_squared_error: 0.0422\n",
            "Epoch 49/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0426 - accuracy: 0.3737 - mean_squared_error: 0.0426 - val_loss: 0.0422 - val_accuracy: 0.3690 - val_mean_squared_error: 0.0422\n",
            "Epoch 50/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0425 - accuracy: 0.3737 - mean_squared_error: 0.0425 - val_loss: 0.0422 - val_accuracy: 0.3702 - val_mean_squared_error: 0.0422\n",
            "Epoch 51/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0425 - accuracy: 0.3738 - mean_squared_error: 0.0425 - val_loss: 0.0422 - val_accuracy: 0.3706 - val_mean_squared_error: 0.0422\n",
            "Epoch 52/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0425 - accuracy: 0.3738 - mean_squared_error: 0.0425 - val_loss: 0.0422 - val_accuracy: 0.3711 - val_mean_squared_error: 0.0422\n",
            "Epoch 53/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0424 - accuracy: 0.3739 - mean_squared_error: 0.0424 - val_loss: 0.0422 - val_accuracy: 0.3721 - val_mean_squared_error: 0.0422\n",
            "Epoch 54/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0424 - accuracy: 0.3744 - mean_squared_error: 0.0424 - val_loss: 0.0423 - val_accuracy: 0.3733 - val_mean_squared_error: 0.0423\n",
            "Epoch 55/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0424 - accuracy: 0.3744 - mean_squared_error: 0.0424 - val_loss: 0.0424 - val_accuracy: 0.3739 - val_mean_squared_error: 0.0424\n",
            "Epoch 56/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0424 - accuracy: 0.3745 - mean_squared_error: 0.0424 - val_loss: 0.0423 - val_accuracy: 0.3741 - val_mean_squared_error: 0.0423\n",
            "Epoch 57/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0424 - accuracy: 0.3747 - mean_squared_error: 0.0424 - val_loss: 0.0423 - val_accuracy: 0.3742 - val_mean_squared_error: 0.0423\n",
            "Epoch 58/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0423 - accuracy: 0.3749 - mean_squared_error: 0.0423 - val_loss: 0.0423 - val_accuracy: 0.3745 - val_mean_squared_error: 0.0423\n",
            "Epoch 59/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0423 - accuracy: 0.3749 - mean_squared_error: 0.0423 - val_loss: 0.0424 - val_accuracy: 0.3747 - val_mean_squared_error: 0.0424\n",
            "Epoch 60/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0423 - accuracy: 0.3750 - mean_squared_error: 0.0423 - val_loss: 0.0424 - val_accuracy: 0.3746 - val_mean_squared_error: 0.0424\n",
            "Epoch 61/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0423 - accuracy: 0.3750 - mean_squared_error: 0.0423 - val_loss: 0.0424 - val_accuracy: 0.3747 - val_mean_squared_error: 0.0424\n",
            "Epoch 62/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0423 - accuracy: 0.3751 - mean_squared_error: 0.0423 - val_loss: 0.0424 - val_accuracy: 0.3749 - val_mean_squared_error: 0.0424\n",
            "Epoch 63/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0422 - accuracy: 0.3750 - mean_squared_error: 0.0422 - val_loss: 0.0424 - val_accuracy: 0.3753 - val_mean_squared_error: 0.0424\n",
            "Epoch 64/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0422 - accuracy: 0.3753 - mean_squared_error: 0.0422 - val_loss: 0.0423 - val_accuracy: 0.3755 - val_mean_squared_error: 0.0423\n",
            "Epoch 65/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0422 - accuracy: 0.3754 - mean_squared_error: 0.0422 - val_loss: 0.0422 - val_accuracy: 0.3749 - val_mean_squared_error: 0.0422\n",
            "Epoch 66/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0422 - accuracy: 0.3755 - mean_squared_error: 0.0422 - val_loss: 0.0422 - val_accuracy: 0.3746 - val_mean_squared_error: 0.0422\n",
            "Epoch 67/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0422 - accuracy: 0.3755 - mean_squared_error: 0.0422 - val_loss: 0.0422 - val_accuracy: 0.3750 - val_mean_squared_error: 0.0422\n",
            "Epoch 68/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0422 - accuracy: 0.3755 - mean_squared_error: 0.0422 - val_loss: 0.0422 - val_accuracy: 0.3752 - val_mean_squared_error: 0.0422\n",
            "Epoch 69/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0422 - accuracy: 0.3756 - mean_squared_error: 0.0422 - val_loss: 0.0421 - val_accuracy: 0.3749 - val_mean_squared_error: 0.0421\n",
            "Epoch 70/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0421 - accuracy: 0.3758 - mean_squared_error: 0.0421 - val_loss: 0.0421 - val_accuracy: 0.3752 - val_mean_squared_error: 0.0421\n",
            "Epoch 71/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0421 - accuracy: 0.3759 - mean_squared_error: 0.0421 - val_loss: 0.0421 - val_accuracy: 0.3758 - val_mean_squared_error: 0.0421\n",
            "Epoch 72/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0421 - accuracy: 0.3758 - mean_squared_error: 0.0421 - val_loss: 0.0421 - val_accuracy: 0.3757 - val_mean_squared_error: 0.0421\n",
            "Epoch 73/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0421 - accuracy: 0.3758 - mean_squared_error: 0.0421 - val_loss: 0.0421 - val_accuracy: 0.3764 - val_mean_squared_error: 0.0421\n",
            "Epoch 74/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0421 - accuracy: 0.3759 - mean_squared_error: 0.0421 - val_loss: 0.0421 - val_accuracy: 0.3772 - val_mean_squared_error: 0.0421\n",
            "Epoch 75/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0421 - accuracy: 0.3756 - mean_squared_error: 0.0421 - val_loss: 0.0422 - val_accuracy: 0.3775 - val_mean_squared_error: 0.0422\n",
            "Epoch 76/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0420 - accuracy: 0.3759 - mean_squared_error: 0.0420 - val_loss: 0.0422 - val_accuracy: 0.3775 - val_mean_squared_error: 0.0422\n",
            "Epoch 77/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0420 - accuracy: 0.3759 - mean_squared_error: 0.0420 - val_loss: 0.0422 - val_accuracy: 0.3781 - val_mean_squared_error: 0.0422\n",
            "Epoch 78/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0420 - accuracy: 0.3761 - mean_squared_error: 0.0420 - val_loss: 0.0422 - val_accuracy: 0.3784 - val_mean_squared_error: 0.0422\n",
            "Epoch 79/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0420 - accuracy: 0.3762 - mean_squared_error: 0.0420 - val_loss: 0.0422 - val_accuracy: 0.3786 - val_mean_squared_error: 0.0422\n",
            "Epoch 80/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0420 - accuracy: 0.3763 - mean_squared_error: 0.0420 - val_loss: 0.0422 - val_accuracy: 0.3787 - val_mean_squared_error: 0.0422\n",
            "Epoch 81/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0420 - accuracy: 0.3765 - mean_squared_error: 0.0420 - val_loss: 0.0422 - val_accuracy: 0.3785 - val_mean_squared_error: 0.0422\n",
            "Epoch 82/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0420 - accuracy: 0.3766 - mean_squared_error: 0.0420 - val_loss: 0.0422 - val_accuracy: 0.3785 - val_mean_squared_error: 0.0422\n",
            "Epoch 83/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0420 - accuracy: 0.3765 - mean_squared_error: 0.0420 - val_loss: 0.0421 - val_accuracy: 0.3785 - val_mean_squared_error: 0.0421\n",
            "Epoch 84/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0419 - accuracy: 0.3766 - mean_squared_error: 0.0419 - val_loss: 0.0420 - val_accuracy: 0.3783 - val_mean_squared_error: 0.0420\n",
            "Epoch 85/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0419 - accuracy: 0.3767 - mean_squared_error: 0.0419 - val_loss: 0.0420 - val_accuracy: 0.3786 - val_mean_squared_error: 0.0420\n",
            "Epoch 86/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0419 - accuracy: 0.3769 - mean_squared_error: 0.0419 - val_loss: 0.0419 - val_accuracy: 0.3786 - val_mean_squared_error: 0.0419\n",
            "Epoch 87/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0419 - accuracy: 0.3770 - mean_squared_error: 0.0419 - val_loss: 0.0419 - val_accuracy: 0.3785 - val_mean_squared_error: 0.0419\n",
            "Epoch 88/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0419 - accuracy: 0.3770 - mean_squared_error: 0.0419 - val_loss: 0.0419 - val_accuracy: 0.3785 - val_mean_squared_error: 0.0419\n",
            "Epoch 89/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0419 - accuracy: 0.3771 - mean_squared_error: 0.0419 - val_loss: 0.0420 - val_accuracy: 0.3787 - val_mean_squared_error: 0.0420\n",
            "Epoch 90/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0419 - accuracy: 0.3771 - mean_squared_error: 0.0419 - val_loss: 0.0420 - val_accuracy: 0.3785 - val_mean_squared_error: 0.0420\n",
            "Epoch 91/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0419 - accuracy: 0.3772 - mean_squared_error: 0.0419 - val_loss: 0.0419 - val_accuracy: 0.3783 - val_mean_squared_error: 0.0419\n",
            "Epoch 92/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0419 - accuracy: 0.3774 - mean_squared_error: 0.0419 - val_loss: 0.0420 - val_accuracy: 0.3786 - val_mean_squared_error: 0.0420\n",
            "Epoch 93/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0419 - accuracy: 0.3774 - mean_squared_error: 0.0419 - val_loss: 0.0420 - val_accuracy: 0.3790 - val_mean_squared_error: 0.0420\n",
            "Epoch 94/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0419 - accuracy: 0.3774 - mean_squared_error: 0.0419 - val_loss: 0.0419 - val_accuracy: 0.3789 - val_mean_squared_error: 0.0419\n",
            "Epoch 95/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0419 - accuracy: 0.3776 - mean_squared_error: 0.0419 - val_loss: 0.0419 - val_accuracy: 0.3791 - val_mean_squared_error: 0.0419\n",
            "Epoch 96/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0418 - accuracy: 0.3778 - mean_squared_error: 0.0418 - val_loss: 0.0419 - val_accuracy: 0.3790 - val_mean_squared_error: 0.0419\n",
            "Epoch 97/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0418 - accuracy: 0.3778 - mean_squared_error: 0.0418 - val_loss: 0.0420 - val_accuracy: 0.3790 - val_mean_squared_error: 0.0420\n",
            "Epoch 98/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0418 - accuracy: 0.3779 - mean_squared_error: 0.0418 - val_loss: 0.0419 - val_accuracy: 0.3790 - val_mean_squared_error: 0.0419\n",
            "Epoch 99/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0418 - accuracy: 0.3781 - mean_squared_error: 0.0418 - val_loss: 0.0420 - val_accuracy: 0.3791 - val_mean_squared_error: 0.0420\n",
            "Epoch 100/100\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.0418 - accuracy: 0.3781 - mean_squared_error: 0.0418 - val_loss: 0.0420 - val_accuracy: 0.3794 - val_mean_squared_error: 0.0420\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.optimizers import RMSprop, Adam, Nadam, Lion\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(),\n",
        "              loss='MSE',  \n",
        "              metrics=['accuracy', 'mean_squared_error'])\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 100\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "    history = model.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset)\n",
        "\n",
        "# Dense: 1.3\n",
        "# CNN: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "cvpE_X1iTXcB",
        "outputId": "eff0e5f5-5b26-46ea-ec6b-491d1de9944c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkgAAAHHCAYAAABEEKc/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABkdUlEQVR4nO3dd3wUZeLH8c9udtM7IQk99ISONMGCnpwJeCCIByJSFOVERZSzYQPld4J6djw5C6AnCOKJp6gooqBCKFJFmnSEFEJJJWV35/fHkoUlARJIdhf4vl+veSU788zMMwO4X5955nlMhmEYiIiIiIiL2dsVEBEREfE1CkgiIiIip1BAEhERETmFApKIiIjIKRSQRERERE6hgCQiIiJyCgUkERERkVMoIImIiIicQgFJRERE5BQKSCIiF7lrrrmGVq1aebsaIhcUBSQRqbAZM2ZgMpkwmUz8/PPPZbYbhkG9evUwmUz85S9/cduWl5fH+PHjadWqFSEhIdSoUYN27doxZswYDhw44Co3YcIE1znKW9LT06v9OivrmmuuOW19ExMTvV09ETkHFm9XQEQuPIGBgcyaNYsrr7zSbf2SJUv4448/CAgIcFtfUlLC1VdfzZYtWxg2bBijR48mLy+P3377jVmzZtGvXz9q167tts9bb71FaGhomXNHRkZW+fVUhbp16zJp0qQy6yMiIrxQGxE5XwpIIlJpvXr1Yu7cubz++utYLCf+MzJr1iw6dOhAVlaWW/nPPvuMtWvXMnPmTG699Va3bYWFhRQXF5c5x80330xMTEz1XEA1iIiI4LbbbvN2NUSkiugRm4hU2qBBgzh06BALFy50rSsuLuaTTz4pE4AAduzYAcAVV1xRZltgYCDh4eFVUq9WrVpx7bXXllnvcDioU6cON998s2vd7Nmz6dChA2FhYYSHh9O6dWtee+21KqnH6ZQ+PtyyZQsDBgwgPDycGjVqMGbMGAoLC93K2mw2Jk6cSOPGjQkICCAhIYHHH3+coqKiMsf9+uuv6d69u+taOnXqxKxZs8qU27RpE9deey3BwcHUqVOHF154odquVeRCp4AkIpWWkJBA165d+eijj1zrvv76a7Kzs7nlllvKlG/QoAEAH3zwAYZhVOgchw8fJisry205evToGfcZOHAgP/74Y5l+Sj///DMHDhxw1W3hwoUMGjSIqKgonn/+eSZPnsw111zD0qVLK1S38tjt9jL1zcrKIj8/v0zZAQMGUFhYyKRJk+jVqxevv/46I0eOdCtz55138vTTT3PZZZfxyiuv0L17dyZNmlTm/s6YMYMbbriBw4cPM27cOCZPnky7du1YsGCBW7kjR46QkpJC27Zteemll0hMTOTRRx/l66+/PudrFrmoGSIiFTR9+nQDMFatWmVMmTLFCAsLMwoKCgzDMIy//vWvxrXXXmsYhmE0aNDAuOGGG1z7FRQUGM2bNzcAo0GDBsbw4cON9957z8jIyChzjvHjxxtAuUvz5s3PWL+tW7cagPHGG2+4rb/nnnuM0NBQV13HjBljhIeHGzab7bzuR6nu3bufts5/+9vfylxbnz59ytQPMNavX28YhmGsW7fOAIw777zTrdxDDz1kAMb3339vGIZhHD161AgLCzO6dOliHDt2zK2sw+EoU78PPvjAta6oqMiIj483+vfvXyX3QORioxYkETknAwYM4NixY8yfP5/c3Fzmz59f7uM1gKCgIFasWMHDDz8MOFs9RowYQa1atRg9enS5j43++9//snDhQrdl+vTpZ6xTs2bNaNeuHXPmzHGts9vtfPLJJ/Tu3ZugoCDA2dE7Pz/f7RHh+UpISChT34ULF/LAAw+UKXvvvfe6fR49ejQAX331ldvPsWPHupX7+9//DsCXX34JOFvCcnNzeeyxxwgMDHQrazKZ3D6Hhoa69ZHy9/enc+fO7Ny5s7KXKnJJUCdtETknNWvWpEePHsyaNYuCggLsdrtbH59TRURE8MILL/DCCy+wZ88eFi1axD//+U+mTJlCREQE//d//+dW/uqrrz6nTtoDBw7k8ccfZ//+/dSpU4fFixeTmZnJwIEDXWXuuecePv74Y3r27EmdOnW4/vrrGTBgACkpKZU+X6mQkBB69OhRobJNmzZ1+9y4cWPMZjO7d+8GYM+ePZjNZpo0aeJWLj4+nsjISPbs2QOc6NtVkTGO6tatWyY0RUVFsWHDhgrVWeRSoxYkETlnt956K19//TVTp06lZ8+eFX4Fv0GDBtxxxx0sXbqUyMhIZs6cWWV1GjhwIIZhMHfuXAA+/vhjIiIi3MJPbGws69at4/PPP6dPnz788MMP9OzZk2HDhlVZPSrj1OBytvXnws/Pr9z1RgX7hIlcahSQROSc9evXD7PZzPLly0/7eO1MoqKiaNy4MWlpaVVWp4YNG9K5c2fmzJmDzWbj008/pW/fvmXGZvL396d3797861//YseOHfztb3/jgw8+YPv27VVWl9P5/fff3T5v374dh8NBQkIC4AyQDoejTLmMjAyOHj3q6vTeuHFjADZu3FjtdRa51Cggicg5Cw0N5a233mLChAn07t37tOXWr19fZmwkcD5K2rRpE82bN6/Seg0cOJDly5czbdo0srKy3B6vARw6dMjts9lspk2bNgCu/lAlJSVs2bKlSsNbqTfffNPt8xtvvAFAz549Aec4UwCvvvqqW7mXX34ZgBtuuAGA66+/nrCwMCZNmlRmmAC1DImcH/VBEpHzUpHHUgsXLmT8+PH06dOHyy+/nNDQUHbu3Mm0adMoKipiwoQJZfb55JNPyh1J+89//jNxcXFnPN+AAQN46KGHeOihh4iOji7TN+jOO+/k8OHD/OlPf6Ju3brs2bOHN954g3bt2pGUlATA/v37SUpKYtiwYcyYMeOs15idnc2HH35Y7rZTB5DctWsXffr0ISUlhdTUVD788ENuvfVW2rZtC0Dbtm0ZNmwYb7/9NkePHqV79+6sXLmS999/n759+7rGegoPD+eVV17hzjvvpFOnTtx6661ERUWxfv16CgoKeP/9989abxEpnwKSiFS7/v37k5uby7fffsv333/P4cOHiYqKonPnzvz9738vd3DHUaNGlXusH3744awBqW7dunTr1o2lS5dy5513YrVa3bbfdtttvP322/zrX//i6NGjxMfHM3DgQCZMmIDZfG4N63/88QdDhgwpd9upAWnOnDk8/fTTPPbYY1gsFu677z5efPFFtzLvvvsujRo1YsaMGcybN4/4+HjGjRvH+PHj3cqNGDGC2NhYJk+ezMSJE7FarSQmJvLggw+e03WIiJPJUDusiIhHTJgwgWeeeYaDBw9eUNOoiFyK1AdJRERE5BQKSCIiIiKnUEASEREROYX6IImIiIicQi1IIiIiIqdQQBIRERE5hcZBOkcOh4MDBw4QFhZWpfMliYiISPUxDIPc3Fxq1659xnHPFJDO0YEDB6hXr563qyEiIiLnYN++fdStW/e02xWQzlFYWBjgvMHh4eFero2IiIhURE5ODvXq1XN9j5+OAtI5Kn2sFh4eroAkIiJygTlb9xh10hYRERE5hQKSiIiIyCkUkEREREROoT5IIiLiNXa7nZKSEm9XQy4iVqsVPz+/8z6OApKIiHicYRikp6dz9OhRb1dFLkKRkZHEx8ef1ziFCkgiIuJxpeEoNjaW4OBgDbgrVcIwDAoKCsjMzASgVq1a53wsBSQREfEou93uCkc1atTwdnXkIhMUFARAZmYmsbGx5/y4TZ20RUTEo0r7HAUHB3u5JnKxKv27dT792xSQRETEK/RYTapLVfzdUkASEREROYUCkoiIiBclJCTw6quvVrj84sWLMZlMegOwmikgiYiIVIDJZDrjMmHChHM67qpVqxg5cmSFy3fr1o20tDQiIiLO6XwVVRrEoqKiKCwsdNu2atUq13Wf7J133qFt27aEhoYSGRlJ+/btmTRpkmv7hAkTyr13iYmJ1Xot50JvsfmYw/nF5BfZiArxJzRAfzwiIr4iLS3N9fucOXN4+umn2bp1q2tdaGio63fDMLDb7VgsZ//veM2aNStVD39/f+Lj4yu1z/kICwtj3rx5DBo0yLXuvffeo379+uzdu9e1btq0aTzwwAO8/vrrdO/enaKiIjZs2MDGjRvdjteyZUu+++47t3UVuU+ephYkH3P/R2u56oUf+G5ThrerIiIiJ4mPj3ctERERmEwm1+ctW7YQFhbG119/TYcOHQgICODnn39mx44d3HjjjcTFxREaGkqnTp3KhINTH7GZTCbeffdd+vXrR3BwME2bNuXzzz93bT/1EduMGTOIjIzkm2++ISkpidDQUFJSUtwCnc1m4/777ycyMpIaNWrw6KOPMmzYMPr27XvW6x42bBjTpk1zfT527BizZ89m2LBhbuU+//xzBgwYwIgRI2jSpAktW7Zk0KBB/OMf/3ArZ7FY3O5lfHw8MTExZ62Hpykg+RiLn7O5ssTu8HJNREQ8xzAMCoptHl8Mw6jS63jssceYPHkymzdvpk2bNuTl5dGrVy8WLVrE2rVrSUlJoXfv3m4tL+V55plnGDBgABs2bKBXr14MHjyYw4cPn7Z8QUEB//znP/nPf/7Djz/+yN69e3nooYdc259//nlmzpzJ9OnTWbp0KTk5OXz22WcVuqYhQ4bw008/uer83//+l4SEBC677DK3cvHx8Sxfvpw9e/ZU6Li+zvfatC5xVj9nZi2xV+0/WhERX3asxE6Lp7/x+Hk3PZtMsH/VfRU+++yz/PnPf3Z9jo6Opm3btq7PEydOZN68eXz++efcd999pz3O8OHDXY+0nnvuOV5//XVWrlxJSkpKueVLSkqYOnUqjRs3BuC+++7j2WefdW1/4403GDduHP369QNgypQpfPXVVxW6ptjYWHr27MmMGTN4+umnmTZtGnfccUeZcuPHj+emm24iISGBZs2a0bVrV3r16sXNN9+M2XyiPebXX391exwJcNtttzF16tQK1cdT1ILkY6zHW5BsDrUgiYhcaDp27Oj2OS8vj4ceeoikpCQiIyMJDQ1l8+bNZ21BatOmjev3kJAQwsPDXdNnlCc4ONgVjsA5xUZp+ezsbDIyMujcubNru5+fHx06dKjwdd1xxx3MmDGDnTt3kpqayuDBg8uUqVWrFqmpqfz666+MGTMGm83GsGHDSElJwXHSd1rz5s1Zt26d23JymPMVakHyMRazWpBE5NITZPVj07PJXjlvVQoJCXH7/NBDD7Fw4UL++c9/0qRJE4KCgrj55pspLi4+43GsVqvbZ5PJ5BYyKlK+Kh8f9uzZk5EjRzJixAh69+59xiliWrVqRatWrbjnnnu4++67ueqqq1iyZAnXXnst4Oxk3qRJkyqrW3VRQPIxpX2QbOqDJCKXEJPJVKWPunzF0qVLGT58uOvRVl5eHrt37/ZoHSIiIoiLi2PVqlVcffXVgHM+vDVr1tCuXbsKHcNisTB06FBeeOEFvv766wqfu0WLFgDk5+dXut7edvH9bbzAWY+3INkcakESEbnQNW3alE8//ZTevXtjMpl46qmnztgSVF1Gjx7NpEmTaNKkCYmJibzxxhscOXKkUlNyTJw4kYcffvi0rUejRo2idu3a/OlPf6Ju3bqkpaXxf//3f9SsWZOuXbu6ytlsNtLT0932NZlMxMXFndvFVRMFJB9jtTj/shbb1IIkInKhe/nll7njjjvo1q0bMTExPProo+Tk5Hi8Ho8++ijp6ekMHToUPz8/Ro4cSXJycqVmuvf39z/j6/g9evRg2rRpvPXWWxw6dIiYmBi6du3KokWL3ELVb7/9Rq1atdz2DQgIKDMYpbeZjKp+x/ESkZOTQ0REBNnZ2YSHh1fZcSd8/hszlu3m3msb83Cy740sKiJyvgoLC9m1axcNGzYkMDDQ29W5JDkcDpKSkhgwYAATJ070dnWq3Jn+jlX0+1stSD7G9RabOmmLiEgV2bNnD99++61rhOspU6awa9cubr31Vm9XzWfpNX8fY9E4SCIiUsXMZjMzZsygU6dOXHHFFfz666989913JCUlebtqPsvrAenNN98kISGBwMBAunTpwsqVK89Yfu7cuSQmJhIYGEjr1q3LHehq8+bN9OnTh4iICEJCQujUqZPbmBPXXHNNmYny7r777iq/tnNhNWscJBERqVr16tVj6dKlZGdnk5OTw7Jly1xvtEn5vBqQ5syZw9ixYxk/fjxr1qyhbdu2JCcnn3YwrGXLljFo0CBGjBjB2rVr6du3L3379nWbCG/Hjh1ceeWVJCYmsnjxYjZs2MBTTz1V5hnkXXfdRVpammt54YUXqvVaK+pEC5ICkoiIiLd4NSC9/PLL3HXXXdx+++20aNGCqVOnEhwc7DYp3slee+01UlJSePjhh0lKSmLixIlcdtllTJkyxVXmiSeeoFevXrzwwgu0b9+exo0b06dPH2JjY92OFRwc7DZRXlV2tD4fmmpERETE+7wWkIqLi1m9ejU9evQ4URmzmR49epCamlruPqmpqW7lAZKTk13lHQ4HX375Jc2aNSM5OZnY2Fi6dOlS7oR8M2fOJCYmhlatWjFu3DgKCgrOWN+ioiJycnLclupg1UCRIiIiXue1gJSVlYXdbi8zMFRcXFyZAaRKpaenn7F8ZmYmeXl5TJ48mZSUFL799lv69evHTTfdxJIlS1z73HrrrXz44Yf88MMPjBs3jv/85z/cdtttZ6zvpEmTiIiIcC316tU7l8s+K8vxPkglGihSRETEay6q1/xLRye98cYbefDBBwFo164dy5YtY+rUqXTv3h2AkSNHuvZp3bo1tWrV4rrrrmPHjh1uk/2dbNy4cYwdO9b1OScnp1pCUmkfJLUgiYiIeI/XWpBiYmLw8/MjIyPDbX1GRgbx8fHl7hMfH3/G8jExMVgsFtfcL6WSkpLOOHNyly5dANi+fftpywQEBBAeHu62VAeNgyQiIuJ9XgtI/v7+dOjQgUWLFrnWORwOFi1a5DZny8lKhyw/2cKFC13l/f396dSpE1u3bnUrs23bNho0aHDauqxbtw6gzNDn3lDaSbtYLUgiIhela665hgceeMD1OSEhgVdfffWM+5hMpnL701ZWVR3nUuDVt9jGjh3LO++8w/vvv8/mzZsZNWoU+fn53H777QAMHTqUcePGucqPGTOGBQsW8NJLL7FlyxYmTJjAL7/8wn333ecq8/DDDzNnzhzeeecdtm/fzpQpU/jiiy+45557AOcwABMnTmT16tXs3r2bzz//nKFDh3L11VfTpk0bz96Acpx4xKYWJBERX9K7d29SUlLK3fbTTz9hMpnYsGFDpY+7atUqt64fVWHChAm0a9euzPq0tDR69uxZpec61YwZMzCZTOUOQjl37lxMJhMJCQmudXa7ncmTJ5OYmEhQUBDR0dF06dKFd99911Vm+PDhZcYvNJlMp/3zqApe7YM0cOBADh48yNNPP016ejrt2rVjwYIFro7Ye/fuxWw+keG6devGrFmzePLJJ3n88cdp2rQpn332Ga1atXKV6devH1OnTmXSpEncf//9NG/enP/+979ceeWVgLOV6bvvvuPVV18lPz+fevXq0b9/f5588knPXvxpaKBIERHfNGLECPr3788ff/xB3bp13bZNnz6djh07ntP/aNesWbOqqnhWp+vCUtVCQkLIzMwkNTXV7anQe++9R/369d3KPvPMM/z73/9mypQpdOzYkZycHH755ReOHDniVi4lJYXp06e7rQsICKi+izDknGRnZxuAkZ2dXaXH/fa3dKPBo/ONG6f8XKXHFRHxFceOHTM2bdpkHDt2zNtVqZSSkhIjLi7OmDhxotv63NxcIzQ01HjrrbeMrKws45ZbbjFq165tBAUFGa1atTJmzZrlVr579+7GmDFjXJ8bNGhgvPLKK67P27ZtM6666iojICDASEpKMr799lsDMObNm+cq88gjjxhNmzY1goKCjIYNGxpPPvmkUVxcbBiGYUyfPt0A3Jbp06cbhmGUOc6GDRuMa6+91ggMDDSio6ONu+66y8jNzXVtHzZsmHHjjTcaL774ohEfH29ER0cb99xzj+tc5Zk+fboRERFh3Hfffcadd97pWr9v3z4jICDAeOyxx4wGDRq41rdt29aYMGHCaY93cj0q6kx/xyr6/e31qUbEncVPLUgicgkyDCjO9/xiVLw7g8ViYejQocyYMQPjpP3mzp2L3W5n0KBBFBYW0qFDB7788ks2btzIyJEjGTJkyFmn0SrlcDi46aab8Pf3Z8WKFUydOpVHH320TLmwsDBmzJjBpk2beO2113jnnXd45ZVXAOfTmb///e+0bNnSNVvEwIEDyxwjPz+f5ORkoqKiWLVqFXPnzuW7775z67YC8MMPP7Bjxw5++OEH3n//fWbMmMGMGTPOei133HEHH3/8sWucwRkzZpCSklJmuJ74+Hi+//57Dh48WKF75CkX1Wv+FwP/0pG0beqDJCKXkJICeK6258/7+AHwD6lw8TvuuIMXX3yRJUuWcM011wDOx2v9+/d3jZP30EMPucqPHj2ab775ho8//pjOnTuf9fjfffcdW7Zs4ZtvvqF2bef9eO6558r0Gzq5W0hCQgIPPfQQs2fP5pFHHiEoKIjQ0FAsFssZH6nNmjWLwsJCPvjgA0JCnPdgypQp9O7dm+eff94VZKKiopgyZQp+fn4kJiZyww03sGjRIu66664zXkv79u1p1KgRn3zyCUOGDGHGjBm8/PLL7Ny5063cyy+/zM0330x8fDwtW7akW7du3HjjjWWuef78+YSGhrqte/zxx3n88cfPWI9zpRYkH3NioEi1IImI+JrExES6devmmhJr+/bt/PTTT4wYMQJwdjieOHEirVu3Jjo6mtDQUL755pszDjVzss2bN1OvXj1XOALKfbN7zpw5XHHFFcTHxxMaGsqTTz5Z4XOcfK62bdu6whHAFVdcgcPhcHsbvGXLlvj5+bk+16pV67Rzpp7qjjvuYPr06SxZsoT8/Hx69epVpkyLFi3YuHEjy5cv54477iAzM5PevXtz5513upW79tprWbdundtSnRPNqwXJx+gtNhG5JFmDna053jhvJY0YMYLRo0fz5ptvMn36dBo3buwaiPjFF1/ktdde49VXX6V169aEhITwwAMPUFxcXGVVTk1NZfDgwTzzzDMkJycTERHB7Nmzeemll6rsHCezWq1un00mk2tg5rMZPHgwjzzyCBMmTGDIkCFYLOXHDrPZTKdOnejUqRMPPPAAH374IUOGDOGJJ56gYcOGgLPjd5MmTc7vYipBAcnHaC42EbkkmUyVetTlTQMGDGDMmDHMmjWLDz74gFGjRmEyOf/bvXTpUm688UbX9FUOh4Nt27aVGcD4dJKSkti3bx9paWmusfmWL1/uVmbZsmU0aNCAJ554wrVuz549bmX8/f2x2+1nPdeMGTPIz893tSItXboUs9lM8+bNK1Tfs4mOjqZPnz58/PHHTJ06tcL7ld6v/Pz8KqnHudAjNh9jOT6sgeZiExHxTaGhoQwcOJBx48aRlpbG8OHDXduaNm3KwoULWbZsGZs3b+Zvf/tbmRkgzqRHjx40a9aMYcOGsX79en766Se3IFR6jr179zJ79mx27NjB66+/zrx589zKJCQksGvXLtatW0dWVhZFRUVlzjV48GACAwMZNmwYGzdu5IcffmD06NEMGTKkTEfq8zFjxgyysrJITEwsd/vNN9/MK6+8wooVK9izZw+LFy/m3nvvpVmzZm77FBUVkZ6e7rZkZWVVWT1PpYDkY/wtx/sgqQVJRMRnjRgxgiNHjpCcnOzWX+jJJ5/ksssuIzk5mWuuuYb4+Hj69u1b4eOazWbmzZvHsWPH6Ny5M3feeSf/+Mc/3Mr06dOHBx98kPvuu8813+hTTz3lVqZ///6kpKRw7bXXUrNmTT766KMy5woODuabb77h8OHDdOrUiZtvvpnrrruOKVOmVO5mnEVQUBA1atQ47fbk5GS++OILevfu7QqHiYmJfPvtt26P5BYsWECtWrXcltIxDquDyTAq8Y6juOTk5BAREUF2dnaVzsu2Oyufa/65mNAACxufSa6y44qI+IrCwkJ27dpFw4YNCQwM9HZ15CJ0pr9jFf3+VguSjykdB0ktSCIiIt6jgORjSiertakPkoiIiNcoIPmY0nGQ7A4Dh0KSiIiIVygg+Rir5cQfiQaLFBER8Q4FJB9jNZ/4I9FgkSJyMdM7QlJdquLvlgKSjyntpA0KSCJycSodmbl0ElORqlb6d+vUUcArQyNp+5jSPkigR2wicnHy8/MjMjLSNZ9XcHCwayRqkfNhGAYFBQVkZmYSGRnpNodcZSkg+RiTyYTFbMLmMNSCJCIXrdJZ5is66alIZURGRrr+jp0rBSQfZPFzBiSNhSQiFyuTyUStWrWIjY2lpKTE29WRi4jVaj2vlqNSCkg+yOpnprDEoYAkIhc9Pz+/KvkyE6lq6qTtgzRYpIiIiHcpIPmg0o7aakESERHxDgUkH+RqQVInbREREa9QQPJBpWMh2fSav4iIiFcoIPmg0hakYptakERERLxBAckHlfZBUguSiIiIdygg+SD1QRIREfEuBSQfVNoHSW+xiYiIeIcCkg+ymjUOkoiIiDcpIPkgq0UtSCIiIt6kgOSDLMdbkErUB0lERMQrFJB8kLV0HCS1IImIiHiFApIPcrUgqQ+SiIiIVygg+SCLWpBERES8SgHJB/n7lfZBUkASERHxBgUkH3RiHCQ9YhMREfEGBSQfZNFI2iIiIl6lgOSDrJqLTURExKsUkHyQxU/jIImIiHiTApIP0lxsIiIi3qWA5IP8XX2QFJBERES8QQHJB2mgSBEREe9SQPJBGihSRETEuxSQfNCJudjUgiQiIuINCkg+SI/YREREvEsByQdZLccDkk2P2ERERLxBAckHaaBIERER71JA8kEaKFJERMS7FJB8kKuTtlqQREREvEIByQe5OmmrBUlERMQrFJB8kFVTjYiIiHiVApIPsrqmGlELkoiIiDcoIPkgTVYrIiLiXQpIPqi0D5JNA0WKiIh4hQKSD7JqLjYRERGvUkDyQVaNgyQiIuJVCkg+SH2QREREvEsByQe53mJTHyQRERGvUEDyQRazWpBERES8SQHJB2kcJBEREe/yekB68803SUhIIDAwkC5durBy5cozlp87dy6JiYkEBgbSunVrvvrqqzJlNm/eTJ8+fYiIiCAkJIROnTqxd+9e1/bCwkLuvfdeatSoQWhoKP379ycjI6PKr+1cneikrRYkERERb/BqQJozZw5jx45l/PjxrFmzhrZt25KcnExmZma55ZctW8agQYMYMWIEa9eupW/fvvTt25eNGze6yuzYsYMrr7ySxMREFi9ezIYNG3jqqacIDAx0lXnwwQf54osvmDt3LkuWLOHAgQPcdNNN1X69FWVxTVZrYBhqRRIREfE0k+HFb+AuXbrQqVMnpkyZAoDD4aBevXqMHj2axx57rEz5gQMHkp+fz/z5813rLr/8ctq1a8fUqVMBuOWWW7BarfznP/8p95zZ2dnUrFmTWbNmcfPNNwOwZcsWkpKSSE1N5fLLL69Q3XNycoiIiCA7O5vw8PBKXffZZBeU0PbZbwH4/R89XS1KIiIicn4q+v3ttW/e4uJiVq9eTY8ePU5UxmymR48epKamlrtPamqqW3mA5ORkV3mHw8GXX35Js2bNSE5OJjY2li5duvDZZ5+5yq9evZqSkhK34yQmJlK/fv3TntfTSluQQP2QREREvMFrASkrKwu73U5cXJzb+ri4ONLT08vdJz09/YzlMzMzycvLY/LkyaSkpPDtt9/Sr18/brrpJpYsWeI6hr+/P5GRkRU+L0BRURE5OTluS3U5OSCVONQPSURExNMs3q5AVXIcDxM33ngjDz74IADt2rVj2bJlTJ06le7du5/zsSdNmsQzzzxTJfU8G6v5RG4tsSkgiYiIeJrXWpBiYmLw8/Mr8/ZYRkYG8fHx5e4THx9/xvIxMTFYLBZatGjhViYpKcn1Flt8fDzFxcUcPXq0wucFGDduHNnZ2a5l3759FbrOc2E2m/Azn+ioLSIiIp7ltYDk7+9Phw4dWLRokWudw+Fg0aJFdO3atdx9unbt6lYeYOHCha7y/v7+dOrUia1bt7qV2bZtGw0aNACgQ4cOWK1Wt+Ns3bqVvXv3nva8AAEBAYSHh7st1UmDRYqIiHiPVx+xjR07lmHDhtGxY0c6d+7Mq6++Sn5+PrfffjsAQ4cOpU6dOkyaNAmAMWPG0L17d1566SVuuOEGZs+ezS+//MLbb7/tOubDDz/MwIEDufrqq7n22mtZsGABX3zxBYsXLwYgIiKCESNGMHbsWKKjowkPD2f06NF07dq1wm+weYLVz0yRzaFO2iIiIl7g1YA0cOBADh48yNNPP016ejrt2rVjwYIFro7Ye/fuxXxSf5xu3boxa9YsnnzySR5//HGaNm3KZ599RqtWrVxl+vXrx9SpU5k0aRL3338/zZs357///S9XXnmlq8wrr7yC2Wymf//+FBUVkZyczL/+9S/PXXgFnBgLSS1IIiIinubVcZAuZNU5DhJAx//7jqy8Ir4ecxVJtar3cZ6IiMilwufHQZIz8/dTHyQRERFvUUDyURbXfGxq4BMREfE0BSQf5eqDpBYkERERj1NA8lGlg0VqHCQRERHPU0DyURb1QRIREfEaBSQfZVUfJBEREa9RQPJRVvVBEhER8RoFJB9lOd4HqUR9kERERDxOAclH6S02ERER71FA8lGlfZA0F5uIiIjnKSD5qNI+SMVqQRIREfE4BSQfZXG1ICkgiYiIeJoCko+ymo/3QVInbREREY9TQPJRmotNRETEexSQfJTGQRIREfEeBSQf5RoHSQFJRETE4xSQfJRrqhH1QRIREfE4BSQfpUdsIiIi3qOA5KNKR9JWJ20RERHPU0DyUaV9kGwOtSCJiIh4mgKSjyp9xFZiUwuSiIiIpykg+agTnbTVgiQiIuJpCkg+yqLJakVERLxGAclHud5iUwuSiIiIxykg+agTA0WqBUlERMTTFJB8lEXjIImIiHiNApKP8tdktSIiIl6jgOSjTgwUqRYkERERT1NA8lEnBopUC5KIiIinKSD5KM3FJiIi4j0KSD7Koj5IIiIiXqOA5KOs6oMkIiLiNQpIPqp0qhH1QRIREfE8BSQfZTGrBUlERMRbFJB8lFVzsYmIiHiNApKPsmguNhEREa9RQPJRpS1IxTYFJBEREU9TQPJRVg0UKSIi4jUKSD7qxGS1CkgiIiKepoDko1xzsakPkoiIiMcpIPmo0kdshgF2PWYTERHxKAUkH1XaggQaC0lERMTTFJB8VOlbbKCAJCIi4mkKSD7q5ICkjtoiIiKepYDko/zMJkzHn7Kpo7aIiIhnKSD5MNdYSGpBEhER8SgFJB+msZBERES8QwHJh7mmG1EnbREREY9SQPJhVk1YKyIi4hUKSD7Moj5IIiIiXqGA5MNc043oEZuIiIhHKSD5sNI+SDZNNSIiIuJRCkg+rLQPUolNLUgiIiKepIDkw0r7IJWoBUlERMSjFJB8mOstNvVBEhER8SgFJB9mOd4HqURvsYmIiHiUApIPs5g1DpKIiIg3KCD5MH9LaQuSApKIiIgnKSD5sNIWJD1iExER8SyfCEhvvvkmCQkJBAYG0qVLF1auXHnG8nPnziUxMZHAwEBat27NV1995bZ9+PDhmEwmtyUlJcWtTEJCQpkykydPrvJrOx+lfZA0kraIiIhneT0gzZkzh7FjxzJ+/HjWrFlD27ZtSU5OJjMzs9zyy5YtY9CgQYwYMYK1a9fSt29f+vbty8aNG93KpaSkkJaW5lo++uijMsd69tln3cqMHj26Wq7xXGkuNhEREe/wekB6+eWXueuuu7j99ttp0aIFU6dOJTg4mGnTppVb/rXXXiMlJYWHH36YpKQkJk6cyGWXXcaUKVPcygUEBBAfH+9aoqKiyhwrLCzMrUxISEi1XOO5co2DpBYkERERj/JqQCouLmb16tX06NHDtc5sNtOjRw9SU1PL3Sc1NdWtPEBycnKZ8osXLyY2NpbmzZszatQoDh06VOZYkydPpkaNGrRv354XX3wRm8122roWFRWRk5PjtlQ3zcUmIiLiHRZvnjwrKwu73U5cXJzb+ri4OLZs2VLuPunp6eWWT09Pd31OSUnhpptuomHDhuzYsYPHH3+cnj17kpqaip+fHwD3338/l112GdHR0Sxbtoxx48aRlpbGyy+/XO55J02axDPPPHM+l1tp/q4+SApIIiIinuTVgFRdbrnlFtfvrVu3pk2bNjRu3JjFixdz3XXXATB27FhXmTZt2uDv78/f/vY3Jk2aREBAQJljjhs3zm2fnJwc6tWrV41XcXILkh6xiYiIeJJXH7HFxMTg5+dHRkaG2/qMjAzi4+PL3Sc+Pr5S5QEaNWpETEwM27dvP22ZLl26YLPZ2L17d7nbAwICCA8Pd1uqW2kfJHXSFhER8SyvBiR/f386dOjAokWLXOscDgeLFi2ia9eu5e7TtWtXt/IACxcuPG15gD/++INDhw5Rq1at05ZZt24dZrOZ2NjYSl5F9TkxF5takERERDypUgHphRde4NixY67PS5cupaioyPU5NzeXe+65p1IVGDt2LO+88w7vv/8+mzdvZtSoUeTn53P77bcDMHToUMaNG+cqP2bMGBYsWMBLL73Eli1bmDBhAr/88gv33XcfAHl5eTz88MMsX76c3bt3s2jRIm688UaaNGlCcnIy4Ozo/eqrr7J+/Xp27tzJzJkzefDBB7ntttvKfdvNW0rHQSpWHyQRERHPMirBbDYbGRkZrs9hYWHGjh07XJ/T09MNs9lcmUMahmEYb7zxhlG/fn3D39/f6Ny5s7F8+XLXtu7duxvDhg1zK//xxx8bzZo1M/z9/Y2WLVsaX375pWtbQUGBcf311xs1a9Y0rFar0aBBA+Ouu+4y0tPTXWVWr15tdOnSxYiIiDACAwONpKQk47nnnjMKCwsrXOfs7GwDMLKzsyt9vRX10rdbjQaPzjeenPdrtZ1DRETkUlLR72+TYRgVfn5jNptJT093PYYKCwtj/fr1NGrUCHD2BapduzZ2u706spxPycnJISIiguzs7Grrj/TGot95aeE2BnWux6Sb2lTLOURERC4lFf3+9vpAkXJ6pY/Y9BabiIiIZykg+bATnbTVB0lERMSTKj0O0rvvvktoaCgANpuNGTNmEBMTAzg7aUvVsZiPj4PkUAuSiIiIJ1UqINWvX5933nnH9Tk+Pp7//Oc/ZcpI1bBajj9is6kFSURExJMqFZBON4iiVA+ra6BItSCJiIh4kvog+TBNVisiIuIdlQpIqampzJ8/323dBx98QMOGDYmNjWXkyJFuA0fK+bG4JqtVC5KIiIgnVSogPfvss/z222+uz7/++isjRoygR48ePPbYY3zxxRdMmjSpyit5qbIe76StudhEREQ8q1IBad26dVx33XWuz7Nnz6ZLly688847jB07ltdff52PP/64yit5qbK6phpRC5KIiIgnVSogHTlyhLi4ONfnJUuW0LNnT9fnTp06sW/fvqqr3SXOonGQREREvKJSASkuLo5du3YBUFxczJo1a7j88std23Nzc7FarVVbw0uYVX2QREREvKJSAalXr1489thj/PTTT4wbN47g4GCuuuoq1/YNGzbQuHHjKq/kperEQJFqQRIREfGkSo2DNHHiRG666Sa6d+9OaGgoM2bMwN/f37V92rRpXH/99VVeyUuV3mITERHxjkoFpJiYGH788Ueys7MJDQ3Fz8/PbfvcuXMJCwur0gpeyvxdk9WqBUlERMSTKhWQ7rjjjgqVmzZt2jlVRtydGChSLUgiIiKeVKmANGPGDBo0aED79u0xDH1pVzern8ZBEhER8YZKBaRRo0bx0UcfsWvXLm6//XZuu+02oqOjq6tulzyLWX2QREREvKFSb7G9+eabpKWl8cgjj/DFF19Qr149BgwYwDfffKMWpWqgudhERES8o9KT1QYEBDBo0CAWLlzIpk2baNmyJffccw8JCQnk5eVVRx0vWVZ10hYREfGKSgckt53NZkwmE4ZhYLfbq6pOclxpQHIY4HCohU5ERMRTKh2QioqK+Oijj/jzn/9Ms2bN+PXXX5kyZQp79+4lNDS0Oup4ySp9xAYaLFJERMSTKtVJ+5577mH27NnUq1ePO+64g48++oiYmJjqqtslz2o+kV9tdoOASv1piYiIyLmq1Ffu1KlTqV+/Po0aNWLJkiUsWbKk3HKffvpplVTuUndyC5LeZBMREfGcSgWkoUOHYjKZzl5QqkTpXGwAxeqoLSIi4jGVHihSPMdkMmH1M1FiNzRYpIiIiAed11tsUv00WKSIiIjnKSD5OA0WKSIi4nkKSD6udCwkm8ZBEhER8RgFJB9X2lFbLUgiIiKeo4Dk405MN6IWJBEREU9RQPJx1uN9kGxqQRIREfEYBSQfZ1ELkoiIiMcpIPm40j5IGgdJRETEcxSQfJzrLTa1IImIiHiMApKPK+2DpKlGREREPEcBycdZ1IIkIiLicQpIPs71Fpv6IImIiHiMApKPK52LTW+xiYiIeI4Cko/TOEgiIiKep4Dk4060ICkgiYiIeIoCko+zWvSITURExNMUkHycVQNFioiIeJwCko+zHO+DpBYkERERz1FA8nEaB0lERMTzFJB8XOkjNnXSFhER8RwFJB9XOhdbifogiYiIeIwCko/TIzYRERHPU0DycRooUkRExPMUkHyca6BIh1qQREREPEUBycdZ1IIkIiLicQpIPs7fTyNpi4iIeJoCko87MVCkWpBEREQ8RQHJx+ktNhEREc9TQPJxmotNRETE8xSQfJxFfZBEREQ8TgHJx1nVB0lERMTjFJB8nFV9kERERDxOAcnHWUonq1UfJBEREY/xiYD05ptvkpCQQGBgIF26dGHlypVnLD937lwSExMJDAykdevWfPXVV27bhw8fjslkcltSUlLcyhw+fJjBgwcTHh5OZGQkI0aMIC8vr8qv7XypBUlERMTzvB6Q5syZw9ixYxk/fjxr1qyhbdu2JCcnk5mZWW75ZcuWMWjQIEaMGMHatWvp27cvffv2ZePGjW7lUlJSSEtLcy0fffSR2/bBgwfz22+/sXDhQubPn8+PP/7IyJEjq+06z5XGQRIREfE8k2EYXm2a6NKlC506dWLKlCkAOBwO6tWrx+jRo3nsscfKlB84cCD5+fnMnz/fte7yyy+nXbt2TJ06FXC2IB09epTPPvus3HNu3ryZFi1asGrVKjp27AjAggUL6NWrF3/88Qe1a9c+a71zcnKIiIggOzub8PDwyl726e36CVLfhJv+DYERLN95iFveXk7jmiEs+vs1VXceERGRS1BFv7+92oJUXFzM6tWr6dGjh2ud2WymR48epKamlrtPamqqW3mA5OTkMuUXL15MbGwszZs3Z9SoURw6dMjtGJGRka5wBNCjRw/MZjMrVqwo97xFRUXk5OS4LVXO4YAv/w7bvoYV/wZOvMVm02S1IiIiHuPVgJSVlYXdbicuLs5tfVxcHOnp6eXuk56eftbyKSkpfPDBByxatIjnn3+eJUuW0LNnT+x2u+sYsbGxbsewWCxER0ef9ryTJk0iIiLCtdSrV6/S13tWZjN0f8T5e+oUOHYUi1l9kERERDzN632QqsMtt9xCnz59aN26NX379mX+/PmsWrWKxYsXn/Mxx40bR3Z2tmvZt29f1VX4ZC37Qc1EKMyGFVPVB0lERMQLvBqQYmJi8PPzIyMjw219RkYG8fHx5e4THx9fqfIAjRo1IiYmhu3bt7uOcWoncJvNxuHDh097nICAAMLDw92WamH2g+6POn9P/Rdh5ANwtKCEI/nF1XNOERERcePVgOTv70+HDh1YtGiRa53D4WDRokV07dq13H26du3qVh5g4cKFpy0P8Mcff3Do0CFq1arlOsbRo0dZvXq1q8z333+Pw+GgS5cu53NJVaNFX4htAUXZ1NsynZa1wym2O/ho1V5v10xEROSS4PVHbGPHjuWdd97h/fffZ/PmzYwaNYr8/Hxuv/12AIYOHcq4ceNc5ceMGcOCBQt46aWX2LJlCxMmTOCXX37hvvvuAyAvL4+HH36Y5cuXs3v3bhYtWsSNN95IkyZNSE5OBiApKYmUlBTuuusuVq5cydKlS7nvvvu45ZZbKvQGW7Uzm12tSKYVUxnZKQqA/6Tu0aM2ERERD/B6QBo4cCD//Oc/efrpp2nXrh3r1q1jwYIFro7Ye/fuJS0tzVW+W7duzJo1i7fffpu2bdvyySef8Nlnn9GqVSsA/Pz82LBhA3369KFZs2aMGDGCDh068NNPPxEQEOA6zsyZM0lMTOS6666jV69eXHnllbz99tuevfgzSeoDca2gKIcb8j4lJtSftOxCvv0t4+z7ioiIyHnx+jhIF6pqGwfpZJs+h4+HgH8o/2r7KS/8lEXHBlF8Mqpb9ZxPRETkIndBjIMkZ5H4F4hrDcV5DDPNx+pn4pc9R/j1j2xv10xEROSipoDky8xmuNbZ/ypk3XRuahEBwPSlu7xZKxERkYueApKva9YTajSB4lxG11wDwBcbDpCZW+jliomIiFy8FJB8ndkMne4EoO7vM2lfL4ISu8GsFXrlX0REpLooIF0I2g4CazAc3MxDiVkAfLh8L0U2u5crJiIicnFSQLoQBEVCmwEAdM2aR3x4IFl5Rfxv3QHv1ktEROQipYB0oeh0FwDmrfO5t2MIAK8s3MaxYrUiiYiIVDUFpAtFfCuo3xUcNgb5LaJOZBBp2YW89/NOb9dMRETkoqOAdCE53lnbsvYDHr2+EQD/WrxDb7SJiIhUMQWkC0lSHwiJhbx0evuvoV29SAqK7byycJu3ayYiInJRUUC6kFj8ocNwAEyr3uWpvyQBMGfVPrak53ixYiIiIhcXBaQLTcfbweQHe5bSITCNG1rXwmHAP77cjKbVExERqRoKSBea8NqQ9Bfn75/fz6N/boi/n5mffs9i8baD3q2biIjIRUIB6UL052chMBL2/0L9Vf/H8CsSAHjm8984cPSYV6smIiJyMVBAuhBFJUD/dwETrHqXB2quJjYsgN2HCugz5WdW7jrs7RqKiIhc0BSQLlRN/wzXPAZA8Dd/5/Obw2lRK5ysvGJufWc5Hy7f4+UKioiIXLgUkC5kVz8CTa8HWyHxC+7iv8Nb8Jc2tbA5DJ78bCPjPv2VwhKNtC0iIlJZCkgXMrMZ+v0bIhvAkd0EfTacN3rX5pGU5phM8NHKvXR5bhHPfPEbv2fkeru2IiIiFwyToXfDz0lOTg4RERFkZ2cTHh7u3cqkrYf3nC1JBERAj6f5IfQvPPm/Tew/qdN2xwZR9O9Ql8sb1SChRjAmk8mLlRYREfG8in5/KyCdI58KSAAH1sEXYyBtnfNznY7Yb3iZn3JrMWvFXhZtycTuOPFHHRPqT8cG0XRMiKJ9/Sha1g4n0OrnlaqLiIh4igJSNfO5gATgsMOq9+D7iVCUAyYzNLwamiaTVbs7s3f4s2TbQdbvy6bY7nDb1WI2kVgrjLZ1I2lXL5IODaJoGBOiViYREbmoKCBVM58MSKVy02HBOPjtU/f10Y2g0TWUxLRgh6k+S3PiWLa/hPV/HCUrr7jMYaKCrXRoEMVlDaLo0jCaNnUjsfqp25qIiFy4FJCqmU8HpFKHdsC2BfD7t7B7KThKypYJr4NRuz3ZMe3Z7JfEj/l1+OWPAtb/kU2xzb2VKTTAQpeG0XRrEsNVTWNoFhfmoQsRERGpGgpI1eyCCEgnK8qFnYvhj1WQuRkyNkHOH2XLma1Quz22RtexPbIrP+fV4Zc92SzfdYijBe4Bq3WdCG7pXI8+bWsTFmj1zHWIiIicBwWkanbBBaTyHDsKGb/B/l9g30rnkp/pXiakJjT5M47Ev7A5pDM/78ph6Y5DLN9xyNWPKdjfj95tanPHlQ1pHq9WJRER8V0KSNXsoghIpzIMOLIbdv0I2xfCjsVQfNL4ScE1oFV/aHsLhyNa8ena/cxauZedB/MBZ0fvB3o05e7ujbGor5KIiPggBaRqdlEGpFPZimHfctj6NWz8L+RlnNgW0wza3YrRZiCrDgXy9o87+G6zs/XpsvqRvDygHQkxIV6quIiISPkUkKrZJRGQTma3Ofswrf8ItnwJtuMDUJrM0KQHRrvBzCtow9PzfyevyEawvx9P3tCCQZ3raagAERHxGQpI1eySC0gnK8yBTZ/B2pnOFqZSNZNI6/0hD3x9kBW7DgNwU/s6TOrfmgCLBqEUERHvU0CqZpd0QDpZ1u+wbias+QAKDkFEPRyDP+XdLX48v2ArdodB54Ropg7pQHSIv7drKyIil7iKfn+rJ62cn5im0GMCjFwMNZpA9j7MM1IY2Tib6cM7ERZgYeXuw/T711J2HMzzdm1FREQqRAFJqkZkfbjjG6jVztmS9H5vrvbbyKf3dKNuVBB7DhXQ782lLNue5e2aioiInJUCklSdkBgYPt85/1txHswaQNO9H/PZPd24rH4kOYU2hk5byX+W7/F2TUVERM5IAUmqVkAY3DoXkvqAvRi+HEvMF8OZdWsTbmxXG5vD4KnPNvL4vF/LTGUiIiLiKxSQpOpZA+Gv78P1/wd+/rDtawLfvYpXO2TxWM9ETCaYtWIvt727gqy8Im/XVkREpAwFJKkeZjN0Gw13fQ81EyEvA9PMm7m7cBrThnZwdd6+ccpSNu7P9nZtRURE3CggSfWKb+18w63zSOfn1Clcu30y8+7pRqOYEPYfPcZfp6by5YY0r1ZTRETkZApIUv2sQdDrRej3b8AEq6fT5JdnmHdPN7o3q8mxEjv3zlrDywu34XBoWC4REfE+BSTxnLa3wI1vAiZY9Q4RS55m2rCO3HVVQwBeX/Q7o2auJr/I5t16iojIJU8BSTyr/WDo/Zrz9xVv4ffdUzzRK4l//rUt/n5mvvktg/5vLWP/0WPeraeIiFzSFJDE8zoMg7+84vw9dQr8px8319jNR3d1ISY0gC3pufR9cykb/jjq1WqKiMilSwFJvKPjHdDrn2Dyg50/wIwb6LDoFhbcUEBiXCgHc4sY8O9Uvvkt3ds1FRGRS5ACknhP57tg9GpnWPILgH0riPnfEL4MeYaeTYIoLHFw94erefennWhOZRER8SQFJPGu6IbOx20PbIBu94N/KH4HVvNm0Fvc1rkuhgH/9+VmnvrfRmx2jbwtIiKeoYAkviEsHq6fCMO/BEsg5t+/ZWLkfJ68IQmTCT5cvpc7P/iFPL3hJiIiHqCAJL6ldjvXW26mH1/gzphNTL2tA4FWM4u3HuSvU1NJy9YbbiIiUr0UkMT3tL0Fuoxy/j7vbpJjs5kzsisxof5sTsuh75tL+e2ApicREZHqo4Akvun6idDgSijOhdmDaVvTxLx7rqBpbCgZOUX8dWoqCzZqehIREakeCkjim/ys8NcZEF4HDv0OswdTL9Tgk1HduKJJDQqK7dz94Rr++c1W7JqeREREqpgCkviu0Jow8EPwD4PdP8GH/YkwHeP92zsz4krn9CRTftjOiPdXkX2sxMuVFRGRi4kCkvi2OpfB0M8gIAL2psJ/+mEpzuGpv7Tg1YHtCLA4O2/fOOVntqbneru2IiJykVBAEt9XtyMM+x8ERcH+X+CDPlBwmL7t6/DfUd2oExnE7kMF9PvXUuZvOODt2oqIyEVAAUkuDLXbw7D5EBwDaethxl8g7yCt6kTwxegrXf2S7pu1lue+2qxBJUVE5LwoIMmFI76VcyDJ0DjI/A3e7w15mUSH+PP+7Z35W/dGALz9406GvLeSQ3lFXq6wiIhcqBSQ5MISmwjDv4KwWnBws7MlKTcDi5+ZcT2T+Nfgywj29yN15yF6v/Ezmw7keLvGIiJyAVJAkgtPTBNnS1J4HcjaCu//BXLTAejVuhb/u/cKGsWEcCC7kAH/TmXp9iwvV1hERC40CkhyYarRGIbPh/C6kLUNZtwAOc4O2k3jwph37xV0aRhNXpGN4dNX8tna/V6usIiIXEgUkOTCFd0Ibv8SIurDoe3w7p/hwDoAIoKsfDCiM39pU4sSu8EDc9bx1uIdGIYGlRQRkbPziYD05ptvkpCQQGBgIF26dGHlypVnLD937lwSExMJDAykdevWfPXVV6cte/fdd2MymXj11Vfd1ickJGAymdyWyZMnV8XliCdFJThbkmo0gZw/YFoy/PoJAAEWP16/pT13XeUcVPL5BVt4fN5GCkvsXqywiIhcCLwekObMmcPYsWMZP348a9asoW3btiQnJ5OZmVlu+WXLljFo0CBGjBjB2rVr6du3L3379mXjxo1lys6bN4/ly5dTu3btco/17LPPkpaW5lpGjx5dpdcmHhLVAO5cBE2vB1sh/HcELHwaHHbMZhNP3NCCp//SApMJPlq5l95v/MzmNHXeFhGR0/N6QHr55Ze56667uP3222nRogVTp04lODiYadOmlVv+tddeIyUlhYcffpikpCQmTpzIZZddxpQpU9zK7d+/n9GjRzNz5kysVmu5xwoLCyM+Pt61hISEVPn1iYcERcKg2XDlWOfnpa/BzL/CoR0A3HFlQ2bc3pmaYQH8npnHjVOW8t7Pu3BoHjcRESmHVwNScXExq1evpkePHq51ZrOZHj16kJqaWu4+qampbuUBkpOT3co7HA6GDBnCww8/TMuWLU97/smTJ1OjRg3at2/Piy++iM1mO88rEq8y+0GP8dD/PbAEwY5F8EYH+Hgo7F9D92Y1WTDmKnokxVJsdzBx/iaGz1hFenaht2suIiI+xqsBKSsrC7vdTlxcnNv6uLg40tPTy90nPT39rOWff/55LBYL999//2nPff/99zN79mx++OEH/va3v/Hcc8/xyCOPnLZ8UVEROTk5bov4qNY3w12LoFlPwIBN/4N3roX3+1DjyAbeGdqRiX1bEWAx8+O2g/z5lSXMXrlXHbhFRMTF4u0KVLXVq1fz2muvsWbNGkwm02nLjR071vV7mzZt8Pf3529/+xuTJk0iICCgTPlJkybxzDPPVEudpRrEtYRbZ0PGJufjto2fwK4lMCMVU//3GHJ5H7o2iubvczewft9RHvv0V77YcIDJN7WhXnSwt2svIiJe5tUWpJiYGPz8/MjIyHBbn5GRQXx8fLn7xMfHn7H8Tz/9RGZmJvXr18disWCxWNizZw9///vfSUhIOG1dunTpgs1mY/fu3eVuHzduHNnZ2a5l3759Fb9Q8Z64FnDTv+H+dZD4F7AXw9xhsHYmTWLD+HRUN57olUSAxczS7Ye4/pUfefennZrLTUTkEufVgOTv70+HDh1YtGiRa53D4WDRokV07dq13H26du3qVh5g4cKFrvJDhgxhw4YNrFu3zrXUrl2bhx9+mG+++ea0dVm3bh1ms5nY2NhytwcEBBAeHu62yAUksh4M+ADaDwHDAf+7B5a/hZ/ZxF1XN+KbB66mS8NojpXY+b8vN9Pr9Z9YphG4RUQuWV5/xDZ27FiGDRtGx44d6dy5M6+++ir5+fncfvvtAAwdOpQ6deowadIkAMaMGUP37t156aWXuOGGG5g9eza//PILb7/9NgA1atSgRo0abuewWq3Ex8fTvHlzwNnRe8WKFVx77bWEhYWRmprKgw8+yG233UZUVJQHr148yuwHfd6AgHBY/iYseAwKc6D7IyTEhPDRXZcze9U+XvxmC9sy8rj13RX0ah3PEze0oE5kkLdrLyIiHuT1gDRw4EAOHjzI008/TXp6Ou3atWPBggWujth79+7FbD7R0NWtWzdmzZrFk08+yeOPP07Tpk357LPPaNWqVYXPGRAQwOzZs5kwYQJFRUU0bNiQBx980K1fklykTCZI/odzWIAf/gGLn4Md38M1j2FudA23dqlPr9bxvLxwGx8u38NXv6bz/ZZMhndryMirGxEd4u/tKxAREQ8wGXp155zk5OQQERFBdna2HrddqFa+A988AfYi5+d6XeCax6DRtWAysTkth/Gf/8bKXYcBCPH3Y1i3BO66qhFRCkoiIhekin5/KyCdIwWki0ROmvMtt9XTnaNwAzS4Am54CWKTMAyDRZszeeW7bfx2wDm0Q2iAhaFdG3Db5Q2orUdvIiIXFAWkaqaAdJHJTXcGpV+mOYOS2QLdRsPVj4B/MIZh8O2mDF797nfXNCV+ZhN/TopjaNcGdG1c44zDSoiIiG9QQKpmCkgXqew/4OtHYct85+fIBnDDy9DUOXq7w+EMStOX7mLF8UdvAE1iQxlyeQNuuqwOYYHlT20jIiLep4BUzRSQLnJbvoSvHoGcP5yfm14Pf3oSarV1Fdmanst/lu9m3pr95BfbAWc/pX6X1WFo1wSaxYV5o+YiInIGCkjVTAHpElCUB4snwfK3wHAGIFrcCNc8DrGJrmK5hSV8umY/H6TuZsfBfNf6Lg2jGd4tgT+3iMPi5/V5oUVEBAWkaqeAdAk5tMMZlH79BDDAZIZW/aHLKKjbwVXMMAxSdxzig9Q9fLspHcfxf1m1IgK57fIG3NKpHjVCy05jIyIinqOAVM0UkC5BGb/BD8+d6J8EUPsy6HwXtLwJrIGu1QeOHmPmij18tHIfh/OLAfC3mOnZKp7+l9XliiYx+JnVqVtExNMUkKqZAtIl7MBa52O33+Y553YDCIqGDsOdYSm8tqtoYYmdLzek8X7qbjb8ke1aHx8eSN/2deh/WR2aqq+SiIjHKCBVMwUkIe8grP0AVk070ZnbbHG2JnW9B2q3dxU1DIMNf2Tzyeo/+Hz9AbKPlbi2NYkNJbllHMkt42ldJ0LDBYiIVCMFpGqmgCQudhts/crZqrR32Yn19S6Hjrc7O3ZbTwwoWWSz8/3mTP675g8Wbz2IzXHin2DtiEB6tIjj2sRYujaqQaDVz5NXIiJy0VNAqmYKSFKuA2sh9V/w26fgsDnXBURAmwHQYRjEt3Yrnn2shMVbM/nmt3QWbz1IwfHhAgACrWauaBzDtYmx/CkxVqN2i4hUAQWkaqaAJGeUkwZrP3Q+gju698T6uNbQsi+07Ac1GrvtUlhi5+ffs/h+aybfb84kPafQbXtifBh/Oh6W2tePUidvEZFzoIBUzRSQpEIcDti1GFa/7xx80nGi7xG12jqDUsubIKqB226GYbA5LZcftmby/ZZM1u49wklP4ogIsnJV0xi6N6tJ92Y1iQ0PpEoU58OR3XB0nzPYZe8FsxUuvwdCa1bNOUREvEgBqZopIEmlFRx2DhHw2zzYueTE4JMAdTtBq5udrUth8WV2PZxfzI/bDrJoSyZLtmaSU2hz254YH8ZVTWPo1iSGzgnRhARYKl6vvIPOem36H+z+6cSjwZOF1Ya/zoD6XSp+XBERH6SAVM0UkOS85GfB5s+dYWnXT0DpP0MT1O8KzVOgeS+IaVpmV5vdwfo/jrJk60GWbDvIhv3ZnPyv2Opnon29KC5vXIOODaJoXz/yxPxwJYVweAcc3ApZvzsD0Z6lYDhOHCAoCiLqQWR958/t38Gh351v6P15Ilw+CvSmnYhcoBSQqpkCklSZ3HT47TPY+F/4Y6X7tujG0PTPENXQ2bJUupgtUJgNhdnkHM1i+559pO/fS07WfoKKDxFDNmGmAiw4sGAj2AKhfjYiSg5iwlG2DrXbQ1If5xt3p/SNoigXPh/tDHPgLNNnCgTq772IXHgUkKqZApJUi6N7YesC2Pa1s2Xp5D5LVSTHCGYndTgS3JCSmBZYW/6FxKRW1Io4w1tyhgEr34FvHnfWqWYiDJnnNiimiMiFQAGpmikgSbUryoUd38OeVMhNc7Y05aZBXgY47BAUCYERJ5aQWAiNhdA458/ACDBbOFxosPXgMTZlFLL8UBA/p/txrKRsK1KtiEDa14+kVZ0IkuLDSawVRnx4oPvAlftWwcdDnPWIrA9D/wfRjTx3T0REzpMCUjVTQBKvKf0ne479gOwOgx0H81i/7yjr/zjKmj1H2ZKe4/aWXKnIYCuJ8WEkxoeTVCuMpFrhNA84QsCsfnBklzOMDZkHcS3P44JERDxHAamaKSDJxSS/yMav+7NZu/com9Ny2JKew46D+djLSU1mE1wWXcyrJc9St3gnJdYIsvp+SFzSVZg1NpOI+DgFpGqmgCQXu8ISO9sz844Hply2pOewOS2Xw/nOCXrDyWO6/4t0MP9OgRHAa8YAVtToR0JcNE3jwmgSG0qT2FAaRAdj8TN7+WpERJwUkKqZApJcigzDIDO3iC3puWxNz2Hn/kxu3vE4HW1rANhv1OBVW38+tV+FHec8clY/Ewk1QmgaF0qzuDAS48NoHh9O/ehgjQYuIh6ngFTNFJBEjrPbsK+dibF4Epa8NADS/Rvwnv9tfJjdutwO4eCca655XBgtaoeTVCucFrXCSawVTmhlBrkUEakkBaRqpoAkcoqSY7DqXfjpJTh2BACjTicOdn2cTdZW/J6Rx9YM56O63zPyKLKVH5zqRAbRODaUJjWdj+ga1wyhQY0QYsMC1MdJRM6bAlI1U0ASOY3CbFj6Oiz/F5QUONc1S4HrxkNcC8D5Jt3uQ/lsSctlU1o2m9Ny2XQgp8wEvSfzt5ipFxVE/ehg6kUHUycyiLpRwdSNCqJOVBA1QvzdhyQQESmHAlI1U0ASOYvcdFjyvHOiXsMOmCDxBuh6r3M6lXLCzJH8YrYfzOP3jDy2Z+ax/WAeu7LyOHC0sNw36k4W7O9H3ShnaKp3/GftyCBqRwZSJyqImBC1QImIAlK1U0ASqaCs7fD9s87JcEvVaguX3wst+4HF/6yHsNkdpGUXsvdwAXsOFbD/aAF/HDnGH0eOsf/IMTJyCznbf8n8/czUDAsgLjyAuPBAYsMCiA0PpGZoADXDTizRIf5Y9dadyEVLAamaKSCJVFLmZlj+FmyYA7bjj9ICIqBBV2hwBSRcAfFtwa/ynbSLbHYOHC1k32FncNp3pID9R45x4Ogx9h89RkZOYbkDYZ5OeKCFmFBnWIoJdYaq2PBA4sIDiQsPoEaIc1tUiJUAi1+l6ysi3qOAVM0UkETOUf4hWD3dObdbXrr7Nv9Q5zxvMc0gponzZ42mEJUA1sBzPmWJ3UFGTiGZuUVk5hSSkVNERk4hB3OLOJhX5PyZW8Sh/OKzPso7VWiAhagQKzVCAqgR4k90iD81QgOICrYSFexPZLCVqBB/ooKthAVaCQ+0Emg1n1t/qZw0yNgIZj+whoA1CKzBYAkAP6tzEmOzn/MnnBh1HQMwOdeXllN/LblEKSBVMwUkkfNkt0H6etizDHYvhb3LnB28y2WC8DpQo5Fz7rfoRhDd+PjPhs6gUAUcDoPsYyUcyi/iUF4xh/KLOZhbRGbuiVCVmeMMUkcKzhymIsijhXkPiaa9JJn2Em86TB5B5BrB5JuCKbGEUGyNwB4YhREYhSk0Bv+QSCIDzIT7G4T7Q7jVQY2CnURm/UJw+ir8svdWyXUCYPJzBtD6XaDe5c6fUQ0VnOSip4BUzRSQRKqYww4Ht0LWVme/paxtcOh35+/FuWfYsTQ8NT6+NHEuEfUgvBYERlbLl75hGOQcs3E4v4jczF3Y96/HkvkroUc2EZ2zhYiSzCo/p90wsYM6YPIjxFREIMUEUoQ/xZgNO36UP3RChYXEQp3LoPZlJ36G1Kiayov4CAWkaqaAJOIhhgH5WXB45/Flx4nfD+2EotO1Oh1nCYKweAir5fyyD46BkBjnT/9gMJndFz8r+PmfWEwmsBeDvcT5s+QYHNkNh7ZD1u9waMfpA1xkfYhr7ZzMN6oBRnE+xQXZlOQfxVaQjT3/CBQcwlx4BGvxUSy2POz4YcNCieFHMX6kE8MqezOWljRjjaMJeQSf9lJNOPA7vrhuH6bj2wz8cGDBjgU7QRTR0rybLpbf6WjeRgt2YsVW5phF5iAKLeGUWMIo8Q/H5h9BcWAMtqAYbME1MUJqYgmJITgsgpCwSELDIwkIja6yVr0KO3YEdi6G7YucfzY1E50hr04H5+9mP3A4nOXyDzp/2ovBYXOGc4fN+ed98t+R8q6huMD5aDg3w/kzPwsCwk78HQuLh4BwtcT5MAWkaqaAJOIDDAMKDjtD06Edzi/GQ9udv+f84RqwstqZLc4v4fg2zjf0arVxhqLAiCo7hcNhkFdsI7fQRm5hCXmFzt9zCkvIK7JRUGQnr8hGfpGN/GIb+UV2CortFBTbyC+2U1Bko6DYTn6xs2yx3b21KYBiWpp208a8kzbmnbQ17aCxOe2c67vbqMVvpiZsNjVhk7kpB621CA7wJyTQn9AAK2GBFqKsJUT5FRJpPkaEuZAAqxlC4zGFxeMXGkPQ8XIRgRYi/IqwFmc7/7wLspw/87MgL8P5mHb/L2CcpgXNGuIMwwWHTl+m3P2CAZNzmAqH3fmzIvsHhDsfX9ZMhNhEqJkEdTtCUGTFzy3VRgGpmikgiVwASgohN+34ku78gszPcn7B5mc536YzDOeXnuFwfgHabcdbjIqcrUaG40RrkiXA+TOy3olHeTWaODuRWwK8fbWVUmSzu0JVabDKLbJxrNhO/vEwVZSfjV/BQYzCo5iLcvArOoqlKJuQksOE2g4RajtChP0wQfY8gowCgoxjhFKI2XT+Xyslhh9ZRGDFRgT5WE32s+6TGZDA7sjLORKeSHzxbuJzfyM6+zes9gK3craACBwBUZisAZj9rJj9LJjMFrAVnfi74Sg5/YksgRAa52wtCqkJxXnOv1+5aafvR+fnD02vh9Y3OwdOPbV1qvSrWC1P1U4BqZopIImIuHM4DHILi8k7nIFfxq9YM9YQkLGOgMz1WI8dLFPebvKjyBzKMXMwBaYQHIaDCPthIh1Hyz1+kWHlKCEcNsI5ZIRxhDAOGeFsMerzo70NB4gps48ZBw1NafhjI8sI5whh2Cg7lESQ1Y+QAD8CrX4EWszUsBQR45dHoL+FAIuFAH8rAf5WzP4hOPzDsFrMWMxmrBYTwVY/ggMshAZYCDUXE1WSTkTudkJzdhB0dBv+Wb/hd2TniZP5h0HDq8F2DPIOQn6mM5SF1IQm1zmXRtdCcHTl/xCOHXX+j4A1yBnkSn+eLXgZhrPFNTf9xCPE4jznSxAxzZ39/MyVHB/MMJyBsfCosw7+oeAfcqIudpvznMcOO386TgrBpWVikyAoqnLnPQsFpGqmgCQiUgmG4d5ah3Gij9ep7CWQl+l8fGYJxB4YSS4hHC2xkFtoo9Bm51ixnWMlzp8Fx1u98opsFBTbyCuyc6z00eLxx41FNgcldudSbHNQZHOQX2Q77ZyAVa25aS99/ZZxo98yapuyzlregZkDgU0o8QvGzwRmk4HZZMLuF0hRQA2KAmtQElADR0A4Ycf+IDL3d8KytxF4LL3MsQxMOELjMCLqYUQ0wBTVAHNgGObsfXB0DxzZA0f3OltNT8caDDFNnSGnKBeK850BqqTQ2XpaOuSENcjZApuf5Qxqp7bEmczOY2A6e/9BgNs+dQbGKqSAVM0UkERELnzFx4NSbqGNghIbhSUOjhXbKSw5KYCVOANXQbGdwhIHtuNBq8RhUGxzcKzk+GPJ448sC4ptrn0LSxxu/b1MOLjM9DvtzNvJJpQsI4KDRiSHjDAamw/Q3byBq80bSDTvO+dryjMCCaCkQo8lT5VNGIfN0RwxR1NiDqCOfT/x9gNYqPyxStnMgfg5ijGd5i1Lm38EtoBIMFswmUyYSl8tMJlw9H6dgEZXnPO5y1PR7+/KD1krIiJykfC3mPG3+BMVcvYpb86Vze4MUQXFpcvVHCu2U2I3KLE7sDkcFNsMiu0Oikrs/GJzsDYvjcjDG7DZbZTYDWx2B8UOA3NJASHFhwmxHSak5AhB9hwyzbHsMjdgh7k+v1OPw/Zgim0OHCXFYC/EXFJATeMQdU0HqWc6SF3TQUJNx9hvxLDPiGWfUZN9Rk0yjGiKsZapvwUb9UwHaWw6gD8lFBBInhFIAYEU4o8/NoIoItBUTDCF2LCQZYRz2AjnMGEU4Q8YBFJMGMcIMR3DjMERI5RsQnEUnv7R3YySplxTbX8yZ6aAJCIiUo0sfmbC/MyEBZYNH6fXALi8yupgGAZ2h4Ht+FJic7ZsFZ/088QjSMO1zna8pay0vM3uoPh4YHOWN7A5HNjshuv3ErtB7EnbS/crDYSl6wMdBrGO4/s7jOPHcN/P34vzIiogiYiIXORMJhMWPxOuqQMvrJcuvUJTVouIiIicQgFJRERE5BQKSCIiIiKnUEASEREROYUCkoiIiMgpFJBERERETqGAJCIiInIKBSQRERGRUyggiYiIiJxCAUlERETkFApIIiIiIqdQQBIRERE5hQKSiIiIyCkUkEREREROYfF2BS5UhmEAkJOT4+WaiIiISEWVfm+Xfo+fjgLSOcrNzQWgXr16Xq6JiIiIVFZubi4RERGn3W4yzhahpFwOh4MDBw4QFhaGyWSqsuPm5ORQr1499u3bR3h4eJUdV8rSvfYs3W/P0b32HN1rz6mqe20YBrm5udSuXRuz+fQ9jdSCdI7MZjN169attuOHh4frH5uH6F57lu635+hee47utedUxb0+U8tRKXXSFhERETmFApKIiIjIKRSQfExAQADjx48nICDA21W56Olee5but+foXnuO7rXnePpeq5O2iIiIyCnUgiQiIiJyCgUkERERkVMoIImIiIicQgFJRERE5BQKSD7mzTffJCEhgcDAQLp06cLKlSu9XaUL3qRJk+jUqRNhYWHExsbSt29ftm7d6lamsLCQe++9lxo1ahAaGkr//v3JyMjwUo0vDpMnT8ZkMvHAAw+41uk+V639+/dz2223UaNGDYKCgmjdujW//PKLa7thGDz99NPUqlWLoKAgevTowe+//+7FGl+Y7HY7Tz31FA0bNiQoKIjGjRszceJEt7m8dK/PzY8//kjv3r2pXbs2JpOJzz77zG17Re7r4cOHGTx4MOHh4URGRjJixAjy8vLOu24KSD5kzpw5jB07lvHjx7NmzRratm1LcnIymZmZ3q7aBW3JkiXce++9LF++nIULF1JSUsL1119Pfn6+q8yDDz7IF198wdy5c1myZAkHDhzgpptu8mKtL2yrVq3i3//+N23atHFbr/tcdY4cOcIVV1yB1Wrl66+/ZtOmTbz00ktERUW5yrzwwgu8/vrrTJ06lRUrVhASEkJycjKFhYVerPmF5/nnn+ett95iypQpbN68meeff54XXniBN954w1VG9/rc5Ofn07ZtW958881yt1fkvg4ePJjffvuNhQsXMn/+fH788UdGjhx5/pUzxGd07tzZuPfee12f7Xa7Ubt2bWPSpElerNXFJzMz0wCMJUuWGIZhGEePHjWsVqsxd+5cV5nNmzcbgJGamuqtal6wcnNzjaZNmxoLFy40unfvbowZM8YwDN3nqvboo48aV1555Wm3OxwOIz4+3njxxRdd644ePWoEBAQYH330kSeqeNG44YYbjDvuuMNt3U033WQMHjzYMAzd66oCGPPmzXN9rsh93bRpkwEYq1atcpX5+uuvDZPJZOzfv/+86qMWJB9RXFzM6tWr6dGjh2ud2WymR48epKamerFmF5/s7GwAoqOjAVi9ejUlJSVu9z4xMZH69evr3p+De++9lxtuuMHtfoLuc1X7/PPP6dixI3/961+JjY2lffv2vPPOO67tu3btIj093e1+R0RE0KVLF93vSurWrRuLFi1i27ZtAKxfv56ff/6Znj17ArrX1aUi9zU1NZXIyEg6duzoKtOjRw/MZjMrVqw4r/NrslofkZWVhd1uJy4uzm19XFwcW7Zs8VKtLj4Oh4MHHniAK664glatWgGQnp6Ov78/kZGRbmXj4uJIT0/3Qi0vXLNnz2bNmjWsWrWqzDbd56q1c+dO3nrrLcaOHcvjjz/OqlWruP/++/H392fYsGGue1ref1N0vyvnscceIycnh8TERPz8/LDb7fzjH/9g8ODBALrX1aQi9zU9PZ3Y2Fi37RaLhejo6PO+9wpIckm599572bhxIz///LO3q3LR2bdvH2PGjGHhwoUEBgZ6uzoXPYfDQceOHXnuuecAaN++PRs3bmTq1KkMGzbMy7W7uHz88cfMnDmTWbNm0bJlS9atW8cDDzxA7dq1da8vYnrE5iNiYmLw8/Mr80ZPRkYG8fHxXqrVxeW+++5j/vz5/PDDD9StW9e1Pj4+nuLiYo4ePepWXve+clavXk1mZiaXXXYZFosFi8XCkiVLeP3117FYLMTFxek+V6FatWrRokULt3VJSUns3bsXwHVP9d+U8/fwww/z2GOPccstt9C6dWuGDBnCgw8+yKRJkwDd6+pSkfsaHx9f5kUmm83G4cOHz/veKyD5CH9/fzp06MCiRYtc6xwOB4sWLaJr165erNmFzzAM7rvvPubNm8f3339Pw4YN3bZ36NABq9Xqdu+3bt3K3r17de8r4brrruPXX39l3bp1rqVjx44MHjzY9bvuc9W54oorygxXsW3bNho0aABAw4YNiY+Pd7vfOTk5rFixQve7kgoKCjCb3b8u/fz8cDgcgO51danIfe3atStHjx5l9erVrjLff/89DoeDLl26nF8FzquLt1Sp2bNnGwEBAcaMGTOMTZs2GSNHjjQiIyON9PR0b1ftgjZq1CgjIiLCWLx4sZGWluZaCgoKXGXuvvtuo379+sb3339v/PLLL0bXrl2Nrl27erHWF4eT32IzDN3nqrRy5UrDYrEY//jHP4zff//dmDlzphEcHGx8+OGHrjKTJ082IiMjjf/973/Ghg0bjBtvvNFo2LChcezYMS/W/MIzbNgwo06dOsb8+fONXbt2GZ9++qkRExNjPPLII64yutfnJjc311i7dq2xdu1aAzBefvllY+3atcaePXsMw6jYfU1JSTHat29vrFixwvj555+Npk2bGoMGDTrvuikg+Zg33njDqF+/vuHv72907tzZWL58uberdMEDyl2mT5/uKnPs2DHjnnvuMaKioozg4GCjX79+RlpamvcqfZE4NSDpPletL774wmjVqpUREBBgJCYmGm+//bbbdofDYTz11FNGXFycERAQYFx33XXG1q1bvVTbC1dOTo4xZswYo379+kZgYKDRqFEj44knnjCKiopcZXSvz80PP/xQ7n+fhw0bZhhGxe7roUOHjEGDBhmhoaFGeHi4cfvttxu5ubnnXTeTYZw0FKiIiIiIqA+SiIiIyKkUkEREREROoYAkIiIicgoFJBEREZFTKCCJiIiInEIBSUREROQUCkgiIiIip1BAEhGpIiaTic8++8zb1RCRKqCAJCIXheHDh2MymcosKSkp3q6aiFyALN6ugIhIVUlJSWH69Olu6wICArxUGxG5kKkFSUQuGgEBAcTHx7stUVFRgPPx11tvvUXPnj0JCgqiUaNGfPLJJ277//rrr/zpT38iKCiIGjVqMHLkSPLy8tzKTJs2jZYtWxIQEECtWrW477773LZnZWXRr18/goODadq0KZ9//nn1XrSIVAsFJBG5ZDz11FP079+f9evXM3jwYG655RY2b94MQH5+PsnJyURFRbFq1Srmzp3Ld9995xaA3nrrLe69915GjhzJr7/+yueff06TJk3czvHMM88wYMAANmzYQK9evRg8eDCHDx/26HWKSBU47+luRUR8wLBhwww/Pz8jJCTEbfnHP/5hGIZhAMbdd9/ttk+XLl2MUaNGGYZhGG+//bYRFRVl5OXlubZ/+eWXhtlsNtLT0w3DMIzatWsbTzzxxGnrABhPPvmk63NeXp4BGF9//XWVXaeIeIb6IInIRePaa6/lrbfeclsXHR3t+r1r165u27p27cq6desA2Lx5M23btiUkJMS1/YorrsDhcLB161ZMJhMHDhzguuuuO2Md2rRp4/o9JCSE8PBwMjMzz/WSRMRLFJBE5KIREhJS5pFXVQkKCqpQOavV6vbZZDLhcDiqo0oiUo3UB0lELhnLly8v8zkpKQmApKQk1q9fT35+vmv70qVLMZvNNG/enLCwMBISEli0aJFH6ywi3qEWJBG5aBQVFZGenu62zmKxEBMTA8DcuXPp2LEjV155JTNnzmTlypW89957AAwePJjx48czbNgwJkyYwMGDBxk9ejRDhgwhLi4OgAkTJnD33XcTGxtLz549yc3NZenSpYwePdqzFyoi1U4BSUQuGgsWLKBWrVpu65o3b86WLVsA5xtms2fP5p577qFWrVp89NFHtGjRAoDg4GC++eYbxowZQ6dOnQgODqZ///68/PLLrmMNGzaMwsJCXnnlFR566CFiYmK4+eabPXeBIuIxJsMwDG9XQkSkuplMJubNm0ffvn29XRURuQCoD5KIiIjIKRSQRERERE6hPkgicklQbwIRqQy1IImIiIicQgFJRERE5BQKSCIiIiKnUEASEREROYUCkoiIiMgpFJBERERETqGAJCIiInIKBSQRERGRUyggiYiIiJzi/wH+kGxjy9i7PAAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "#plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.plot(history.history['mean_squared_error'], label='Training MSE')\n",
        "plt.plot(history.history['val_mean_squared_error'], label='Validation MSE')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE')\n",
        "plt.title('MSE vs. Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# MIN LOSS = 0.0128 c/fund 50epochs MSE\n",
        "##         = 0.0118 s/fund 50epochs MSE\n",
        "##         = 0.0039 s/fund 50epochs MSE m=4 d=6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRlZuRUNa6Yb",
        "outputId": "85850559-311b-4cf4-ea5b-465a9ee8a7af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8/8 [==============================] - 0s 1ms/step\n"
          ]
        }
      ],
      "source": [
        "# Assuming you have a validation dataset (val_dataset)\n",
        "iterador = iter(val_dataset)\n",
        "sample = next(iterador)\n",
        "next_sample = next(iterador)\n",
        "input_data = sample[0]  # Assuming your dataset provides input data as the first element\n",
        "actual_values = sample[1]  # Assuming your dataset provides actual labels as the second element\n",
        "\n",
        "# Predict using the model\n",
        "predictions = model.predict(input_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[0.08029006 0.5129678  0.72444449 0.09086507 0.95799577 0.27082367\n",
            " 0.83613026 0.34585325 0.89458545 0.01424909], shape=(10,), dtype=float64)\n",
            "[0.5276974  0.8086654  0.5859325  0.11536771 0.4732831  0.49887455\n",
            " 0.6672565  0.48704726 0.8210559  0.5011102 ]\n",
            "tf.Tensor(\n",
            "[0.3968758  0.77954891 0.72990047 0.4603898  0.04982733 0.95023109\n",
            " 0.15865586 0.52114702 0.91438482 0.72519945], shape=(10,), dtype=float64)\n",
            "[0.4125358  0.78646535 0.786339   0.44413492 0.472513   0.57365656\n",
            " 0.10689276 0.33242258 0.838688   0.519114  ]\n",
            "tf.Tensor(\n",
            "[0.34639894 0.46120908 0.80125104 0.34096438 0.0875456  0.65780645\n",
            " 0.08153186 0.14800143 0.87828616 0.08201991], shape=(10,), dtype=float64)\n",
            "[0.51713777 0.40635043 0.79492396 0.39569786 0.47272515 0.5667049\n",
            " 0.06988022 0.1196807  0.53764856 0.42503557]\n",
            "tf.Tensor(\n",
            "[0.32522964 0.35690478 0.32443866 0.71342362 0.51789229 0.43971866\n",
            " 0.60556211 0.1290665  0.44032872 0.5363084 ], shape=(10,), dtype=float64)\n",
            "[0.53973675 0.49210846 0.32535028 0.72628164 0.46592978 0.5768182\n",
            " 0.66629833 0.12619102 0.50188935 0.44413316]\n",
            "0.18311669038201067 0.39753404735447045\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Vemos algunos valores\n",
        "for e in val_dataset:\n",
        "    for i in range(0, 4):\n",
        "        print(e[1][i])\n",
        "        print(predictions[i])\n",
        "    break\n",
        "    \n",
        "\n",
        "RMSE_pred = mean_squared_error(actual_values, predictions, squared=False)\n",
        "RMSE_rand = mean_squared_error(actual_values, next_sample[1], squared=False)\n",
        "print(RMSE_pred, RMSE_rand)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4.662921348314606"
            ]
          },
          "execution_count": 163,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "0.415/0.089"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "ds5iD1OMbZu3"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (4,4,1,1) (8,8,70,70) ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[164], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m val_dataset:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m printear \u001b[38;5;28;01melse\u001b[39;00m batch_size):\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;66;03m# Valores actuales\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;66;03m#h = e[1][i].numpy().reshape(basis.size,basis.size)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m         h_true \u001b[38;5;241m=\u001b[39m \u001b[43mgen_to_h\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrho_1_arrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;66;03m#print(h) if printear else 0\u001b[39;00m\n\u001b[1;32m     14\u001b[0m         r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39meigvals(e[\u001b[38;5;241m0\u001b[39m][i]))\n",
            "Cell \u001b[0;32mIn[67], line 22\u001b[0m, in \u001b[0;36mgen_to_h\u001b[0;34m(base, rho_1_arrays)\u001b[0m\n\u001b[1;32m     20\u001b[0m triag \u001b[38;5;241m=\u001b[39m fill_triangular_np(base)\n\u001b[1;32m     21\u001b[0m body_gen \u001b[38;5;241m=\u001b[39m triag \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(triag)\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mdiag(np\u001b[38;5;241m.\u001b[39mdiag(triag))\n\u001b[0;32m---> 22\u001b[0m h \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mbase_hamiltonian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrho_1_arrays\u001b[49m\u001b[43m)\u001b[49m)  \n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h\n",
            "Cell \u001b[0;32mIn[3], line 35\u001b[0m, in \u001b[0;36mbase_hamiltonian\u001b[0;34m(mat, basis, rho_1_gen)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbase_hamiltonian\u001b[39m(mat, basis, rho_1_gen):\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbase_hamiltonian_aux\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrho_1_gen\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[3], line 31\u001b[0m, in \u001b[0;36mbase_hamiltonian_aux\u001b[0;34m(mat, size, d, rho_1_gen)\u001b[0m\n\u001b[1;32m     29\u001b[0m rho_1_gen_transposed \u001b[38;5;241m=\u001b[39m rho_1_gen\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     30\u001b[0m mat_expanded \u001b[38;5;241m=\u001b[39m mat[:, :, np\u001b[38;5;241m.\u001b[39mnewaxis, np\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[0;32m---> 31\u001b[0m h \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[43mmat_expanded\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrho_1_gen_transposed\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m, axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,4,1,1) (8,8,70,70) "
          ]
        }
      ],
      "source": [
        "m_size = basis.size\n",
        "rho_1_pred = []\n",
        "rho_1_actual = []\n",
        "norm = []\n",
        "norm_rand = []\n",
        "printear =  False\n",
        "\n",
        "for e in val_dataset:\n",
        "    for i in range(0, 3 if printear else batch_size):\n",
        "        # Valores actuales\n",
        "        #h = e[1][i].numpy().reshape(basis.size,basis.size)\n",
        "        h_true = gen_to_h(e[1][i], rho_1_arrays)\n",
        "        #print(h) if printear else 0\n",
        "        r = max(np.linalg.eigvals(e[0][i]))\n",
        "        rho_1_actual.append(r)\n",
        "\n",
        "        print(h_true) if printear else 0\n",
        "        print(r) if printear else 0\n",
        "\n",
        "        # Valores predichos\n",
        "        #h = predictions[i].reshape(basis.size,basis.size)\n",
        "        h_pred = gen_to_h(predictions[i], rho_1_arrays)\n",
        "        beta = 1\n",
        "        # Estado térmico\n",
        "        state = thermal_state(h_pred, beta)\n",
        "        # Estado puro\n",
        "        #state = pure_state(h_pred)\n",
        "        rho1 = np.array(rho_1(basis.d, state, rho_1_arrays))\n",
        "        r = max(np.sort(linalg_d.eigvals(rho1).real))\n",
        "        rho_1_pred.append(r)\n",
        "\n",
        "        print(h_pred) if printear else 0\n",
        "        print(r) if printear else 0\n",
        "        \n",
        "\n",
        "        # Normas\n",
        "        norm.append(np.linalg.norm(h_true-h_pred, ord='fro'))\n",
        "        print(f'Norma {norm[-1]}') if printear else 0\n",
        "        ## Vamos a comparar con un h aleatorio\n",
        "        size = basis.d*(basis.d+1)//2\n",
        "        base = np.random.uniform(low=0, high=1.0, size=(size,))\n",
        "        h_rand = gen_to_h(base, rho_1_arrays)\n",
        "        norm_rand.append(np.linalg.norm(h_true-h_rand, ord='fro'))\n",
        "        #print(f'Norma random {norm_rand[-1]}') if printear else 0\n",
        "        print('') if printear else 0\n",
        "        \n",
        "\n",
        "\n",
        "    # e contiene todo el batch y nos basta con uno\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(e[1][10])\n",
        "predictions[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "AL2EC9Ci-0HG",
        "outputId": "545ebe57-d3de-490f-f076-709d5c47b5f2"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "f=1\n",
        "rho_1_actual = np.array(rho_1_actual)\n",
        "rho_1_pred = np.array(rho_1_pred)\n",
        "#print(mean_squared_error(rho_1_pred, rho_1_actual))\n",
        "\n",
        "print('Rho1 based statistics')\n",
        "print(np.mean(np.abs(rho_1_actual-rho_1_pred)))\n",
        "print(np.mean(rho_1_actual)*f)\n",
        "print('std')\n",
        "print(np.std(rho_1_actual-rho_1_pred)*f)\n",
        "print(np.std(rho_1_actual)*f)\n",
        "print(np.std(rho_1_pred)*f)\n",
        "plt.hist(np.array(rho_1_pred-rho_1_actual), bins=50)\n",
        "plt.show()\n",
        "print('H based statistics')\n",
        "print(np.mean(norm), np.mean(norm_rand))\n",
        "print(np.mean(norm_rand)/np.mean(norm))\n",
        "\n",
        "\n",
        "# BEST: FACTOR 1/8 c/fund\n",
        "## 500 epochs, 10M dataset\n",
        "# BEST: FACTOR 1/9 s/fund\n",
        "## 50 epochs, 5M dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "6.25/1.30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 25 epochs d = m*2\n",
        "res = {}\n",
        "res[5] = 35/8.19 \n",
        "res[4] = 15/2.47\n",
        "res[3] = 6.2/1.73"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YioVllOX3M1N",
        "outputId": "b7715c37-1400-4c04-8be3-dd247b4b9db9"
      },
      "outputs": [],
      "source": [
        "# Get the weights of all dense layers in the model\n",
        "dense_weights = []\n",
        "for layer in model.layers:\n",
        "    if isinstance(layer, tf.keras.layers.Dense):\n",
        "        weights = layer.get_weights()\n",
        "        if len(weights) > 0:\n",
        "            dense_weights.append(weights[0])\n",
        "\n",
        "# Visualize the weights of each dense layer\n",
        "for i, weights in enumerate(dense_weights):\n",
        "    plt.figure()\n",
        "    plt.imshow(weights, cmap='viridis', aspect='auto')\n",
        "    plt.colorbar()\n",
        "    plt.title(f\"Dense Layer {i+1} Weights Visualization\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rho_2_arrays[9,0,0,9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 0 0 0 1 1] [0 1 1 0 0 0]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Genera las matrices de rho2\n",
        "def rho_2_gen(basis, mll_basis, t_basis):\n",
        "    size = t_basis.size\n",
        "    s = basis.size\n",
        "    # La entrada i, j contiene C_j^\\dag C_i    i, j \\in t_basis\n",
        "    mat = np.empty((size, size, s, s), dtype=np.float32)\n",
        "    for i, v in enumerate(t_basis.base):\n",
        "        for j, w in enumerate(t_basis.base):\n",
        "            c_i = b_gen(basis, mll_basis, rep_to_exi(v))\n",
        "            cdag_j = bd_gen(mll_basis, basis, rep_to_exi(w))\n",
        "            mat[i, j, :, :] = np.dot(cdag_j, c_i)\n",
        "            if mat[i,j,0,9] != 0:\n",
        "                print(v,w)\n",
        "\n",
        "    return mat\n",
        "\n",
        "r = rho_2_gen(basis, basis_m2, t_basis)\n",
        "r[9,0,0,9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "print(b_gen(basis, mll_basis, rep_to_exi([0,0,0,1,1,0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 1, 1, 0, 0, 0],\n",
              "       [1, 1, 0, 1, 0, 0],\n",
              "       [1, 1, 0, 0, 1, 0],\n",
              "       [1, 1, 0, 0, 0, 1],\n",
              "       [1, 0, 1, 1, 0, 0],\n",
              "       [1, 0, 1, 0, 1, 0],\n",
              "       [1, 0, 1, 0, 0, 1],\n",
              "       [1, 0, 0, 1, 1, 0],\n",
              "       [1, 0, 0, 1, 0, 1],\n",
              "       [1, 0, 0, 0, 1, 1],\n",
              "       [0, 1, 1, 1, 0, 0],\n",
              "       [0, 1, 1, 0, 1, 0],\n",
              "       [0, 1, 1, 0, 0, 1],\n",
              "       [0, 1, 0, 1, 1, 0],\n",
              "       [0, 1, 0, 1, 0, 1],\n",
              "       [0, 1, 0, 0, 1, 1],\n",
              "       [0, 0, 1, 1, 1, 0],\n",
              "       [0, 0, 1, 1, 0, 1],\n",
              "       [0, 0, 1, 0, 1, 1],\n",
              "       [0, 0, 0, 1, 1, 1]])"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "basis.base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(6, 20)\n",
            "[array([0, 1, 0, 1, 1, 0])] [0 1 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "col = 1\n",
        "b = b_gen(basis, mll_basis, rep_to_exi([0,0,0,1,1,0]))\n",
        "print(b.shape)\n",
        "for x in range(0,b.shape[1]):\n",
        "    if b[col,x] != 0:\n",
        "        ind = x\n",
        "        break\n",
        "else:\n",
        "    ind = NaN\n",
        "\n",
        "print([basis.base[ind]], mll_basis.base[col])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "m = 3\n",
        "d = 2*m\n",
        "basis = fixed_basis(m, d)\n",
        "t_basis = fixed_basis(2, d)\n",
        "basis_m1 = fixed_basis(m-1, d)\n",
        "basis_m2 = fixed_basis(m-2, d)\n",
        "ml_basis = basis_m1\n",
        "mll_basis = basis_m2\n",
        "rho_1_arrays = rho_1_gen(basis)\n",
        "rho_2_arrays = rho_2_gen(basis, mll_basis, t_basis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t_basis = fixed_basis(2, d)\n",
        "mll_basis = fixed_basis(basis.m-2, d)\n",
        "rho_1_arrays = rho_1_gen(basis)\n",
        "rho_2_arrays = rho_2_gen(basis, mll_basis, t_basis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def two_body_hamiltonian(t_basis_size, m, energy, G, rho_1_arrays, rho_2_arrays):\n",
        "    # Creamos la mat diagonal de d*d con los elementos de energy\n",
        "    # cada uno de estos, se contraen con los elementos de rho_1_arrays\n",
        "    # la mat energy contiene las energias de cada termino c^\\dag_k c_k para k kbar (iguales)\n",
        "    # por ello los elementos se repiten \n",
        "    energy_matrix = np.diagflat(np.kron(energy, np.ones(2)))\n",
        "    \n",
        "    # Construimos la mat de energía\n",
        "    h0 = np.sum(energy_matrix[:, :, np.newaxis, np.newaxis] * rho_1_arrays[:, :, :, :], axis=(0, 1))\n",
        "\n",
        "    # Pasamos ahora a la matrix de interacción con la misma estrategia\n",
        "    # dada G que indica la interacción entre los pares k' k'bar k kbar \n",
        "    # (que son elementos particulares de t_basis)\n",
        "    # transladamos estos coeficientes a una matriz en t_basis\n",
        "    # y multiplicamos por rho_2_arrays\n",
        "    \n",
        "    # Primero determinamos, dada t_basis, cuales son los indices de pares kkbar\n",
        "    offset = 4*m+1\n",
        "    indices = [-(k-1)*(2*k-offset) for k in range(1,m+1)] # Lo saque de Mathmatica, vos confia\n",
        "    i, j = np.meshgrid(indices, indices, indexing='ij') # Lo usamos para rellenar la mat deseada\n",
        "    \n",
        "    # Contruimos la mat que contraeremos con rho_2_arrays\n",
        "    mat = np.zeros((t_basis_size, t_basis_size))\n",
        "    mat[i, j] = G\n",
        "\n",
        "    hi = -np.sum(mat[:, :, np.newaxis, np.newaxis] * rho_2_arrays[:, :, :, :], axis=(0, 1))\n",
        "\n",
        "    return (h0, hi)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(h02,hi2) = two_body_hamiltonian(t_basis.size, m, [0,1,2], np.ones((3,3)), rho_1_arrays, rho_2_arrays)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[[[1., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 1., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 1., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [1., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 1., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [1., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 1., 0., ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[1., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [1., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 1., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 1., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "       ...,\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 1.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., ..., 1., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 1.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 1., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 1.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 1., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., ..., 0., 1., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 1.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 1., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 1., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 1.]]]], dtype=float32)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rho_2_arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0\n"
          ]
        }
      ],
      "source": [
        "print(rho_2_arrays[9,0,0,9])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "rho_1_arrays = rho_1_gen(basis)\n",
        "\n",
        "A = np.array([0, 1, 2])  # Your list with d elements\n",
        "\n",
        "# Create a diagonal matrix with each element repeated twice\n",
        "result_matrix = np.diagflat(np.kron(A, np.ones(2)))\n",
        "\n",
        "print(result_matrix)\n",
        "np.kron(A, np.ones(2))\n",
        "\n",
        "mat = np.zeros((basis.size, basis.size))\n",
        "for i in range(0,2*d):\n",
        "    for j in range(0, 2*d):\n",
        "        mat += result_matrix[i,j] * rho_1_arrays[i,j,:,:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mat = np.sum(result_matrix[:, :, np.newaxis, np.newaxis] * rho_1_arrays[:, :, :, :], axis=(0, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "h0 == mat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1 1 0 0 0 0]\n",
            "[0 0 1 1 0 0]\n",
            "[0 0 0 0 1 1]\n",
            "[0, 9, 14]\n",
            "[0, 9, 14]\n"
          ]
        }
      ],
      "source": [
        "d = 3\n",
        "t_basis = fixed_basis(2, 2*d)\n",
        "basis = fixed_basis(d, 2*d)\n",
        "size = t_basis.size\n",
        "#basis = fixed_basis(d, 2*d)\n",
        "diag_elem = []\n",
        "for x in t_basis.base:\n",
        "    if all([x[i] == x[i+1] for i in range(0, 2*d, 2)]):\n",
        "        print(x)\n",
        "        diag_elem.append(t_basis.rep_to_index(x))\n",
        "\n",
        "print(diag_elem)\n",
        "# Veamos el GALERAZO de Wolfram\n",
        "n = 4*d+1\n",
        "print([-(k-1)*(2*k-n) for k in range(1,d+1)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "m2_basis = fixed_basis(2, d)\n",
        "print(m2_basis.size)\n",
        "nm2_basis = fixed_basis(basis.m-2, d)\n",
        "print(nm2_basis.base)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "W = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
        "index = [0,9,14]\n",
        "mat = np.zeros((size,size))\n",
        "for i in range(0,3):\n",
        "    for j in range(0,3):\n",
        "        mat[index[i], index[j]] = W[i,j]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "rho_2_arrays = rho_2_gen(basis, nm2_basis, m2_basis)\n",
        "#rho_2_arrays_tf = tf.constant(rho_2_arrays, dtype=tf.float32)\n",
        "\n",
        "W = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "W = np.ones((3,3))\n",
        "index = [0, 9, 14]\n",
        "size = 15  # Assuming size is the size of the matrix\n",
        "\n",
        "# Create a meshgrid of indices\n",
        "i, j = np.meshgrid(index, index, indexing='ij')\n",
        "\n",
        "# Use the meshgrid indices to assign values from W to the specified positions in mat\n",
        "mat = np.zeros((size, size))\n",
        "mat[i, j] = W\n",
        "\n",
        "# La mat... mat corresponde a los coeficientes en t_basis\n",
        "inte = np.zeros((basis.size, basis.size))\n",
        "for i in range(0, t_basis.size):\n",
        "    for j in range(0, t_basis.size):\n",
        "        inte += - mat[i, j] * rho_2_arrays[i,j,:,:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inte == hi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "m = 3\n",
        "basis = fixed_basis(m, d)\n",
        "basis_m1 = fixed_basis(m-1, d)\n",
        "basis_m2 = fixed_basis(m-2, d)\n",
        "\n",
        "from numba import njit\n",
        "\n",
        "# Parametros hamiltoniano\n",
        "e = 1\n",
        "eps = 0\n",
        "e0 = np.zeros(2*d)\n",
        "eigenspace_tol = 0.0001\n",
        "for k in range(0, d):\n",
        "    r = random.random() * eps * 0\n",
        "    e0[2*k] = k*e+r\n",
        "    e0[2*k+1] = k*e+r\n",
        "\n",
        "@njit(parallel=True)\n",
        "def base_hamiltonian_aux(basis, size, d, basis_m1, basis_m2):\n",
        "    # Construccion de H\n",
        "    d = d//2\n",
        "    h0 = np.zeros((size,size), dtype=np.float32)\n",
        "    for k in prange(0,2*d):\n",
        "        h0 += e0[k] * np.dot(bd_aux(basis_m1, basis, k),b_aux(basis, basis_m1, k))\n",
        "    hi = np.zeros((size, size), dtype=np.float32)\n",
        "    for k in prange(0,d):\n",
        "        for kb in prange(0,d):\n",
        "            bd_terms = np.dot(bd_aux(basis_m1, basis, 2*k),bd_aux(basis_m2, basis_m1, 2*k+1))\n",
        "            b_terms = np.dot(b_aux(basis_m1, basis_m2, 2*kb+1),b_aux(basis, basis_m1, 2*kb))\n",
        "            hi += -1*np.dot(bd_terms,b_terms)\n",
        "\n",
        "    return (h0, hi)\n",
        "\n",
        "def base_hamiltonian(basis, basis_m1, basis_m2):\n",
        "    return base_hamiltonian_aux(basis.base, basis.size, basis.d, basis_m1.base, basis_m2.base)\n",
        "\n",
        "h0, hi = base_hamiltonian(basis, basis_m1, basis_m2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "oapxWkD16fHg"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
