{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aguschanchu/FermionicML/blob/main/FermionicML_thermal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXz5cOlVwrzZ"
      },
      "source": [
        "# FermionicML:\n",
        "\n",
        "Code based on aguschanchu/Bosonic.py\n",
        "\n",
        "A diferencia del código anterior, este modelo trabaja sobre estados térmicos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLD2Yai55rMm"
      },
      "source": [
        "## Código base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgf9ExZN4jA7"
      },
      "source": [
        "Cargamos el código de Bosonic.py básico, branch fermionic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Gydz4kCH4l5w"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-15 15:44:46.897606: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-04-15 15:44:46.925796: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-15 15:44:46.925820: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-15 15:44:46.926644: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-15 15:44:46.931426: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-04-15 15:44:46.931920: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-04-15 15:44:47.555911: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/tmp/ipykernel_62778/1952354829.py:325: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
            "  @nb.jit\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.special import binom\n",
        "from scipy.sparse import dok_matrix, linalg\n",
        "from scipy import linalg as linalg_d\n",
        "from joblib import Memory\n",
        "import random\n",
        "import plotly.graph_objects as go\n",
        "from joblib import Parallel, delayed\n",
        "from numba import jit, prange, njit\n",
        "import numba as nb\n",
        "import pickle\n",
        "import math\n",
        "import tensorflow_probability as tfp\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from itertools import combinations\n",
        "import scipy\n",
        "\n",
        "# Funciones auxiliares optimiadas\n",
        "@nb.jit(nopython=True, parallel=True)\n",
        "def int_to_tuple_arr(ni,nf, b, digits=None):\n",
        "    sol = np.zeros((nf-ni, digits), dtype=np.int64)\n",
        "    for n in prange(ni, nf):\n",
        "        r = np.zeros(digits, dtype=np.int64)\n",
        "        ncop = n\n",
        "        idx = 0\n",
        "        while n != 0:\n",
        "            r[idx] = n % b\n",
        "            n = n // b\n",
        "            idx += 1\n",
        "        if digits is not None:\n",
        "            if idx < digits:\n",
        "                for i in range(idx, digits):\n",
        "                    r[i] = 0\n",
        "                idx = digits\n",
        "        sol[ncop-ni,:] = r[:idx]\n",
        "    return sol\n",
        "\n",
        "def tuple_to_int(t, d):\n",
        "    b = d-1\n",
        "    l = len(t)\n",
        "    s = [t[k]*b**(l-k-1) for k in range(0,l)]\n",
        "    return sum(s)\n",
        "\n",
        "def create_basis_(m, d, size):\n",
        "    base = []\n",
        "    index = 0\n",
        "    chunk_size = 1000000\n",
        "    for x in range(0,(m+1)**d, chunk_size):\n",
        "        start_index = x\n",
        "        end_index = min(x + chunk_size, (m+1)**d)\n",
        "        arr = int_to_tuple_arr(start_index, end_index, m+1, d)\n",
        "        sums = np.sum(arr, axis=1)\n",
        "        rows = np.where(sums == m)[0]\n",
        "        for row in [arr[i] for i in rows]:\n",
        "            if np.all(np.logical_or(row == 0, row == 1)):\n",
        "                base.append(row)\n",
        "\n",
        "    # Como consecuencia de la paralelizacion, es necesario reordenar la base\n",
        "    sorted_base = sorted(base, key=lambda x: tuple_to_int(x, d), reverse=True)\n",
        "    assert len(base) == size\n",
        "\n",
        "    return sorted_base\n",
        "\n",
        "def custom_base_representation_tf(n_min, n_max, base, num_digits):\n",
        "    # Generate a range of numbers from n_min to n_max\n",
        "    numbers = tf.range(n_min, n_max + 1, dtype=tf.int64)\n",
        "    \n",
        "    # Calculate the digits in the custom base using broadcasting\n",
        "    digits = tf.pow(tf.cast(base, dtype=tf.float64), tf.cast(tf.range(num_digits), dtype=tf.float64))\n",
        "    \n",
        "    # Reshape the digits to [1, num_digits] for broadcasting\n",
        "    digits = tf.reshape(digits, [1, -1])\n",
        "    \n",
        "    # Reshape numbers to [batch_size, 1]\n",
        "    numbers = tf.reshape(tf.cast(numbers, dtype=tf.float64), [-1, 1])\n",
        "    \n",
        "    # Calculate the digits in the custom base for each number using broadcasting\n",
        "    result = tf.cast(tf.math.floormod(tf.math.floordiv(numbers, digits), base), dtype=tf.int32)\n",
        "    \n",
        "    # Pad the result to have exactly num_digits columns\n",
        "    result = tf.pad(result, paddings=[[0, 0], [0, num_digits - tf.shape(result)[1]]], constant_values=0)\n",
        "    \n",
        "    # Reverse the order of columns\n",
        "    #result = tf.reverse(result, axis=[1])\n",
        "\n",
        "    return result\n",
        "\n",
        "def select_rows_with_sum(arr, m):\n",
        "    # Create a mask based on the criteria\n",
        "    mask = tf.reduce_all(tf.math.logical_or(tf.equal(arr, 0), tf.equal(arr, 1)), axis=1) & (tf.reduce_sum(arr, axis=1) == m)\n",
        "    \n",
        "    # Use the mask to select the rows\n",
        "    result = tf.boolean_mask(arr, mask, axis=0)\n",
        "    \n",
        "    return result\n",
        "\n",
        "def create_basis_tf_(m, d):\n",
        "    base = []\n",
        "    index = 0\n",
        "    chunk_size = 10000000\n",
        "    for x in tqdm(range(0,(m+1)**d, chunk_size)):\n",
        "        start_index = x\n",
        "        end_index = min(x + chunk_size, (m+1)**d)\n",
        "        res = custom_base_representation_tf(start_index, end_index, m+1, d)\n",
        "        arr = select_rows_with_sum(res, m)\n",
        "        base.append(arr.numpy())\n",
        "\n",
        "    return np.concatenate(base)\n",
        "\n",
        "def create_fermionic_base_(m, d):\n",
        "    indices = list(range(d))\n",
        "    combinations_list = list(combinations(indices, m))\n",
        "    \n",
        "    vectors = []\n",
        "    for combo in combinations_list:\n",
        "        vector = [1 if i in combo else 0 for i in indices]\n",
        "        vectors.append(vector)\n",
        "    \n",
        "    return vectors\n",
        "\n",
        "# Dada una base, devuelve los vectores que estan dados de a pares\n",
        "def get_kkbar_indices_(base):\n",
        "    indices = []\n",
        "    for i, v in enumerate(base):\n",
        "        if np.all(v[::2] == v[1::2]):\n",
        "            indices.append(i)\n",
        "    return indices\n",
        "\n",
        "class fixed_basis:\n",
        "\n",
        "    # Convierte a un enterno n a su escritura en base b\n",
        "    def _int_to_tuple(self, n, b, digits = None):\n",
        "        rep = np.base_repr(n, b)\n",
        "        rep_int = [int(x,b) for x in rep]\n",
        "        if digits is not None:\n",
        "            zeros = [0 for i in range(0,digits-len(rep))]\n",
        "            return zeros + rep_int\n",
        "        else:\n",
        "            return rep_int\n",
        "\n",
        "    # Revierte la transformacion anterior\n",
        "    def tuple_to_int(self, t):\n",
        "        b = self.d-1\n",
        "        l = len(t)\n",
        "        s = [t[k]*b**(l-k-1) for k in range(0,l)]\n",
        "        return sum(s)\n",
        "\n",
        "    # Convierte el vector en su representacion\n",
        "    def vect_to_repr(self, vect):\n",
        "        for i, k in enumerate(vect):\n",
        "            if k == 1. or k == 1:\n",
        "                break\n",
        "        else:\n",
        "            return 0\n",
        "        return self.base[i,:]\n",
        "\n",
        "    def rep_to_vect(self, rep):\n",
        "        rep = list(rep)\n",
        "        for i, r in [(j, self.base[j,:]) for j in range(0,self.size)]:\n",
        "            if list(r) == rep:\n",
        "                return self.canonicals[:,i]\n",
        "        else:\n",
        "            None\n",
        "\n",
        "    def rep_to_index(self, rep):\n",
        "        try:\n",
        "            return self.base.tolist().index(list(rep))\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    @staticmethod\n",
        "    def rep_to_exi(rep):\n",
        "        r = []\n",
        "        for i, k in enumerate(rep):\n",
        "            r += [i for x in range(0,k)]\n",
        "        return r\n",
        "\n",
        "    # Crea base de M particulas en D estados (repr y base canonica)\n",
        "    def create_basis(self, m, d, pairs = False):\n",
        "        #print(\"Creating basis: \", m, d)\n",
        "        #base = np.array(create_basis_tf_(m, d)) CASO GENERICO\n",
        "        base = np.array(create_fermionic_base_(m,d)) # UNICAMENTE FERMIONICO\n",
        "        if pairs:\n",
        "            base = base[get_kkbar_indices_(base)]\n",
        "        length = base.shape[0]\n",
        "        # Asignamos a cada uno de ellos un canónico\n",
        "        canonicals = np.eye(length)\n",
        "        return base, canonicals\n",
        "    \n",
        "    def __init__(self, m, d, pairs = False):\n",
        "        self.m = m\n",
        "        self.d = d\n",
        "        (self.base, self.canonicals) = self.create_basis(m, d, pairs)\n",
        "        self.size = self.base.shape[0]\n",
        "\n",
        "# Matrices de aniquilación y creación endomórficas. Estan fuera de la clase para poder ser cacheadas\n",
        "#@memory.cache\n",
        "def bdb(basis, i, j):\n",
        "    mat = dok_matrix((basis.size, basis.size), dtype=np.float32)\n",
        "    if i != j:\n",
        "        for k, v in enumerate(basis.base):\n",
        "            if v[j] != 0 and v[i] != 1:\n",
        "                #print(v)\n",
        "                dest = list(v.copy())\n",
        "                dest[j] -= 1\n",
        "                dest[i] += 1\n",
        "                tar = basis.rep_to_index(dest)\n",
        "                if tar is None:\n",
        "                    pass\n",
        "                else:\n",
        "                    mat[tar, k] = np.sqrt(v[i]+1)*np.sqrt(v[j])\n",
        "    else:\n",
        "        for k, v in enumerate(basis.base):\n",
        "            if v[j] != 0:\n",
        "                mat[k, k] = v[i] \n",
        "    return mat\n",
        "\n",
        "#@memory.cache\n",
        "def bbd(basis, i, j):\n",
        "    mat = dok_matrix((basis.size, basis.size), dtype=np.float32)\n",
        "    if i != j:\n",
        "        for k, v in enumerate(basis.base):\n",
        "            if v[i] != 0 and v[j] != 1:\n",
        "                dest = list(v.copy())\n",
        "                dest[i] -= 1\n",
        "                dest[j] += 1\n",
        "                tar = basis.rep_to_index(dest)\n",
        "                mat[tar, k] = np.sqrt(v[j]+1)*np.sqrt(v[i])\n",
        "    else:\n",
        "        for k, v in enumerate(basis.base):\n",
        "            if v[i] != 1:\n",
        "                mat[k, k] = v[i]+1\n",
        "    return mat\n",
        "\n",
        "# Matrices de aniquilación y creación.Toman la base de origen y destino (basis_o, basis_d) resp\n",
        "#@nb.jit(nopython=True, parallel=True)\n",
        "@nb.jit(nopython=True)\n",
        "def b_aux(basis_o, basis_d, i):\n",
        "    mat = np.zeros((len(basis_d), len(basis_o)), dtype=np.float32)\n",
        "    for k in prange(len(basis_o)):\n",
        "        if basis_o[k][i] != 0:\n",
        "            dest = list(basis_o[k].copy())\n",
        "            dest[i] -= 1\n",
        "            for j in prange(len(basis_d)):\n",
        "                if list(basis_d[j]) == dest:\n",
        "                    tar = j\n",
        "                    mat[tar, k] = np.sqrt(basis_o[k][i])\n",
        "    return mat\n",
        "\n",
        "def b(basis_o, basis_d, i):\n",
        "    return b_aux(basis_o.base, basis_d.base, i)\n",
        "\n",
        "#@nb.jit(nopython=True, parallel=True)\n",
        "@nb.jit(nopython=True)\n",
        "def bd_aux(basis_o, basis_d, i):\n",
        "    mat = np.zeros((len(basis_d), len(basis_o)), dtype=np.float32)\n",
        "    for k in prange(len(basis_o)):\n",
        "        if basis_o[k][i] != 1:\n",
        "            dest = list(basis_o[k].copy())\n",
        "            dest[i] += 1\n",
        "            for j in prange(len(basis_d)):\n",
        "                if list(basis_d[j]) == dest:\n",
        "                    tar = j\n",
        "                    mat[tar, k] = np.sqrt(basis_o[k][i]+1)\n",
        "    return mat\n",
        "\n",
        "def bd(basis_o, basis_d, i):\n",
        "    return bd_aux(basis_o.base, basis_d.base, i)\n",
        "\n",
        "\n",
        "# Acepta una lista de indices a crear\n",
        "@nb.jit(nopython=True, parallel=True)\n",
        "def bd_gen_aux(basis_o, basis_d, gen_list):\n",
        "    mat = np.zeros((len(basis_d), len(basis_o)), dtype=np.float32)\n",
        "    for k in prange(len(basis_o)):\n",
        "        conds = np.zeros(len(gen_list), dtype=np.int64)\n",
        "        for i in range(len(gen_list)):\n",
        "            if basis_o[k][gen_list[i]] != 1:\n",
        "                conds[i] = 1\n",
        "        if np.all(conds):\n",
        "            dest = list(basis_o[k].copy())\n",
        "            for i in gen_list:\n",
        "                dest[i] += 1\n",
        "            for j in prange(len(basis_d)):\n",
        "                if list(basis_d[j]) == dest:\n",
        "                    tar = j\n",
        "                    mat[tar, k] = np.sqrt(basis_o[k][i]+1)\n",
        "    return mat\n",
        "\n",
        "def bd_gen(basis_o, basis_d, i):\n",
        "    return bd_gen_aux(basis_o.base, basis_d.base, np.array(i))\n",
        "\n",
        "def b_gen(basis_o, basis_d, i):\n",
        "    return np.transpose(bd_gen(basis_d, basis_o, i))\n",
        "\n",
        "# Volvemos a definir la función para compilarla\n",
        "@nb.jit(forceobj=True)\n",
        "def _rep_to_index(base, rep):\n",
        "    return base.tolist().index(list(rep))\n",
        "\n",
        "# Funciones auxiliares para calcular rho2kkbar y gamma_p\n",
        "@nb.jit(nopython=True)\n",
        "def rep_to_exi(rep):\n",
        "    r = []\n",
        "    for i in range(len(rep)):\n",
        "        for j in range(rep[i]):\n",
        "            r.append(i)\n",
        "    return r\n",
        "\n",
        "@nb.njit\n",
        "def factorial(n):\n",
        "    result = 1\n",
        "    for i in range(1, n + 1):\n",
        "        result *= i\n",
        "    return result\n",
        "\n",
        "@nb.njit\n",
        "def gamma_lamba(x):\n",
        "    res = 1.0\n",
        "    for o in x:\n",
        "        res *= math.sqrt(factorial(o))\n",
        "    return res\n",
        "\n",
        "@nb.jit\n",
        "def gamma_lamba_inv(x):\n",
        "    res = 1.0\n",
        "    for o in x:\n",
        "        res *= 1.0 / np.sqrt(factorial(o))\n",
        "    return res\n",
        "\n",
        "@nb.njit\n",
        "def rep_to_index_np(base, rep):\n",
        "    for i in range(len(base)):\n",
        "        if np.all(base[i] == rep):\n",
        "            return i\n",
        "    return -1\n",
        "\n",
        "\n",
        "def gamma_p(basis, m, vect, m_basis = None, nm_basis = None):\n",
        "    d = basis.d\n",
        "    if not m_basis or not nm_basis:\n",
        "        m_basis = fixed_basis(m, d)\n",
        "        nm_basis = fixed_basis(basis.m-m,d)\n",
        "    return gamma_p_aux(basis.base, vect, m_basis.base, nm_basis.base)\n",
        "\n",
        "@nb.njit()\n",
        "def gamma_p_aux(basis, vect, m_basis, nm_basis):\n",
        "    mat = np.zeros((len(m_basis), len(nm_basis)), dtype=np.float32)\n",
        "    for i in prange(len(m_basis)):\n",
        "        v = m_basis[i]\n",
        "        for j in prange(len(nm_basis)):\n",
        "            w = nm_basis[j]\n",
        "            targ = v + w\n",
        "            index = rep_to_index_np(basis, targ)\n",
        "            if index != -1:\n",
        "                coef = vect[index]\n",
        "                if coef != 0:\n",
        "                    coef = coef * gamma_lamba_inv(v) * gamma_lamba_inv(w) * gamma_lamba(targ)\n",
        "                mat[i, j] = coef\n",
        "    return mat\n",
        "# Devuelve la matriz rho M asociada al vector\n",
        "def rho_m(basis, m, vect, m_basis = None, nm_basis = None):\n",
        "    g = gamma_p(basis, m, vect, m_basis, nm_basis)\n",
        "    return np.dot(g,np.transpose(g))\n",
        "\n",
        "# Devuelve la matriz gamma asociada a la descomposición (M,N-M) del vector\n",
        "@jit(forceobj=True)\n",
        "def gamma(basis, m, vect, m_basis = None, nm_basis = None):\n",
        "    d = basis.d\n",
        "    if not m_basis or not nm_basis:\n",
        "        m_basis = fixed_basis(m, d)\n",
        "        nm_basis = fixed_basis(basis.m-m,d)\n",
        "    mat = dok_matrix((m_basis.size, nm_basis.size), dtype=np.float32)\n",
        "    for i, v in enumerate(m_basis.base):\n",
        "        for j, w in enumerate(nm_basis.base):\n",
        "            targ = v+w\n",
        "            # Revisamos que sea un estado fermionico valido\n",
        "            arr = np.asarray(targ)\n",
        "            if not np.all(np.logical_or(arr == 0, arr == 1)):\n",
        "                continue\n",
        "            index = _rep_to_index(basis.base, targ)\n",
        "            coef = vect[index]\n",
        "            if coef != 0:\n",
        "                aux = lambda x: np.prod(np.reciprocal(np.sqrt([np.math.factorial(o) for o in x])))\n",
        "                aux_inv = lambda x: np.prod(np.sqrt([np.math.factorial(o) for o in x]))\n",
        "                coef = coef * aux(v) * aux(w) * aux_inv(targ)\n",
        "                #coef = coef\n",
        "                #print(v,w,coef)\n",
        "            mat[i,j] = coef\n",
        "    return mat\n",
        "\n",
        "# Genera las matrices de rho1\n",
        "def rho_1_gen(basis):\n",
        "    d = basis.d\n",
        "    s = basis.size\n",
        "    mat = np.empty((d,d,s,s), dtype=np.float32)\n",
        "    for i in range(0, d):\n",
        "        for j in range(0, d):\n",
        "            mat[i,j,:,:] = np.array(bdb(basis,j, i).todense())\n",
        "    return mat\n",
        "\n",
        "#@jit(parallel=True, nopython=True)\n",
        "def rho_1(d, state, rho_1_arrays):\n",
        "    state_expanded = state[np.newaxis, np.newaxis, :, :]\n",
        "    product = state_expanded * rho_1_arrays\n",
        "    mat = np.sum(product, axis=(-2, -1))\n",
        "\n",
        "    return mat\n",
        "\n",
        "\n",
        "# Genera las matrices de rho2\n",
        "def rho_2_gen(basis, mll_basis, t_basis):\n",
        "    size = t_basis.size\n",
        "    s = basis.size\n",
        "    # La entrada i, j contiene C_j^\\dag C_i    i, j \\in t_basis\n",
        "    mat = np.empty((size, size, s, s), dtype=np.float32)\n",
        "    for i, v in enumerate(t_basis.base):\n",
        "        for j, w in enumerate(t_basis.base):\n",
        "            c_i = b_gen(basis, mll_basis, rep_to_exi(v))\n",
        "            cdag_j = bd_gen(mll_basis, basis, rep_to_exi(w))\n",
        "            mat[i, j, :, :] = np.dot(cdag_j, c_i)\n",
        "\n",
        "    return mat\n",
        "\n",
        "def rho_2(size, state, rho_2_arrays):\n",
        "    state_expanded = np.expand_dims(state, axis=1)\n",
        "    state_expanded = np.expand_dims(state_expanded, axis=1)\n",
        "    rho_2_arrays = rho_2_arrays[np.newaxis, :, :, :, :]\n",
        "    print(state_expanded.shape, rho_2_arrays.shape)\n",
        "    product = state_expanded * rho_2_arrays\n",
        "    mat = np.sum(product, axis=(-2, -1))\n",
        "    return mat\n",
        "\n",
        "def get_kkbar_indices(t_basis):\n",
        "    indices = []\n",
        "    for i, v in enumerate(t_basis.base):\n",
        "        if np.all(v[::2] == v[1::2]):\n",
        "            indices.append(i)\n",
        "    return indices\n",
        "\n",
        "def rho_2_kkbar_gen(t_basis, rho_2_arrays):\n",
        "    indices = get_kkbar_indices(t_basis)\n",
        "    i, j = np.meshgrid(indices, indices, indexing='ij') # Lo usamos para rellenar la mat deseada\n",
        "\n",
        "    rho_2_arrays_kkbar = rho_2_arrays[i, j, :, :]\n",
        "\n",
        "    return rho_2_arrays_kkbar\n",
        "\n",
        "# Devuelve la matriz rho 2 asociada al bloque kkbar\n",
        "def rho_2_kkbar(basis, vect, ml_basis = None, mll_basis = None, t_basis = None):\n",
        "    d = basis.d\n",
        "    # Creo las bases si no están dadas\n",
        "    if ml_basis == None or mll_basis == None or t_basis == None:\n",
        "        ml_basis = fixed_basis(m-1,d)\n",
        "        mll_basis = fixed_basis(m-2,d)\n",
        "        t_basis = fixed_basis(2,d)\n",
        "    diag = []\n",
        "    for v in t_basis.base:\n",
        "        for j in range(0, d, 2):\n",
        "            if v[j] == v[j+1]:\n",
        "                continue\n",
        "            else:\n",
        "                break\n",
        "        else:\n",
        "            diag.append(v)\n",
        "    diag = np.array(diag)\n",
        "    return rho_2_kkbar_aux(diag, vect, basis.base, ml_basis.base, mll_basis.base, t_basis.base)\n",
        "\n",
        "@nb.njit\n",
        "def rho_2_kkbar_lambda(x):\n",
        "    res = 1.0\n",
        "    for o in x:\n",
        "        res *= 1.0 / math.sqrt(factorial(o))\n",
        "    return res\n",
        "\n",
        "#@nb.njit(parallel=True)\n",
        "def rho_2_kkbar_aux(diag, vect, basis, ml_basis, mll_basis, t_basis):\n",
        "    mat = np.zeros((len(diag), len(diag)), dtype=np.float32)\n",
        "    for i in prange(len(diag)):\n",
        "        for j in prange(len(diag)):\n",
        "            v = diag[i]\n",
        "            w = diag[j]\n",
        "            # Creacion de los a\n",
        "            i_set = rep_to_exi(v)\n",
        "            b_m = b_aux(ml_basis, mll_basis, i_set[1]) @ b_aux(basis, ml_basis, i_set[0])\n",
        "            # Creacion de los ad\n",
        "            i_set = rep_to_exi(w)\n",
        "            bd_m = bd_aux(ml_basis, basis, i_set[1]) @ bd_aux(mll_basis, ml_basis, i_set[0])\n",
        "            # v1 = vect @ bd_m @ b_m @ vect Para estados puros\n",
        "            # Mult de b's y filleo de mat\n",
        "            coef = np.trace(vect @ bd_m @ b_m)\n",
        "            mat[i,j] = coef * rho_2_kkbar_lambda(v) * rho_2_kkbar_lambda(w)\n",
        "    return mat\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dga5Xx_5vDf"
      },
      "source": [
        "## Definicion de Hamiltoniano"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myiTq53L5E1U"
      },
      "source": [
        "Cargamos el código de creación y resolución de Hamiltonianos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h5FXWv849Mq",
        "outputId": "49dd47b5-8c16-4ad4-92e7-e172462229b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "70\n"
          ]
        }
      ],
      "source": [
        "m = 8\n",
        "d = 2*m\n",
        "pairs = True # Usar solo para estados puros\n",
        "# Creo las bases para no tener que recrearlas luego\n",
        "basis = fixed_basis(m, d, pairs = pairs)\n",
        "#basis_m1 = fixed_basis(m-1, d, pairs = True)\n",
        "basis_m2 = fixed_basis(m-2, d, pairs = pairs)\n",
        "print(basis.size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PToiSs915TXw"
      },
      "outputs": [],
      "source": [
        "## Usamos este approach si queremos guardar los generadores\n",
        "# Dados 1/2 (d^2+d) elementos, genera una mat de dxd:\n",
        "eps = 0.00001\n",
        "\n",
        "def sym_mat_gen(vect, d):\n",
        "    matrix = fill_matrix(vect, d)\n",
        "    return matrix + matrix.T - np.diag(matrix.diagonal())\n",
        "\n",
        "@jit(nopython=True)\n",
        "def fill_matrix(vect, d):\n",
        "    matrix = np.zeros((d, d))\n",
        "    idx = 0\n",
        "    for i in prange(d):\n",
        "        for j in prange(i, d):\n",
        "            matrix[i, j] = vect[idx]\n",
        "            idx += 1\n",
        "    return matrix\n",
        "\n",
        "# Generamos una matrix aleatoria. Cuidado con la distribución, ver https://stackoverflow.com/questions/56605189/is-there-an-efficient-way-to-generate-a-symmetric-random-matrix\n",
        "def hamil_base_gen(d):\n",
        "    U = np.random.uniform(low=0, high=1.0, size=(d, d))\n",
        "    hamil_base = np.tril(U) + np.tril(U, -1).T\n",
        "    return hamil_base\n",
        "\n",
        "# Dada un a mat dxd simetrica, contruye el hamiltoniano de un cuerpo a_{ij} c^{dag}_i c_j\n",
        "# Alternativamente podemos construirlo a partir de rho_1_gen\n",
        "def base_hamiltonian_aux(mat, size, d, rho_1_gen):\n",
        "    # Construccion de H\n",
        "    rho_1_gen_transposed = rho_1_gen.transpose(1, 0, 2, 3)\n",
        "    mat_expanded = mat[:, :, np.newaxis, np.newaxis]\n",
        "    h = np.sum(mat_expanded * rho_1_gen_transposed[:, :, :, :], axis=(0, 1))\n",
        "    return h.astype(np.float32)\n",
        "\n",
        "def base_hamiltonian(mat, basis, rho_1_gen):\n",
        "    return base_hamiltonian_aux(mat, basis.size, basis.d, rho_1_gen)\n",
        "\n",
        "def two_body_hamiltonian(t_basis_size, m, energy, G, rho_1_arrays, rho_2_arrays, indices):\n",
        "    # Creamos la mat diagonal de d*d con los elementos de energy\n",
        "    # cada uno de estos, se contraen con los elementos de rho_1_arrays\n",
        "    # la mat energy contiene las energias de cada termino c^\\dag_k c_k para k kbar (iguales)\n",
        "    # por ello los elementos se repiten \n",
        "    energy_matrix = np.diagflat(np.kron(energy, np.ones(2))) + eps * np.random.random((2*m,2*m))\n",
        "    \n",
        "    # Construimos la mat de energía\n",
        "    rho_1_arrays_t = tf.transpose(rho_1_arrays,perm=[1, 0, 2, 3])\n",
        "    h0 = np.sum(energy_matrix[:, :, np.newaxis, np.newaxis] * rho_1_arrays_t[:, :, :, :], axis=(0, 1))\n",
        "\n",
        "    # Pasamos ahora a la matrix de interacción con la misma estrategia\n",
        "    # dada G que indica la interacción entre los pares k' k'bar k kbar \n",
        "    # (que son elementos particulares de t_basis)\n",
        "    # transladamos estos coeficientes a una matriz en t_basis\n",
        "    # y multiplicamos por rho_2_arrays\n",
        "    \n",
        "    # Primero determinamos, dada t_basis, cuales son los indices de pares kkbar\n",
        "    i, j = np.meshgrid(indices, indices, indexing='ij') # Lo usamos para rellenar la mat deseada\n",
        "    rho_2_arrays_t = tf.transpose(rho_2_arrays,perm=[1, 0, 2, 3])\n",
        "\n",
        "    # Contruimos la mat que contraeremos con rho_2_arrays\n",
        "    mat = np.zeros((t_basis_size, t_basis_size))\n",
        "    mat[i, j] = G\n",
        "    hi = np.sum(mat[:, :, np.newaxis, np.newaxis] * rho_2_arrays_t[:, :, :, :], axis=(0, 1))\n",
        "    return (h0, hi)\n",
        "\n",
        "def solve(h, last_step = None):\n",
        "    sol = linalg.eigsh(h, which='SA',k=19)\n",
        "    eigenspace_tol = 0.0001\n",
        "    if type(last_step) != type(None):\n",
        "        # Seleccionamos todos los autovects que difieren sus autovalores menos que tol (mismo autoespacio)\n",
        "        # y tomamos la proyección en el autoespacio de la solución del paso anterior (last_step)\n",
        "        eig = sol[0].real\n",
        "        eigv = sol[1]\n",
        "        cand = [eigv[:,i].real  for (i, x) in enumerate(eig) if abs(x-min(eig)) < eigenspace_tol]\n",
        "        cand_norm = [x/np.linalg.norm(x) for x in cand]\n",
        "        fund = np.zeros(len(cand[0]))\n",
        "        for x in cand_norm:\n",
        "            fund += np.dot(last_step,x) * x\n",
        "    else:\n",
        "        argmin = np.argmin(sol[0].real)\n",
        "        fund = sol[1][:,argmin]\n",
        "    fund = fund.real / np.linalg.norm(fund)\n",
        "    return fund\n",
        "\n",
        "# Generacion de H basada en TF\n",
        "\n",
        "# Funciones auxiliares de gen de H basado en TF\n",
        "## Dada matrix de indices, genera los indices de updates de TF\n",
        "def gen_update_indices(t_basis, batch_size):\n",
        "    # Calculamos los indices de kkbar en t_basis\n",
        "    indices = tf.constant(get_kkbar_indices(t_basis))\n",
        "    # Creamos el array de indices x indices\n",
        "    i, j = tf.meshgrid(indices, indices, indexing='ij')\n",
        "    matrix = tf.reshape(tf.stack([i, j], axis=-1), (-1, 2))\n",
        "\n",
        "    # Repeat the matrix along the first axis (axis=0) 'b' times\n",
        "    repeated_matrix = tf.repeat(tf.expand_dims(matrix, axis=0), repeats=batch_size, axis=0)\n",
        "\n",
        "    # Create an index array from 0 to b-1\n",
        "    indices = tf.range(batch_size, dtype=tf.int32)\n",
        "\n",
        "    # Expand the index array to have the same shape as the repeated matrix\n",
        "    indices = tf.expand_dims(indices, axis=-1)\n",
        "    indices = tf.expand_dims(indices, axis=-1)\n",
        "    indices = tf.tile(indices, multiples=[1,matrix.shape[0],1]) \n",
        "\n",
        "    # Concatenate the index array to the repeated matrix along a new axis\n",
        "    tiled_matrix = tf.concat([indices, repeated_matrix], axis=-1)\n",
        "    tiled_matrix = tf.reshape(tiled_matrix, [-1,3])\n",
        "    return tiled_matrix\n",
        "\n",
        "\n",
        "def two_body_hamiltonian_tf(t_basis, m, energy_batch, G_batched, rho_1_arrays, rho_2_arrays, indices):\n",
        "    # SECCIÓN ENERGIAS\n",
        "    ## Dado un batch de niveles, lo pasamos a TF\n",
        "    energy_matrix = tf.constant(energy_batch, dtype=tf.float32)\n",
        "    ## Repetimos los niveles para cada uno de los pares (por el nivel k y kbar)\n",
        "    energy_matrix = tf.repeat(energy_matrix, repeats=2, axis=1)\n",
        "    ## Generamos la matrix diagonal y expandimos\n",
        "    energy_matrix_expanded = tf.linalg.diag(energy_matrix)\n",
        "    energy_matrix_expanded = energy_matrix_expanded[:, :, :, np.newaxis, np.newaxis]\n",
        "    rho_1_gen_transposed = tf.transpose(rho_1_arrays, perm=[1, 0, 2, 3])\n",
        "    # Multiplicamos por los operadores C^dag C\n",
        "    h0_arr = tf.reduce_sum(energy_matrix_expanded * rho_1_gen_transposed[np.newaxis,:,:,:,:], axis=[1,2])\n",
        "\n",
        "    # SECCIÓN INTERACCIÓN\n",
        "    # Ya tenemos los indices de updates, ahora tomamos la mat en t_basis (una de zeros)\n",
        "    # y updateamos de acuerdo a la lista de G's cada uno flatteneados\n",
        "    G_flatten = np.ndarray.flatten(np.array([np.ndarray.flatten(G) for G in G_batched]))\n",
        "    # Creamos la mat de t_basis y updateamos a partir de los indices de kkbar\n",
        "    mat = tf.zeros((len(energy_batch), t_basis.size, t_basis.size), dtype=tf.float32)\n",
        "    mat = tf.tensor_scatter_nd_update(mat, indices, G_flatten)\n",
        "    # Preparamos las dimensiones y multiplicamos\n",
        "    mat_expanded = mat[:, :, :, np.newaxis, np.newaxis]\n",
        "    rho_2_gen_transposed = tf.transpose(rho_2_arrays, perm=[1, 0, 2, 3])\n",
        "    hi_arr = tf.reduce_sum(mat_expanded * rho_2_gen_transposed[np.newaxis,:,:,:,:], axis=[1,2])\n",
        "\n",
        "    return h0_arr - hi_arr\n",
        "\n",
        "def state_energy(state, h_arr):\n",
        "    return tf.linalg.trace(tf.matmul(state, h_arr))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emVBTg2QD-Fg"
      },
      "source": [
        "## Modelo de ML\n",
        "Basado en matrices densidad de 1 y 2 cuerpos como input, con hamiltoniano como salida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aF_Ec_mCGX96"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-15 15:44:48.373855: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-15 15:44:48.389429: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "''"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tf.test.gpu_device_name()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJDoa6LUJJ8O",
        "outputId": "73481454-fbcb-469f-d72f-cd0f8d534808"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0]\n",
            " [1 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0]\n",
            " [1 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0]\n",
            " [1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            " [1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1]\n",
            " [1 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0]\n",
            " [1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0]\n",
            " [1 1 0 0 1 1 0 0 0 0 1 1 0 0 0 0]\n",
            " [1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 0]\n",
            " [1 1 0 0 1 1 0 0 0 0 0 0 0 0 1 1]\n",
            " [1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0]\n",
            " [1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0]\n",
            " [1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0]\n",
            " [1 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1]\n",
            " [1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0]\n",
            " [1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0]\n",
            " [1 1 0 0 0 0 0 0 1 1 0 0 0 0 1 1]\n",
            " [1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0]\n",
            " [1 1 0 0 0 0 0 0 0 0 1 1 0 0 1 1]\n",
            " [1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1]\n",
            " [0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 0]\n",
            " [0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0]\n",
            " [0 0 1 1 1 1 0 0 0 0 0 0 1 1 0 0]\n",
            " [0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1]\n",
            " [0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0]\n",
            " [0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0]\n",
            " [0 0 1 1 0 0 1 1 0 0 0 0 1 1 0 0]\n",
            " [0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 1]\n",
            " [0 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0]\n",
            " [0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0]\n",
            " [0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 1]\n",
            " [0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0]\n",
            " [0 0 1 1 0 0 0 0 0 0 1 1 0 0 1 1]\n",
            " [0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1]\n",
            " [0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 1 1 1 0 0 1 1 0 0 0 0]\n",
            " [0 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0]\n",
            " [0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1]\n",
            " [0 0 0 0 1 1 0 0 1 1 1 1 0 0 0 0]\n",
            " [0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0]\n",
            " [0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 1]\n",
            " [0 0 0 0 1 1 0 0 0 0 1 1 1 1 0 0]\n",
            " [0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 1]\n",
            " [0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1]\n",
            " [0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 0]\n",
            " [0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1]\n",
            " [0 0 0 0 0 0 1 1 0 0 1 1 1 1 0 0]\n",
            " [0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1]\n",
            " [0 0 0 0 0 0 1 1 0 0 0 0 1 1 1 1]\n",
            " [0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1]\n",
            " [0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 1]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\nm = 1\\nm1_basis = fixed_basis(m, d)\\nprint(m1_basis.size)\\nprint(m1_basis.base)\\nnm1_basis = fixed_basis(basis.m-m, d)\\n'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Construccion de bases para calculo de rho1 y rho2\n",
        "# rho2\n",
        "m = 2\n",
        "m2_basis = fixed_basis(m, d, pairs=pairs)\n",
        "print(m2_basis.size)\n",
        "nm2_basis = fixed_basis(basis.m-m, d, pairs=pairs)\n",
        "print(nm2_basis.base)\n",
        "t_basis = fixed_basis(2, basis.d, pairs=pairs)\n",
        "# rho1\n",
        "\"\"\"\n",
        "m = 1\n",
        "m1_basis = fixed_basis(m, d)\n",
        "print(m1_basis.size)\n",
        "print(m1_basis.base)\n",
        "nm1_basis = fixed_basis(basis.m-m, d)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oapxWkD16fHg"
      },
      "source": [
        "### Algunos benchmarks y funciones auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "umCIrxCZKXQd"
      },
      "outputs": [],
      "source": [
        "# Given h calculo en rho2 y rho1 máximo\n",
        "def rho1_rho2(h, beta):\n",
        "    fund = thermal_state(h, beta)\n",
        "    rho2 = np.array(rho_2(basis, m2_basis.size, state, rho_2_arrays))\n",
        "    r = np.sort(linalg_d.eigvals(rho2).real)\n",
        "    rho_2_max = r[0]\n",
        "    rho1 = np.array(rho_1(basis, state, rho_1_arrays))\n",
        "    r = np.sort(linalg_d.eigvals(rho1).real)\n",
        "    rho_1_max = r[0]\n",
        "\n",
        "    return (rho_1_max, rho_2_max)\n",
        "\n",
        "def fill_triangular_np(x):\n",
        "    m = x.shape[0]\n",
        "    n = np.int32(np.sqrt(.25 + 2 * m) - .5)\n",
        "    x_tail = x[(m - (n**2 - m)):]\n",
        "    return np.triu(np.concatenate([x, x_tail[::-1]], 0).reshape(n, n))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QaNnIIc5bZux"
      },
      "outputs": [],
      "source": [
        "# TEST: Las funciones de TF y comunes coinciden\n",
        "\n",
        "# Dado h, \\beta, construyo el estado térmico\n",
        "from scipy.linalg import expm\n",
        "\n",
        "def thermal_state(h, beta):\n",
        "    quotient = expm(-beta*h)\n",
        "    return quotient / np.trace(quotient)\n",
        "\n",
        "## NO usar para mat no hermiticas\n",
        "@nb.jit(nopython=True)\n",
        "def thermal_state_eig(h, beta):\n",
        "    w, v = np.linalg.eigh(-beta*h)\n",
        "    D = np.diag(np.exp(w))\n",
        "    mat = v @ D @ v.T\n",
        "    mat = mat / np.trace(mat)\n",
        "    return mat\n",
        "    \n",
        "def gen_to_h(base, rho_1_arrays):\n",
        "    triag = fill_triangular_np(base)\n",
        "    body_gen = triag + np.transpose(triag)-np.diag(np.diag(triag))\n",
        "    h = np.array(base_hamiltonian(body_gen, basis, rho_1_arrays))  \n",
        "    return h \n",
        "\n",
        "def gen_to_h_1b(hamil_base):\n",
        "    triag = tfp.math.fill_triangular(hamil_base, upper=True)\n",
        "    body_gen = triag + tf.transpose(triag, perm=[0,2,1])-tf.linalg.diag(tf.linalg.diag_part(triag))\n",
        "    return body_gen\n",
        "\n",
        "def gen_to_h_tf(hamil_base, rho_1_arrays):\n",
        "    triag = tfp.math.fill_triangular(hamil_base, upper=True)\n",
        "    body_gen = triag + tf.transpose(triag, perm=[0,2,1])-tf.linalg.diag(tf.linalg.diag_part(triag)) # Simetrizamos y generamos la matriz de h\n",
        "    hamil_expanded = body_gen[:, :, :, np.newaxis, np.newaxis]\n",
        "    rho_1_gen_transposed = tf.transpose(rho_1_arrays, perm=[1, 0, 2, 3])\n",
        "    h_arr = tf.reduce_sum(hamil_expanded * rho_1_gen_transposed[np.newaxis,:,:,:,:], axis=[1,2])\n",
        "    return h_arr\n",
        "\n",
        "def thermal_state_tf(h):\n",
        "    # Assume beta=1\n",
        "    exp_hamiltonian = tf.linalg.expm(-h)\n",
        "    partition_function = tf.linalg.trace(exp_hamiltonian)\n",
        "    partition_function = tf.expand_dims(partition_function, axis=1)\n",
        "    partition_function = tf.expand_dims(partition_function, axis=1)\n",
        "    \n",
        "    rho = exp_hamiltonian / partition_function\n",
        "\n",
        "    return rho\n",
        "\n",
        "def rho_1_tf(state, rho_1_arrays):\n",
        "    state = tf.expand_dims(state, axis=1)  # Shape: (5120, 10, 1, 10)\n",
        "    state_expanded = tf.expand_dims(state, axis=1)\n",
        "    rho_1_arrays_expanded = tf.expand_dims(rho_1_arrays, axis=0)  # Shape: (1, 5, 5, 10, 10)\n",
        "    product = state_expanded * rho_1_arrays_expanded  # Shape: (5120, 10, 5, 10, 10)\n",
        "    mat = tf.reduce_sum(product, axis=[-2, -1])  # Shape: (5120, 5, 5)\n",
        "    \n",
        "    return mat\n",
        "\n",
        "def rho_2_tf(state, rho_2_arrays):\n",
        "    state = tf.expand_dims(state, axis=1)  # Shape: (5120, 10, 1, 10)\n",
        "    state_expanded = tf.expand_dims(state, axis=1)\n",
        "    rho_2_arrays_expanded = tf.expand_dims(rho_2_arrays, axis=0)  # Shape: (1, 5, 5, 10, 10)\n",
        "    product = state_expanded * rho_2_arrays_expanded  # Shape: (5120, 10, 5, 10, 10)\n",
        "    mat = tf.reduce_sum(product, axis=[-2, -1])  # Shape: (5120, 5, 5)\n",
        "    \n",
        "    return mat\n",
        "\n",
        "# NOTA: para calcular el bloque rho2kkbar, utilizar en lugar\n",
        "\n",
        "def rho_1_gc_tf(hamil_base):\n",
        "    e, v = tf.linalg.eigh(gen_to_h_1b(hamil_base))\n",
        "    result = 1 / (1 + tf.exp(e))\n",
        "    result = tf.linalg.diag(result)\n",
        "    res = tf.linalg.matmul(v,result)\n",
        "    res = tf.linalg.matmul(res,v,adjoint_b=True)\n",
        "    \n",
        "    return tf.cast(res, tf.float32)\n",
        "\n",
        "# Aux function\n",
        "def outer_product(vector):\n",
        "    return tf.einsum('i,j->ij', vector, vector)\n",
        "\n",
        "def pure_state(h):\n",
        "    e, v = tf.linalg.eigh(h)\n",
        "    fund = v[:,:,0]\n",
        "    d = tf.map_fn(outer_product, fund)\n",
        "    return d\n",
        "\n",
        "# Casos de entrenamiento tipo mat gaussianas\n",
        "def gen_gauss_mat(G, sigma_sq, size):\n",
        "    indices = np.arange(size)\n",
        "    mat = G * np.exp(-((indices - indices[:, np.newaxis])**2) / (2 * sigma_sq))\n",
        "    return mat\n",
        "\n",
        "def gen_gauss_mat_np(G_values, sigma_sq_values, size):\n",
        "    indices = np.arange(size, dtype=np.float32)\n",
        "    indices_diff = indices - indices[:, np.newaxis]\n",
        "\n",
        "    mat = G_values[:, np.newaxis, np.newaxis] * np.exp(-np.square(indices_diff) / (2 * sigma_sq_values[:, np.newaxis, np.newaxis]))\n",
        "\n",
        "    return mat\n",
        "\n",
        "# Casos de entrenamiento tipo matriz vectorial\n",
        "def gen_vect_mat(size, g_init, g_stop, sym = True):\n",
        "    if sym:\n",
        "        vect = np.sort(np.random.uniform(g_init, g_stop, size // 2))[::-1]\n",
        "        vect = np.repeat(vect, 2)\n",
        "        if size % 2 != 0: # TODO: Agregar tipo en el medio\n",
        "            raise ValueError\n",
        "    else:\n",
        "        vect = np.sort(np.random.uniform(g_init, g_stop, size))[::-1]\n",
        "    indices = np.abs(np.arange(size)[:, np.newaxis] - np.arange(size))\n",
        "    mat = vect[indices]\n",
        "\n",
        "    return vect, mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylpy_BCw6jxF"
      },
      "source": [
        "### Construccion de dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Version sincrónica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2is_Eo_qGpEz",
        "outputId": "9a968190-59f2-4695-ef18-b99ff5b4a212"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-15 15:44:48.756252: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-15 15:44:48.756473: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "from typing import Literal\n",
        "\n",
        "# Config\n",
        "num_samples = 1000\n",
        "gpu_batch_size = 256\n",
        "en_batch = [np.arange(0, basis.m) - basis.m//2 + 1/2 for _ in range(0,gpu_batch_size)] \n",
        "\n",
        "# Beta\n",
        "beta = 1\n",
        "\n",
        "# Construccion de parametros y matrices auxiliares\n",
        "#rho1_size = m1_basis.size\n",
        "rho2_size = m2_basis.size\n",
        "rho2kkbar_size = basis.m\n",
        "input_shape = (basis.m,basis.m, 1) # Usando rho2kkbar como input batcheado\n",
        "fund_size = basis.size\n",
        "hamil_base_size = basis.d*(basis.d+1)//2\n",
        "rho_1_arrays = rho_1_gen(basis)\n",
        "rho_1_arrays_tf = tf.constant(rho_1_arrays, dtype=tf.float32)\n",
        "rho_2_arrays = rho_2_gen(basis, nm2_basis, m2_basis)\n",
        "rho_2_arrays_tf = tf.constant(rho_2_arrays, dtype=tf.float32)\n",
        "rho_2_arrays_kkbar = rho_2_kkbar_gen(t_basis, rho_2_arrays)\n",
        "rho_2_arrays_kkbar_tf = tf.constant(rho_2_arrays_kkbar, dtype=tf.float32)\n",
        "k_indices = get_kkbar_indices(t_basis)\n",
        "k_indices_tf = gen_update_indices(t_basis, gpu_batch_size)\n",
        "\n",
        "\n",
        "# Generación de dataset (params)\n",
        "# h_type = {const, gaussian, random}: const = proporcional a ones, gaussian = proporcional a mat gaussiana, random = full random \n",
        "# g_init, g_stop: rango de Gs (aplica a los 3 casos)\n",
        "# state_type = {thermal, gs}: tipo de estado (térmico o funalmental)\n",
        "# input_type = {rho2, rho1}: tipo de feature a calcular\n",
        "valid_h_type = Literal['const', 'gaussian', 'vect', 'random']\n",
        "valid_state_type = Literal['thermal', 'gs']\n",
        "valid_input_type = Literal['rho2', 'rho1', 'rho1+rho2']\n",
        "\n",
        "def gen_dataset(h_type: valid_h_type, g_init: float, g_stop: float, state_type: valid_state_type, input_type: valid_input_type, include_energy: bool):\n",
        "    print(tf.test.gpu_device_name())\n",
        "    datasets = []\n",
        "    for i in tqdm(range(num_samples//gpu_batch_size+1)):\n",
        "        size = basis.m*(basis.m+1)//2\n",
        "        # En una primera versión vamos a pasar una mat proporcional a range(0,m) para energias (DEFINIDO EN CONFIG)\n",
        "\n",
        "        ## Caso G proporcional a ones\n",
        "        if h_type == 'const':\n",
        "            label_size = 1 \n",
        "            h_labels = [np.random.uniform(g_init, g_stop) for _ in range(0,gpu_batch_size)]\n",
        "            g_arr = [np.ones((basis.m, basis.m))*g_seed for g_seed in h_labels]\n",
        "            g_arr = tf.constant(g_arr, dtype=tf.float32)\n",
        "\n",
        "\n",
        "        ## Caso generico\n",
        "        elif h_type == 'random':\n",
        "            label_size = basis.m*(basis.m+1)// 2 # CASO GENERICO elementos independientes de una mat de m x m\n",
        "            h_labels = [np.random.uniform(g_init, g_stop, label_size) for _ in range(0,gpu_batch_size)] \n",
        "            # Construimos la mat G\n",
        "            triag = tfp.math.fill_triangular(h_labels, upper=True)\n",
        "            g_arr = triag + tf.transpose(triag, perm=[0,2,1])-tf.linalg.diag(tf.linalg.diag_part(triag))\n",
        "\n",
        "        ## Caso vectorial\n",
        "        elif h_type == 'vect':\n",
        "            symmetry = True # TODO: Agregar elemento si no hay simetria\n",
        "            label_size = basis.m // 2 if symmetry else basis.m\n",
        "            labels_gen = lambda x: np.sort(np.random.uniform(g_init, g_stop, basis.m // 2 if symmetry else 1))[::-1]\n",
        "            h_labels = [labels_gen(0) for _ in range(0, gpu_batch_size)]\n",
        "            indices = np.abs(np.arange(basis.m)[:, np.newaxis] - np.arange(basis.m))\n",
        "            g_arr = [np.repeat(x,2)[indices] if symmetry else x[indices] for x in h_labels]\n",
        "            h_labels = tf.constant(h_labels, dtype=tf.float32)\n",
        "            g_arr = tf.constant(g_arr, dtype=tf.float32)\n",
        "\n",
        "        ## Caso reducido\n",
        "        else: \n",
        "            label_size = 2\n",
        "            h_labels = np.array([[np.random.uniform(g_init, g_stop), np.random.random()*2 + 0.1] for _ in range(0, gpu_batch_size)])\n",
        "            g_arr = gen_gauss_mat_np(h_labels[:,0], h_labels[:,1], basis.m)\n",
        "            h_labels = tf.constant(h_labels, dtype=tf.float32)\n",
        "            g_arr = tf.constant(g_arr, dtype=tf.float32)\n",
        "\n",
        "        \n",
        "        # Construimos los hamiltonianos basados en g_arr\n",
        "        h_arr = two_body_hamiltonian_tf(t_basis, basis.m, en_batch, g_arr.numpy(), rho_1_arrays, rho_2_arrays, k_indices_tf)\n",
        "\n",
        "        # Calculamos los estados\n",
        "        if state_type == 'thermal':\n",
        "            state = thermal_state_tf(h_arr*beta) \n",
        "            state = tf.cast(state, dtype=tf.float32)\n",
        "        else:\n",
        "            state = pure_state(h_arr)\n",
        "        \n",
        "        # Calculamos la feature\n",
        "        if input_type == 'rho2':\n",
        "            rho_input = rho_2_tf(state, rho_2_arrays_kkbar_tf)\n",
        "        elif input_type == 'rho1+rho2':\n",
        "            rho_2_input = rho_2_tf(state, rho_2_arrays_kkbar_tf)\n",
        "            rho_1_input = rho_1_tf(state, rho_1_arrays_tf)\n",
        "        else:\n",
        "            rho_input = rho_1_tf(state, rho_1_arrays_tf)\n",
        "        \n",
        "        # Calculamos la enegia\n",
        "        if include_energy:\n",
        "            energy = state_energy(state, h_arr)\n",
        "\n",
        "        # OUTPUTS\n",
        "        # Caso input eigvals\n",
        "        #input_shape = (basis.m, 1)\n",
        "        #rho_2_input = tf.linalg.eigvals(rho_2_input)\n",
        "        #rho_2_input = tf.sort(tf.math.real(rho_2_input), axis=-1)\n",
        "\n",
        "        # Caso PCA\n",
        "        #input_shape = (num_gen, 1)\n",
        "        #rflat = np.array([np.ndarray.flatten(x) for x in rho_2_input.numpy()])\n",
        "        #rho_2_input = np.dot(rflat, P)\n",
        "\n",
        "        # Generación de dataset\n",
        "        # Tradicional (rho2 tipo matricial)\n",
        "        if input_type == 'rho1' or input_type == 'rho2':\n",
        "            if include_energy:\n",
        "                datasets.append(tf.data.Dataset.from_tensor_slices(((rho_input, energy), h_labels)))\n",
        "            else:\n",
        "                datasets.append(tf.data.Dataset.from_tensor_slices(((rho_input), h_labels)))\n",
        "        else:\n",
        "            datasets.append(tf.data.Dataset.from_tensor_slices(((rho_1_input, rho_2_input), h_labels)))\n",
        "        # Rho2 flatteneada, requerido para DF\n",
        "        #datasets.append(tf.data.Dataset.from_tensor_slices(((tf.reshape(rho_2_input, (gpu_batch_size,basis.m*basis.m))), h_labels)))\n",
        "        #datasets.append(tf.data.Dataset.from_tensor_slices(((rho_1_input, rho_2_input), h_labels)))\n",
        "        #datasets.append(tf.data.Dataset.from_tensor_slices(((rho_1_input, rho_2_input, state), h_labels)))\n",
        "    ds = tf.data.Dataset.from_tensor_slices(datasets)\n",
        "    dataset = ds.interleave(\n",
        "        lambda x: x,\n",
        "        cycle_length=1,\n",
        "        num_parallel_calls=tf.data.AUTOTUNE,\n",
        "    )\n",
        "\n",
        "    return dataset, label_size\n",
        "\n",
        "\n",
        "#batch_size = 32\n",
        "#dataset = dataset.shuffle(buffer_size=num_samples).batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 8, 8), dtype=float32, numpy=\n",
              "array([[[0.9555735 , 0.9555735 , 0.3592694 , 0.3592694 , 0.1659846 ,\n",
              "         0.1659846 , 0.07144889, 0.07144889],\n",
              "        [0.9555735 , 0.9555735 , 0.9555735 , 0.3592694 , 0.3592694 ,\n",
              "         0.1659846 , 0.1659846 , 0.07144889],\n",
              "        [0.3592694 , 0.9555735 , 0.9555735 , 0.9555735 , 0.3592694 ,\n",
              "         0.3592694 , 0.1659846 , 0.1659846 ],\n",
              "        [0.3592694 , 0.3592694 , 0.9555735 , 0.9555735 , 0.9555735 ,\n",
              "         0.3592694 , 0.3592694 , 0.1659846 ],\n",
              "        [0.1659846 , 0.3592694 , 0.3592694 , 0.9555735 , 0.9555735 ,\n",
              "         0.9555735 , 0.3592694 , 0.3592694 ],\n",
              "        [0.1659846 , 0.1659846 , 0.3592694 , 0.3592694 , 0.9555735 ,\n",
              "         0.9555735 , 0.9555735 , 0.3592694 ],\n",
              "        [0.07144889, 0.1659846 , 0.1659846 , 0.3592694 , 0.3592694 ,\n",
              "         0.9555735 , 0.9555735 , 0.9555735 ],\n",
              "        [0.07144889, 0.07144889, 0.1659846 , 0.1659846 , 0.3592694 ,\n",
              "         0.3592694 , 0.9555735 , 0.9555735 ]]], dtype=float32)>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "symmetry = True # TODO: Agregar elemento si no hay simetria\n",
        "label_size = basis.m // 2 if symmetry else basis.m\n",
        "labels_gen = lambda x: np.sort(np.random.uniform(0, 1, basis.m // 2 if symmetry else 1))[::-1]\n",
        "h_labels = [labels_gen(0) for _ in range(0, 1)]\n",
        "indices = np.abs(np.arange(basis.m)[:, np.newaxis] - np.arange(basis.m))\n",
        "g_arr = [np.repeat(x,2)[indices] if symmetry else x[indices] for x in h_labels]\n",
        "h_labels = tf.constant(h_labels, dtype=tf.float32)\n",
        "g_arr = tf.constant(g_arr, dtype=tf.float32)\n",
        "g_arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Filleo de dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "# Save and load dataset\n",
        "save_dataset = False\n",
        "load_dataset = False\n",
        "path = \"/home/agus/TF\"\n",
        "#num_samples = 5000000\n",
        "if save_dataset:\n",
        "    tf.data.Dataset.save(dataset, path)\n",
        "    with open(\"/home/agus/\"+'/file.pkl', 'wb') as file:\n",
        "        pickle.dump(beta_input, file)\n",
        "if load_dataset:\n",
        "    dataset = tf.data.Dataset.load(path)\n",
        "    with open(\"/home/agus/\"+'file.pkl', 'rb') as file:\n",
        "        beta_input = pickle.load(file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8moZIlfabZuy"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'dataset' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Dividimos los datasets\u001b[39;00m\n\u001b[1;32m      2\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.8\u001b[39m \u001b[38;5;241m*\u001b[39m num_samples)\n\u001b[0;32m----> 4\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241m.\u001b[39mtake(train_size)\n\u001b[1;32m      5\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mskip(train_size)\n\u001b[1;32m      8\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m gpu_batch_size\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ],
      "source": [
        "# Dividimos los datasets\n",
        "train_size = int(0.8 * num_samples)\n",
        "\n",
        "train_dataset = dataset.take(train_size)\n",
        "val_dataset = dataset.skip(train_size)\n",
        "\n",
        "\n",
        "batch_size = gpu_batch_size\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "val_dataset = val_dataset.batch(batch_size)\n",
        "\n",
        "#beta_val = beta_input[train_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Dataset Size: -2\n"
          ]
        }
      ],
      "source": [
        "# Cardinality no funciona con los datasets generados por GPU\n",
        "val_size = tf.data.experimental.cardinality(val_dataset).numpy()\n",
        "print(\"Validation Dataset Size:\", val_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n# Correr una vez para definir la transformacion y lyego volver a correr la gen de dataset\\nnum_gen = 10\\nrflat = np.array([np.ndarray.flatten(x) for x in rho_input.numpy()])\\nrflat = rflat - rflat.mean()\\nrflat = rflat / rflat.std()\\nU, S, Vh = np.linalg.svd(rflat)\\nprint(S)\\n\\n# Determinado automáticamente\\nnum_gen = np.where(S < 0.1)[0][0]\\nprint(num_gen)\\n\\nZ = np.dot(rflat.T, rflat)\\neigenvalues, eigenvectors = np.linalg.eig(Z)\\nD = np.diag(eigenvalues)\\nP = eigenvectors[:,0:num_gen]\\nZ_new = np.dot(Z, P)\\nZ_new.shape\\n'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# PCA\n",
        "\"\"\"\n",
        "# Correr una vez para definir la transformacion y lyego volver a correr la gen de dataset\n",
        "num_gen = 10\n",
        "rflat = np.array([np.ndarray.flatten(x) for x in rho_input.numpy()])\n",
        "rflat = rflat - rflat.mean()\n",
        "rflat = rflat / rflat.std()\n",
        "U, S, Vh = np.linalg.svd(rflat)\n",
        "print(S)\n",
        "\n",
        "# Determinado automáticamente\n",
        "num_gen = np.where(S < 0.1)[0][0]\n",
        "print(num_gen)\n",
        "\n",
        "Z = np.dot(rflat.T, rflat)\n",
        "eigenvalues, eigenvectors = np.linalg.eig(Z)\n",
        "D = np.diag(eigenvalues)\n",
        "P = eigenvectors[:,0:num_gen]\n",
        "Z_new = np.dot(Z, P)\n",
        "Z_new.shape\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DNN y CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYEEjNB-7b8y"
      },
      "source": [
        "### Definición de modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8kkhJr5K0ZQ",
        "outputId": "f1b731f1-6a02-4181-f0b5-5677a2a85784"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'label_size' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m flatten_rho2 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mFlatten()(conv_rho2)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#flatten_rho1 = tf.keras.layers.Dense(rho1_size*rho1_size, activation='relu')(flatten_rho1)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#local_size = basis.size*basis.size\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m local_size \u001b[38;5;241m=\u001b[39m label_size\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#dense1 = tf.keras.layers.Dense(8*8*4*4, activation='relu')(dense1)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#dense1 = tf.keras.layers.Dense(512, activation='relu')(flatten_rho1)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#dense1 = tf.keras.layers.Dense(128, activation='relu')(flatten_rho1)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m dense1 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(initial_dense, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)(flatten_rho2)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'label_size' is not defined"
          ]
        }
      ],
      "source": [
        "# Definicion de layers basado en Conv 2D\n",
        "\n",
        "# Factor de cantidad de filtros\n",
        "lf = 16 \n",
        "conv_limit = (rho2kkbar_size - 4)\n",
        "initial_dense = (lf*2**(conv_limit-1)*((rho2kkbar_size-(conv_limit-1))//2)**2)\n",
        "## rho 1\n",
        "rho2_layer =  tf.keras.layers.Input(shape=(rho2kkbar_size,rho2kkbar_size, 1), name='rho2')\n",
        "\n",
        "# Procesamos el primer input\n",
        "conv_rho2 = tf.keras.layers.Conv2D(lf*2**conv_limit, (2, 2), activation='relu')(rho2_layer)\n",
        "conv_rho2 = tf.keras.layers.BatchNormalization()(conv_rho2)\n",
        "for j in [(2**conv_limit - 2**k) for k in range(1,conv_limit)]:\n",
        "    conv_rho2 = tf.keras.layers.Conv2D(lf*j, (2, 2), activation='relu')(conv_rho2 if 2**j != 1 else rho1_layer)\n",
        "    conv_rho2 = tf.keras.layers.BatchNormalization()(conv_rho2)\n",
        "\n",
        "#conv_rho2 = tf.keras.layers.MaxPooling2D((2, 2))(conv_rho2)\n",
        "\n",
        "flatten_rho2 = tf.keras.layers.Flatten()(conv_rho2)\n",
        "#flatten_rho1 = tf.keras.layers.Dense(rho1_size*rho1_size, activation='relu')(flatten_rho1)\n",
        "\n",
        "#local_size = basis.size*basis.size\n",
        "local_size = label_size\n",
        "\n",
        "#dense1 = tf.keras.layers.Dense(8*8*4*4, activation='relu')(dense1)\n",
        "#dense1 = tf.keras.layers.Dense(512, activation='relu')(flatten_rho1)\n",
        "#dense1 = tf.keras.layers.Dense(128, activation='relu')(flatten_rho1)\n",
        "dense1 = tf.keras.layers.Dense(initial_dense, activation='relu')(flatten_rho2)\n",
        "#dense1 = tf.keras.layers.Dense(initial_dense//2, activation='relu')(dense1)\n",
        "\n",
        "output = tf.keras.layers.Dense(local_size)(dense1)\n",
        "\n",
        "\n",
        "# Creamos el modelo y compulamos\n",
        "#model = tf.keras.models.Model(inputs=[rho1_layer, rho2_layer, fund_layer], outputs=output)\n",
        "#model = tf.keras.models.Model(inputs=[rho1_layer, rho2_layer], outputs=output)\n",
        "model = tf.keras.models.Model(inputs=[rho2_layer], outputs=output)\n",
        "\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZBtonvGbZuz",
        "outputId": "f197277e-a84b-4ffd-c81f-c81581707fb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rho2 (InputLayer)           [(None, 4, 4, 1)]         0         \n",
            "                                                                 \n",
            " flatten_11 (Flatten)        (None, 16)                0         \n",
            "                                                                 \n",
            " concatenate_11 (Concatenat  (None, 16)                0         \n",
            " e)                                                              \n",
            "                                                                 \n",
            " dense_41 (Dense)            (None, 20)                340       \n",
            "                                                                 \n",
            " dense_42 (Dense)            (None, 32)                672       \n",
            "                                                                 \n",
            " dense_43 (Dense)            (None, 64)                2112      \n",
            "                                                                 \n",
            " dense_44 (Dense)            (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3189 (12.46 KB)\n",
            "Trainable params: 3189 (12.46 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Modelo denso + fundamental\n",
        "rho2_layer =  tf.keras.layers.Input(shape=input_shape, name='rho2')\n",
        "flatten_rho2 = tf.keras.layers.Flatten()(rho2_layer)\n",
        "#flatten_rho2 = tf.keras.layers.BatchNormalization()(flatten_rho2)\n",
        "#rho2_layer =  tf.keras.layers.Input(shape=(rho2_size,rho2_size, 1), name='rho2')\n",
        "#flatten_rho2 = tf.keras.layers.Flatten()(rho2_layer)\n",
        "#fund_layer =  tf.keras.layers.Input(shape=(fund_size, fund_size, 1 ), name='fund')\n",
        "#flatten_fund = tf.keras.layers.Flatten()(fund_layer)\n",
        "\n",
        "dense1 = tf.keras.layers.concatenate([flatten_rho2])\n",
        "#dense1 = tf.keras.layers.concatenate([dense1, flatten_fund])\n",
        "#dense1 = tf.keras.layers.Dense(rho1_size*rho1_size, activation='relu')(dense1)\n",
        "\n",
        "local_size = label_size\n",
        "l = 4\n",
        "layer_s = [32//i*2 for i in reversed(range(1,l))]\n",
        "for i in range(0,l-1):\n",
        "    dense1 = tf.keras.layers.Dense(layer_s[i], activation='relu')(dense1)\n",
        "\n",
        "output = tf.keras.layers.Dense(local_size)(dense1)\n",
        "# Creamos el modelo y compulamos\n",
        "model = tf.keras.models.Model(inputs=[rho2_layer], outputs=output)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gen_dnn_model(label_size, input_type, include_energy: bool):\n",
        "    if input_type == 'rho1':\n",
        "        rho_layer =  tf.keras.layers.Input(shape=(basis.d, basis.d, 1), name='rho')\n",
        "    elif input_type == 'rho2':\n",
        "        rho_layer =  tf.keras.layers.Input(shape=(basis.m, basis.m, 1), name='rho')\n",
        "    else:\n",
        "        rho_1_layer =  tf.keras.layers.Input(shape=(basis.d, basis.d, 1), name='rho1')\n",
        "        rho_2_layer =  tf.keras.layers.Input(shape=(basis.m, basis.m, 1), name='rho2')\n",
        "\n",
        "    if include_energy:\n",
        "        energy_layer = tf.keras.layers.Input(shape=(1), name='energy')\n",
        "\n",
        "    if input_type == 'rho1' or input_type == 'rho2':\n",
        "        flatten_rho = tf.keras.layers.Flatten()(rho_layer)\n",
        "        #flatten_rho2 = tf.keras.layers.BatchNormalization()(flatten_rho2)\n",
        "        if include_energy:\n",
        "            dense1 = tf.keras.layers.concatenate([flatten_rho, energy_layer]) \n",
        "        else:\n",
        "            dense1 = tf.keras.layers.concatenate([flatten_rho]) \n",
        "    else:\n",
        "        flatten_rho_1 = tf.keras.layers.Flatten()(rho_1_layer)\n",
        "        flatten_rho_2 = tf.keras.layers.Flatten()(rho_2_layer)\n",
        "        #flatten_rho2 = tf.keras.layers.BatchNormalization()(flatten_rho2)\n",
        "        dense1 = tf.keras.layers.concatenate([flatten_rho_1, flatten_rho_2])    \n",
        "\n",
        "    local_size = label_size\n",
        "    l = 4\n",
        "    layer_s = [16//i*2 for i in reversed(range(1,l))]\n",
        "    for i in range(0,l-1):\n",
        "        dense1 = tf.keras.layers.Dense(layer_s[i], activation='sigmoid')(dense1)\n",
        "        #dense1 = tf.keras.layers.Dense(layer_s[i], activation='sigmoid', kernel_regularizer=tf.keras.regularizers.L1(0.001))(dense1)\n",
        "        #dense1 = tf.keras.layers.Dropout(0.1)(dense1)\n",
        "\n",
        "    output = tf.keras.layers.Dense(local_size)(dense1)\n",
        "    if input_type == 'rho1' or input_type == 'rho2':\n",
        "        if include_energy:\n",
        "            model = tf.keras.models.Model(inputs=[rho_layer, energy_layer], outputs=output)\n",
        "        else:\n",
        "            model = tf.keras.models.Model(inputs=[rho_layer], outputs=output)\n",
        "    else:\n",
        "        model = tf.keras.models.Model(inputs=[rho_1_layer, rho_2_layer], outputs=output)\n",
        "\n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_8\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " rho (InputLayer)            [(None, 4, 4, 1)]            0         []                            \n",
            "                                                                                                  \n",
            " flatten_12 (Flatten)        (None, 16)                   0         ['rho[0][0]']                 \n",
            "                                                                                                  \n",
            " energy (InputLayer)         [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " concatenate_12 (Concatenat  (None, 17)                   0         ['flatten_12[0][0]',          \n",
            " e)                                                                  'energy[0][0]']              \n",
            "                                                                                                  \n",
            " dense_32 (Dense)            (None, 10)                   180       ['concatenate_12[0][0]']      \n",
            "                                                                                                  \n",
            " dense_33 (Dense)            (None, 16)                   176       ['dense_32[0][0]']            \n",
            "                                                                                                  \n",
            " dense_34 (Dense)            (None, 32)                   544       ['dense_33[0][0]']            \n",
            "                                                                                                  \n",
            " dense_35 (Dense)            (None, 1)                    33        ['dense_34[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 933 (3.64 KB)\n",
            "Trainable params: 933 (3.64 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.engine.functional.Functional at 0x7f349db83f70>"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gen_dnn_model(label_size, input_type, include_energy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RgoMlCyyfBe-"
      },
      "outputs": [],
      "source": [
        "# LOSS FUNCTIONS\n",
        "r_size = basis.size\n",
        "\n",
        "# Custom loss function based on GS MSE\n",
        "def gs_loss(h_pred, h_true):\n",
        "    h_pred = tf.reshape(h_pred, shape=(-1,r_size,r_size))\n",
        "    e, v = tf.linalg.eigh(h_pred)\n",
        "    gs_pred = v[:, 0]\n",
        "\n",
        "    h_true = tf.reshape(h_true, shape=(-1,r_size,r_size))\n",
        "    e, v = tf.linalg.eigh(h_true)\n",
        "    gs_true = v[:, 0]\n",
        "\n",
        "    gs_diff = tf.norm(gs_true - gs_pred)\n",
        "\n",
        "    return gs_diff + tf.reduce_mean(tf.square(h_true - h_pred)) * 100\n",
        "\n",
        "def distance_to_hermitian(matrix):\n",
        "    hermitian_part = 0.5 * (matrix + tf.linalg.adjoint(matrix))\n",
        "    distance = tf.norm(matrix - hermitian_part, ord='euclidean')\n",
        "    return distance\n",
        "\n",
        "# Custom loss function based on MSE + non-hermitian penalization\n",
        "def herm_loss(h_pred, h_true):\n",
        "    h_pred_arr = tf.reshape(h_pred, shape=(-1,r_size,r_size))\n",
        "    return tf.reduce_mean(tf.square(h_true - h_pred)) + distance_to_hermitian(h_pred_arr)\n",
        "\n",
        "# Custom loss function based on h eigenvalues\n",
        "def eig_loss(h_pred, h_true):\n",
        "    eig_true = tf.sort(tf.math.real(tf.linalg.eigvals(tf.reshape(h_true, (-1, fund_size, fund_size)))))\n",
        "    eig_pred = tf.sort(tf.math.real(tf.linalg.eigvals(tf.reshape(h_pred, (-1, fund_size, fund_size)))))\n",
        "    return tf.reduce_mean(tf.square(eig_true - eig_pred))\n",
        "\n",
        "# MSE with a factor\n",
        "def mse_f(h_pred, h_true):\n",
        "    f = 1000\n",
        "    return tf.reduce_mean(tf.square(h_true - h_pred))*f\n",
        "\n",
        "# Spectral radius loss\n",
        "def spectral_loss(h_pred, h_true):\n",
        "    eig = tf.math.real(tf.linalg.eigvals(tf.reshape(h_true-h_pred, (-1, fund_size, fund_size))))\n",
        "    return tf.math.reduce_max(tf.abs(eig))\n",
        "\n",
        "# Hamiltonian MSE loss (using generators)\n",
        "def base_mse_loss(base_pred, base_true):\n",
        "    h_pred = gen_to_h_tf(base_pred, rho_1_arrays_tf)\n",
        "    h_true = gen_to_h_tf(base_true, rho_1_arrays_tf)\n",
        "    mat = tf.reshape(h_pred-h_true, (-1, fund_size, fund_size))\n",
        "    return tf.norm(mat, ord='fro', axis=[-1, -2])\n",
        "\n",
        "# Custom loss function based on h eigenvalues (using generators)\n",
        "def base_eig_loss(base_pred, base_true):\n",
        "    h_pred = gen_to_h_tf(base_pred, rho_1_arrays_tf)\n",
        "    h_true = gen_to_h_tf(base_true, rho_1_arrays_tf)\n",
        "    eig_true = tf.sort(tf.math.real(tf.linalg.eigvals(tf.reshape(h_true, (-1, fund_size, fund_size)))))\n",
        "    eig_pred = tf.sort(tf.math.real(tf.linalg.eigvals(tf.reshape(h_pred, (-1, fund_size, fund_size)))))\n",
        "    return tf.reduce_mean(tf.square(eig_true - eig_pred))\n",
        "\n",
        "# Custom loss function based on rho1 eigenvals\n",
        "## Auxiliary function\n",
        "def base_to_rho_1_tf(base_pred):\n",
        "    h = gen_to_h_tf(base_pred, rho_1_arrays_tf)\n",
        "    h = tf.reshape(h, (-1, fund_size, fund_size))\n",
        "    state = thermal_state_tf(h)\n",
        "    rho1 = rho_1_tf(state, rho_1_arrays_tf)\n",
        "    return rho1\n",
        "    \n",
        "def rho1_loss(base_pred, base_true):\n",
        "    mat = base_to_rho_1_tf(base_pred) - base_to_rho_1_tf(base_true)\n",
        "    return tf.norm(mat, ord='fro', axis=[-1, -2])\n",
        "\n",
        "# Custom loss function based on rho1 eigenvals (using generators)\n",
        "def base_rho1_loss(base_pred, base_true):\n",
        "    h_pred = gen_to_h_tf(base_pred, rho_1_arrays_tf)\n",
        "    h_true = gen_to_h_tf(base_true, rho_1_arrays_tf)\n",
        "    return tf.reduce_mean(tf.square(rho_1_eig_tf(h_pred) - rho_1_eig_tf(h_true)))*1000\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiWk9piJtNIZ"
      },
      "source": [
        "### Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhJCHf0fQdRl",
        "outputId": "1821cf27-9ff5-4d67-e9f5-956d20eda5e2"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import RMSprop, Adam, Nadam, Lion\n",
        "\n",
        "def dnn_fit(dataset, label_size, input_type, include_energy):\n",
        "    model = gen_dnn_model(label_size, input_type, include_energy)\n",
        "\n",
        "    # Dividimos los datasets\n",
        "    train_size = int(0.8 * num_samples)\n",
        "\n",
        "    train_dataset = dataset.take(train_size)\n",
        "    val_dataset = dataset.skip(train_size)\n",
        "    \n",
        "    batch_size = gpu_batch_size\n",
        "    train_dataset = train_dataset.batch(batch_size)\n",
        "    val_dataset = val_dataset.batch(batch_size)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(),\n",
        "                loss='MSE',\n",
        "                metrics=['accuracy', 'mean_squared_error'])\n",
        "\n",
        "    # Train the model\n",
        "    num_epochs = 100\n",
        "    device_name = tf.test.gpu_device_name()\n",
        "\n",
        "    with tf.device('/gpu:0'):\n",
        "        history = model.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset)\n",
        "\n",
        "    return model, val_dataset, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "cvpE_X1iTXcB",
        "outputId": "eff0e5f5-5b26-46ea-ec6b-491d1de9944c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "#plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.plot(history.history['mean_squared_error'][75:], label='Training MSE')\n",
        "plt.plot(history.history['val_mean_squared_error'][75:], label='Validation MSE')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE')\n",
        "plt.title('MSE vs. Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# MIN LOSS = 0.0128 c/fund 50epochs MSE\n",
        "##         = 0.0118 s/fund 50epochs MSE\n",
        "##         = 0.0039 s/fund 50epochs MSE m=4 d=6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRlZuRUNa6Yb",
        "outputId": "85850559-311b-4cf4-ea5b-465a9ee8a7af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8/8 [==============================] - 0s 702us/step\n"
          ]
        }
      ],
      "source": [
        "# Assuming you have a validation dataset (val_dataset)\n",
        "iterador = iter(val_dataset)\n",
        "sample = next(iterador)\n",
        "next_sample = next(iterador)\n",
        "input_data = sample[0]  # Assuming your dataset provides input data as the first element\n",
        "actual_values = sample[1]  # Assuming your dataset provides actual labels as the second element\n",
        "\n",
        "# Predict using the model\n",
        "predictions = model.predict(input_data)\n",
        "\n",
        "#mean_squared_error(predictions, actual_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Diferencias en error RMSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def dnn_error_coef(model, val_dataset):\n",
        "    # Assuming you have a validation dataset (val_dataset)\n",
        "    iterador = iter(val_dataset)\n",
        "    sample = next(iterador)\n",
        "    next_sample = next(iterador)\n",
        "    input_data = sample[0]  # Assuming your dataset provides input data as the first element\n",
        "    actual_values = sample[1]  # Assuming your dataset provides actual labels as the second element\n",
        "\n",
        "    # Predict using the model\n",
        "    predictions = model.predict(input_data)\n",
        "\n",
        "    #mean_squared_error(predictions, actual_values)\n",
        "\n",
        "    # Vemos algunos valores\n",
        "    for e in val_dataset:\n",
        "        for i in range(0, 4):\n",
        "            print(e[1][i])\n",
        "            print(predictions[i])\n",
        "        break\n",
        "        \n",
        "    # Veamos el MSE de los valores de G      \n",
        "    #RMSE_pred = mean_squared_error(actual_values, predictions, squared=False)\n",
        "    #RMSE_rand = mean_squared_error(actual_values, next_sample[1], squared=False)\n",
        "    #print(RMSE_pred, RMSE_rand)\n",
        "    #rint(RMSE_rand/RMSE_pred)\n",
        "    # Veamos los errores en términos de norma 2 (equiv a lo anterior)\n",
        "    if predictions.shape[1] == 1:\n",
        "        norm_pred = np.mean(np.abs(predictions.T-actual_values))\n",
        "        norm_rand = np.mean(np.abs(next_sample[1]-actual_values))\n",
        "    else:\n",
        "        norm_pred = np.mean(np.linalg.norm(predictions-actual_values,ord=2, axis=1))\n",
        "        norm_rand = np.mean(np.linalg.norm(next_sample[1]-actual_values,ord=2, axis=1))\n",
        "    print(norm_pred, norm_rand)\n",
        "    print(norm_rand / norm_pred)\n",
        "    return(norm_rand / norm_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Análisis rho2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reconstruye rho a partir de G\n",
        "# Codigo medio copiado de gen_dataset, not goot\n",
        "def rho_reconstruction(g_arr, h_type, state_type):\n",
        "    ## Caso G proporcional a ones\n",
        "    if h_type == 'const':\n",
        "        g_arr = [np.ones((basis.m, basis.m))*g_seed for g_seed in g_arr]\n",
        "        g_arr = tf.constant(g_arr, dtype=tf.float32)\n",
        "\n",
        "    ## Caso generico\n",
        "    elif h_type == 'random':\n",
        "        triag = tfp.math.fill_triangular(g_arr, upper=True)\n",
        "        g_arr = triag + tf.transpose(triag, perm=[0,2,1])-tf.linalg.diag(tf.linalg.diag_part(triag))\n",
        "\n",
        "    ## Caso reducido\n",
        "    else: \n",
        "        g_arr = gen_gauss_mat_np(g_arr[:,0], g_arr[:,1], basis.m)\n",
        "        g_arr = tf.constant(g_arr, dtype=tf.float32)   \n",
        "\n",
        "    # Construimos los hamiltonianos basados en g_arr\n",
        "    h_arr = two_body_hamiltonian_tf(t_basis, basis.m, en_batch, g_arr.numpy(), rho_1_arrays, rho_2_arrays, k_indices_tf)\n",
        "\n",
        "    # Calculamos los estados\n",
        "    if state_type == 'thermal':\n",
        "        state = thermal_state_tf(h_arr*beta) \n",
        "        state = tf.cast(state, dtype=tf.float32)\n",
        "    else:\n",
        "        state = pure_state(h_arr)\n",
        "\n",
        "    rho_input = rho_2_tf(state, rho_2_arrays_kkbar_tf)\n",
        "    \n",
        "    return rho_input\n",
        "\n",
        "# Vemos algunos valores\n",
        "def dnn_rho_reconstruction_error(model, val_dataset, h_type, state_type):\n",
        "    iterador = iter(val_dataset)\n",
        "    sample = next(iterador)\n",
        "    next_sample = next(iterador)\n",
        "    input_data = sample[0]  # Assuming your dataset provides input data as the first element\n",
        "    actual_values = sample[1].numpy()\n",
        "    predictions = model.predict(input_data)\n",
        "\n",
        "    # Calculamos los rho\n",
        "    rho_pred = rho_reconstruction(predictions, h_type, state_type)\n",
        "    rho_true = rho_reconstruction(actual_values, h_type, state_type)\n",
        "    rho_rand = rho_reconstruction(next_sample[1].numpy(), h_type, state_type)\n",
        "\n",
        "    \n",
        "    rho_2_s = lambda x: np.sort(np.linalg.eigvals(x))\n",
        "\n",
        "    # Analisis RMSE\n",
        "    #RMSE_pred = mean_squared_error(rho_2_true, rho_2_pred, squared=False)\n",
        "    #RMSE_rand = mean_squared_error(rho_2_true, rho_2_rand, squared=False)\n",
        "    #print(RMSE_pred, RMSE_rand)\n",
        "    #print(RMSE_rand/RMSE_pred)\n",
        "    # Printeamos algunos valores\n",
        "    for i in range(0, 2):\n",
        "        print(\"true: \" + str(rho_2_s(rho_true[i])))\n",
        "        print(\"pred: \" + str(rho_2_s(rho_pred[i])))\n",
        "\n",
        "    norm_pred = np.mean(np.linalg.norm(rho_true-rho_pred,ord=2, axis=(1,2)))\n",
        "    norm_rand = np.mean(np.linalg.norm(rho_true-rho_rand,ord=2, axis=(1,2)))\n",
        "    print(norm_pred, norm_rand)\n",
        "    print(norm_rand / norm_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-15 15:45:12.878456: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-15 15:45:12.878590: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/4 [00:00<?, ?it/s]2024-04-15 15:45:12.901018: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1284505600 exceeds 10% of free system memory.\n",
            "2024-04-15 15:45:13.201022: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 321126400 exceeds 10% of free system memory.\n",
            "2024-04-15 15:45:13.443155: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 321126400 exceeds 10% of free system memory.\n",
            " 25%|██▌       | 1/4 [00:00<00:01,  1.55it/s]2024-04-15 15:45:13.534591: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1284505600 exceeds 10% of free system memory.\n",
            "2024-04-15 15:45:13.839476: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 321126400 exceeds 10% of free system memory.\n",
            "100%|██████████| 4/4 [00:02<00:00,  1.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " rho (InputLayer)            [(None, 8, 8, 1)]            0         []                            \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)         (None, 64)                   0         ['rho[0][0]']                 \n",
            "                                                                                                  \n",
            " energy (InputLayer)         [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 65)                   0         ['flatten_1[0][0]',           \n",
            "                                                                     'energy[0][0]']              \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 10)                   660       ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 16)                   176       ['dense[0][0]']               \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 32)                   544       ['dense_1[0][0]']             \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 4)                    132       ['dense_2[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1512 (5.91 KB)\n",
            "Trainable params: 1512 (5.91 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "2024-04-15 15:45:15.419368: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-15 15:45:15.419489: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 44ms/step - loss: 3.3700 - accuracy: 0.0000e+00 - mean_squared_error: 3.3700 - val_loss: 3.0087 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.0087\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 3.0681 - accuracy: 0.0000e+00 - mean_squared_error: 3.0681 - val_loss: 2.7328 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.7328\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 2.7898 - accuracy: 0.0000e+00 - mean_squared_error: 2.7898 - val_loss: 2.4714 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.4714\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 2.5274 - accuracy: 0.0000e+00 - mean_squared_error: 2.5274 - val_loss: 2.2287 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.2287\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 2.2840 - accuracy: 0.0000e+00 - mean_squared_error: 2.2840 - val_loss: 2.0056 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.0056\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 2.0599 - accuracy: 0.0000e+00 - mean_squared_error: 2.0599 - val_loss: 1.8016 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.8016\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.8548 - accuracy: 0.0000e+00 - mean_squared_error: 1.8548 - val_loss: 1.6162 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.6162\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.6683 - accuracy: 0.0000e+00 - mean_squared_error: 1.6683 - val_loss: 1.4483 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.4483\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.4991 - accuracy: 0.0000e+00 - mean_squared_error: 1.4991 - val_loss: 1.2965 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.2965\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.3462 - accuracy: 0.0000e+00 - mean_squared_error: 1.3462 - val_loss: 1.1595 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.1595\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.2080 - accuracy: 0.0000e+00 - mean_squared_error: 1.2080 - val_loss: 1.0360 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.0360\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.0833 - accuracy: 0.0000e+00 - mean_squared_error: 1.0833 - val_loss: 0.9247 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.9247\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.9709 - accuracy: 0.0000e+00 - mean_squared_error: 0.9709 - val_loss: 0.8247 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.8247\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.8696 - accuracy: 0.0000e+00 - mean_squared_error: 0.8696 - val_loss: 0.7348 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.7348\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.7784 - accuracy: 0.0000e+00 - mean_squared_error: 0.7784 - val_loss: 0.6541 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.6541\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6963 - accuracy: 0.0000e+00 - mean_squared_error: 0.6963 - val_loss: 0.5817 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.5817\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6223 - accuracy: 0.0000e+00 - mean_squared_error: 0.6223 - val_loss: 0.5168 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.5168\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.5557 - accuracy: 0.6800 - mean_squared_error: 0.5557 - val_loss: 0.4590 - val_accuracy: 1.0000 - val_mean_squared_error: 0.4590\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.4962 - accuracy: 1.0000 - mean_squared_error: 0.4962 - val_loss: 0.4079 - val_accuracy: 1.0000 - val_mean_squared_error: 0.4079\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.4435 - accuracy: 1.0000 - mean_squared_error: 0.4435 - val_loss: 0.3634 - val_accuracy: 1.0000 - val_mean_squared_error: 0.3634\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.3974 - accuracy: 1.0000 - mean_squared_error: 0.3974 - val_loss: 0.3253 - val_accuracy: 1.0000 - val_mean_squared_error: 0.3253\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.3578 - accuracy: 1.0000 - mean_squared_error: 0.3578 - val_loss: 0.2934 - val_accuracy: 1.0000 - val_mean_squared_error: 0.2934\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.3245 - accuracy: 1.0000 - mean_squared_error: 0.3245 - val_loss: 0.2675 - val_accuracy: 1.0000 - val_mean_squared_error: 0.2675\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2973 - accuracy: 1.0000 - mean_squared_error: 0.2973 - val_loss: 0.2470 - val_accuracy: 1.0000 - val_mean_squared_error: 0.2470\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2756 - accuracy: 1.0000 - mean_squared_error: 0.2756 - val_loss: 0.2314 - val_accuracy: 1.0000 - val_mean_squared_error: 0.2314\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2587 - accuracy: 1.0000 - mean_squared_error: 0.2587 - val_loss: 0.2197 - val_accuracy: 1.0000 - val_mean_squared_error: 0.2197\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2457 - accuracy: 1.0000 - mean_squared_error: 0.2457 - val_loss: 0.2113 - val_accuracy: 1.0000 - val_mean_squared_error: 0.2113\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2359 - accuracy: 1.0000 - mean_squared_error: 0.2359 - val_loss: 0.2053 - val_accuracy: 1.0000 - val_mean_squared_error: 0.2053\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2288 - accuracy: 1.0000 - mean_squared_error: 0.2288 - val_loss: 0.2014 - val_accuracy: 1.0000 - val_mean_squared_error: 0.2014\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2237 - accuracy: 1.0000 - mean_squared_error: 0.2237 - val_loss: 0.1991 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1991\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.2203 - accuracy: 1.0000 - mean_squared_error: 0.2203 - val_loss: 0.1979 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1979\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2181 - accuracy: 1.0000 - mean_squared_error: 0.2181 - val_loss: 0.1974 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1974\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2168 - accuracy: 1.0000 - mean_squared_error: 0.2168 - val_loss: 0.1973 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1973\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2160 - accuracy: 1.0000 - mean_squared_error: 0.2160 - val_loss: 0.1975 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1975\n",
            "Epoch 35/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2156 - accuracy: 1.0000 - mean_squared_error: 0.2156 - val_loss: 0.1978 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1978\n",
            "Epoch 36/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2154 - accuracy: 1.0000 - mean_squared_error: 0.2154 - val_loss: 0.1981 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1981\n",
            "Epoch 37/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2154 - accuracy: 1.0000 - mean_squared_error: 0.2154 - val_loss: 0.1985 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1985\n",
            "Epoch 38/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2154 - accuracy: 1.0000 - mean_squared_error: 0.2154 - val_loss: 0.1988 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1988\n",
            "Epoch 39/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2154 - accuracy: 1.0000 - mean_squared_error: 0.2154 - val_loss: 0.1990 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1990\n",
            "Epoch 40/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2154 - accuracy: 1.0000 - mean_squared_error: 0.2154 - val_loss: 0.1992 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1992\n",
            "Epoch 41/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2154 - accuracy: 1.0000 - mean_squared_error: 0.2154 - val_loss: 0.1992 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1992\n",
            "Epoch 42/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2154 - accuracy: 1.0000 - mean_squared_error: 0.2154 - val_loss: 0.1992 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1992\n",
            "Epoch 43/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2153 - accuracy: 1.0000 - mean_squared_error: 0.2153 - val_loss: 0.1991 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1991\n",
            "Epoch 44/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2151 - accuracy: 1.0000 - mean_squared_error: 0.2151 - val_loss: 0.1989 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1989\n",
            "Epoch 45/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2148 - accuracy: 1.0000 - mean_squared_error: 0.2148 - val_loss: 0.1985 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1985\n",
            "Epoch 46/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2145 - accuracy: 1.0000 - mean_squared_error: 0.2145 - val_loss: 0.1980 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1980\n",
            "Epoch 47/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2140 - accuracy: 1.0000 - mean_squared_error: 0.2140 - val_loss: 0.1974 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1974\n",
            "Epoch 48/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2136 - accuracy: 1.0000 - mean_squared_error: 0.2136 - val_loss: 0.1969 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1969\n",
            "Epoch 49/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2132 - accuracy: 1.0000 - mean_squared_error: 0.2132 - val_loss: 0.1964 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1964\n",
            "Epoch 50/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2128 - accuracy: 1.0000 - mean_squared_error: 0.2128 - val_loss: 0.1960 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1960\n",
            "Epoch 51/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2124 - accuracy: 1.0000 - mean_squared_error: 0.2124 - val_loss: 0.1955 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1955\n",
            "Epoch 52/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2120 - accuracy: 1.0000 - mean_squared_error: 0.2120 - val_loss: 0.1952 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1952\n",
            "Epoch 53/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2117 - accuracy: 1.0000 - mean_squared_error: 0.2117 - val_loss: 0.1949 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1949\n",
            "Epoch 54/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2114 - accuracy: 1.0000 - mean_squared_error: 0.2114 - val_loss: 0.1947 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1947\n",
            "Epoch 55/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2112 - accuracy: 1.0000 - mean_squared_error: 0.2112 - val_loss: 0.1945 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1945\n",
            "Epoch 56/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2109 - accuracy: 1.0000 - mean_squared_error: 0.2109 - val_loss: 0.1943 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1943\n",
            "Epoch 57/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2106 - accuracy: 1.0000 - mean_squared_error: 0.2106 - val_loss: 0.1941 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1941\n",
            "Epoch 58/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2103 - accuracy: 1.0000 - mean_squared_error: 0.2103 - val_loss: 0.1938 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1938\n",
            "Epoch 59/100\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.2100 - accuracy: 1.0000 - mean_squared_error: 0.2100 - val_loss: 0.1935 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1935\n",
            "Epoch 60/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2097 - accuracy: 1.0000 - mean_squared_error: 0.2097 - val_loss: 0.1932 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1932\n",
            "Epoch 61/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2094 - accuracy: 1.0000 - mean_squared_error: 0.2094 - val_loss: 0.1928 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1928\n",
            "Epoch 62/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2091 - accuracy: 1.0000 - mean_squared_error: 0.2091 - val_loss: 0.1925 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1925\n",
            "Epoch 63/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2087 - accuracy: 1.0000 - mean_squared_error: 0.2087 - val_loss: 0.1922 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1922\n",
            "Epoch 64/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2084 - accuracy: 1.0000 - mean_squared_error: 0.2084 - val_loss: 0.1919 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1919\n",
            "Epoch 65/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2080 - accuracy: 1.0000 - mean_squared_error: 0.2080 - val_loss: 0.1915 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1915\n",
            "Epoch 66/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2077 - accuracy: 1.0000 - mean_squared_error: 0.2077 - val_loss: 0.1912 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1912\n",
            "Epoch 67/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2073 - accuracy: 1.0000 - mean_squared_error: 0.2073 - val_loss: 0.1908 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1908\n",
            "Epoch 68/100\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.2069 - accuracy: 1.0000 - mean_squared_error: 0.2069 - val_loss: 0.1905 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1905\n",
            "Epoch 69/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2066 - accuracy: 1.0000 - mean_squared_error: 0.2066 - val_loss: 0.1901 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1901\n",
            "Epoch 70/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2062 - accuracy: 1.0000 - mean_squared_error: 0.2062 - val_loss: 0.1897 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1897\n",
            "Epoch 71/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2058 - accuracy: 1.0000 - mean_squared_error: 0.2058 - val_loss: 0.1893 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1893\n",
            "Epoch 72/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2054 - accuracy: 1.0000 - mean_squared_error: 0.2054 - val_loss: 0.1889 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1889\n",
            "Epoch 73/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2050 - accuracy: 1.0000 - mean_squared_error: 0.2050 - val_loss: 0.1885 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1885\n",
            "Epoch 74/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2045 - accuracy: 1.0000 - mean_squared_error: 0.2045 - val_loss: 0.1881 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1881\n",
            "Epoch 75/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2041 - accuracy: 1.0000 - mean_squared_error: 0.2041 - val_loss: 0.1877 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1877\n",
            "Epoch 76/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2037 - accuracy: 1.0000 - mean_squared_error: 0.2037 - val_loss: 0.1872 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1872\n",
            "Epoch 77/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2032 - accuracy: 1.0000 - mean_squared_error: 0.2032 - val_loss: 0.1868 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1868\n",
            "Epoch 78/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2027 - accuracy: 1.0000 - mean_squared_error: 0.2027 - val_loss: 0.1863 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1863\n",
            "Epoch 79/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2023 - accuracy: 1.0000 - mean_squared_error: 0.2023 - val_loss: 0.1858 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1858\n",
            "Epoch 80/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2018 - accuracy: 1.0000 - mean_squared_error: 0.2018 - val_loss: 0.1854 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1854\n",
            "Epoch 81/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2013 - accuracy: 1.0000 - mean_squared_error: 0.2013 - val_loss: 0.1849 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1849\n",
            "Epoch 82/100\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.2008 - accuracy: 1.0000 - mean_squared_error: 0.2008 - val_loss: 0.1844 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1844\n",
            "Epoch 83/100\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.2003 - accuracy: 1.0000 - mean_squared_error: 0.2003 - val_loss: 0.1839 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1839\n",
            "Epoch 84/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.1998 - accuracy: 1.0000 - mean_squared_error: 0.1998 - val_loss: 0.1834 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1834\n",
            "Epoch 85/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.1992 - accuracy: 1.0000 - mean_squared_error: 0.1992 - val_loss: 0.1828 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1828\n",
            "Epoch 86/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.1987 - accuracy: 1.0000 - mean_squared_error: 0.1987 - val_loss: 0.1823 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1823\n",
            "Epoch 87/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.1981 - accuracy: 1.0000 - mean_squared_error: 0.1981 - val_loss: 0.1818 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1818\n",
            "Epoch 88/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.1976 - accuracy: 1.0000 - mean_squared_error: 0.1976 - val_loss: 0.1812 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1812\n",
            "Epoch 89/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.1970 - accuracy: 1.0000 - mean_squared_error: 0.1970 - val_loss: 0.1806 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1806\n",
            "Epoch 90/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.1964 - accuracy: 1.0000 - mean_squared_error: 0.1964 - val_loss: 0.1800 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1800\n",
            "Epoch 91/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.1958 - accuracy: 1.0000 - mean_squared_error: 0.1958 - val_loss: 0.1795 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1795\n",
            "Epoch 92/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.1952 - accuracy: 1.0000 - mean_squared_error: 0.1952 - val_loss: 0.1789 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1789\n",
            "Epoch 93/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.1946 - accuracy: 1.0000 - mean_squared_error: 0.1946 - val_loss: 0.1782 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1782\n",
            "Epoch 94/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.1939 - accuracy: 1.0000 - mean_squared_error: 0.1939 - val_loss: 0.1776 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1776\n",
            "Epoch 95/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.1933 - accuracy: 1.0000 - mean_squared_error: 0.1933 - val_loss: 0.1770 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1770\n",
            "Epoch 96/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.1926 - accuracy: 1.0000 - mean_squared_error: 0.1926 - val_loss: 0.1763 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1763\n",
            "Epoch 97/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.1920 - accuracy: 1.0000 - mean_squared_error: 0.1920 - val_loss: 0.1757 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1757\n",
            "Epoch 98/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.1913 - accuracy: 1.0000 - mean_squared_error: 0.1913 - val_loss: 0.1750 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1750\n",
            "Epoch 99/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.1906 - accuracy: 1.0000 - mean_squared_error: 0.1906 - val_loss: 0.1743 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1743\n",
            "Epoch 100/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.1899 - accuracy: 1.0000 - mean_squared_error: 0.1899 - val_loss: 0.1736 - val_accuracy: 1.0000 - val_mean_squared_error: 0.1736\n"
          ]
        },
        {
          "ename": "StopIteration",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:810\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 810\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_internal()\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:773\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[0;32m--> 773\u001b[0m   ret \u001b[38;5;241m=\u001b[39m gen_dataset_ops\u001b[38;5;241m.\u001b[39miterator_get_next(\n\u001b[1;32m    774\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource,\n\u001b[1;32m    775\u001b[0m       output_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types,\n\u001b[1;32m    776\u001b[0m       output_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_shapes)\n\u001b[1;32m    778\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3029\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3028\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 3029\u001b[0m   _ops\u001b[38;5;241m.\u001b[39mraise_from_not_ok_status(e, name)\n\u001b[1;32m   3030\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5882\u001b[0m e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5883\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[0;31mOutOfRangeError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} End of sequence [Op:IteratorGetNext] name: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# DNN\u001b[39;00m\n\u001b[1;32m     10\u001b[0m model, val_dataset, history \u001b[38;5;241m=\u001b[39m dnn_fit(dataset, label_size, input_type, include_energy)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(dnn_error_coef(model, val_dataset))\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(dnn_rho_reconstruction_error(model, val_dataset, h_type, state_type))\n",
            "Cell \u001b[0;32mIn[13], line 7\u001b[0m, in \u001b[0;36mdnn_error_coef\u001b[0;34m(model, val_dataset)\u001b[0m\n\u001b[1;32m      5\u001b[0m iterador \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(val_dataset)\n\u001b[1;32m      6\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterador)\n\u001b[0;32m----> 7\u001b[0m next_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterador)\n\u001b[1;32m      8\u001b[0m input_data \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Assuming your dataset provides input data as the first element\u001b[39;00m\n\u001b[1;32m      9\u001b[0m actual_values \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Assuming your dataset provides actual labels as the second element\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:812\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    810\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_internal()\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[0;32m--> 812\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
            "\u001b[0;31mStopIteration\u001b[0m: "
          ]
        }
      ],
      "source": [
        "h_type = 'vect'\n",
        "g_init = 0.01\n",
        "g_stop = 2.5\n",
        "state_type = 'gs' \n",
        "input_type = 'rho2'\n",
        "include_energy = True\n",
        "\n",
        "dataset, label_size = gen_dataset(h_type, g_init, g_stop, state_type, input_type, include_energy)\n",
        "# DNN\n",
        "model, val_dataset, history = dnn_fit(dataset, label_size, input_type, include_energy)\n",
        "print(dnn_error_coef(model, val_dataset))\n",
        "print(dnn_rho_reconstruction_error(model, val_dataset, h_type, state_type))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparación con BCS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Caso G = G"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7/7 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (8,8) (224,1,1,4) ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[25], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m predictions_sort \u001b[38;5;241m=\u001b[39m predictions[g_ids]\n\u001b[1;32m     42\u001b[0m g_true_sort \u001b[38;5;241m=\u001b[39m actual_values\u001b[38;5;241m.\u001b[39mnumpy()[g_ids]\n\u001b[0;32m---> 44\u001b[0m rho_pred \u001b[38;5;241m=\u001b[39m rho_reconstruction(predictions_sort, h_type, state_type)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Calculamos ahora G BCS\u001b[39;00m\n\u001b[1;32m     47\u001b[0m rho_actual \u001b[38;5;241m=\u001b[39m rho_reconstruction(actual_values\u001b[38;5;241m.\u001b[39mnumpy()[g_ids], h_type, state_type)\n",
            "Cell \u001b[0;32mIn[14], line 18\u001b[0m, in \u001b[0;36mrho_reconstruction\u001b[0;34m(g_arr, h_type, state_type)\u001b[0m\n\u001b[1;32m     14\u001b[0m     g_arr \u001b[38;5;241m=\u001b[39m triag \u001b[38;5;241m+\u001b[39m tf\u001b[38;5;241m.\u001b[39mtranspose(triag, perm\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m-\u001b[39mtf\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mdiag(tf\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mdiag_part(triag))\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m## Caso reducido\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[0;32m---> 18\u001b[0m     g_arr \u001b[38;5;241m=\u001b[39m gen_gauss_mat_np(g_arr[:,\u001b[38;5;241m0\u001b[39m], g_arr[:,\u001b[38;5;241m1\u001b[39m], basis\u001b[38;5;241m.\u001b[39mm)\n\u001b[1;32m     19\u001b[0m     g_arr \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(g_arr, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)   \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Construimos los hamiltonianos basados en g_arr\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[8], line 98\u001b[0m, in \u001b[0;36mgen_gauss_mat_np\u001b[0;34m(G_values, sigma_sq_values, size)\u001b[0m\n\u001b[1;32m     95\u001b[0m indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(size, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     96\u001b[0m indices_diff \u001b[38;5;241m=\u001b[39m indices \u001b[38;5;241m-\u001b[39m indices[:, np\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[0;32m---> 98\u001b[0m mat \u001b[38;5;241m=\u001b[39m G_values[:, np\u001b[38;5;241m.\u001b[39mnewaxis, np\u001b[38;5;241m.\u001b[39mnewaxis] \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39msquare(indices_diff) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m sigma_sq_values[:, np\u001b[38;5;241m.\u001b[39mnewaxis, np\u001b[38;5;241m.\u001b[39mnewaxis]))\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mat\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (8,8) (224,1,1,4) "
          ]
        }
      ],
      "source": [
        "e_mean = 1/basis.m * np.sum(en_batch[0])\n",
        "\n",
        "# Calcula rho2 ha partir del delta dado\n",
        "def bcs_delta(delta):\n",
        "    lambda_k = lambda k: np.sqrt((en_batch[0][k] - e_mean)**2 + delta**2)\n",
        "    f_k = lambda k: 1/2 * (1 - (en_batch[0][k] - e_mean)/lambda_k(k))\n",
        "    r_k = lambda k: delta/(2*lambda_k(k))\n",
        "\n",
        "    rho = np.zeros((basis.m, basis.m))\n",
        "    for k in range(0, basis.m):\n",
        "        for kp in range(0, basis.m):\n",
        "            p = f_k(k)**2 if k == kp else 0\n",
        "            rho[k, kp] = r_k(k) * r_k(kp) + p\n",
        "\n",
        "    return rho \n",
        "        \n",
        "#rho_dist = lambda x: np.linalg.norm(bcs_delta(x)-rho_init)\n",
        "#dom = np.linspace(0,10,100)\n",
        "#plt.plot(dom, [rho_dist(x) for x in dom])\n",
        "\n",
        "# Calculamos g_BCS a partir de la rho2 calculada por BCS más cercana a rho dada\n",
        "def g_bcs(rho_init):\n",
        "    rho_dist = lambda x: np.linalg.norm(bcs_delta(x)-rho_init)\n",
        "    opti = scipy.optimize.minimize(rho_dist, 1, method='Nelder-Mead')\n",
        "    delta = opti.x\n",
        "    lambda_k = lambda k: np.sqrt((en_batch[0][k] - e_mean)**2 + delta**2)\n",
        "    G = 1/(np.sum([ 1/(2*lambda_k(x)) for x in range(0, basis.m)])) \n",
        "\n",
        "    return G\n",
        "\n",
        "# Cargamos elementos del conjunto de validación\n",
        "iterador = iter(val_dataset)\n",
        "sample = next(iterador)\n",
        "input_data = sample[0]  \n",
        "actual_values = sample[1]\n",
        "predictions = model.predict(input_data)\n",
        "\n",
        "#if h_type == 'const':\n",
        "# Ordenamos los valores de G con el fin de plotear\n",
        "g_ids = actual_values.numpy().argsort()\n",
        "predictions_sort = predictions[g_ids]\n",
        "g_true_sort = actual_values.numpy()[g_ids]\n",
        "\n",
        "rho_pred = rho_reconstruction(predictions_sort, h_type, state_type)\n",
        "\n",
        "# Calculamos ahora G BCS\n",
        "rho_actual = rho_reconstruction(actual_values.numpy()[g_ids], h_type, state_type)\n",
        "g_bcs_sort = [g_bcs(x) for x in rho_actual.numpy()]\n",
        "rho_bcs = rho_reconstruction(g_bcs_sort, h_type, state_type)\n",
        "\n",
        "rho_error = lambda x: np.linalg.norm(rho_actual.numpy()-x, ord=2, axis=(1,2))\n",
        "\n",
        "plt.plot(g_true_sort, rho_error(rho_pred), label='DNN predictions')\n",
        "plt.plot(g_true_sort, rho_error(rho_bcs), label='BCS')\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel(\"g\")\n",
        "plt.ylabel(\"Rho2 reconstruction error\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABcAUlEQVR4nO3dd3gU1f7H8fduekISCJAEQkLvLRBaQAUUQZpi7/CzF1AxWAC9VDUWmgXFjg27otKbdBAIBKmhdxJ6et2d3x/jjRelJSSZbPJ5PU+e+5zJmZ3vzg27H2fOnGMzDMNARERExCJ2qwsQERGR8k1hRERERCylMCIiIiKWUhgRERERSymMiIiIiKUURkRERMRSCiMiIiJiKYURERERsZS71QVcCqfTyZEjR/D398dms1ldjoiIiFwCwzBITU2levXq2O3nv/7hEmHkyJEjhIeHW12GiIiIFMLBgwepUaPGeX/vEmHE398fMN9MQECAxdWIiIjIpUhJSSE8PDz/e/x8XCKM/PfWTEBAgMKIiIiIi7nYEAsNYBURERFLKYyIiIiIpRRGRERExFIuMWbkUjgcDnJzc60uo0zz8PDAzc3N6jJERKSMKRNhJC0tjUOHDmEYhtWllGk2m40aNWpQoUIFq0sREZEyxOXDiMPh4NChQ/j6+lK1alVNilZMDMPg+PHjHDp0iPr16+sKiYiIFBmXDyO5ubkYhkHVqlXx8fGxupwyrWrVquzbt4/c3FyFERERKTJlZgCrrogUP51jEREpDmUmjIiIiIhrKlAYee+992jRokX+TKjR0dHMnj37gvt8//33NGrUCG9vb5o3b86sWbMuq+DybNSoUURGRlpdhoiISJEqUBipUaMGr776KnFxcaxbt46rr76aG264gS1btpyz/8qVK7nzzjt54IEH2LBhA/369aNfv35s3ry5SIoXERER11egMNK3b1969epF/fr1adCgAS+//DIVKlRg9erV5+z/5ptvct111/Hss8/SuHFjxo4dS+vWrXnnnXeKpHgRERFxfYUeM+JwOPjmm29IT08nOjr6nH1WrVpFt27dztrWo0cPVq1adcHXzs7OJiUl5ayfsiY1NZW7774bPz8/qlWrxsSJE+nSpQuDBw++6L7vv/8+4eHh+Pr6ctttt5GcnHzW7z/55BOaNm2Kl5cX1apVY9CgQYD5eO6oUaOIiIjAy8uL6tWr8+STTxbH2xMRERcxe9NRBn61HofTurm6Cvxo76ZNm4iOjiYrK4sKFSrw888/06RJk3P2TUxMJCQk5KxtISEhJCYmXvAYsbGxjB49uqClAeYXbmauo1D7Xi4fD7dLfuIkJiaGFStW8OuvvxISEsKIESNYv379RceE7Nq1i++++47ffvuNlJQUHnjgAR5//HG++uorwBzXExMTw6uvvkrPnj1JTk5mxYoVAPz4449MnDiRb775hqZNm5KYmMjGjRsv6z2LiIhrMgyDdxfv5o25CQB0qleFu9pHWFJLgcNIw4YNiY+PJzk5mR9++IEBAwawZMmS8waSwhg2bBgxMTH57ZSUFMLDwy9p38xcB01GzC2yWgpi65ge+Hpe/JSmpqby2WefMW3aNK655hoAPv30U6pXr37RfbOysvj8888JCwsD4O2336Z3796MHz+e0NBQXnrpJYYMGcJTTz2Vv0/btm0BOHDgAKGhoXTr1g0PDw8iIiJo165dYd6qiIi4sOw8B8N+3MRPGw4DcF+nWtzWpoZl9RT4No2npyf16tUjKiqK2NhYWrZsyZtvvnnOvqGhoSQlJZ21LSkpidDQ0Asew8vLK/+Jnf/+lCV79uwhNzf3rCAQGBhIw4YNL7pvREREfhABiI6Oxul0kpCQwLFjxzhy5Eh+wPmnW2+9lczMTOrUqcNDDz3Ezz//TF5e3uW/IRERcRkn07K5+8M/+GnDYdzsNsb2a8bIvk1xd7Nuto/LnoHV6XSSnZ19zt9FR0ezcOHCs8ZBzJ8//7xjTIqCj4cbW8f0KLbXv9ixrXSxGWjDw8NJSEhgwYIFzJ8/n8cff5w33niDJUuW4OHhUUJVioiIVXYmpXL/Z2s5eCoTf2933r27NVfWr2p1WQULI8OGDaNnz55ERESQmprKtGnTWLx4MXPnmrdF+vfvT1hYGLGxsQA89dRTdO7cmfHjx9O7d2+++eYb1q1bxwcffFD07+QvNpvtkm6VWKlOnTp4eHiwdu1aIiLM+3PJycns2LGDq6666oL7HjhwgCNHjuTf0lm9ejV2u52GDRvi7+9PrVq1WLhwIV27dj3n/j4+PvTt25e+ffsycOBAGjVqxKZNm2jdunXRvkkRESlVluw4zqCv1pOanUfNyr58PKAt9YJLx8KnBfrWPnbsGP379+fo0aMEBgbSokUL5s6dy7XXXguYX5R2+9+XeTp27Mi0adN48cUXGT58OPXr12f69Ok0a9asaN+Fi/H392fAgAE8++yzBAUFERwczMiRI7Hb7RcdAOvt7c2AAQMYN24cKSkpPPnkk9x22235t75GjRrFo48+SnBwMD179iQ1NZUVK1bwxBNPMHXqVBwOB+3bt8fX15cvv/wSHx8fatasWRJvW0RELPLZyn2M/m0LTgPa1Q7i/XuiqOTnaXVZ+QoURj7++OML/n7x4sX/2nbrrbdy6623Fqio8mDChAk8+uij9OnTh4CAAJ577jkOHjyIt7f3BferV68eN910E7169eLUqVP06dOHd999N//3AwYMICsri4kTJ/LMM89QpUoVbrnlFgAqVqzIq6++SkxMDA6Hg+bNm/Pbb79RuXLlYn2vIiJijTyHkzEztvL5qv0A3BpVg5dvbI6ne+laDcZmGIZ1DxZfopSUFAIDA0lOTv7XYNasrCz27t1L7dq1L/pFXpqlp6cTFhbG+PHjeeCBB6wu55zKyrkWESkPUrJyGfjVepbtPIHNBs9f14hHrqpTooueXuj7+3+V7sEVZdiGDRvYvn077dq1Izk5mTFjxgBwww03WFyZiIi4ugMnM7j/s7XsOpaGj4cbk+6IpEfTCz/JaiWFEQuNGzeOhIQEPD09iYqKYtmyZVSpUsXqskRExIWt2XuKR75Yx+mMXEIDvPloQBuahQVaXdYFKYxYpFWrVsTFxVldhoiIlCE/xB1i2E9/kuswaFEjkA/7tyEkoPTfVlcYERERcXFOp8Eb8xJ4b/FuAHo1D2X8rZH4eFo7/9WlUhgRERFxYRk5ecR8u5E5W8x13564uh5Pd2uA3V5yA1Uvl8KIiIiIi0pMzuLBz9ey+XAKnm52Xr25OTe1tm6NmcJSGBEREXFBmw4l8+Dna0lKySbIz5MP7o2iTa0gq8sqFIURERERFzNn81EGfxtPVq6T+sEV+OT/2hIe5Gt1WYWmMCIiIuIiDMPgvSW7eX1OAgCdG1Tl7btaEeDt2oudKoyIiIi4gOw8B8N+2sRP6w8D8H8da/Fi78a4u5Wuqd0Lw/XfQTkyatQoIiMjrS5DRERK2Kn0HO756A9+Wn8YN7uNsTc0ZdT1TctEEAFdGSmTcnNz8fBw7Ut2IiJi2pmUyv2freXgqUz8vd159+7WXFm/qtVlFamyEalcUGpqKnfffTd+fn5Uq1aNiRMn0qVLFwYPHnzO/lOnTmX06NFs3LgRm82GzWZj6tSpANhsNt577z2uv/56/Pz8ePnll5k6dSoVK1Y86zWmT5/+rwWSfvnlF1q3bo23tzd16tRh9OjR5OXlFcM7FhGRglqy4zg3vbuSg6cyiQjy5efHO5a5IAJl8cqIYUBuhjXH9vCFS1wNMSYmhhUrVvDrr78SEhLCiBEjWL9+/Xlvw9x+++1s3ryZOXPmsGDBAgACA/9ea2DUqFG8+uqrTJo0CXd3dxYtWnTRGpYtW0b//v156623uPLKK9m9ezcPP/wwACNHjryk9yEiIsXj81X7GP3bVhxOg3a1gphybxRBfp5Wl1Usyl4Yyc2AV6pbc+zhR8DT76LdUlNT+eyzz5g2bRrXXHMNAJ9++inVq5+/bh8fHypUqIC7uzuhof9eefGuu+7ivvvuK1C5o0ePZujQoQwYMACAOnXqMHbsWJ577jmFERERi+Q5nIydsZXPVu0H4JaoGrx8YzO83F1javfCKHthxAXs2bOH3Nxc2rVrl78tMDCQhg0bFvo127RpU+B9Nm7cyIoVK3j55ZfztzkcDrKyssjIyMDX13WfWRcRcUUpWbkMmraBpTuOA/D8dY14tHOdf91iL2vKXhjx8DWvUFh1bIv4+Z19RcZut2MYxlnbcnNzz2qnpaUxevRobrrppn+9nrd36V/lUUSkLDlwMoMHPlvLzmNp+Hi4MfH2SK5r9u8r4WVR2QsjNtsl3SqxUp06dfDw8GDt2rVEREQAkJyczI4dO7jqqqvOu5+npycOh+OSjlG1alVSU1NJT0/PDyrx8fFn9WndujUJCQnUq1evcG9ERESKxNp9p3jkizhOpecQEuDFxwPa0iws8OI7lhFlL4y4AH9/fwYMGMCzzz5LUFAQwcHBjBw5ErvdfsFLcbVq1WLv3r3Ex8dTo0YN/P398fLyOmff9u3b4+vry/Dhw3nyySf5448/8p+++a8RI0bQp08fIiIiuOWWW7Db7WzcuJHNmzfz0ksvFeVbFhGR8/gx7hDDftpEjsNJ87BAPuzfhtDA8nV1Wo/2WmTChAlER0fTp08funXrRqdOnWjcuPEFb4/cfPPNXHfddXTt2pWqVavy9ddfn7dvUFAQX375JbNmzaJ58+Z8/fXXjBo16qw+PXr0YMaMGcybN4+2bdvSoUMHJk6cSM2aNYvqbYqIyHk4nQZvzN3OkO83kuNw0rNZKN89El3uggiAzfjnwIJSKCUlhcDAQJKTkwkICDjrd1lZWezdu5fatWu79DiH9PR0wsLCGD9+PA888IDV5ZxTWTnXIiJWy8xxEPNdPLM3JwIwsGtdhlzbELu9bA1UvdD39//SbRqLbNiwge3bt9OuXTuSk5MZM2YMADfccIPFlYmISHFKSsniwc/WselwMp5udl69uTk3ta5hdVmWUhix0Lhx40hISMDT05OoqCiWLVtGlSpVrC5LRESKyebDyTz42ToSU7II8vPk/XujaFsryOqyLKcwYpFWrVoRFxdndRkiIlJC5mxO5Olv48nMdVA/uAIfD2hLRGXN5wQKIyIiIsXKMAymLNnDa3O2A3BVg6q8c1crAry1oOl/KYyIiIgUk+w8B8N/2syP6w8BMCC6Jv/p0wR3Nz3M+r/KTBhxgYeCXJ7OsYjIpTuVnsOjX8SxZt8p3Ow2RvZtQv/oWlaXVSq5fBhxczMXDsrJycHHx8fiasq2nJwc4O9zLiIi57brWCr3T13HgVMZ+Hu5887drencoKrVZZVaLh9G3N3d8fX15fjx43h4eGC369JXcXA6nRw/fhxfX1/c3V3+z0ZEpNgs23mcx79aT2pWHuFBPnwyoC31Q/ytLqtUc/lvFZvNRrVq1di7dy/79++3upwyzW63ExERUeZXjxQRKazPV+1j9G9bcTgN2taqxJR7oqhc4dzLdsjfXD6MgLmAXP369fNvI0jx8PT01JUnEZFzyHM4GTNjK5+vMv+j+KbWYcTe1Bwvd93WvhRlIoyA+V/tmqJcRERKWnJmLoOmrWfZzhPYbPBcj0Y82rmOriIXQJkJIyIiIiVt34l0HvhsLbuPp+Pj4cakOyLp0TTU6rJcjsKIiIhIIazafZLHvorjTEYu1QK9+WhAG5pWD7S6LJekMCIiIlJA3649wAs/bybPadAyvCIf3htFcICGChSWwoiIiMglcjgNXp29jQ+X7QWgb8vqvHFLC7w9NFD1ciiMiIiIXIK07Dye+noDC7cfA2Bwt/o8dU19DVQtAgojIiIiF3HwVAYPfraOhKRUvNztjLu1JX1bVre6rDJDYUREROQC4vaf4uHP4ziZnkNVfy8+7N+GyPCKVpdVpiiMiIiInMdP6w8x9MdN5DicNK0ewEcD2lAtUOugFTWFERERkX9wOg3GzUvg3cW7AejRNISJt0fi66mvzeKgsyoiIvI/MnLyiPl2I3O2JAIwsGtdhlzbELtdA1WLi8KIiIjIX44mZ/LgZ+vYciQFTzc7r97cnJta17C6rDJPYURERATYePAMD32+jmOp2VT28+T9e6NoUyvI6rLKBYUREREp92b8eYQh320kO89JwxB/PhrQhvAgX6vLKjcKtB58bGwsbdu2xd/fn+DgYPr160dCQsIF95k6dSo2m+2sH62uKyIipYFhGLy5YCeDpm0gO8/J1Y2C+eGxaAWRElagKyNLlixh4MCBtG3blry8PIYPH0737t3ZunUrfn5+590vICDgrNCi2epERMRqWbkOnv3hT37beASAB6+ozbBejXHTQNUSV6AwMmfOnLPaU6dOJTg4mLi4OK666qrz7mez2QgN1ZLKIiJSOhxLzeLhz+OIP3gGd7uNl/o14452EVaXVW4V6DbNPyUnJwMQFHThAT5paWnUrFmT8PBwbrjhBrZs2XLB/tnZ2aSkpJz1IyIiUhS2HEmm3zsriD94hoq+HnzxQPvyHUQMA47vsLSEQocRp9PJ4MGD6dSpE82aNTtvv4YNG/LJJ5/wyy+/8OWXX+J0OunYsSOHDh067z6xsbEEBgbm/4SHhxe2TBERkXzztiRy65RVHEnOok5VP6Y/3onoupWtLss6xxNgah/4sCskH7asDJthGEZhdnzssceYPXs2y5cvp0aNS38GOzc3l8aNG3PnnXcyduzYc/bJzs4mOzs7v52SkkJ4eDjJyckEBAQUplwRESnHDMNgypI9vD53O4YBV9avwjt3tSbQx8Pq0qyRmwlLx8GKN8GZC+4+cMvH0Kh3kR4mJSWFwMDAi35/F+rR3kGDBjFjxgyWLl1aoCAC4OHhQatWrdi1a9d5+3h5eeHl5VWY0kRERM6Snedg+E+b+XG9eUX+3g41Gdm3Ce5ulzVSwXXtXACzhsDpfWa7wXXQ83WoVNOykgoURgzD4IknnuDnn39m8eLF1K5du8AHdDgcbNq0iV69ehV4XxERkYI4mZbNo1/GsXbfadzsNkb2bUL/6FpWl2WNlKMwZyhsnW62A8Kg52vQqA9Y/JRrgcLIwIEDmTZtGr/88gv+/v4kJprz9gcGBuLjY65i2L9/f8LCwoiNjQVgzJgxdOjQgXr16nHmzBneeOMN9u/fz4MPPljEb0VERORvO5JSuX/qWg6dzsTf253Jd7XmqgZVrS6r5DkdsOZDWPQS5KSCzQ06PAZdhoKXv9XVAQUMI++99x4AXbp0OWv7p59+yv/93/8BcODAAez2vy99nT59moceeojExEQqVapEVFQUK1eupEmTJpdXuYiIyHn8nnCMJ6ZtIC07j5qVffl4QBvqBZeOL94SdTgOZjwNRzea7bA20GciVGthbV3/UOgBrCXpUgfAiIhI+WYYBp+u2MdLM7fiNKB97SCm3BNFJT9Pq0srWVnJsHAsrP0IMMA7ELqNgtb/B/aSGytTrANYRURESptch5MRv2zh6zUHALi9TThj+zXD070cDVQ1DNj8I8wdDmlJ5rYWt0P3l6BCsLW1XYDCiIiIuLwzGTk8/tV6Vu4+ic0GL/RqzANX1C5fy4+c3A2znoHdi8x25XrQewLU6WxtXZdAYURERFzanuNpPPDZOvaeSMfP04237mzFNY1DrC6r5ORlm/OFLB0Hjmxw84KrnoFOT4G7a0yToTAiIiIua8WuEzz2ZRwpWXmEVfTh4/9rQ6PQcjS2cM8SmBkDJ/+au6tOV+g9HirXtbauAlIYERERl/Tl6v2M/HULDqdB64iKfNC/DVUquMaVgMuWdgzmvQh/fmu2K4RAj1eg2c2WzxlSGAojIiLiUvIcTl6auY2pK/cBcGOrMGJvao63h5u1hZUEpxPWT4UFo8wnZrBB2wfh6hfBp6K1tV0GhREREXEZKVm5DJq2gaU7jgPwbI+GPN6lbvkYqJq4yZwz5NBasx3aAvpOgrAoS8sqCgojIiLiEvafTOeBz9ax61ga3h52Jt4WSc/m1awuq/hlp8HiWFj9HhgO8PQ3r4S0fRDcysbXeNl4FyIiUqat2n2Sx76K40xGLqEB3nw0oA3NwgKtLqt4GQZsnwGzn4eUw+a2Jv3gulgIqG5paUVNYUREREq1aX8cYMQvm8lzGrSsEcgH/dsQEuBtdVnF68wBmPUs7JhjtivWNJ+SqX+ttXUVE4UREREplf45ULVvy+q8cUuLsj1Q1ZELqybDktcgNwPsHuZ8IVcOAU9fq6srNgojIiJS6iRn5jJo2nqW7TwBwDPdGzCwa72yPVB1/ypzgOrxbWa75hXQZwJUbWhtXSVAYUREREqVvSfSeeCztew5no6PhxsTb2/Jdc3K8EDVjFMwfwRs+MJs+1Y215JpeadLzhlSGAojIiJSaqzYdYLHv1pPcmYu1QO9+XBAG5pWL6MDVQ0D4qeZk5dlnjK3te4P3UaDb5C1tZUwhRERESkVvli1j1G/bcXhNGgVUZH3740i2L+MDlQ9tt2cxn3/CrMd3AT6TISIDtbWZRGFERERsVSuw8mY37byxer9QBmfUTUnA5a+ASvfAmceePhCl6HQ4XFw87C6OssojIiIiGXOZOQwcNp6Vuw6ic1mzqj6WOcyOqPqjnkwa4j52C5Ag57Q63WoGGFtXaWAwoiIiFhi17E0HvxsLftOZuDr6cak2yPp3jTU6rKKXsoRmDMUtv5itgNqmCGkUW9r6ypFFEZERKTELd1xnIHT1pOalUdYRR8+GtCGxtUCrC6raDnyYO2HsOglyEkDmxtEPw6dh4JXBaurK1UURkREpMQYhsHUlfsYO2MrTgPa1KzElHujqFLBy+rSitahOJgxGBL/NNs12pkDVEObWVpWaaUwIiIiJSLX4WTEL1v4eo05ZuKWqBq8fGMzvNzL0EDVzDOwaCys/RgwwLsiXDsaWvUHu93i4kovhRERESl2p9NzeOyrOFbvOYXNBsN7NubBK2uXnYGqhgGbf4Q5wyD9mLmtxR3m5GUVqlpbmwtQGBERkWK1MymVBz5bx4FTGVTwcuetOyO5ulGI1WUVnZO7zTlD9iw225Xrm9O4177K0rJcicKIiIgUm98TjvHktA2kZucRHuTDxwPa0iDE3+qyikZeNiyfCMsmgCMb3Lzgqmeh05PgXsbGwBQzhRERESlyhmHw8fK9vDJrG04D2tUOYso9UQT5eVpdWtHYsxhmDoGTu8x23auh1zioXNfSslyVwoiIiBSpnDwnL07fxHfrDgFwe5twxvZrhqd7GRjAmZoE816ATd+b7QqhcF0sNL2x3CxqVxwURkREpMicTMvmsS/Xs2bfKew2eKF3E+7vVMv1B6o6nRD3CSwYA9nJgA3aPQxXvwDeZXQhvxKkMCIiIkUiITGVBz5by6HTmfh7ufP2Xa3o0jDY6rIu39E/YcbTcHid2a4Wac4ZEtba0rLKEoURERG5bAu3JfHk1xtIz3FQs7IvHw9oQ71gFx+omp0Kv8fCH++B4QRPf7hmBLR9AOxlaG6UUkBhRERECs0wDD5YuodX52zHMCC6TmXevbs1lVx5oKphwLbfYPbzkHrE3Nb0RugRCwHVrK2tjFIYERGRQsnOczD8p838uN4cqHp3+whGXd8UDzcXHqh6eh/Meg52zjXblWpB7/FQr5uVVZV5CiMiIlJgx1OzeeSLdaw/cAY3u40RfZrQP7qm6w5UzcuBVe/AktchLxPsHnDFYLhyCHj4WF1dmacwIiIiBbL1SAoPfb6Ow2cy8fd25927W3NlfRee8nz/SnOA6vHtZrvWldB7AlRtYG1d5YjCiIiIXLK5WxJ5+tt4MnIc1K7ix0cD2lC3agWryyqc9JMwfwTEf2m2fatAj5ehxe2aM6SEKYyIiMhFGYbBu4t388bcBACuqFeFyXe1JtDXw+LKCsHphI3TYN5/IPOUuS3q/+CakeAbZGlp5ZXCiIiIXFBmjoPnfvyT3zaaT5b0j67Jf/o0cc2Bqse2wYwYOLDSbAc3NecMiWhvbV3lnMKIiIic19HkTB7+PI5Nh5Nxt9sYfUNT7m5f0+qyCi4nA5a+DivfBmceePhC1+HQ/lFwc8GrO2WMwoiIiJxT3P7TPPJFHCfSsqnk68F790TRoU5lq8squB1zYdYzcOaA2W7UB657FSqGW1uX5FMYERGRf/kh7hDDf9pEjsNJo1B/PuzfhvAgX6vLKpjkwzDneXMCM4CAGtDrDWjUy9q65F8URkREJF+ew8mrs7fz0fK9APRoGsKE2yLx83KhrwtHHqx5H35/BXLSwOYG0QOh8/Pg5aJP/pRxLvTXJSIixSk5M5cnvt7A0h3HAXjymvoMvqY+drsLPeZ6aB38NhiSNpnt8PbmANWQppaWJRemMCIiIuw+nsZDn61jz4l0vD3sjL81kt4tXGgdlszTsHAMrPsUMMC7Ilw7BlrdC3YXfOqnnFEYEREp55bsOM6gaetJzcqjeqA3H/RvQ7OwQKvLujSGAZu+h7nDId28okPLu6D7WPCrYm1tcskURkREyinDMPh4+V5embUNpwFRNSsx5Z4oqvp7WV3apTmxC2bGwN4lZrtKA3Ma99pXWluXFJjCiIhIOZSV6+CFn/9ecfe2NjUY268ZXu5uFld2CXKzYPlEWD4BHDng7g1XPQsdnwR3T6urk0Io0I202NhY2rZti7+/P8HBwfTr14+EhISL7vf999/TqFEjvL29ad68ObNmzSp0wSIicnmOpWRx54er+XH9Iew2GNm3Ca/d3MI1gsjuRfBeNCx51Qwi9brB46vhqmcURFxYgcLIkiVLGDhwIKtXr2b+/Pnk5ubSvXt30tPTz7vPypUrufPOO3nggQfYsGED/fr1o1+/fmzevPmyixcRkYLZdCiZ699ZwYYDZwjwduez+9txX6fa2Er7wnCpSfDDA/DFjXBqD/hXg1s/g7t/gKDaVlcnl8lmGIZR2J2PHz9OcHAwS5Ys4aqrrjpnn9tvv5309HRmzJiRv61Dhw5ERkYyZcqUSzpOSkoKgYGBJCcnExAQUNhyRUTKtV83HuHZ7zeSneekblU/PhrQltpV/Kwu68KcDlj3CSwcC9nJYLNDu4eh6wvgre+D0u5Sv78va8xIcnIyAEFB51/lcNWqVcTExJy1rUePHkyfPv28+2RnZ5OdnZ3fTklJuZwyRUTKNafTYPz8BCb/vhuArg2r8uadrQjwLuVrshyJhxlPw5H1Zrt6K3POkOqtLC1Lil6hw4jT6WTw4MF06tSJZs2anbdfYmIiISEhZ20LCQkhMTHxvPvExsYyevTowpYmIiJ/ScvOY/A38SzYlgTAI53r8FyPRriV5onMslLM2VPXvA+GE7wC4JoR0OZ+sLvAuBYpsEKHkYEDB7J582aWL19elPUAMGzYsLOupqSkpBAergWNREQK4sDJDB78fC07ktLwdLfz2s3NubFVDavLOj/DgK2/wJyhkHrU3NbsZujxCviHWlubFKtChZFBgwYxY8YMli5dSo0aF/7DDg0NJSkp6axtSUlJhIae/w/Ly8sLLy8Xec5dRKQUWrn7BI9/tZ4zGbkE+3vx/r1RtIqoZHVZ53dqL8x6FnbNN9uVakPv8VDvGmvrkhJRoKdpDMNg0KBB/PzzzyxatIjatS8+gjk6OpqFCxeetW3+/PlER0cXrFIREbkkX6zax70fr+FMRi4tagTy66ArSm8QycuBZePh3Q5mEHHzNBe0e3yVgkg5UqArIwMHDmTatGn88ssv+Pv754/7CAwMxMfHB4D+/fsTFhZGbGwsAE899RSdO3dm/Pjx9O7dm2+++YZ169bxwQcfFPFbEREp33LynIz+bQtf/XEAgBsiq/PazS3w9iil4yz2rTAHqJ74a76q2leZM6hWqW9tXVLiChRG3nvvPQC6dOly1vZPP/2U//u//wPgwIED2P9nUaKOHTsybdo0XnzxRYYPH079+vWZPn36BQe9iohIwZxMy+bxr9bzx95T2GzwXI9GPNq5TumcPyT9BMwfAfFfmW2/qua4kOa3QmmsV4rdZc0zUlI0z4iIyPltO5rCQ5+v49DpTCp4ufPmHZFc0zjk4juWNKcT4r80g0jmacAGbe4zn5TxKaW3keSylMg8IyIiYq05mxOJ+S6ejBwHNSv78lH/NtQP8be6rH9L2mrekjm42myHNDfnDAlva21dUioojIiIuCDDMHh70S4mzN8BQKd6lZl8V2sq+pay9Vly0mHJa7BqMjjzwMMPug6H9o+Cm76CxKS/BBERF5Oenccz329k9mbzIYL/61iLF3o3xsOtQA9IFr+E2ebjuskHzXajPtDzNQgsxXOdiCUURkREXMiBkxk8/MU6tiem4uFmY+wNzbijXYTVZZ0t+RDMfh62/7UmWWAE9HodGva0ti4ptRRGRERcxPKdJxj0tTmRWZUKXrx/b2uiap5/bbAS58iDP6aYU7nnpoPdHaIHQefnwLOUL8gnllIYEREp5QzD4OPle3ll1jacBrSsEciUe6OoFuhjdWl/O7jWHKCatMlsh3cwB6iGNLG2LnEJCiMiIqVYVq6D4T9v4qf1hwG4qXUYr9zYvPRMZJZ5GhaMhripgGE+onvtWIi8G+ylbAyLlFoKIyIipdTR5Ewe/SKOjYeScbPbGN6rMfd3qlU6JjIzDPjzO5g7HDJOmNsi7zaDiF9la2sTl6MwIiJSCsXtP8UjX6znRFo2FX09mHxXazrVq2J1WaYTO81bMvuWme0qDaHPBKh1hbV1ictSGBERKWW+XnOAEb9sJtdh0CjUnw/ubUNEZV+ry4LcTFg2AVZMAkcOuPuYg1OjB4F7KZvfRFyKwoiISCmRk+dk7IytfLF6PwA9m4Uy7taW+HmVgo/qXQth5hA4vdds17sWeo+DSrUsLUvKhlLwFy4iIif+WuhuzV8L3Q25tgEDu9azfnxIaiLMGQZbfjLb/tXMicsaX69F7aTIKIyIiFhs8+FkHv58HUeSs6jg5c6k2yPp1sTihe6cDlj7MSwaC9kpYLObU7h3HQ5epXDtG3FpCiMiIhb6Jf4wz//4J1m5TmpX8ePD/lHUC7b4y/7IBnOA6pENZrt6a+g7Caq1tLQsKbsURkRELOBwGrw+ZzvvL90DQJeGVXnzjlYE+nhYV1RWCvz+Mqz5AAwneAVCtxEQdR/YS8m8JlImKYyIiJSw5IxcnvxmA0t2HAfgsS51eaZ7Q9zsFo3BMAzYOh1mD4U0c/E9mt8K3V8Gf4tvF0m5oDAiIlKCdial8tDn69h3MgNvDzuv39KS61tWt66gU3vMlXV3LTDbQXWh93io29W6mqTcURgRESkh87Yk8vS38aTnOAir6MP790bRLCzQmmLysmHlW7B0HORlgZsnXBEDVzwNHt7W1CTllsKIiEgxczoN3l60i4kLdgDQvnYQ797dmsoVvKwpaO8ymBkDJ8x6qN0Zek+AKvWsqUfKPYUREZFilJadxzPfbWTOFnMsxoDomrzYpwkebhYsIpd+Aua9CBu/Ntt+VaFHLDS/RXOGiKUURkREism+E+k8/MU6diSl4elm56V+zbitbXjJF+J0woYvYP4IyDoD2KDN/XDNCPCpWPL1iPyDwoiISDH4ffsxnvpmAylZeVT192LKPVFE1axU8oUkbjbnDDm0xmyHNoc+k6BGm5KvReQ8FEZERIqQ02nw7uJdjJ+/A8OA1hEVee+eKEICSnhQaE46LI6FVe+C4QDPCtD1BWj3MLjpo19KF/1FiogUkbTsPIZ8F8/cLUkA3N0+gpF9m+LpXsLjQ7bPgtnPQfJBs934erjuVQgMK9k6RC6RwoiISBHYfTyNR76IY9cxc3zImBuacke7iJIt4sxBmP08JMw02xUjoNc4aNCjZOsQKSCFERGRyzR/axIx38aTmp1HaIA3793TmlYRJTg+xJELq98zb8vkZoDdHTo+CVc9C56+JVeHSCEpjIiIFJLTafDWop1MWrATgLa1KjH57tYE+5fg+JADf5gDVI9tMdsRHaHPBAhuXHI1iFwmhRERkUJIycol5tt4Fmw7Bpjzh7zQu0nJjQ/JOAULRsH6z8y2TxB0HwuRd2vOEHE5CiMiIgW0MymVR76IY8+JdDzd7bzcrxm3timh+UMMAzZ+Y05elnHC3NbqHug2Bvwql0wNIkVMYUREpADmbE5kyHfm+jLVA72Zcm8ULWpULJmDH99hTuO+b5nZrtrYvCVTs2PJHF+kmCiMiIhcAofTYOL8Hbzz+y7AXF9m8t2tqVIS68vkZsKy8bB8Ejhzwd0HujwPHQaCu2fxH1+kmCmMiIhcRHJGLk99u4HFCccBuL9TbYb1alQy68vsXACzhsDpfWa7fg/o9QZUqln8xxYpIQojIiIXkJCYysNfrGP/yQy83O28enNzbmxVo/gPnHIU5g6DLT+bbf/q0Ot1aNRHA1SlzFEYERE5j5l/HuXZHzaSkeMgrKIP798bRbOwwOI9qNMBaz+ChWMhJxVsbtDhMegyFLz8i/fYIhZRGBER+QeH0+CNuQlMWbIbgE71KvP2na0J8ivm8RmH15tzhhyNN9thbaDPRKjWoniPK2IxhRERkf9xJiOHJ77ewLKd5mOzD19Vh+d6NMS9OMeHZCXDopdgzYeAAV6B0G0kRN0H9hJe10bEAgojIiJ/2Xw4mUe/jOPQ6Uy8Pey8fktLrm9ZvfgOaBiw5SeYMwzSzMX1aH4b9HgZKgQX33FFShmFERER4Lt1B3lx+mZy8pxEBPky5Z4omlQPKL4DntwNs56B3YvMdlBdc86QOl2K75gipZTCiIiUa9l5Dkb9upWv1xwA4OpGwUy8LZJAX4/iOWBeNqx4E5aOA0c2uHnBlUOg01PgUYJr2oiUIgojIlJuHT6TyeNfxrHxUDI2GzzdrQGDutbDbi+mR2f3LoUZMXDSXFiPOl2h93ioXLd4jifiIhRGRKRcWrHrBE98vYFT6TkE+njw5h2RdGlYTOM00o6ba8n8+Y3Z9guG62Kh2c2aM0QEhRERKWcMw+C9JbsZNzcBpwFNqwcw5Z4owoN8i/5gTqe5qu6CkeYTM9ig7YNw9YvgU7HojyfiohRGRKTcSMnK5ZnvNjJvq/nkyq1RNRjbrxneHm5Ff7DETeacIYfWmu3QFtB3EoRFFf2xRFycwoiIlAsJiak8+mUce0+k4+lmZ9T1TbmzXTi2or5Nkp0Gi2Nh9XtgOMDTH65+Ado+BG76yBU5F/3LEJEy79eNR3j+hz/JzHVQPdCbd++JIjK8YtEfaNsMmP0cpBw22036mWNDAopxrhKRMqDAU/stXbqUvn37Ur16dWw2G9OnT79g/8WLF2Oz2f71k5iYWNiaRUQuSa7DyZjftvLk1xvIzHXQqV5lfnviiqIPImcOwLQ74Nu7zSBSsSbc9T3c9pmCiMglKPCVkfT0dFq2bMn999/PTTfddMn7JSQkEBDw9wRCwcGaXVBEis+xlCwGTlvP2n2nAXi8S12GdG+IW1E+tuvIhVWTYclrkJsBdg/o9CRc+Qx4FsOAWJEyqsBhpGfPnvTs2bPABwoODqZixYoF3k9EpKDW7jvF41+t53hqNhW83Bl3a0uuaxZatAc5sNocoHpsq9mu2Ql6T4DgRkV7HJFyoMTGjERGRpKdnU2zZs0YNWoUnTp1Om/f7OxssrOz89spKSklUaKIuDjDMPh0xT5embWNPKdBg5AKTLknijpVKxTdQTJOmY/qrv/cbPtWhu4vQcs7NWeISCEVexipVq0aU6ZMoU2bNmRnZ/PRRx/RpUsX/vjjD1q3bn3OfWJjYxk9enRxlyYiZUhGTh5Df9zErxuPANCnRTVeu7kFfl5F9DFnGLDxa3PysoyT5rbW/aHbaPANKppjiJRTNsMwjELvbLPx888/069fvwLt17lzZyIiIvjiiy/O+ftzXRkJDw8nOTn5rHEnIiIAe0+k8+gXcSQkpeJutzG8V2Pu61Sr6B7bPZ5gTuO+f7nZDm4CfSZCRIeieX2RMiolJYXAwMCLfn9b8mhvu3btWL58+Xl/7+XlhZeXVwlWJCKuat6WRIZ8t5HU7Dyq+nsx+a7WtKtdRFcqcjJg2ThY8RY4c8HDF7oMhQ6Pg1sxLaQnUg5ZEkbi4+OpVq2aFYcWkTIiz+Fk3LwdTFmyG4C2tSox+a7WBAcU0cq3O+fDzCFwZr/ZbtATer0OFSOK5vVFJF+Bw0haWhq7du3Kb+/du5f4+HiCgoKIiIhg2LBhHD58mM8/Nwd3TZo0idq1a9O0aVOysrL46KOPWLRoEfPmzSu6dyEi5cqxlCwGfb2BNXtPAXBfp1oM79UYD7cCT530bylHYM5Q2PqL2Q4Ig56vQ6PeGqAqUkwKHEbWrVtH165d89sxMTEADBgwgKlTp3L06FEOHDiQ//ucnByGDBnC4cOH8fX1pUWLFixYsOCs1xARuVSrdp/kia83cCItGz9PN16/pSW9WxTBlVZHHqz9EBa9BDlpYHODDo9Bl2HgVYRP44jIv1zWANaScqkDYESk7HI6Dd5fuoc35m7HaUCDkAq8d08UdYvisd3DceacIUc3mu0abc0BqqHNL/+1RcqxUj2AVUSkIJIzchnyfTwLth0D4KZWYbx0YzN8PS/zIywrGRaOhbUfAQZ4B5qP6rYeAPYiuOUjIpdEYURESrVNh5J57Ks4Dp3OLLrVdg0DNv8Ic4dDWpK5rcUd5uRlFaoWTeEicskURkSkVDIMg2lrDjD6163kOJyEB/nw7l1RNK8ReHkvfHK3+ZTMnt/NduX60Hs81Ol8+UWLSKEojIhIqZORk8eLP2/mpw2HAejWOJjxt0YS6HsZc3vkZcPySbBsPDiywc0LrnrWXNjOXfMaiVhJYURESpXdx9N47Ms4diSlYbfBc9c14uEr62C/nNV29yw2r4ac/GtagrpXQ69xULlukdQsIpdHYURESo0Zfx7h+R/+JD3HQZUKXrxzVys61Klc+BdMOwZzX4BN35ntCiFwXSw0vUlzhoiUIgojImK5nDwnr8zaxtSV+wBoXzuIt+9sVfjZVJ1OiPsUFo42n5jBBu0ehqtfMJ+YEZFSRWFERCx1+EwmA79aT/zBMwA81qUuQ65tgHthZ1M9+qc5Z8jhdWa7WkvoMwnCzr1KuIhYT2FERCyzZMdxBn+zgdMZuQR4uzPhtki6NQkp3Itlp8LvsfDHe2A4wdMfrvkPtH0Q7G5FW7iIFCmFEREpcXkOJxMX7GDy7+Yid83CAnj3rigiKvsW/MUMA7bPgNnPQ4r59A1Nb4QesRCgBTlFXIHCiIiUqMTkLJ78egNr9pmL3N3TIYIXezfB26MQVy9O74fZz8GOOWa7Ui3oNR7qdyu6gkWk2CmMiEiJWbLjOE9/G8+p9BwqeLkTe1Nz+rasXvAXcuTCqndg8WuQlwl2D7hiMFw5BDx8irxuESleCiMiUuz+eVumSbUAJt/dmtpV/Ar+YvtXwowYOL7NbNe6EnpPgKoNirBiESlJCiMiUqySUrJ44usNrNl7mbdl0k/CghGw4Uuz7VsFerwMLW7XnCEiLk5hRESKzdK/bsucTM/Bz9ON2JtbcH1Bb8sYBsR/BfP+A5lmoKH1AOg2CnyDirxmESl5CiMiUuTyHE4mLdjJ5MW7MAxoXC2AdwtzW+bYNvOWzIGVZju4KfSZCBHti75oEbGMwoiIFKl/3pa5u30E/+lTwNsyORmw9A1Y+RY488DDF7oMgw6PgdtlLJYnIqWSwoiIFJkiuS2zYy7MegbOHDDbDXtDz9egYnjRFywipYLCiIhctjyHkzcX7uSd3/++LTP5rlbUqVrh0l8k+TDMeR62/Wa2A2pAr9ehUe/iKVpESg2FERG5LEkp5iRmfxT2towjD9Z8AL+/DDlpYHOD6IHQ+XnwKkCYERGXpTAiIoW2bOdxBn9zGbdlDsXBjKcgcZPZDm9vzhkS2qx4ChaRUklhREQKLNfhZML8HUxZsrtwt2Uyz8DCMbDuE8AA74pw7RhodS/YC7lar4i4LIURESmQQ6czePLrDaw/cAaAu9pHMOJSb8sYBmz6AeYOh/Rj5raWd0H3seBXpfiKFpFSTWFERC7Z7E1Hef7HP0nJysPf251Xb2pB7xaXuDLuiV0wMwb2LjHbVRqYt2RqX1l8BYuIS1AYEZGLysp18NLMrXy52nzcNjK8Im/f2YrwIN+L75ybBcsnwvIJ4MgBd2+46hno+BS4exZz5SLiChRGROSCdh1LZdC0DWxPTAXg0c51GdK9AR5ulzC2Y/fvMHMInDIXyKNeN+j1BgTVKcaKRcTVKIyIyDkZhsH36w4x8tctZOY6qFLBkwm3RXJVg6oX3zk1Cea9AJu+N9sVQqHnq9Cknxa1E5F/URgRkX9Jzcrlxemb+SX+CABX1KvChNtbEuzvfeEdnQ6I+xQWjIHsZLDZod3D0PUF8A4ogcpFxBUpjIjIWf48dIYnvt7A/pMZuNltDOnegEevqovdfpErGkc3woyn4XCc2a7eylzUrnqr4i9aRFyawoiIAOZtmY+X7+W1OdvJdRiEVfThrTsjiaoZdOEds1Ph91fgjylgOMErAK4ZAW3uB3sBFscTkXJLYUREOJmWzbM//Mmi7ebcH9c1DeW1m1sQ6HuBFXINA7b9CrOHQqp5O4dmN0OPV8A/tASqFpGyQmFEpJxbuesET38XT1JKNp7udv7Tpwn3tI/AdqGBpqf3waxnYec8s12pNvQeD/WuKZGaRaRsURgRKady8swp3d9fak7pXreqH+/c1ZrG1S4w0DQvB1a9DUvegLxMsHvAFU/DlTHg4VNyxYtImaIwIlIO7T2RzlPfbODPQ8kA3Nkugv/0aYyv5wU+EvatMAeonkgw27WuNGdQrdqgBCoWkbJMYUSkHDEMgx/izLlDMnIcBPp48NrNzbmu2QWmdE8/CfNHQPyXZtu3ijkupMVtmjNERIqEwohIOZGcmcvwnzcx88+jAHSoE8TE2yOpFnie2ytOJ8R/BfP/A5mnzW1R90G3keBTqYSqFpHyQGFEpBxYu+8Ug7+J5/CZTNzsNmKubcCjnevidr65Q5K2movaHVhltkOamXOGhLcruaJFpNxQGBEpw/IcTt5atIt3Fu3EaUBEkC9v3hFJq4jzXNnISYclr8Oqd8CZBx5+0HU4tH8U3PRxISLFQ58uImXUwVMZDP42nrj95i2Wm1qHMeaGZlTwOs8/+4Q55uO6yebKvDTqAz1fg8AaJVSxiJRXCiMiZdAv8Yd58efNpGbn4e/lzks3NuOGyLBzd04+BLOfh+0zzHZguLmybsOeJVewiJRrCiMiZUhadh4jftnMT+sPAxBVsxKTbo8kPMj3350deeYU7r+/ArnpYHeH6EHQ+Tnw9CvhykWkPFMYESkj4g+e4alvzAXu7DZ44ur6PHF1Pdzd7P/ufHCtOWdI0iazHd7BHKAa0qRkixYRQWFExOXlOZxMWbKbSQt2kuc0F7ibdEckbWudY4G7zNOwYDTETQUM8xHda8dA5D1gP0doEREpAQojIi5s/8l0nv42nvUHzgDQu0U1XrmxOYE+/1jgzjBg0/cwdzikHze3Rd5tBhG/KiVbtIjIPyiMiLggwzD4bt1Bxvy2lfQcB/5e7oy+oSk3tgr79wJ3J3aac4bsXWq2qzSEPhOg1hUlX7iIyDkU+Lrs0qVL6du3L9WrV8dmszF9+vSL7rN48WJat26Nl5cX9erVY+rUqYUoVUQATqZl88gXcTz/4ybScxy0qx3E7MFXclPrGmcHkdwsc3Dqex3NIOLuDdeMgEeXK4iISKlS4Csj6enptGzZkvvvv5+bbrrpov337t1L7969efTRR/nqq69YuHAhDz74INWqVaNHjx6FKlqkvPp9+zGe/eFPTqRl4+FmY0j3hjx0ZZ1/z6S6ayHMegZO7THb9a41H9cNql3yRYuIXESBw0jPnj3p2fPS5x+YMmUKtWvXZvz48QA0btyY5cuXM3HiRIURkUuUkZPHyzO38dUf5oRkDUIqMPH2SJpWDzy7Y2qiOS5k849m27+aOXFZ4+u1qJ2IlFrFPmZk1apVdOvW7axtPXr0YPDgwefdJzs7m+zs7Px2SkpKcZUnUuptPHiGp7+NZ8+JdADu71Sb565riLeH29+dnA5Y9wksHAPZKWCzm1O4dxkG3gEWVS4icmmKPYwkJiYSEhJy1raQkBBSUlLIzMzEx+ffK4bGxsYyevTo4i5NpFTLczh5d/Fu3ly4E4fTIDTAm3G3tuSK+v94+uVIPMwYDEc2mO3qraHvJKjWsoQrFhEpnFL5NM2wYcOIiYnJb6ekpBAeHm5hRSIla9+JdJ7+Lp4Nfz2y26dFNV7q14yKvp5/d8pKgd9fhjUfgOEErwBzgGqb+8Hudu4XFhEphYo9jISGhpKUlHTWtqSkJAICAs55VQTAy8sLLy+v4i5NpNQxDINv1h5k7IytZOQ48Pd2Z+wNzbghsvrfT8oYBmz9BeYMhdSj5rbmt0L3l8E/5PwvLiJSShV7GImOjmbWrFlnbZs/fz7R0dHFfWgRl3IiLZuhP25iwTYzvHeoE8T42yIJq/g/of3UXnNl3V3zzXZQHeg9HupebUHFIiJFo8BhJC0tjV27duW39+7dS3x8PEFBQURERDBs2DAOHz7M559/DsCjjz7KO++8w3PPPcf999/PokWL+O6775g5c2bRvQsRF7dwWxLP//gnJ9Jy8HSz80yPBjx4RR3s/31kNy8HVr4FS9+AvCxw84QrYuCKp8HD29riRUQuU4HDyLp16+jatWt++79jOwYMGMDUqVM5evQoBw4cyP997dq1mTlzJk8//TRvvvkmNWrU4KOPPtJjvSKYq+y+PHMbX68x/800DPFn0h2RNK72P0/A7FsOM2LgRILZrt0Zek+AKvUsqFhEpOjZDMMwrC7iYlJSUggMDCQ5OZmAAD2mKGXD6j0nefaHjRw8lQnAg1fU5pke//PIbvoJmPcf2DjNbPtVhR6x0PwWzRkiIi7hUr+/S+XTNCJlWVaugzfmJvDJir0YBoRV9OGNW1vQse5fj+w6nbDhC5g/ArLOADbzCZlr/mOusisiUsYojIiUoPiDZ4j5Lp49x80JzO5oG84LvRvj7/3XKrtJW2DG03DwD7Md2hz6TIIabawpWESkBCiMiJSAnDwnby3cybuLd+E0INjfi9dubkHXRsF/dUiHJa/BqsngzAPPCtD1BWj3MLjpn6mIlG36lBMpZtuOphDz3Ua2HTWXNbghsjqjr2/69wRm22fB7Ocg+aDZbtwXrnsNAsMsqlhEpGQpjIgUkzyHk/eX7mHSgh3kOgwq+Xrw8o3N6dW8mtnhzEGY/Twk/PWYe2CEubJuw+usK1pExAIKIyLFYPfxNIZ8t5H4g2cAuLZJCK/c2Jyq/l7gyIU/psDvsZCbDnZ36PgEXPUcePpaW7iIiAUURkSKkNNpMHXlPl6bs53sPCf+3u6M6tuUm1qHmdO5H1xjDlBN2mzuEBENfSZCcGNrCxcRsZDCiEgROXgqg2e+38gfe08BcGX9Krx2cwuqV/SBjFOwcDTETTU7+wRB97HQ8i6w260rWkSkFFAYEblMTqfBl3/s59XZ28nIceDr6cbwXo25u30ENoCN38DcFyDjhLlDq3ug2xjwq2xl2SIipYbCiMhl2H8yned++DP/aki7WkG8cWsLalb2g+M7YGYM7Ftmdq7ayLwlU7OjhRWLiJQ+CiMiheD4a2zIG3O3k5XrxMfDjeeva0j/6FrYHVmw6CVYPgmcueDuA12ehw4Dwd3T6tJFREodhRGRAtp9PI3nfviTuP2nAYiuU5nXbm5BRGVf2LUAZg6B0/vMzvV7mI/rVqppXcEiIqWcwojIJXI4DT5atocJ83eQnefEz9ONYb0ac1e7COxpifD9QNjyk9nZvzr0fM2cwEyL2omIXJDCiMgl2JmUyjM//MnGv+YNubJ+FV69uQVhAZ6w9kNYNBayU8Bmh/aPQddh4OVvbdEiIi5CYUTkAv47i+qbC3aS4zDnDflP7ybc2qYGtqPx8O1gOBpvdg6LMgeoVmtpYcUiIq5HYUTkPLYdTeHZHzay+bC5pszVjYJ55cbmhHplm9O4r/0QDCd4BUK3kRD1f2B3s7ZoEREXpDAi8g85eU7eXbyLyb/vItdhEOjjwci+Tbgxsjq2rdNhzjBISzQ7N78NerwMFYItrVlExJUpjIj8j82Hk3nm+41sT0wFoEfTEMb2a0Zw7hH46hbYvdDsGFQX+kyAOl2sK1ZEpIxQGBEBsvMcvL1wF+8t2Y3DaRDk58no65vSp0kQtpVvw7JxkJcFbl5w5RDo9BR4eFtdtohImaAwIuVe/MEzPPfDRnYkpQHQu0U1xlzflMrH/4ApMXByp9mxThfoPQEq17WuWBGRMkhhRMqtjJw8xs/bwacr9uI0oEoFT17q14zrarnDvCfhz2/Mjn7BcF0sNLtZc4aIiBQDhREpl5bsOM4LP2/i0OlMAPpFVmdkn8ZUSvgG3hkJWWcAG7R9AK7+D/hUtLJcEZEyTWFEypVT6Tm8NGMrP204DEBYRR9evrEZXQKPwTd94dAas2NoC+gzCWpEWVesiEg5oTAi5YJhGPwSf4QxM7ZyKj0Hmw3u61ibIV2q47dqHKx6FwwHeFaAq1+Etg+Bm/55iIiUBH3aSpl36HQGL/y8mSU7jgPQKNSf2Jua0ypjJXx4K6QcMjs2uQGuexUCqltYrYhI+aMwImWWw2nw2cp9jJuXQEaOA093O09dU5+HW7jjMe9xSJhldqxYE3qNgwbdrS1YRKScUhiRMml7YgrP/7gpf2G7drWCiO3XiLq7P4cpr0JuBtg9oNOTcOUz4OlrbcEiIuWYwoiUKZk5Dt5atJMPl+4hz2ng7+XO0F6NuDPkCPafesGxrWbHmp3MOUOCG1lbsIiIKIxI2bE44Rj/+WUzB0+Zj+t2bxLC2O7VCVkTC7M/Nzv5BJlrybS8U3OGiIiUEgoj4vKOpWQxZsZWZvx5FIBqgd6M7tuE7nm/w+e3QcZJs2Ore+HaMeAbZGG1IiLyTwoj4rKcToNpaw7w2pztpGblYbfBfZ1qE9PKwG/eg7B/udkxuAn0mQgRHawtWEREzklhRFzStqMpDP95ExsOnAGgRY1AYvvWp+muD+DjN8GZC+4+0GUoRA8ENw9rCxYRkfNSGBGXkpGTx5sLd/LRsr04nAYVvNx5pnsD7q26C7fp3eH0PrNjg+ug5+tQqaal9YqIyMUpjIjL+H37MV6cvpnDZ8wBqj2bhTK6a2WCV4yE+dPNTgFhZghp1FsDVEVEXITCiJR6SSlZjPltKzM3mQNUwyr6MKZvI65J/RU+ewlyUsHmBh0egy7DwKuCxRWLiEhBKIxIqZXrcPLZyn1MnL+D9BwHbnYbD1xRm5gmqXjPvQOObjQ71mhrDlANbW5twSIiUigKI1Iq/bHnJCN+2UJCUioArSIqEturJo22TIKpHwEGeAdCt9HQegDY7ZbWKyIihacwIqXKsdQsYmdt5+cNhwEI8vNkaI+G3OK9BvsP90FaktmxxR3QfSxUCLawWhERKQoKI1Iq5DmcfLl6P+Pn7SA1Ow+bDe5sF8HQdh4ELHoKdi8yO1auZ07jXqeztQWLiEiRURgRy8XtP81/pm9m69EUwJwz5KU+9Wmx/zP4ZBw4ssHNC656Bjo9Be5eFlcsIiJFSWFELHMyLZvX5mznu3WHAAj08eDZHg25s+pe3H7rBSd3mR3rXg29xkHluhZWKyIixUVhREqcw2nw9ZoDvDE3geTMXABua1ODoVcGEbRiLMz51uxYIQSui4WmN2nOEBGRMkxhRErUun2nGPXbFjYfNm/JNK4WwEs3NCbqxK/w6SjISgZs0O4huPpF84kZEREp0xRGpEQcOZPJq7O38+vGIwD4e7kT070B99ZKwX32HXBordmxWkvoMwnCWltXrIiIlCiFESlWWbkOPli6h/cW7yYz14HNBne0DWdIlzCqrJ0AH70HhgM8/eGa/0DbB8HuZnXZIiJSghRGpFgYhsHszYm8PHNb/loybWtVYmSfJjRLXQZT74AUcy4Rmt4IPWIhoJqFFYuIiFUKNW3l5MmTqVWrFt7e3rRv3541a9act+/UqVOx2Wxn/Xh7exe6YCn9th5J4c4PV/P4V+s5fCaT6oHevH1nK767PYxmSx+Bb+8xg0ilWnD3j3DrVAUREZFyrMBXRr799ltiYmKYMmUK7du3Z9KkSfTo0YOEhASCg889G2ZAQAAJCQn5bZuejCiTTqXnMH5eAl+vOYDTAC93O490rstjV0TgEzcF3n0NcjPA7mHOF3LVM+DhY3XZIiJisQKHkQkTJvDQQw9x3333ATBlyhRmzpzJJ598wtChQ8+5j81mIzQ09PIqlVIrK9fBZyv3Mfn3XaRk5QHQu0U1hvVsRI2UjfBJfzi+zexc8wroMwGqNrSwYhERKU0KFEZycnKIi4tj2LBh+dvsdjvdunVj1apV590vLS2NmjVr4nQ6ad26Na+88gpNmzY9b//s7Gyys7Pz2ykpKQUpU0qI02kwPf4w4+ftyB8X0rhaACP7NqFDqA3mPwcbvjA7+1aG7i9Dyzs0Z4iIiJylQGHkxIkTOBwOQkJCztoeEhLC9u3bz7lPw4YN+eSTT2jRogXJycmMGzeOjh07smXLFmrUqHHOfWJjYxk9enRBSpMStnTHcWJnb2fbX1O4Vwv0JubaBtzUKgy3P7+G71+EzFNm59YDoNso8A2yrmARESm1iv1pmujoaKKjo/PbHTt2pHHjxrz//vuMHTv2nPsMGzaMmJiY/HZKSgrh4eHFXapcgs2Hk3ltznaW7TwBmPOFPN61Hvd1qoX36Z3weR/Yv8LsHNwU+kyEiPYWViwiIqVdgcJIlSpVcHNzIykp6aztSUlJlzwmxMPDg1atWrFr167z9vHy8sLLS4uhlSaHTmcwft4Oft5gPo7r4Wbj3g61GHR1PYI88mDJS7DyLXDmgYcvdBkGHR4DNw+LKxcRkdKuQGHE09OTqKgoFi5cSL9+/QBwOp0sXLiQQYMGXdJrOBwONm3aRK9evQpcrJS8Y6lZvLd4N1+tPkCOwwnA9S2r80z3hkRU9oUd82DWEDhzwNyhYS/o+TpU1JUsERG5NAW+TRMTE8OAAQNo06YN7dq1Y9KkSaSnp+c/XdO/f3/CwsKIjY0FYMyYMXTo0IF69epx5swZ3njjDfbv38+DDz5YtO9EitTJtGzeX7qHz1ftIyvXDCHRdSozvFdjmtcIhJQj8N2jsPUXc4eAGtDrdWjU28KqRUTEFRU4jNx+++0cP36cESNGkJiYSGRkJHPmzMkf1HrgwAHs9r/nUjt9+jQPPfQQiYmJVKpUiaioKFauXEmTJk2K7l1IkUlMzuLDZXuY9scBMnMdAESGV2RI9wZcUa8KNqcDVr0Lv78MOWlgc4PogdD5efCqYHH1IiLiimyGYRhWF3ExKSkpBAYGkpycTEBAgNXllEn7T6YzZckefow7lH87pnlYIE9fW5+uDYPNieoOxcGMwZD4p7lTjXbmANXQZtYVLiIipdalfn9rbZpyLv7gGT5dsZffNh7B+VcsbVc7iIFd63FV/SpmCMk8A4vGwtqPAQO8K8K1o6FVf7AXakUBERGRfAoj5VBWroNfNx7hy9X7+fNQcv72Lg2rMrBrPdrW+ms+EMOATT/AnGGQfszc1vJOuHYsVKhqQeUiIlIWKYyUIwdOZvDlH/v5bt1BzmTkAuDpZqdPi2rcf0VtmoUF/t355G6YGQN7FpvtyvXNadxrX1XyhYuISJmmMFLG5TmcLN15nC9W7WfxjuP8d4RQWEUf7ulQk9va1KByhf+Z0yU3C1ZMgmUTwJEN7t7mgnYdnwR3zf0iIiJFT2GkDDIMg42Hkpm+4TAz/jzKibS/1/np3KAq93aoSddGwbjZ/7FGzO7fYeYQOLXbbNe9BnqPg6A6JVi9iIiUNwojZYRhGGw6nMy8LUnM+PMI+05m5P8uyM+Tm1qFcU+HmtSq4vfvnVOTYN4LsOl7s10hFHq+Ck36aVE7EREpdgojLiw7z8GavaeYtyWJ+VuTSEzJyv+dj4cb3ZuGcENkda6sXxUPt3M89eJ0QtwnsGAMZCeDzQ5tH4KrXwRvPUItIiIlQ2HEheQ5nGw+ksKKXSdYtfsk6/afyp8dFcDX040uDavSo2ko3RqH4Od1gf97j/4JM56Gw+vMdrVIc86QsNbF+yZERET+QWGkFMtzOElISmX1nlOs2n2CP/acIjU776w+Vf296NY4mO5NQomuWxlvD7cLv2h2KvweC3+8B4YTvALgmhHQ5n6wX2RfERGRYqAwUkoYhsGh05lsOZJM/MFkNhw4zabDyWTkOM7qF+DtToc6lelYtzKd6lWhXnAFc2Kyix8Atv0Gs5+H1CPmtqY3QY9XIKBaMbwjERGRS6MwYoFT6TnsPp7GrmNp7ExKY+vRZLYeSSElK+9fff083WhdsxKd6lWhU90qNKke8O+nYC7m9D6Y9RzsnGu2K9U2n5Kp1+3y34yIiMhlUhgpJrkOJ0fOZLLneDq7jqWx+3hafgA5/deEY//k4WajfrA/LcMDiQyvSGR4JeoFVyh4+PivvBxY9Q4seR3yMsHuAVc8DVfGgIfPZbw7ERGRoqMwUgiGYZCSlUdSShaJyVkk/vW/B09lcPB0BgdPZXI0OTN/rZdzCavoQ93gCtSt6kfj0ACaVA+gQYg/nu5FtNbL/pXmANXj2812rSuh9wSo2qBoXl9ERKSIKIz8j6xcByfTcziVlsPJ9GxOpedwMi2HE+nZJP0VOpJSsklMziIz13HR1/N0t1Onih91q1bIDx51q1agTlU/fD2L6dSnn4T5IyD+S7PtW8UcF9LiNs0ZIiIipVK5DiMvTt/EpsMpnErP5lRaDuk5Fw8Y/yvQx4PQAG9CAr0JDfCiRiVfwoN8CK/kS3iQL1UreGEv7C2WgnI6If4rmP8fyDxtbou6D7qNBJ9KJVODiIhIIZTrMJKQmMrGg2fO2ubhZiPIz5MgPy+qVPAkyM+Tyn5ehAR4ERroTUiAtxlAArzx8Swlj8Ie2wYzYuDASrMd0sycMyS8nbV1iYiIXIJyHUaeuqYBGTl5VK5gBo6gCp74e7lf2qOypUFOBix9HVa+Dc488PCDrsOg/WPgVq7/rxURERdSrr+xrqhfxeoSCm/HXJj1DJw5YLYb9YHrXoWK4dbWJSIiUkDlOoy4pOTDMOd5cwIzgMBw6Pk6NOplbV0iIiKFpDDiKhx5sOZ9+P0VyEkDuztED4TOz4PnOVbiFRERcREKI67g0Dr4bTAkbTLb4R2gzwQIaWppWSIiIkVBYaQ0yzwNC8fAuk8Bw3xE99oxEHkP2ItocjQRERGLKYyURoYBm76HucMh/bi5LfJuM4j4ufCgWxERkXNQGCltTuyCmTGwd4nZrtLQvCVT6wpr6xIRESkmCiOlRW4WLJ8IyyeAIwfcvaHzcxD9BLh7Wl2diIhIsVEYKQ12L4KZQ+DUHrNd71ro9QYE1ba2LhERkRKgMGKl1CRzXMjmH8y2fzXo+Ro0vl6L2omISLmhMGIFpwPWfQILx0J2Mtjs0O4R6DocvAOsrk5ERKREKYyUtCPxMONpOLLebFdvbS5qVz3SyqpEREQsozBSUrJSzNlT17wPhhO8AuCaEdDmfrCXktV/RURELKAwUtwMA7b+AnOGQupRc1uzW6DHy+Afam1tIiIipYDCSHE6tRdmPQu75pvtoDrQezzUvdraukREREoRhZHikJcDq96GJa9DXha4ecIVMXDF0+DhbXV1IiIipYrCSFHbt8IcoHoiwWzX7gy9J0CVetbWJSIiUkopjBSV9BMwfwTEf2W2/apCj1eg+a2aM0REROQCFEYul9MJ8V+aQSTzNGCDNveZT8r4VLK6OhERkVJPYeRyJG01b8kcXG22Q5qbc4aEt7W2LhEREReiMFIYOemw5DVYNRmceeDhB1e/YM6i6qZTKiIiUhD65iyohNnm47rJB812475w3asQWMPaukRERFyUwsilSj4Es5+H7TPMdmCEubJuw+usrUtERMTFKYxcjCMX/pgCv8dCbjrY3aHjE3DVs+DpZ3V1IiIiLk9h5EIOroUZgyFps9mOiDYHqAY3trQsERGRskRh5FwyT8OC0RA3FTDAJwi6j4WWd4HdbnV1IiIiZYrCyP8yDPjzO5g7HDJOmNta3QPdxoBfZWtrExERKaMURv7rxE5zzpB9y8x21UbmNO61Ollbl4iISBlXqHsOkydPplatWnh7e9O+fXvWrFlzwf7ff/89jRo1wtvbm+bNmzNr1qxCFVsscjNh0cvwXkcziLj7wDUj4ZFlCiIiIiIloMBh5NtvvyUmJoaRI0eyfv16WrZsSY8ePTh27Ng5+69cuZI777yTBx54gA0bNtCvXz/69evH5s2bL7v4y7ZrIbwbDUtfB0cO1O8OA1fDlTHg7ml1dSIiIuWCzTAMoyA7tG/fnrZt2/LOO+8A4HQ6CQ8P54knnmDo0KH/6n/77beTnp7OjBkz8rd16NCByMhIpkyZcknHTElJITAwkOTkZAICAgpS7vnlZsFbkZB6FPyrQ8/XzAnMtKidiIhIkbjU7+8CXRnJyckhLi6Obt26/f0CdjvdunVj1apV59xn1apVZ/UH6NGjx3n7A2RnZ5OSknLWT5Hz8Iaer0OHgTBoDTS5XkFERETEAgUKIydOnMDhcBASEnLW9pCQEBITE8+5T2JiYoH6A8TGxhIYGJj/Ex4eXpAyL12T6+G6V8DLv3heX0RERC6qVE6aMWzYMJKTk/N/Dh48aHVJIiIiUkwK9GhvlSpVcHNzIykp6aztSUlJhIaGnnOf0NDQAvUH8PLywsvLqyCliYiIiIsq0JURT09PoqKiWLhwYf42p9PJwoULiY6OPuc+0dHRZ/UHmD9//nn7i4iISPlS4EnPYmJiGDBgAG3atKFdu3ZMmjSJ9PR07rvvPgD69+9PWFgYsbGxADz11FN07tyZ8ePH07t3b7755hvWrVvHBx98ULTvRERERFxSgcPI7bffzvHjxxkxYgSJiYlERkYyZ86c/EGqBw4cwP4/67d07NiRadOm8eKLLzJ8+HDq16/P9OnTadasWdG9CxEREXFZBZ5nxArFMs+IiIiIFKtimWdEREREpKgpjIiIiIilFEZERETEUgojIiIiYimFEREREbGUwoiIiIhYSmFERERELFXgSc+s8N+pUFJSUiyuRERERC7Vf7+3LzalmUuEkdTUVADCw8MtrkREREQKKjU1lcDAwPP+3iVmYHU6nRw5cgR/f39sNttlv15KSgrh4eEcPHhQM7oWM53rkqNzXXJ0rkuOznXJKY5zbRgGqampVK9e/aylYv7JJa6M2O12atSoUeSvGxAQoD/uEqJzXXJ0rkuOznXJ0bkuOUV9ri90ReS/NIBVRERELKUwIiIiIpYql2HEy8uLkSNH4uXlZXUpZZ7OdcnRuS45OtclR+e65Fh5rl1iAKuIiIiUXeXyyoiIiIiUHgojIiIiYimFEREREbGUwoiIiIhYqsyGkcmTJ1OrVi28vb1p3749a9asuWD/77//nkaNGuHt7U3z5s2ZNWtWCVXq+gpyrqdOnYrNZjvrx9vbuwSrdV1Lly6lb9++VK9eHZvNxvTp0y+6z+LFi2ndujVeXl7Uq1ePqVOnFnudZUFBz/XixYv/9Xdts9lITEwsmYJdVGxsLG3btsXf35/g4GD69etHQkLCRffT53XBFeZcl+TndZkMI99++y0xMTGMHDmS9evX07JlS3r06MGxY8fO2X/lypXceeedPPDAA2zYsIF+/frRr18/Nm/eXMKVu56CnmswZ/c7evRo/s/+/ftLsGLXlZ6eTsuWLZk8efIl9d+7dy+9e/ema9euxMfHM3jwYB588EHmzp1bzJW6voKe6/9KSEg46287ODi4mCosG5YsWcLAgQNZvXo18+fPJzc3l+7du5Oenn7effR5XTiFOddQgp/XRhnUrl07Y+DAgflth8NhVK9e3YiNjT1n/9tuu83o3bv3Wdvat29vPPLII8VaZ1lQ0HP96aefGoGBgSVUXdkFGD///PMF+zz33HNG06ZNz9p2++23Gz169CjGysqeSznXv//+uwEYp0+fLpGayqpjx44ZgLFkyZLz9tHnddG4lHNdkp/XZe7KSE5ODnFxcXTr1i1/m91up1u3bqxateqc+6xateqs/gA9evQ4b38xFeZcA6SlpVGzZk3Cw8O54YYb2LJlS0mUW+7o77rkRUZGUq1aNa699lpWrFhhdTkuJzk5GYCgoKDz9tHfddG4lHMNJfd5XebCyIkTJ3A4HISEhJy1PSQk5Lz3bxMTEwvUX0yFOdcNGzbkk08+4ZdffuHLL7/E6XTSsWNHDh06VBIllyvn+7tOSUkhMzPToqrKpmrVqjFlyhR+/PFHfvzxR8LDw+nSpQvr16+3ujSX4XQ6GTx4MJ06daJZs2bn7afP68t3qee6JD+vXWLVXik7oqOjiY6Ozm937NiRxo0b8/777zN27FgLKxMpvIYNG9KwYcP8dseOHdm9ezcTJ07kiy++sLAy1zFw4EA2b97M8uXLrS6lzLvUc12Sn9dl7spIlSpVcHNzIykp6aztSUlJhIaGnnOf0NDQAvUXU2HO9T95eHjQqlUrdu3aVRwllmvn+7sOCAjAx8fHoqrKj3bt2unv+hINGjSIGTNm8Pvvv1OjRo0L9tXn9eUpyLn+p+L8vC5zYcTT05OoqCgWLlyYv83pdLJw4cKzEt7/io6OPqs/wPz588/bX0yFOdf/5HA42LRpE9WqVSuuMsst/V1bKz4+Xn/XF2EYBoMGDeLnn39m0aJF1K5d+6L76O+6cApzrv+pWD+vS2SYbAn75ptvDC8vL2Pq1KnG1q1bjYcfftioWLGikZiYaBiGYdx7773G0KFD8/uvWLHCcHd3N8aNG2ds27bNGDlypOHh4WFs2rTJqrfgMgp6rkePHm3MnTvX2L17txEXF2fccccdhre3t7Flyxar3oLLSE1NNTZs2GBs2LDBAIwJEyYYGzZsMPbv328YhmEMHTrUuPfee/P779mzx/D19TWeffZZY9u2bcbkyZMNNzc3Y86cOVa9BZdR0HM9ceJEY/r06cbOnTuNTZs2GU899ZRht9uNBQsWWPUWXMJjjz1mBAYGGosXLzaOHj2a/5ORkZHfR5/XRaMw57okP6/LZBgxDMN4++23jYiICMPT09No166dsXr16vzfde7c2RgwYMBZ/b/77jujQYMGhqenp9G0aVNj5syZJVyx6yrIuR48eHB+35CQEKNXr17G+vXrLaja9fz38dF//vz3/A4YMMDo3Lnzv/aJjIw0PD09jTp16hiffvppidftigp6rl977TWjbt26hre3txEUFGR06dLFWLRokTXFu5BznWPgrL9TfV4XjcKc65L8vLb9VaSIiIiIJcrcmBERERFxLQojIiIiYimFEREREbGUwoiIiIhYSmFERERELKUwIiIiIpZSGBERERFLKYyIiIiIpRRGRERExFIKIyIiImIphRERERGxlMKIiIiIWOr/AaRtOGLngPxtAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(g_true_sort, g_bcs_sort, label='g bcs')\n",
        "plt.plot(g_true_sort, g_true_sort, label='g true')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Caso G = G(k-k')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 11152.20819529, -16966.67721849,   9464.80487942,  13174.54418182,\n",
              "       -18629.13698067,  13159.91936489,  -1295.98295914, -17375.76301624])"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Calcula rho_2 en función del delta_k dado\n",
        "import scipy.optimize\n",
        "\n",
        "\n",
        "def bcs_deltak_rho(delta_k):\n",
        "    sq = lambda k: np.sqrt(en_batch[0][k]**2+delta_k[k]**2)\n",
        "    vk = lambda k: np.sqrt(1/2 * (1 - en_batch[0][k]/sq(k)))\n",
        "    uk = lambda k: np.sqrt(1/2 * (1 + en_batch[0][k]/sq(k)))\n",
        "\n",
        "    rho = np.zeros((basis.m, basis.m))\n",
        "    for k in range(0, basis.m):\n",
        "        for kp in range(0, basis.m):\n",
        "            p = vk(k)**4 if k == kp else 0\n",
        "            rho[k, kp] = uk(k)*vk(k)*uk(kp)*vk(kp) + p\n",
        "\n",
        "    return rho \n",
        "\n",
        "\n",
        "# Calcula G_kk' en función del delta dado\n",
        "def bcs_rho_g(rho_init):\n",
        "    # Buscamos delta_k\n",
        "    dist = lambda delta_k: np.linalg.norm(bcs_deltak_rho(delta_k)-rho_init)\n",
        "    opti = scipy.optimize.minimize(dist, np.random.rand(basis.m), method='Nelder-Mead')\n",
        "    delta_k = opti.x\n",
        "    # Redefinimos las funciones para este delta\n",
        "    sq = lambda k: np.sqrt(en_batch[0][k]**2+delta_k[k]**2)\n",
        "    vk = lambda k: np.sqrt(1/2 * (1 - en_batch[0][k]/sq(k)))\n",
        "    uk = lambda k: np.sqrt(1/2 * (1 + en_batch[0][k]/sq(k)))\n",
        "\n",
        "    # Calculamos las ecuaciones\n",
        "    delta_expr = lambda k, g: np.sum([-g[np.abs(k-kp)]*uk(kp)*vk(kp) for kp in range(0, basis.m)])\n",
        "    func = lambda g: np.array([delta_expr(k, g)-delta_k[k] for k in range(0,basis.m)])\n",
        "    g_res = scipy.optimize.fsolve(func, np.random.rand(basis.m))\n",
        "    \n",
        "    return g_res\n",
        "\n",
        "\n",
        "iterador = iter(val_dataset)\n",
        "sample = next(iterador)\n",
        "input_data = sample[0][0][0]\n",
        "rho_init = input_data\n",
        "actual_values = sample[1][0]\n",
        "bcs_rho_g(input_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 1.15463195e-14, -1.24344979e-14, -1.82076576e-14,  2.13162821e-14,\n",
              "       -6.21724894e-15,  7.99360578e-15, -2.62012634e-14,  4.21884749e-15])"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Buscamos delta_k\n",
        "dist = lambda delta_k: np.linalg.norm(bcs_deltak_rho(delta_k)-rho_init)\n",
        "opti = scipy.optimize.minimize(dist, np.random.rand(basis.m), method='Nelder-Mead')\n",
        "delta_k = opti.x\n",
        "# Redefinimos las funciones para este delta\n",
        "sq = lambda k: np.sqrt(en_batch[0][k]**2+delta_k[k]**2)\n",
        "vk = lambda k: np.sqrt(1/2 * (1 - en_batch[0][k]/sq(k)))\n",
        "uk = lambda k: np.sqrt(1/2 * (1 + en_batch[0][k]/sq(k)))\n",
        "\n",
        "# Calculamos las ecuaciones\n",
        "delta_expr = lambda k, g: np.sum([-g[np.abs(k-kp)]*uk(kp)*vk(kp) for kp in range(0, basis.m)])\n",
        "func = lambda g: np.array([delta_expr(k, g)-delta_k[k] for k in range(0,basis.m)])\n",
        "g_res = scipy.optimize.fsolve(func, np.random.rand(basis.m))\n",
        "func(g_res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       message: Optimization terminated successfully.\n",
            "       success: True\n",
            "        status: 0\n",
            "           fun: 6.583580935645797\n",
            "             x: [ 0.000e+00  7.167e-11  0.000e+00  8.857e-10  0.000e+00\n",
            "                  4.970e-09  8.428e-05  7.647e+00]\n",
            "           nit: 1137\n",
            "          nfev: 1591\n",
            " final_simplex: (array([[ 0.000e+00,  7.167e-11, ...,  8.428e-05,\n",
            "                         7.647e+00],\n",
            "                       [ 0.000e+00,  1.629e-10, ...,  9.213e-05,\n",
            "                         7.647e+00],\n",
            "                       ...,\n",
            "                       [ 0.000e+00,  4.820e-11, ...,  8.241e-05,\n",
            "                         7.647e+00],\n",
            "                       [ 0.000e+00,  2.802e-10, ...,  1.014e-04,\n",
            "                         7.647e+00]]), array([ 6.584e+00,  6.584e+00,  6.584e+00,  6.584e+00,\n",
            "                        6.584e+00,  6.584e+00,  6.584e+00,  6.584e+00,\n",
            "                        6.584e+00]))\n"
          ]
        }
      ],
      "source": [
        "bounds = [(0,10) for x in range(0,basis.m)]\n",
        "res = scipy.optimize.minimize(lambda x: np.linalg.norm(func(x)), x0=np.random.rand(basis.m), method = 'Nelder-Mead', bounds=bounds)\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-1.53904299 -2.06091161  2.59598808  3.27063042  3.26651694  2.59642177\n",
            "  2.06118041 -1.53866189]\n",
            "[[0.95770364 0.06401127 0.08713254 0.09947651 0.09947364 0.08713618\n",
            "  0.06401624 0.04049897]\n",
            " [0.06401127 0.88580694 0.13769009 0.15719649 0.15719197 0.13769584\n",
            "  0.10116086 0.06399799]\n",
            " [0.08713254 0.13769009 0.75015077 0.21397683 0.21397067 0.18743243\n",
            "  0.13770078 0.08711446]\n",
            " [0.09947651 0.15719649 0.21397683 0.57556001 0.24428366 0.21398577\n",
            "  0.1572087  0.09945586]\n",
            " [0.09947364 0.15719197 0.21397067 0.24428366 0.42434702 0.21397961\n",
            "  0.15720417 0.099453  ]\n",
            " [0.08713618 0.13769584 0.18743243 0.21398577 0.21397961 0.24988055\n",
            "  0.13770654 0.0871181 ]\n",
            " [0.06401624 0.10116086 0.13770078 0.1572087  0.15720417 0.13770654\n",
            "  0.11421342 0.06400296]\n",
            " [0.04049897 0.06399799 0.08711446 0.09945586 0.099453   0.0871181\n",
            "  0.06400296 0.042278  ]]\n"
          ]
        }
      ],
      "source": [
        "dist = lambda delta_k: np.linalg.norm(bcs_deltak_rho(delta_k)-input_data)\n",
        "opti = scipy.optimize.minimize(dist, np.random.rand(basis.m), method='Nelder-Mead')\n",
        "delta_k = opti.x\n",
        "print(delta_k)\n",
        "print(bcs_deltak_rho(delta_k))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(8, 8), dtype=float32, numpy=\n",
              "array([[0.9688839 , 0.04984007, 0.07421588, 0.10022894, 0.1074377 ,\n",
              "        0.09971628, 0.06921853, 0.04621751],\n",
              "       [0.04984007, 0.91645753, 0.12104012, 0.16098046, 0.17579783,\n",
              "        0.15192974, 0.11499333, 0.06921854],\n",
              "       [0.07421588, 0.12104011, 0.8042377 , 0.24403635, 0.2495444 ,\n",
              "        0.22269703, 0.15192974, 0.09971628],\n",
              "       [0.10022895, 0.16098043, 0.24403636, 0.602141  , 0.34425622,\n",
              "        0.24954438, 0.17579785, 0.10743771],\n",
              "       [0.1074377 , 0.17579785, 0.2495444 , 0.34425622, 0.39785984,\n",
              "        0.24403635, 0.16098046, 0.10022897],\n",
              "       [0.09971628, 0.15192974, 0.22269703, 0.24954438, 0.24403636,\n",
              "        0.19576319, 0.12104011, 0.07421589],\n",
              "       [0.06921854, 0.11499333, 0.15192974, 0.17579785, 0.16098046,\n",
              "        0.12104011, 0.08354335, 0.04984008],\n",
              "       [0.0462175 , 0.06921854, 0.09971628, 0.10743771, 0.10022896,\n",
              "        0.07421589, 0.04984008, 0.031117  ]], dtype=float32)>"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Análisis para G cte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "ename": "InvalidArgumentError",
          "evalue": "{{function_node __wrapped__TensorScatterUpdate_device_/job:localhost/replica:0/task:0/device:CPU:0}} Outer dimensions of indices and update must match. Indices shape: [4096,3], updates shape:[8192] [Op:TensorScatterUpdate] name: ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[83], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m g_arr \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mones((basis\u001b[38;5;241m.\u001b[39mm, basis\u001b[38;5;241m.\u001b[39mm))\u001b[38;5;241m*\u001b[39mg_seed \u001b[38;5;28;01mfor\u001b[39;00m g_seed \u001b[38;5;129;01min\u001b[39;00m h_labels]\n\u001b[1;32m      5\u001b[0m g_arr \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(g_arr, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m----> 6\u001b[0m h_arr \u001b[38;5;241m=\u001b[39m two_body_hamiltonian_tf(t_basis, basis\u001b[38;5;241m.\u001b[39mm, en_batch, g_arr\u001b[38;5;241m.\u001b[39mnumpy(), rho_1_arrays, rho_2_arrays, k_indices_tf)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Estados térmicos\u001b[39;00m\n\u001b[1;32m      9\u001b[0m state \u001b[38;5;241m=\u001b[39m thermal_state_tf(h_arr\u001b[38;5;241m*\u001b[39mbeta) \n",
            "Cell \u001b[0;32mIn[44], line 130\u001b[0m, in \u001b[0;36mtwo_body_hamiltonian_tf\u001b[0;34m(t_basis, m, energy_batch, G_batched, rho_1_arrays, rho_2_arrays, indices)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Creamos la mat de t_basis y updateamos a partir de los indices de kkbar\u001b[39;00m\n\u001b[1;32m    129\u001b[0m mat \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(energy_batch), t_basis\u001b[38;5;241m.\u001b[39msize, t_basis\u001b[38;5;241m.\u001b[39msize), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m--> 130\u001b[0m mat \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtensor_scatter_nd_update(mat, indices, G_flatten)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Preparamos las dimensiones y multiplicamos\u001b[39;00m\n\u001b[1;32m    132\u001b[0m mat_expanded \u001b[38;5;241m=\u001b[39m mat[:, :, :, np\u001b[38;5;241m.\u001b[39mnewaxis, np\u001b[38;5;241m.\u001b[39mnewaxis]\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__TensorScatterUpdate_device_/job:localhost/replica:0/task:0/device:CPU:0}} Outer dimensions of indices and update must match. Indices shape: [4096,3], updates shape:[8192] [Op:TensorScatterUpdate] name: "
          ]
        }
      ],
      "source": [
        "# Generacion de elementos, rho2 a partir de ellos, y comparación con la predicción\n",
        "# Nuevamente, el resultado depende pura y exclusivamente del modelo, y no de los ptos tomados\n",
        "h_labels = np.linspace(0.1,1,512)\n",
        "g_arr = [np.ones((basis.m, basis.m))*g_seed for g_seed in h_labels]\n",
        "g_arr = tf.constant(g_arr, dtype=tf.float32)\n",
        "h_arr = two_body_hamiltonian_tf(t_basis, basis.m, en_batch, g_arr.numpy(), rho_1_arrays, rho_2_arrays, k_indices_tf)\n",
        "\n",
        "# Estados térmicos\n",
        "state = thermal_state_tf(h_arr*beta) \n",
        "state = tf.cast(state, dtype=tf.float32)\n",
        "# Estados puros\n",
        "#state = pure_state(h_arr)\n",
        "\n",
        "rho_2_input = rho_2_tf(state, rho_2_arrays_kkbar_tf)\n",
        "predictions = model.predict(rho_2_input).T\n",
        "G_err = np.abs(predictions-h_labels).T\n",
        "plt.plot(h_labels, G_err)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 0s 1ms/step\n",
            "16/16 [==============================] - 0s 1ms/step\n",
            "16/16 [==============================] - 0s 1ms/step\n",
            "16/16 [==============================] - 0s 1ms/step\n",
            "16/16 [==============================] - 0s 1ms/step\n",
            "16/16 [==============================] - 0s 1ms/step\n",
            "16/16 [==============================] - 0s 1ms/step\n",
            "16/16 [==============================] - 0s 1ms/step\n",
            "16/16 [==============================] - 0s 1ms/step\n",
            "16/16 [==============================] - 0s 1ms/step\n",
            "16/16 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Ploteo de varios elementos de val_dataset\n",
        "# No sirve de mucho, depende del modelo y no la muestra\n",
        "max_plt = 10\n",
        "idx = 0\n",
        "for e in val_dataset:\n",
        "    predictions = model.predict(e[0])\n",
        "    pred_ids = predictions.T.argsort()\n",
        "    predictions_sort = predictions[pred_ids][0]\n",
        "    G_true_sorted = e[1].numpy()[pred_ids].T\n",
        "    G_err = np.abs(predictions_sort-G_true_sorted)\n",
        "    plt.plot(predictions_sort,G_err)\n",
        "    idx += 1\n",
        "    if idx > max_plt:  \n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modelos Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tenemos que trabajar con DataFrames para trabajar con xgboost, por eso inicialmente desempaquetamos el dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow_decision_forests as tfdf\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def rf_fit(dataset):\n",
        "    # Generación de dataset\n",
        "    ds_f = {} # features\n",
        "    ds_l = {} # labels\n",
        "\n",
        "    # Generamos las etiquetas\n",
        "    for i in range(basis.m*basis.m):\n",
        "        ds_f[f'{i}'] = []\n",
        "    # Generacion de labels TODO: Escribir todo en función del label size y fue\n",
        "    if label_size == 1:\n",
        "        ds_l['g'] = []\n",
        "    elif label_size == 2:\n",
        "        ds_l['g'] = []\n",
        "        ds_l['sigma'] = []  \n",
        "    else:\n",
        "        for i in range(0, label_size):\n",
        "            ds_l[f'l{i}'] = []\n",
        "\n",
        "    # Poblamos el DF\n",
        "    for e in list(dataset.as_numpy_iterator()):\n",
        "        # Elementos de rho2\n",
        "        for i in range(0,basis.m*basis.m):\n",
        "            ds_f[f'{i}'].append(np.ndarray.flatten(e[0])[i])\n",
        "        # Labels\n",
        "        if label_size == 1:\n",
        "            ds_l['g'].append(e[1])\n",
        "        elif label_size == 2:\n",
        "            ds_l['g'].append(e[1][0])\n",
        "            ds_l['sigma'].append(e[1][1])\n",
        "        else:\n",
        "            for i in range(0, label_size):\n",
        "                ds_l[f'l{i}'].append(e[1][i])\n",
        "\n",
        "    ds_l = pd.DataFrame(ds_l)\n",
        "    ds_f = pd.DataFrame(ds_f)\n",
        "\n",
        "    # Spliteamos los datasets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(ds_f, ds_l, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Entrenamos\n",
        "    regressor = xgb.XGBRegressor(objective='reg:squarederror', max_depth=20)\n",
        "    regressor.fit(X_train, y_train)\n",
        "    predictions = regressor.predict(X_test)\n",
        "\n",
        "    # Evaluamos\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    print(f'Mean Squared Error: {mse}')\n",
        "\n",
        "    return regressor, X_test, y_test, y_train\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Análicemos los resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rf_error_coef(regressor, X_test, y_test, y_train):\n",
        "    predictions = regressor.predict(X_test)\n",
        "    # Printeamos algunos valores\n",
        "    for i in range(0, 10):\n",
        "        print(predictions[i], y_test.to_numpy()[i])\n",
        "\n",
        "    if label_size == 1:\n",
        "        actual_values = y_test.to_numpy()\n",
        "        norm_pred = np.mean(np.abs(predictions-actual_values.T))\n",
        "        norm_rand = np.mean(np.abs(y_train.to_numpy()[:len(actual_values)]-actual_values))\n",
        "    elif label_size > 1:\n",
        "        norm_pred = np.mean(np.linalg.norm(predictions-y_test.to_numpy(),ord=2, axis=1))\n",
        "        norm_rand = np.mean(np.linalg.norm(y_train.to_numpy()[:len(predictions)]-y_test.to_numpy(),ord=2, axis=1))\n",
        "        \n",
        "    print(norm_pred, norm_rand)\n",
        "    print(norm_rand / norm_pred)\n",
        "    return norm_rand / norm_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Análisis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ejemplo de uso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-26 20:27:52.258336: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/device:GPU:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-26 20:27:52.258686: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-02-26 20:27:52.258864: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-02-26 20:27:52.259105: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-02-26 20:27:52.259290: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-02-26 20:27:52.259445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n",
            "100%|██████████| 391/391 [00:19<00:00, 20.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Squared Error: 0.010034733451902866\n",
            "0.5682996 [0.50319576]\n",
            "0.422716 [0.14772558]\n",
            "4.3965306 [4.388036]\n",
            "3.6091523 [3.6028416]\n",
            "1.2004422 [1.1912681]\n",
            "2.4397368 [2.4369164]\n",
            "2.7130573 [2.7192173]\n",
            "4.144267 [4.1379166]\n",
            "1.2973745 [1.2939128]\n",
            "1.2589029 [1.2602155]\n",
            "0.039318 1.6313978\n",
            "41.492393\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "41.492393"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset, label_size = gen_dataset('const', 0.1, 5, 'thermal', 'rho1')\n",
        "# DNN\n",
        "#model, val_dataset = dnn_fit(dataset, label_size)\n",
        "#dnn_error_coef(model, val_dataset) \n",
        "# RF\n",
        "regressor, X_test, y_test, y_train = rf_fit(dataset)\n",
        "rf_error_coef(regressor, X_test, y_test, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 16:54:22.680337: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.01\n",
            "/device:GPU:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 16:54:22.680662: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 16:54:22.680848: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "  0%|          | 0/196 [00:00<?, ?it/s]local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 16:54:22.681293: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 16:54:22.681447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n",
            "100%|██████████| 196/196 [01:55<00:00,  1.69it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rho (InputLayer)            [(None, 8, 8, 1)]         0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 64)                0         \n",
            "                                                                 \n",
            " concatenate_1 (Concatenate  (None, 64)                0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 16)                176       \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 32)                544       \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1403 (5.48 KB)\n",
            "Trainable params: 1403 (5.48 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "2024-03-12 16:56:18.586544: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 16:56:18.586831: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 16:56:18.587001: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 16:56:18.587213: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 16:56:18.587400: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 16:56:18.587555: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n",
            "2024-03-12 16:56:19.830289: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fe270079b00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2024-03-12 16:56:19.830325: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
            "2024-03-12 16:56:19.857132: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2024-03-12 16:56:22.341353: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8900\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1710262582.456168    4846 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    156/Unknown - 5s 3ms/step - loss: 0.0310 - accuracy: 0.0000e+00 - mean_squared_error: 0.0310"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 16:56:23.755923: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 16:56:23.755976: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n",
            "2024-03-12 16:56:23.755985: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 407123110838793230\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 6s 7ms/step - loss: 0.0310 - accuracy: 0.0000e+00 - mean_squared_error: 0.0310 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 2/100\n",
            " 51/157 [========>.....................] - ETA: 0s - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 16:56:24.254918: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 16:56:24.254989: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 3/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - mean_squared_error: 0.0204 - val_loss: 0.0203 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0203\n",
            "Epoch 4/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0201 - accuracy: 0.0000e+00 - mean_squared_error: 0.0201 - val_loss: 0.0199 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0199\n",
            "Epoch 5/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0195 - accuracy: 0.0000e+00 - mean_squared_error: 0.0195 - val_loss: 0.0193 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0193\n",
            "Epoch 6/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0185 - accuracy: 0.0000e+00 - mean_squared_error: 0.0185 - val_loss: 0.0180 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0180\n",
            "Epoch 7/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0165 - accuracy: 0.0000e+00 - mean_squared_error: 0.0165 - val_loss: 0.0152 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0152\n",
            "Epoch 8/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0128 - accuracy: 0.0000e+00 - mean_squared_error: 0.0128 - val_loss: 0.0101 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0101\n",
            "Epoch 9/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0068 - accuracy: 0.0000e+00 - mean_squared_error: 0.0068 - val_loss: 0.0039 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0039\n",
            "Epoch 10/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0023 - accuracy: 0.0000e+00 - mean_squared_error: 0.0023 - val_loss: 0.0015 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0015\n",
            "Epoch 11/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0014 - accuracy: 0.0000e+00 - mean_squared_error: 0.0014 - val_loss: 0.0013 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0013\n",
            "Epoch 12/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0013 - accuracy: 0.0000e+00 - mean_squared_error: 0.0013 - val_loss: 0.0013 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0013\n",
            "Epoch 13/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0012 - accuracy: 0.0000e+00 - mean_squared_error: 0.0012 - val_loss: 0.0012 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0012\n",
            "Epoch 14/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0012 - accuracy: 0.0000e+00 - mean_squared_error: 0.0012 - val_loss: 0.0012 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0012\n",
            "Epoch 15/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0011 - accuracy: 0.0000e+00 - mean_squared_error: 0.0011 - val_loss: 0.0011 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0011\n",
            "Epoch 16/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0011 - accuracy: 0.0000e+00 - mean_squared_error: 0.0011 - val_loss: 0.0011 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0011\n",
            "Epoch 17/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0010 - accuracy: 0.0000e+00 - mean_squared_error: 0.0010 - val_loss: 0.0010 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0010\n",
            "Epoch 18/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0010 - accuracy: 0.0000e+00 - mean_squared_error: 0.0010 - val_loss: 9.9995e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 9.9995e-04\n",
            "Epoch 19/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 9.6925e-04 - accuracy: 0.0000e+00 - mean_squared_error: 9.6925e-04 - val_loss: 9.6603e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 9.6603e-04\n",
            "Epoch 20/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 9.3491e-04 - accuracy: 0.0000e+00 - mean_squared_error: 9.3491e-04 - val_loss: 9.3470e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 9.3470e-04\n",
            "Epoch 21/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 9.0222e-04 - accuracy: 0.0000e+00 - mean_squared_error: 9.0222e-04 - val_loss: 9.0338e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 9.0338e-04\n",
            "Epoch 22/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 8.7100e-04 - accuracy: 0.0000e+00 - mean_squared_error: 8.7100e-04 - val_loss: 8.7131e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 8.7131e-04\n",
            "Epoch 23/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 8.4104e-04 - accuracy: 0.0000e+00 - mean_squared_error: 8.4104e-04 - val_loss: 8.3851e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 8.3851e-04\n",
            "Epoch 24/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 8.1183e-04 - accuracy: 0.0000e+00 - mean_squared_error: 8.1183e-04 - val_loss: 8.0574e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 8.0574e-04\n",
            "Epoch 25/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 7.8274e-04 - accuracy: 0.0000e+00 - mean_squared_error: 7.8274e-04 - val_loss: 7.7433e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 7.7433e-04\n",
            "Epoch 26/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 7.5348e-04 - accuracy: 0.0000e+00 - mean_squared_error: 7.5348e-04 - val_loss: 7.4480e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 7.4480e-04\n",
            "Epoch 27/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 7.2415e-04 - accuracy: 0.0000e+00 - mean_squared_error: 7.2415e-04 - val_loss: 7.1666e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 7.1666e-04\n",
            "Epoch 28/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 6.9503e-04 - accuracy: 0.0000e+00 - mean_squared_error: 6.9503e-04 - val_loss: 6.8915e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 6.8915e-04\n",
            "Epoch 29/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 6.6639e-04 - accuracy: 0.0000e+00 - mean_squared_error: 6.6639e-04 - val_loss: 6.6185e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 6.6185e-04\n",
            "Epoch 30/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 6.3835e-04 - accuracy: 0.0000e+00 - mean_squared_error: 6.3835e-04 - val_loss: 6.3472e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 6.3472e-04\n",
            "Epoch 31/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 6.1085e-04 - accuracy: 0.0000e+00 - mean_squared_error: 6.1085e-04 - val_loss: 6.0810e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 6.0810e-04\n",
            "Epoch 32/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.8377e-04 - accuracy: 0.0000e+00 - mean_squared_error: 5.8377e-04 - val_loss: 5.8283e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 5.8283e-04\n",
            "Epoch 33/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.5707e-04 - accuracy: 0.0000e+00 - mean_squared_error: 5.5707e-04 - val_loss: 5.6005e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 5.6005e-04\n",
            "Epoch 34/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.3085e-04 - accuracy: 0.0000e+00 - mean_squared_error: 5.3085e-04 - val_loss: 5.4061e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 5.4061e-04\n",
            "Epoch 35/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.0531e-04 - accuracy: 0.0000e+00 - mean_squared_error: 5.0531e-04 - val_loss: 5.2313e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 5.2313e-04\n",
            "Epoch 36/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.8061e-04 - accuracy: 0.0000e+00 - mean_squared_error: 4.8061e-04 - val_loss: 5.0404e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 5.0404e-04\n",
            "Epoch 37/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.5692e-04 - accuracy: 0.0000e+00 - mean_squared_error: 4.5692e-04 - val_loss: 4.8148e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.8148e-04\n",
            "Epoch 38/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.3435e-04 - accuracy: 0.0000e+00 - mean_squared_error: 4.3435e-04 - val_loss: 4.5690e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.5690e-04\n",
            "Epoch 39/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.1301e-04 - accuracy: 0.0000e+00 - mean_squared_error: 4.1301e-04 - val_loss: 4.3310e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.3310e-04\n",
            "Epoch 40/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.9300e-04 - accuracy: 0.0000e+00 - mean_squared_error: 3.9300e-04 - val_loss: 4.1184e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.1184e-04\n",
            "Epoch 41/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.7440e-04 - accuracy: 0.0000e+00 - mean_squared_error: 3.7440e-04 - val_loss: 3.9351e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.9351e-04\n",
            "Epoch 42/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.5718e-04 - accuracy: 0.0000e+00 - mean_squared_error: 3.5718e-04 - val_loss: 3.7773e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.7773e-04\n",
            "Epoch 43/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.4133e-04 - accuracy: 0.0000e+00 - mean_squared_error: 3.4133e-04 - val_loss: 3.6379e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.6379e-04\n",
            "Epoch 44/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.2682e-04 - accuracy: 0.0000e+00 - mean_squared_error: 3.2682e-04 - val_loss: 3.5120e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.5120e-04\n",
            "Epoch 45/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.1365e-04 - accuracy: 0.0000e+00 - mean_squared_error: 3.1365e-04 - val_loss: 3.3974e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.3974e-04\n",
            "Epoch 46/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.0181e-04 - accuracy: 0.0000e+00 - mean_squared_error: 3.0181e-04 - val_loss: 3.2938e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.2938e-04\n",
            "Epoch 47/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.9128e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.9128e-04 - val_loss: 3.2010e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.2010e-04\n",
            "Epoch 48/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.8202e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.8202e-04 - val_loss: 3.1189e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.1189e-04\n",
            "Epoch 49/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.7394e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.7394e-04 - val_loss: 3.0469e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.0469e-04\n",
            "Epoch 50/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.6696e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.6696e-04 - val_loss: 2.9843e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.9843e-04\n",
            "Epoch 51/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.6097e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.6097e-04 - val_loss: 2.9301e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.9301e-04\n",
            "Epoch 52/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.5587e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.5587e-04 - val_loss: 2.8834e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.8834e-04\n",
            "Epoch 53/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.5154e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.5154e-04 - val_loss: 2.8433e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.8433e-04\n",
            "Epoch 54/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.4788e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.4788e-04 - val_loss: 2.8089e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.8089e-04\n",
            "Epoch 55/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.4480e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.4480e-04 - val_loss: 2.7793e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.7793e-04\n",
            "Epoch 56/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.4219e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.4219e-04 - val_loss: 2.7537e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.7537e-04\n",
            "Epoch 57/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.3998e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.3998e-04 - val_loss: 2.7315e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.7315e-04\n",
            "Epoch 58/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.3810e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.3810e-04 - val_loss: 2.7120e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.7120e-04\n",
            "Epoch 59/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.3647e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.3647e-04 - val_loss: 2.6948e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.6948e-04\n",
            "Epoch 60/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.3507e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.3507e-04 - val_loss: 2.6796e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.6796e-04\n",
            "Epoch 61/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.3383e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.3383e-04 - val_loss: 2.6658e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.6658e-04\n",
            "Epoch 62/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.3273e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.3273e-04 - val_loss: 2.6532e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.6532e-04\n",
            "Epoch 63/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.3173e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.3173e-04 - val_loss: 2.6418e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.6418e-04\n",
            "Epoch 64/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.3083e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.3083e-04 - val_loss: 2.6311e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.6311e-04\n",
            "Epoch 65/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.2999e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.2999e-04 - val_loss: 2.6212e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.6212e-04\n",
            "Epoch 66/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.2921e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.2921e-04 - val_loss: 2.6119e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.6119e-04\n",
            "Epoch 67/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.2848e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.2848e-04 - val_loss: 2.6032e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.6032e-04\n",
            "Epoch 68/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.2778e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.2778e-04 - val_loss: 2.5949e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.5949e-04\n",
            "Epoch 69/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.2712e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.2712e-04 - val_loss: 2.5870e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.5870e-04\n",
            "Epoch 70/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.2649e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.2649e-04 - val_loss: 2.5794e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.5794e-04\n",
            "Epoch 71/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.2588e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.2588e-04 - val_loss: 2.5721e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.5721e-04\n",
            "Epoch 72/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.2530e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.2530e-04 - val_loss: 2.5651e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.5651e-04\n",
            "Epoch 73/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.2473e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.2473e-04 - val_loss: 2.5584e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.5584e-04\n",
            "Epoch 74/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.2418e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.2418e-04 - val_loss: 2.5518e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.5518e-04\n",
            "Epoch 75/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.2364e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.2364e-04 - val_loss: 2.5455e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.5455e-04\n",
            "Epoch 76/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.2312e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.2312e-04 - val_loss: 2.5393e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.5393e-04\n",
            "Epoch 77/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.2261e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.2261e-04 - val_loss: 2.5333e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.5333e-04\n",
            "Epoch 78/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.2211e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.2211e-04 - val_loss: 2.5274e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.5274e-04\n",
            "Epoch 79/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.2162e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.2162e-04 - val_loss: 2.5216e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.5216e-04\n",
            "Epoch 80/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.2113e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.2113e-04 - val_loss: 2.5159e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.5159e-04\n",
            "Epoch 81/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.2066e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.2066e-04 - val_loss: 2.5103e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.5103e-04\n",
            "Epoch 82/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.2019e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.2019e-04 - val_loss: 2.5048e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.5048e-04\n",
            "Epoch 83/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.1973e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.1973e-04 - val_loss: 2.4993e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.4993e-04\n",
            "Epoch 84/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.1927e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.1927e-04 - val_loss: 2.4939e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.4939e-04\n",
            "Epoch 85/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.1881e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.1881e-04 - val_loss: 2.4885e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.4885e-04\n",
            "Epoch 86/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.1837e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.1837e-04 - val_loss: 2.4832e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.4832e-04\n",
            "Epoch 87/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.1792e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.1792e-04 - val_loss: 2.4779e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.4779e-04\n",
            "Epoch 88/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.1747e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.1747e-04 - val_loss: 2.4726e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.4726e-04\n",
            "Epoch 89/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.1703e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.1703e-04 - val_loss: 2.4673e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.4673e-04\n",
            "Epoch 90/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.1659e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.1659e-04 - val_loss: 2.4621e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.4621e-04\n",
            "Epoch 91/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.1616e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.1616e-04 - val_loss: 2.4568e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.4568e-04\n",
            "Epoch 92/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.1572e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.1572e-04 - val_loss: 2.4515e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.4515e-04\n",
            "Epoch 93/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.1528e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.1528e-04 - val_loss: 2.4462e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.4462e-04\n",
            "Epoch 94/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.1484e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.1484e-04 - val_loss: 2.4410e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.4410e-04\n",
            "Epoch 95/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.1441e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.1441e-04 - val_loss: 2.4356e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.4356e-04\n",
            "Epoch 96/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.1397e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.1397e-04 - val_loss: 2.4303e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.4303e-04\n",
            "Epoch 97/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.1353e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.1353e-04 - val_loss: 2.4250e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.4250e-04\n",
            "Epoch 98/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.1309e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.1309e-04 - val_loss: 2.4196e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.4196e-04\n",
            "Epoch 99/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.1265e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.1265e-04 - val_loss: 2.4142e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.4142e-04\n",
            "Epoch 100/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.1221e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.1221e-04 - val_loss: 2.4088e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.4088e-04\n",
            "8/8 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 16:57:42.051708: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(0.37187418, shape=(), dtype=float32)\n",
            "[0.39007065]\n",
            "tf.Tensor(0.3176473, shape=(), dtype=float32)\n",
            "[0.33264667]\n",
            "tf.Tensor(0.050901804, shape=(), dtype=float32)\n",
            "[0.07034963]\n",
            "tf.Tensor(0.22918922, shape=(), dtype=float32)\n",
            "[0.225677]\n",
            "0.01243853 0.1620189\n",
            "13.025566\n",
            "0.5357894736842106\n",
            "/device:GPU:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 16:57:42.051977: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 16:57:42.052156: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 16:57:42.052363: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 16:57:42.052531: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 16:57:42.052670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n",
            "100%|██████████| 196/196 [01:55<00:00,  1.69it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rho (InputLayer)            [(None, 8, 8, 1)]         0         \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 64)                0         \n",
            "                                                                 \n",
            " concatenate_2 (Concatenate  (None, 64)                0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 16)                176       \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 32)                544       \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1403 (5.48 KB)\n",
            "Trainable params: 1403 (5.48 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "2024-03-12 16:59:38.145924: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 16:59:38.146220: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 16:59:38.146414: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 16:59:38.146633: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 16:59:38.146801: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 16:59:38.146941: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    153/Unknown - 2s 3ms/step - loss: 0.5148 - accuracy: 0.0000e+00 - mean_squared_error: 0.5148"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 16:59:39.724225: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 16:59:39.724299: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 2s 6ms/step - loss: 0.5046 - accuracy: 0.0000e+00 - mean_squared_error: 0.5046 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 2/100\n",
            " 52/157 [========>.....................] - ETA: 0s - loss: 0.0204 - accuracy: 0.0000e+00 - mean_squared_error: 0.0204"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 16:59:40.124839: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 16:59:40.124888: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 3/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - mean_squared_error: 0.0205 - val_loss: 0.0205 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0205\n",
            "Epoch 4/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - mean_squared_error: 0.0204 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 5/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0202 - accuracy: 0.0000e+00 - mean_squared_error: 0.0202 - val_loss: 0.0202 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0202\n",
            "Epoch 6/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0200 - accuracy: 0.0000e+00 - mean_squared_error: 0.0200 - val_loss: 0.0199 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0199\n",
            "Epoch 7/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0196 - accuracy: 0.0000e+00 - mean_squared_error: 0.0196 - val_loss: 0.0195 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0195\n",
            "Epoch 8/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0192 - accuracy: 0.0000e+00 - mean_squared_error: 0.0192 - val_loss: 0.0189 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0189\n",
            "Epoch 9/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0185 - accuracy: 0.0000e+00 - mean_squared_error: 0.0185 - val_loss: 0.0181 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0181\n",
            "Epoch 10/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0175 - accuracy: 0.0000e+00 - mean_squared_error: 0.0175 - val_loss: 0.0170 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0170\n",
            "Epoch 11/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0162 - accuracy: 0.0000e+00 - mean_squared_error: 0.0162 - val_loss: 0.0155 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0155\n",
            "Epoch 12/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0146 - accuracy: 0.0000e+00 - mean_squared_error: 0.0146 - val_loss: 0.0137 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0137\n",
            "Epoch 13/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0125 - accuracy: 0.0000e+00 - mean_squared_error: 0.0125 - val_loss: 0.0114 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0114\n",
            "Epoch 14/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0101 - accuracy: 0.0000e+00 - mean_squared_error: 0.0101 - val_loss: 0.0087 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0087\n",
            "Epoch 15/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0073 - accuracy: 0.0000e+00 - mean_squared_error: 0.0073 - val_loss: 0.0058 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0058\n",
            "Epoch 16/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0044 - accuracy: 0.0000e+00 - mean_squared_error: 0.0044 - val_loss: 0.0031 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0031\n",
            "Epoch 17/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0021 - accuracy: 0.0000e+00 - mean_squared_error: 0.0021 - val_loss: 0.0013 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0013\n",
            "Epoch 18/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 7.3905e-04 - accuracy: 0.0000e+00 - mean_squared_error: 7.3905e-04 - val_loss: 3.7137e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.7137e-04\n",
            "Epoch 19/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.0178e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.0178e-04 - val_loss: 1.0089e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.0089e-04\n",
            "Epoch 20/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 6.4142e-05 - accuracy: 0.0000e+00 - mean_squared_error: 6.4142e-05 - val_loss: 4.5702e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.5702e-05\n",
            "Epoch 21/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.8978e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.8978e-05 - val_loss: 3.6521e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.6521e-05\n",
            "Epoch 22/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.4783e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.4783e-05 - val_loss: 3.4657e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.4657e-05\n",
            "Epoch 23/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.3806e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.3806e-05 - val_loss: 3.4015e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.4015e-05\n",
            "Epoch 24/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.3446e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.3446e-05 - val_loss: 3.3677e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.3677e-05\n",
            "Epoch 25/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.3271e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.3271e-05 - val_loss: 3.3438e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.3438e-05\n",
            "Epoch 26/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.3160e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.3160e-05 - val_loss: 3.3248e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.3248e-05\n",
            "Epoch 27/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.3060e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.3060e-05 - val_loss: 3.3104e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.3104e-05\n",
            "Epoch 28/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.2945e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.2945e-05 - val_loss: 3.2968e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.2968e-05\n",
            "Epoch 29/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.2811e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.2811e-05 - val_loss: 3.2851e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.2851e-05\n",
            "Epoch 30/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 3.2660e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.2660e-05 - val_loss: 3.2836e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.2836e-05\n",
            "Epoch 31/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 3.2500e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.2500e-05 - val_loss: 3.2880e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.2880e-05\n",
            "Epoch 32/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.2337e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.2337e-05 - val_loss: 3.2880e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.2880e-05\n",
            "Epoch 33/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.2163e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.2163e-05 - val_loss: 3.2739e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.2739e-05\n",
            "Epoch 34/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.1946e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.1946e-05 - val_loss: 3.2330e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.2330e-05\n",
            "Epoch 35/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.1670e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.1670e-05 - val_loss: 3.1768e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.1768e-05\n",
            "Epoch 36/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.1358e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.1358e-05 - val_loss: 3.1314e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.1314e-05\n",
            "Epoch 37/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.1036e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.1036e-05 - val_loss: 3.0977e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.0977e-05\n",
            "Epoch 38/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.0702e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.0702e-05 - val_loss: 3.0628e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.0628e-05\n",
            "Epoch 39/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.0344e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.0344e-05 - val_loss: 3.0242e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.0242e-05\n",
            "Epoch 40/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.9942e-05 - accuracy: 0.0000e+00 - mean_squared_error: 2.9942e-05 - val_loss: 2.9896e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.9896e-05\n",
            "Epoch 41/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.9476e-05 - accuracy: 0.0000e+00 - mean_squared_error: 2.9476e-05 - val_loss: 2.9667e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.9667e-05\n",
            "Epoch 42/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.8942e-05 - accuracy: 0.0000e+00 - mean_squared_error: 2.8942e-05 - val_loss: 2.9313e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.9313e-05\n",
            "Epoch 43/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.8351e-05 - accuracy: 0.0000e+00 - mean_squared_error: 2.8351e-05 - val_loss: 2.8623e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.8623e-05\n",
            "Epoch 44/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.7705e-05 - accuracy: 0.0000e+00 - mean_squared_error: 2.7705e-05 - val_loss: 2.7615e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.7615e-05\n",
            "Epoch 45/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.6992e-05 - accuracy: 0.0000e+00 - mean_squared_error: 2.6992e-05 - val_loss: 2.6659e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.6659e-05\n",
            "Epoch 46/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.6255e-05 - accuracy: 0.0000e+00 - mean_squared_error: 2.6255e-05 - val_loss: 2.5872e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.5872e-05\n",
            "Epoch 47/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.5503e-05 - accuracy: 0.0000e+00 - mean_squared_error: 2.5503e-05 - val_loss: 2.5068e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.5068e-05\n",
            "Epoch 48/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.4698e-05 - accuracy: 0.0000e+00 - mean_squared_error: 2.4698e-05 - val_loss: 2.4240e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.4240e-05\n",
            "Epoch 49/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.3872e-05 - accuracy: 0.0000e+00 - mean_squared_error: 2.3872e-05 - val_loss: 2.3402e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.3402e-05\n",
            "Epoch 50/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.3070e-05 - accuracy: 0.0000e+00 - mean_squared_error: 2.3070e-05 - val_loss: 2.2572e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.2572e-05\n",
            "Epoch 51/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.2303e-05 - accuracy: 0.0000e+00 - mean_squared_error: 2.2303e-05 - val_loss: 2.1750e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.1750e-05\n",
            "Epoch 52/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.1555e-05 - accuracy: 0.0000e+00 - mean_squared_error: 2.1555e-05 - val_loss: 2.0977e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.0977e-05\n",
            "Epoch 53/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.0810e-05 - accuracy: 0.0000e+00 - mean_squared_error: 2.0810e-05 - val_loss: 2.0355e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.0355e-05\n",
            "Epoch 54/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.0040e-05 - accuracy: 0.0000e+00 - mean_squared_error: 2.0040e-05 - val_loss: 1.9476e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.9476e-05\n",
            "Epoch 55/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.9300e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.9300e-05 - val_loss: 1.8855e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.8855e-05\n",
            "Epoch 56/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.8713e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.8713e-05 - val_loss: 1.8385e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.8385e-05\n",
            "Epoch 57/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.8187e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.8187e-05 - val_loss: 1.8231e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.8231e-05\n",
            "Epoch 58/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.7779e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.7779e-05 - val_loss: 1.8664e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.8664e-05\n",
            "Epoch 59/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.7459e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.7459e-05 - val_loss: 1.8066e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.8066e-05\n",
            "Epoch 60/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.7063e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.7063e-05 - val_loss: 1.6156e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.6156e-05\n",
            "Epoch 61/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.6508e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.6508e-05 - val_loss: 1.5614e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.5614e-05\n",
            "Epoch 62/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.6113e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.6113e-05 - val_loss: 1.6306e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.6306e-05\n",
            "Epoch 63/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.5884e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.5884e-05 - val_loss: 1.5508e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.5508e-05\n",
            "Epoch 64/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.5695e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.5695e-05 - val_loss: 1.4367e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.4367e-05\n",
            "Epoch 65/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.5545e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.5545e-05 - val_loss: 1.4005e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.4005e-05\n",
            "Epoch 66/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.5481e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.5481e-05 - val_loss: 1.4025e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.4025e-05\n",
            "Epoch 67/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.5361e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.5361e-05 - val_loss: 1.4460e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.4460e-05\n",
            "Epoch 68/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.5189e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.5189e-05 - val_loss: 1.4946e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.4946e-05\n",
            "Epoch 69/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.5028e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.5028e-05 - val_loss: 1.5165e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.5165e-05\n",
            "Epoch 70/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.4877e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.4877e-05 - val_loss: 1.4997e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.4997e-05\n",
            "Epoch 71/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.4711e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.4711e-05 - val_loss: 1.4573e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.4573e-05\n",
            "Epoch 72/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.4523e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.4523e-05 - val_loss: 1.4032e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.4032e-05\n",
            "Epoch 73/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.4322e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.4322e-05 - val_loss: 1.3470e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.3470e-05\n",
            "Epoch 74/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.4119e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.4119e-05 - val_loss: 1.2946e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.2946e-05\n",
            "Epoch 75/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.3920e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.3920e-05 - val_loss: 1.2494e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.2494e-05\n",
            "Epoch 76/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.3727e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.3727e-05 - val_loss: 1.2120e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.2120e-05\n",
            "Epoch 77/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.3539e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.3539e-05 - val_loss: 1.1814e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.1814e-05\n",
            "Epoch 78/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.3354e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.3354e-05 - val_loss: 1.1559e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.1559e-05\n",
            "Epoch 79/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.3172e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.3172e-05 - val_loss: 1.1340e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.1340e-05\n",
            "Epoch 80/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.2991e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.2991e-05 - val_loss: 1.1148e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.1148e-05\n",
            "Epoch 81/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.2810e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.2810e-05 - val_loss: 1.0976e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.0976e-05\n",
            "Epoch 82/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.2631e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.2631e-05 - val_loss: 1.0821e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.0821e-05\n",
            "Epoch 83/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.2453e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.2453e-05 - val_loss: 1.0682e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.0682e-05\n",
            "Epoch 84/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.2276e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.2276e-05 - val_loss: 1.0557e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.0557e-05\n",
            "Epoch 85/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.2100e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.2100e-05 - val_loss: 1.0447e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.0447e-05\n",
            "Epoch 86/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.1926e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.1926e-05 - val_loss: 1.0352e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.0352e-05\n",
            "Epoch 87/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.1753e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.1753e-05 - val_loss: 1.0272e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.0272e-05\n",
            "Epoch 88/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.1581e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.1581e-05 - val_loss: 1.0208e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.0208e-05\n",
            "Epoch 89/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.1410e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.1410e-05 - val_loss: 1.0159e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.0159e-05\n",
            "Epoch 90/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.1241e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.1241e-05 - val_loss: 1.0128e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.0128e-05\n",
            "Epoch 91/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.1072e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.1072e-05 - val_loss: 1.0113e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.0113e-05\n",
            "Epoch 92/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.0905e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.0905e-05 - val_loss: 1.0115e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.0115e-05\n",
            "Epoch 93/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.0739e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.0739e-05 - val_loss: 1.0135e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.0135e-05\n",
            "Epoch 94/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.0574e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.0574e-05 - val_loss: 1.0172e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.0172e-05\n",
            "Epoch 95/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.0410e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.0410e-05 - val_loss: 1.0225e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.0225e-05\n",
            "Epoch 96/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.0247e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.0247e-05 - val_loss: 1.0295e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.0295e-05\n",
            "Epoch 97/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.0086e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.0086e-05 - val_loss: 1.0381e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.0381e-05\n",
            "Epoch 98/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 9.9251e-06 - accuracy: 0.0000e+00 - mean_squared_error: 9.9251e-06 - val_loss: 1.0481e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.0481e-05\n",
            "Epoch 99/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 9.7657e-06 - accuracy: 0.0000e+00 - mean_squared_error: 9.7657e-06 - val_loss: 1.0596e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.0596e-05\n",
            "Epoch 100/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 9.6075e-06 - accuracy: 0.0000e+00 - mean_squared_error: 9.6075e-06 - val_loss: 1.0722e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.0722e-05\n",
            "8/8 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:00:59.059640: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(0.85048723, shape=(), dtype=float32)\n",
            "[0.85117614]\n",
            "tf.Tensor(0.97012335, shape=(), dtype=float32)\n",
            "[0.9683809]\n",
            "tf.Tensor(0.68311346, shape=(), dtype=float32)\n",
            "[0.6781809]\n",
            "tf.Tensor(0.8307189, shape=(), dtype=float32)\n",
            "[0.830904]\n",
            "0.0027387468 0.17746772\n",
            "64.798874\n",
            "1.0615789473684212\n",
            "/device:GPU:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:00:59.059898: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:00:59.060060: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:00:59.060261: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:00:59.060421: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:00:59.060563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n",
            "100%|██████████| 196/196 [01:55<00:00,  1.70it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rho (InputLayer)            [(None, 8, 8, 1)]         0         \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 64)                0         \n",
            "                                                                 \n",
            " concatenate_3 (Concatenate  (None, 64)                0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 16)                176       \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 32)                544       \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1403 (5.48 KB)\n",
            "Trainable params: 1403 (5.48 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "2024-03-12 17:02:54.672798: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:02:54.673110: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:02:54.673295: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:02:54.673544: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:02:54.673750: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:02:54.673908: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    142/Unknown - 2s 3ms/step - loss: 2.1155 - accuracy: 0.0000e+00 - mean_squared_error: 2.1155"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:02:56.594654: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 17:02:56.594713: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 2s 6ms/step - loss: 1.9254 - accuracy: 0.0000e+00 - mean_squared_error: 1.9254 - val_loss: 0.0260 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0260\n",
            "Epoch 2/100\n",
            " 48/157 [========>.....................] - ETA: 0s - loss: 0.0221 - accuracy: 0.0000e+00 - mean_squared_error: 0.0221"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:02:57.037410: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0212 - accuracy: 0.0000e+00 - mean_squared_error: 0.0212 - val_loss: 0.0212 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0212\n",
            "Epoch 3/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0212 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0212\n",
            "Epoch 4/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0212 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0212\n",
            "Epoch 5/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0212 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0212\n",
            "Epoch 6/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0212 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0212\n",
            "Epoch 7/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 8/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 9/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 10/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 11/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 12/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 13/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 14/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 15/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - mean_squared_error: 0.0205 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 16/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - mean_squared_error: 0.0205 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 17/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - mean_squared_error: 0.0204 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 18/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0203 - accuracy: 0.0000e+00 - mean_squared_error: 0.0203 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 19/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0202 - accuracy: 0.0000e+00 - mean_squared_error: 0.0202 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 20/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0200 - accuracy: 0.0000e+00 - mean_squared_error: 0.0200 - val_loss: 0.0202 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0202\n",
            "Epoch 21/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0197 - accuracy: 0.0000e+00 - mean_squared_error: 0.0197 - val_loss: 0.0199 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0199\n",
            "Epoch 22/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0194 - accuracy: 0.0000e+00 - mean_squared_error: 0.0194 - val_loss: 0.0196 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0196\n",
            "Epoch 23/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0190 - accuracy: 0.0000e+00 - mean_squared_error: 0.0190 - val_loss: 0.0191 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0191\n",
            "Epoch 24/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0184 - accuracy: 0.0000e+00 - mean_squared_error: 0.0184 - val_loss: 0.0184 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0184\n",
            "Epoch 25/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.0176 - accuracy: 0.0000e+00 - mean_squared_error: 0.0176 - val_loss: 0.0175 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0175\n",
            "Epoch 26/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0165 - accuracy: 0.0000e+00 - mean_squared_error: 0.0165 - val_loss: 0.0161 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0161\n",
            "Epoch 27/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0148 - accuracy: 0.0000e+00 - mean_squared_error: 0.0148 - val_loss: 0.0142 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0142\n",
            "Epoch 28/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0125 - accuracy: 0.0000e+00 - mean_squared_error: 0.0125 - val_loss: 0.0114 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0114\n",
            "Epoch 29/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0095 - accuracy: 0.0000e+00 - mean_squared_error: 0.0095 - val_loss: 0.0079 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0079\n",
            "Epoch 30/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0059 - accuracy: 0.0000e+00 - mean_squared_error: 0.0059 - val_loss: 0.0043 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0043\n",
            "Epoch 31/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0028 - accuracy: 0.0000e+00 - mean_squared_error: 0.0028 - val_loss: 0.0017 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0017\n",
            "Epoch 32/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 9.5250e-04 - accuracy: 0.0000e+00 - mean_squared_error: 9.5250e-04 - val_loss: 4.7525e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.7525e-04\n",
            "Epoch 33/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.6222e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.6222e-04 - val_loss: 1.4055e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.4055e-04\n",
            "Epoch 34/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.0006e-04 - accuracy: 0.0000e+00 - mean_squared_error: 1.0006e-04 - val_loss: 7.9697e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 7.9697e-05\n",
            "Epoch 35/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 7.0707e-05 - accuracy: 0.0000e+00 - mean_squared_error: 7.0707e-05 - val_loss: 6.7175e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 6.7175e-05\n",
            "Epoch 36/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 6.2306e-05 - accuracy: 0.0000e+00 - mean_squared_error: 6.2306e-05 - val_loss: 6.0433e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 6.0433e-05\n",
            "Epoch 37/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.6424e-05 - accuracy: 0.0000e+00 - mean_squared_error: 5.6424e-05 - val_loss: 5.4617e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 5.4617e-05\n",
            "Epoch 38/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.1058e-05 - accuracy: 0.0000e+00 - mean_squared_error: 5.1058e-05 - val_loss: 4.9292e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.9292e-05\n",
            "Epoch 39/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.6071e-05 - accuracy: 0.0000e+00 - mean_squared_error: 4.6071e-05 - val_loss: 4.4343e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.4343e-05\n",
            "Epoch 40/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.1455e-05 - accuracy: 0.0000e+00 - mean_squared_error: 4.1455e-05 - val_loss: 3.9765e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.9765e-05\n",
            "Epoch 41/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.7202e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.7202e-05 - val_loss: 3.5602e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.5602e-05\n",
            "Epoch 42/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.3303e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.3303e-05 - val_loss: 3.1872e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.1872e-05\n",
            "Epoch 43/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.9746e-05 - accuracy: 0.0000e+00 - mean_squared_error: 2.9746e-05 - val_loss: 2.8608e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.8608e-05\n",
            "Epoch 44/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.6508e-05 - accuracy: 0.0000e+00 - mean_squared_error: 2.6508e-05 - val_loss: 2.5700e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.5700e-05\n",
            "Epoch 45/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.3567e-05 - accuracy: 0.0000e+00 - mean_squared_error: 2.3567e-05 - val_loss: 2.2733e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.2733e-05\n",
            "Epoch 46/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.0919e-05 - accuracy: 0.0000e+00 - mean_squared_error: 2.0919e-05 - val_loss: 1.9835e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.9835e-05\n",
            "Epoch 47/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.8543e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.8543e-05 - val_loss: 1.7412e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.7412e-05\n",
            "Epoch 48/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.6410e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.6410e-05 - val_loss: 1.5349e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.5349e-05\n",
            "Epoch 49/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.4455e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.4455e-05 - val_loss: 1.3491e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.3491e-05\n",
            "Epoch 50/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.2683e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.2683e-05 - val_loss: 1.1852e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.1852e-05\n",
            "Epoch 51/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.1107e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.1107e-05 - val_loss: 1.0372e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.0372e-05\n",
            "Epoch 52/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 9.7148e-06 - accuracy: 0.0000e+00 - mean_squared_error: 9.7148e-06 - val_loss: 9.0659e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 9.0659e-06\n",
            "Epoch 53/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 8.4793e-06 - accuracy: 0.0000e+00 - mean_squared_error: 8.4793e-06 - val_loss: 7.9184e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 7.9184e-06\n",
            "Epoch 54/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 7.4099e-06 - accuracy: 0.0000e+00 - mean_squared_error: 7.4099e-06 - val_loss: 6.9364e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 6.9364e-06\n",
            "Epoch 55/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 6.4835e-06 - accuracy: 0.0000e+00 - mean_squared_error: 6.4835e-06 - val_loss: 6.0116e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 6.0116e-06\n",
            "Epoch 56/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.6803e-06 - accuracy: 0.0000e+00 - mean_squared_error: 5.6803e-06 - val_loss: 5.4186e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 5.4186e-06\n",
            "Epoch 57/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.0137e-06 - accuracy: 0.0000e+00 - mean_squared_error: 5.0137e-06 - val_loss: 5.2412e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 5.2412e-06\n",
            "Epoch 58/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.4130e-06 - accuracy: 0.0000e+00 - mean_squared_error: 4.4130e-06 - val_loss: 4.7213e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.7213e-06\n",
            "Epoch 59/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.8558e-06 - accuracy: 0.0000e+00 - mean_squared_error: 3.8558e-06 - val_loss: 3.8160e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.8160e-06\n",
            "Epoch 60/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.3929e-06 - accuracy: 0.0000e+00 - mean_squared_error: 3.3929e-06 - val_loss: 3.1344e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.1344e-06\n",
            "Epoch 61/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.0519e-06 - accuracy: 0.0000e+00 - mean_squared_error: 3.0519e-06 - val_loss: 2.8327e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.8327e-06\n",
            "Epoch 62/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.8107e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.8107e-06 - val_loss: 2.5005e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.5005e-06\n",
            "Epoch 63/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.5235e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.5235e-06 - val_loss: 2.2709e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.2709e-06\n",
            "Epoch 64/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.2740e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.2740e-06 - val_loss: 2.1029e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.1029e-06\n",
            "Epoch 65/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.0599e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.0599e-06 - val_loss: 2.0519e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.0519e-06\n",
            "Epoch 66/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.9163e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.9163e-06 - val_loss: 1.8392e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.8392e-06\n",
            "Epoch 67/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.8123e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.8123e-06 - val_loss: 1.9436e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.9436e-06\n",
            "Epoch 68/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.7738e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.7738e-06 - val_loss: 1.7323e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.7323e-06\n",
            "Epoch 69/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.6916e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.6916e-06 - val_loss: 1.6339e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.6339e-06\n",
            "Epoch 70/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.7374e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.7374e-06 - val_loss: 1.5404e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.5404e-06\n",
            "Epoch 71/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.7971e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.7971e-06 - val_loss: 1.5122e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.5122e-06\n",
            "Epoch 72/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.7674e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.7674e-06 - val_loss: 1.5361e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.5361e-06\n",
            "Epoch 73/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.6588e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.6588e-06 - val_loss: 1.4656e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.4656e-06\n",
            "Epoch 74/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.6560e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.6560e-06 - val_loss: 1.7710e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.7710e-06\n",
            "Epoch 75/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.7937e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.7937e-06 - val_loss: 2.4531e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.4531e-06\n",
            "Epoch 76/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.8616e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.8616e-06 - val_loss: 2.8313e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.8313e-06\n",
            "Epoch 77/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.8949e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.8949e-06 - val_loss: 3.8206e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.8206e-06\n",
            "Epoch 78/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.9796e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.9796e-06 - val_loss: 3.4334e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.4334e-06\n",
            "Epoch 79/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.0168e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.0168e-06 - val_loss: 2.6457e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.6457e-06\n",
            "Epoch 80/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.9647e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.9647e-06 - val_loss: 2.4259e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.4259e-06\n",
            "Epoch 81/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.0183e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.0183e-06 - val_loss: 2.4594e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.4594e-06\n",
            "Epoch 82/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.1354e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.1354e-06 - val_loss: 2.1462e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.1462e-06\n",
            "Epoch 83/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.2054e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.2054e-06 - val_loss: 1.8477e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.8477e-06\n",
            "Epoch 84/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.2437e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.2437e-06 - val_loss: 1.6821e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.6821e-06\n",
            "Epoch 85/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 2.2717e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.2717e-06 - val_loss: 1.5917e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.5917e-06\n",
            "Epoch 86/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.2934e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.2934e-06 - val_loss: 1.5386e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.5386e-06\n",
            "Epoch 87/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.3098e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.3098e-06 - val_loss: 1.5041e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.5041e-06\n",
            "Epoch 88/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.3216e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.3216e-06 - val_loss: 1.4793e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.4793e-06\n",
            "Epoch 89/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.3312e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.3312e-06 - val_loss: 1.4614e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.4614e-06\n",
            "Epoch 90/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.3375e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.3375e-06 - val_loss: 1.4475e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.4475e-06\n",
            "Epoch 91/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.3427e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.3427e-06 - val_loss: 1.4370e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.4370e-06\n",
            "Epoch 92/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.3460e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.3460e-06 - val_loss: 1.4279e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.4279e-06\n",
            "Epoch 93/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.3483e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.3483e-06 - val_loss: 1.4207e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.4207e-06\n",
            "Epoch 94/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.3496e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.3496e-06 - val_loss: 1.4145e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.4145e-06\n",
            "Epoch 95/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.3499e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.3499e-06 - val_loss: 1.4093e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.4093e-06\n",
            "Epoch 96/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.3501e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.3501e-06 - val_loss: 1.4044e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.4044e-06\n",
            "Epoch 97/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.3496e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.3496e-06 - val_loss: 1.4000e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.4000e-06\n",
            "Epoch 98/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.3486e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.3486e-06 - val_loss: 1.3960e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.3960e-06\n",
            "Epoch 99/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.3477e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.3477e-06 - val_loss: 1.3923e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.3923e-06\n",
            "Epoch 100/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.3462e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.3462e-06 - val_loss: 1.3890e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.3890e-06\n",
            "8/8 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:04:15.678691: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:04:15.678947: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:04:15.679116: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:04:15.679316: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:04:15.679476: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:04:15.679610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(1.1377666, shape=(), dtype=float32)\n",
            "[1.1370573]\n",
            "tf.Tensor(1.32209, shape=(), dtype=float32)\n",
            "[1.3221892]\n",
            "tf.Tensor(1.4103558, shape=(), dtype=float32)\n",
            "[1.4115281]\n",
            "tf.Tensor(1.3767936, shape=(), dtype=float32)\n",
            "[1.3777275]\n",
            "0.0010052831 0.17870386\n",
            "177.76471\n",
            "1.5873684210526318\n",
            "/device:GPU:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 196/196 [01:53<00:00,  1.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rho (InputLayer)            [(None, 8, 8, 1)]         0         \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 64)                0         \n",
            "                                                                 \n",
            " concatenate_4 (Concatenate  (None, 64)                0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 16)                176       \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 32)                544       \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1403 (5.48 KB)\n",
            "Trainable params: 1403 (5.48 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "2024-03-12 17:06:08.861090: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:06:08.861389: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:06:08.861559: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:06:08.861768: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:06:08.861935: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:06:08.862081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    149/Unknown - 2s 3ms/step - loss: 0.3898 - accuracy: 0.0000e+00 - mean_squared_error: 0.3898"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:06:10.459708: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 17:06:10.459796: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n",
            "2024-03-12 17:06:10.459812: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 1353796229350139122\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 2s 6ms/step - loss: 0.3727 - accuracy: 0.0000e+00 - mean_squared_error: 0.3727 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 2/100\n",
            " 52/157 [========>.....................] - ETA: 0s - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:06:10.867204: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 17:06:10.867263: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 3/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 4/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 5/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 6/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 7/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 8/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 9/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 10/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 11/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 12/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 13/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 14/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 15/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 16/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - mean_squared_error: 0.0205 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 17/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - mean_squared_error: 0.0204 - val_loss: 0.0205 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0205\n",
            "Epoch 18/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0203 - accuracy: 0.0000e+00 - mean_squared_error: 0.0203 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 19/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0202 - accuracy: 0.0000e+00 - mean_squared_error: 0.0202 - val_loss: 0.0203 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0203\n",
            "Epoch 20/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0200 - accuracy: 0.0000e+00 - mean_squared_error: 0.0200 - val_loss: 0.0201 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0201\n",
            "Epoch 21/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0198 - accuracy: 0.0000e+00 - mean_squared_error: 0.0198 - val_loss: 0.0197 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0197\n",
            "Epoch 22/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0194 - accuracy: 0.0000e+00 - mean_squared_error: 0.0194 - val_loss: 0.0192 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0192\n",
            "Epoch 23/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0188 - accuracy: 0.0000e+00 - mean_squared_error: 0.0188 - val_loss: 0.0185 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0185\n",
            "Epoch 24/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0179 - accuracy: 0.0000e+00 - mean_squared_error: 0.0179 - val_loss: 0.0174 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0174\n",
            "Epoch 25/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0165 - accuracy: 0.0000e+00 - mean_squared_error: 0.0165 - val_loss: 0.0156 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0156\n",
            "Epoch 26/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0143 - accuracy: 0.0000e+00 - mean_squared_error: 0.0143 - val_loss: 0.0130 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0130\n",
            "Epoch 27/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0112 - accuracy: 0.0000e+00 - mean_squared_error: 0.0112 - val_loss: 0.0093 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0093\n",
            "Epoch 28/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0073 - accuracy: 0.0000e+00 - mean_squared_error: 0.0073 - val_loss: 0.0052 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0052\n",
            "Epoch 29/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0034 - accuracy: 0.0000e+00 - mean_squared_error: 0.0034 - val_loss: 0.0019 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0019\n",
            "Epoch 30/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0010 - accuracy: 0.0000e+00 - mean_squared_error: 0.0010 - val_loss: 4.3185e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.3185e-04\n",
            "Epoch 31/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.2172e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.2172e-04 - val_loss: 9.9928e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 9.9928e-05\n",
            "Epoch 32/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 7.2595e-05 - accuracy: 0.0000e+00 - mean_squared_error: 7.2595e-05 - val_loss: 5.8122e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 5.8122e-05\n",
            "Epoch 33/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.4537e-05 - accuracy: 0.0000e+00 - mean_squared_error: 5.4537e-05 - val_loss: 5.2934e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 5.2934e-05\n",
            "Epoch 34/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.0484e-05 - accuracy: 0.0000e+00 - mean_squared_error: 5.0484e-05 - val_loss: 4.9274e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.9274e-05\n",
            "Epoch 35/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.7396e-05 - accuracy: 0.0000e+00 - mean_squared_error: 4.7396e-05 - val_loss: 4.6353e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.6353e-05\n",
            "Epoch 36/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.4404e-05 - accuracy: 0.0000e+00 - mean_squared_error: 4.4404e-05 - val_loss: 4.3656e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.3656e-05\n",
            "Epoch 37/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.1514e-05 - accuracy: 0.0000e+00 - mean_squared_error: 4.1514e-05 - val_loss: 4.0568e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.0568e-05\n",
            "Epoch 38/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.8704e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.8704e-05 - val_loss: 3.7396e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.7396e-05\n",
            "Epoch 39/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.6030e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.6030e-05 - val_loss: 3.4474e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.4474e-05\n",
            "Epoch 40/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.3519e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.3519e-05 - val_loss: 3.1814e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.1814e-05\n",
            "Epoch 41/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.1172e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.1172e-05 - val_loss: 2.9553e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.9553e-05\n",
            "Epoch 42/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.9042e-05 - accuracy: 0.0000e+00 - mean_squared_error: 2.9042e-05 - val_loss: 2.7869e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.7869e-05\n",
            "Epoch 43/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.6967e-05 - accuracy: 0.0000e+00 - mean_squared_error: 2.6967e-05 - val_loss: 2.6692e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.6692e-05\n",
            "Epoch 44/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.4956e-05 - accuracy: 0.0000e+00 - mean_squared_error: 2.4956e-05 - val_loss: 2.4460e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.4460e-05\n",
            "Epoch 45/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.3048e-05 - accuracy: 0.0000e+00 - mean_squared_error: 2.3048e-05 - val_loss: 2.1223e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.1223e-05\n",
            "Epoch 46/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.1068e-05 - accuracy: 0.0000e+00 - mean_squared_error: 2.1068e-05 - val_loss: 1.9807e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.9807e-05\n",
            "Epoch 47/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.9317e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.9317e-05 - val_loss: 1.8942e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.8942e-05\n",
            "Epoch 48/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.7813e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.7813e-05 - val_loss: 1.7510e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.7510e-05\n",
            "Epoch 49/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.6334e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.6334e-05 - val_loss: 1.5363e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.5363e-05\n",
            "Epoch 50/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.4802e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.4802e-05 - val_loss: 1.3614e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.3614e-05\n",
            "Epoch 51/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.3468e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.3468e-05 - val_loss: 1.2268e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.2268e-05\n",
            "Epoch 52/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.2246e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.2246e-05 - val_loss: 1.1137e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.1137e-05\n",
            "Epoch 53/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.1226e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.1226e-05 - val_loss: 1.0212e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.0212e-05\n",
            "Epoch 54/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.0249e-05 - accuracy: 0.0000e+00 - mean_squared_error: 1.0249e-05 - val_loss: 9.7041e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 9.7041e-06\n",
            "Epoch 55/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 9.3798e-06 - accuracy: 0.0000e+00 - mean_squared_error: 9.3798e-06 - val_loss: 1.1024e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.1024e-05\n",
            "Epoch 56/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 8.6442e-06 - accuracy: 0.0000e+00 - mean_squared_error: 8.6442e-06 - val_loss: 1.0731e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.0731e-05\n",
            "Epoch 57/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 7.8701e-06 - accuracy: 0.0000e+00 - mean_squared_error: 7.8701e-06 - val_loss: 9.0009e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 9.0009e-06\n",
            "Epoch 58/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 7.1612e-06 - accuracy: 0.0000e+00 - mean_squared_error: 7.1612e-06 - val_loss: 1.0390e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.0390e-05\n",
            "Epoch 59/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 6.6227e-06 - accuracy: 0.0000e+00 - mean_squared_error: 6.6227e-06 - val_loss: 9.2088e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 9.2088e-06\n",
            "Epoch 60/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.8148e-06 - accuracy: 0.0000e+00 - mean_squared_error: 5.8148e-06 - val_loss: 5.9506e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 5.9506e-06\n",
            "Epoch 61/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.2658e-06 - accuracy: 0.0000e+00 - mean_squared_error: 5.2658e-06 - val_loss: 4.4113e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.4113e-06\n",
            "Epoch 62/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.8604e-06 - accuracy: 0.0000e+00 - mean_squared_error: 4.8604e-06 - val_loss: 4.3073e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.3073e-06\n",
            "Epoch 63/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.4061e-06 - accuracy: 0.0000e+00 - mean_squared_error: 4.4061e-06 - val_loss: 5.5508e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 5.5508e-06\n",
            "Epoch 64/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.0340e-06 - accuracy: 0.0000e+00 - mean_squared_error: 4.0340e-06 - val_loss: 4.0621e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.0621e-06\n",
            "Epoch 65/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.5855e-06 - accuracy: 0.0000e+00 - mean_squared_error: 3.5855e-06 - val_loss: 3.7769e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.7769e-06\n",
            "Epoch 66/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.3476e-06 - accuracy: 0.0000e+00 - mean_squared_error: 3.3476e-06 - val_loss: 3.4450e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.4450e-06\n",
            "Epoch 67/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.2436e-06 - accuracy: 0.0000e+00 - mean_squared_error: 3.2436e-06 - val_loss: 5.3120e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 5.3120e-06\n",
            "Epoch 68/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.2377e-06 - accuracy: 0.0000e+00 - mean_squared_error: 3.2377e-06 - val_loss: 3.7842e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.7842e-06\n",
            "Epoch 69/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.2020e-06 - accuracy: 0.0000e+00 - mean_squared_error: 3.2020e-06 - val_loss: 2.9043e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.9043e-06\n",
            "Epoch 70/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.0491e-06 - accuracy: 0.0000e+00 - mean_squared_error: 3.0491e-06 - val_loss: 2.2219e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.2219e-06\n",
            "Epoch 71/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.8265e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.8265e-06 - val_loss: 1.7477e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.7477e-06\n",
            "Epoch 72/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.4109e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.4109e-06 - val_loss: 1.9828e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.9828e-06\n",
            "Epoch 73/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.3430e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.3430e-06 - val_loss: 1.8244e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.8244e-06\n",
            "Epoch 74/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.5221e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.5221e-06 - val_loss: 1.6463e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.6463e-06\n",
            "Epoch 75/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.4400e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.4400e-06 - val_loss: 1.4895e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.4895e-06\n",
            "Epoch 76/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.3601e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.3601e-06 - val_loss: 1.3406e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.3406e-06\n",
            "Epoch 77/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.2834e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.2834e-06 - val_loss: 1.1873e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.1873e-06\n",
            "Epoch 78/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.2093e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.2093e-06 - val_loss: 1.0346e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.0346e-06\n",
            "Epoch 79/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.1258e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.1258e-06 - val_loss: 9.0927e-07 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 9.0927e-07\n",
            "Epoch 80/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.0242e-06 - accuracy: 0.0000e+00 - mean_squared_error: 2.0242e-06 - val_loss: 9.0576e-07 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 9.0576e-07\n",
            "Epoch 81/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.9481e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.9481e-06 - val_loss: 9.3685e-07 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 9.3685e-07\n",
            "Epoch 82/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.9675e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.9675e-06 - val_loss: 8.1559e-07 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 8.1559e-07\n",
            "Epoch 83/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.9468e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.9468e-06 - val_loss: 7.8630e-07 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 7.8630e-07\n",
            "Epoch 84/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.9049e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.9049e-06 - val_loss: 7.2583e-07 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 7.2583e-07\n",
            "Epoch 85/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.8767e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.8767e-06 - val_loss: 6.8924e-07 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 6.8924e-07\n",
            "Epoch 86/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.8467e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.8467e-06 - val_loss: 6.4764e-07 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 6.4764e-07\n",
            "Epoch 87/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.8206e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.8206e-06 - val_loss: 6.1266e-07 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 6.1266e-07\n",
            "Epoch 88/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.7965e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.7965e-06 - val_loss: 5.8049e-07 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 5.8049e-07\n",
            "Epoch 89/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.7747e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.7747e-06 - val_loss: 5.5608e-07 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 5.5608e-07\n",
            "Epoch 90/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.7534e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.7534e-06 - val_loss: 5.3814e-07 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 5.3814e-07\n",
            "Epoch 91/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.7355e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.7355e-06 - val_loss: 5.3227e-07 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 5.3227e-07\n",
            "Epoch 92/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.7165e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.7165e-06 - val_loss: 5.5073e-07 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 5.5073e-07\n",
            "Epoch 93/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.6978e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.6978e-06 - val_loss: 6.1931e-07 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 6.1931e-07\n",
            "Epoch 94/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.6810e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.6810e-06 - val_loss: 7.7835e-07 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 7.7835e-07\n",
            "Epoch 95/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.6655e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.6655e-06 - val_loss: 1.0514e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.0514e-06\n",
            "Epoch 96/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.6514e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.6514e-06 - val_loss: 1.4312e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.4312e-06\n",
            "Epoch 97/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.6426e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.6426e-06 - val_loss: 1.8159e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.8159e-06\n",
            "Epoch 98/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.6311e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.6311e-06 - val_loss: 2.2215e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.2215e-06\n",
            "Epoch 99/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.6258e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.6258e-06 - val_loss: 2.5566e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.5566e-06\n",
            "Epoch 100/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.6152e-06 - accuracy: 0.0000e+00 - mean_squared_error: 1.6152e-06 - val_loss: 2.8893e-06 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.8893e-06\n",
            "8/8 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:07:28.633052: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:07:28.633355: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:07:28.633533: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:07:28.633750: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(1.7816738, shape=(), dtype=float32)\n",
            "[1.7798456]\n",
            "tf.Tensor(1.7721716, shape=(), dtype=float32)\n",
            "[1.7702669]\n",
            "tf.Tensor(1.6252642, shape=(), dtype=float32)\n",
            "[1.6238693]\n",
            "tf.Tensor(2.019789, shape=(), dtype=float32)\n",
            "[2.0181742]\n",
            "0.0016306015 0.16461052\n",
            "100.9508\n",
            "2.113157894736842\n",
            "/device:GPU:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "L355\n",
            "2024-03-12 17:07:28.633926: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:07:28.634082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n",
            "100%|██████████| 196/196 [01:54<00:00,  1.72it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rho (InputLayer)            [(None, 8, 8, 1)]         0         \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 64)                0         \n",
            "                                                                 \n",
            " concatenate_5 (Concatenate  (None, 64)                0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 16)                176       \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 32)                544       \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1403 (5.48 KB)\n",
            "Trainable params: 1403 (5.48 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "2024-03-12 17:09:22.752729: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:09:22.753055: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:09:22.753243: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:09:22.753479: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:09:22.753639: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:09:22.753771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    151/Unknown - 2s 3ms/step - loss: 1.5316 - accuracy: 0.0000e+00 - mean_squared_error: 1.5316"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:09:24.341948: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 17:09:24.342033: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n",
            "2024-03-12 17:09:24.342049: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 1353796229350139122\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 2s 6ms/step - loss: 1.4809 - accuracy: 0.0000e+00 - mean_squared_error: 1.4809 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 2/100\n",
            " 50/157 [========>.....................] - ETA: 0s - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:09:24.739264: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 17:09:24.739319: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 3/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 4/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 5/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 6/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 7/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 8/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 9/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 10/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 11/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 12/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 13/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 14/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 15/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 16/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 17/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 18/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 19/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 20/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 21/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 22/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 23/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 24/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 25/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 26/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - mean_squared_error: 0.0205 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 27/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - mean_squared_error: 0.0204 - val_loss: 0.0205 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0205\n",
            "Epoch 28/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - mean_squared_error: 0.0204 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 29/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0203 - accuracy: 0.0000e+00 - mean_squared_error: 0.0203 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 30/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0202 - accuracy: 0.0000e+00 - mean_squared_error: 0.0202 - val_loss: 0.0203 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0203\n",
            "Epoch 31/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0201 - accuracy: 0.0000e+00 - mean_squared_error: 0.0201 - val_loss: 0.0201 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0201\n",
            "Epoch 32/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0199 - accuracy: 0.0000e+00 - mean_squared_error: 0.0199 - val_loss: 0.0200 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0200\n",
            "Epoch 33/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0198 - accuracy: 0.0000e+00 - mean_squared_error: 0.0198 - val_loss: 0.0198 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0198\n",
            "Epoch 34/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0196 - accuracy: 0.0000e+00 - mean_squared_error: 0.0196 - val_loss: 0.0195 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0195\n",
            "Epoch 35/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0193 - accuracy: 0.0000e+00 - mean_squared_error: 0.0193 - val_loss: 0.0193 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0193\n",
            "Epoch 36/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0190 - accuracy: 0.0000e+00 - mean_squared_error: 0.0190 - val_loss: 0.0189 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0189\n",
            "Epoch 37/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0187 - accuracy: 0.0000e+00 - mean_squared_error: 0.0187 - val_loss: 0.0185 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0185\n",
            "Epoch 38/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0182 - accuracy: 0.0000e+00 - mean_squared_error: 0.0182 - val_loss: 0.0180 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0180\n",
            "Epoch 39/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0177 - accuracy: 0.0000e+00 - mean_squared_error: 0.0177 - val_loss: 0.0174 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0174\n",
            "Epoch 40/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0170 - accuracy: 0.0000e+00 - mean_squared_error: 0.0170 - val_loss: 0.0167 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0167\n",
            "Epoch 41/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0162 - accuracy: 0.0000e+00 - mean_squared_error: 0.0162 - val_loss: 0.0157 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0157\n",
            "Epoch 42/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0151 - accuracy: 0.0000e+00 - mean_squared_error: 0.0151 - val_loss: 0.0145 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0145\n",
            "Epoch 43/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0136 - accuracy: 0.0000e+00 - mean_squared_error: 0.0136 - val_loss: 0.0128 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0128\n",
            "Epoch 44/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0118 - accuracy: 0.0000e+00 - mean_squared_error: 0.0118 - val_loss: 0.0107 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0107\n",
            "Epoch 45/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0096 - accuracy: 0.0000e+00 - mean_squared_error: 0.0096 - val_loss: 0.0083 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0083\n",
            "Epoch 46/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0070 - accuracy: 0.0000e+00 - mean_squared_error: 0.0070 - val_loss: 0.0057 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0057\n",
            "Epoch 47/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0045 - accuracy: 0.0000e+00 - mean_squared_error: 0.0045 - val_loss: 0.0034 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0034\n",
            "Epoch 48/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0025 - accuracy: 0.0000e+00 - mean_squared_error: 0.0025 - val_loss: 0.0017 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0017\n",
            "Epoch 49/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0011 - accuracy: 0.0000e+00 - mean_squared_error: 0.0011 - val_loss: 6.5982e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 6.5982e-04\n",
            "Epoch 50/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.9207e-04 - accuracy: 0.0000e+00 - mean_squared_error: 3.9207e-04 - val_loss: 2.1861e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.1861e-04\n",
            "Epoch 51/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.3873e-04 - accuracy: 0.0000e+00 - mean_squared_error: 1.3873e-04 - val_loss: 8.7975e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 8.7975e-05\n",
            "Epoch 52/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 7.0267e-05 - accuracy: 0.0000e+00 - mean_squared_error: 7.0267e-05 - val_loss: 6.1147e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 6.1147e-05\n",
            "Epoch 53/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.6068e-05 - accuracy: 0.0000e+00 - mean_squared_error: 5.6068e-05 - val_loss: 5.6765e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 5.6765e-05\n",
            "Epoch 54/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.3249e-05 - accuracy: 0.0000e+00 - mean_squared_error: 5.3249e-05 - val_loss: 5.3077e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 5.3077e-05\n",
            "Epoch 55/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.2530e-05 - accuracy: 0.0000e+00 - mean_squared_error: 5.2530e-05 - val_loss: 5.2739e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 5.2739e-05\n",
            "Epoch 56/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.1994e-05 - accuracy: 0.0000e+00 - mean_squared_error: 5.1994e-05 - val_loss: 5.0337e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 5.0337e-05\n",
            "Epoch 57/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.1531e-05 - accuracy: 0.0000e+00 - mean_squared_error: 5.1531e-05 - val_loss: 4.9545e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.9545e-05\n",
            "Epoch 58/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.1167e-05 - accuracy: 0.0000e+00 - mean_squared_error: 5.1167e-05 - val_loss: 4.9439e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.9439e-05\n",
            "Epoch 59/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.0907e-05 - accuracy: 0.0000e+00 - mean_squared_error: 5.0907e-05 - val_loss: 4.8496e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.8496e-05\n",
            "Epoch 60/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.0946e-05 - accuracy: 0.0000e+00 - mean_squared_error: 5.0946e-05 - val_loss: 4.7970e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.7970e-05\n",
            "Epoch 61/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.0297e-05 - accuracy: 0.0000e+00 - mean_squared_error: 5.0297e-05 - val_loss: 4.8519e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.8519e-05\n",
            "Epoch 62/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.9741e-05 - accuracy: 0.0000e+00 - mean_squared_error: 4.9741e-05 - val_loss: 5.2795e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 5.2795e-05\n",
            "Epoch 63/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.0060e-05 - accuracy: 0.0000e+00 - mean_squared_error: 5.0060e-05 - val_loss: 6.0654e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 6.0654e-05\n",
            "Epoch 64/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.0434e-05 - accuracy: 0.0000e+00 - mean_squared_error: 5.0434e-05 - val_loss: 6.3492e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 6.3492e-05\n",
            "Epoch 65/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.0625e-05 - accuracy: 0.0000e+00 - mean_squared_error: 5.0625e-05 - val_loss: 5.9497e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 5.9497e-05\n",
            "Epoch 66/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.0444e-05 - accuracy: 0.0000e+00 - mean_squared_error: 5.0444e-05 - val_loss: 5.3917e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 5.3917e-05\n",
            "Epoch 67/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.9889e-05 - accuracy: 0.0000e+00 - mean_squared_error: 4.9889e-05 - val_loss: 4.8591e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.8591e-05\n",
            "Epoch 68/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.8835e-05 - accuracy: 0.0000e+00 - mean_squared_error: 4.8835e-05 - val_loss: 4.4706e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.4706e-05\n",
            "Epoch 69/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.7224e-05 - accuracy: 0.0000e+00 - mean_squared_error: 4.7224e-05 - val_loss: 4.2977e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.2977e-05\n",
            "Epoch 70/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.5578e-05 - accuracy: 0.0000e+00 - mean_squared_error: 4.5578e-05 - val_loss: 4.2625e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.2625e-05\n",
            "Epoch 71/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.4788e-05 - accuracy: 0.0000e+00 - mean_squared_error: 4.4788e-05 - val_loss: 4.3103e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.3103e-05\n",
            "Epoch 72/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.5080e-05 - accuracy: 0.0000e+00 - mean_squared_error: 4.5080e-05 - val_loss: 4.3671e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.3671e-05\n",
            "Epoch 73/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.5466e-05 - accuracy: 0.0000e+00 - mean_squared_error: 4.5466e-05 - val_loss: 4.4008e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.4008e-05\n",
            "Epoch 74/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.5390e-05 - accuracy: 0.0000e+00 - mean_squared_error: 4.5390e-05 - val_loss: 4.4090e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.4090e-05\n",
            "Epoch 75/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.5080e-05 - accuracy: 0.0000e+00 - mean_squared_error: 4.5080e-05 - val_loss: 4.3907e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.3907e-05\n",
            "Epoch 76/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.4656e-05 - accuracy: 0.0000e+00 - mean_squared_error: 4.4656e-05 - val_loss: 4.3619e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.3619e-05\n",
            "Epoch 77/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 4.4188e-05 - accuracy: 0.0000e+00 - mean_squared_error: 4.4188e-05 - val_loss: 4.3279e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.3279e-05\n",
            "Epoch 78/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.3705e-05 - accuracy: 0.0000e+00 - mean_squared_error: 4.3705e-05 - val_loss: 4.2940e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.2940e-05\n",
            "Epoch 79/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.3218e-05 - accuracy: 0.0000e+00 - mean_squared_error: 4.3218e-05 - val_loss: 4.2619e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.2619e-05\n",
            "Epoch 80/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.2738e-05 - accuracy: 0.0000e+00 - mean_squared_error: 4.2738e-05 - val_loss: 4.2324e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.2324e-05\n",
            "Epoch 81/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.2266e-05 - accuracy: 0.0000e+00 - mean_squared_error: 4.2266e-05 - val_loss: 4.2063e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.2063e-05\n",
            "Epoch 82/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.1802e-05 - accuracy: 0.0000e+00 - mean_squared_error: 4.1802e-05 - val_loss: 4.1834e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.1834e-05\n",
            "Epoch 83/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.1351e-05 - accuracy: 0.0000e+00 - mean_squared_error: 4.1351e-05 - val_loss: 4.1616e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.1616e-05\n",
            "Epoch 84/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.0907e-05 - accuracy: 0.0000e+00 - mean_squared_error: 4.0907e-05 - val_loss: 4.1406e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.1406e-05\n",
            "Epoch 85/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.0471e-05 - accuracy: 0.0000e+00 - mean_squared_error: 4.0471e-05 - val_loss: 4.1215e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.1215e-05\n",
            "Epoch 86/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.0046e-05 - accuracy: 0.0000e+00 - mean_squared_error: 4.0046e-05 - val_loss: 4.1028e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.1028e-05\n",
            "Epoch 87/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.9626e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.9626e-05 - val_loss: 4.0840e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.0840e-05\n",
            "Epoch 88/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.9216e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.9216e-05 - val_loss: 4.0667e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.0667e-05\n",
            "Epoch 89/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.8814e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.8814e-05 - val_loss: 4.0485e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.0485e-05\n",
            "Epoch 90/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.8417e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.8417e-05 - val_loss: 4.0317e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.0317e-05\n",
            "Epoch 91/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.8030e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.8030e-05 - val_loss: 4.0144e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.0144e-05\n",
            "Epoch 92/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.7649e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.7649e-05 - val_loss: 3.9994e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.9994e-05\n",
            "Epoch 93/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.7278e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.7278e-05 - val_loss: 3.9832e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.9832e-05\n",
            "Epoch 94/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.6912e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.6912e-05 - val_loss: 3.9682e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.9682e-05\n",
            "Epoch 95/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.6553e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.6553e-05 - val_loss: 3.9551e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.9551e-05\n",
            "Epoch 96/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.6201e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.6201e-05 - val_loss: 3.9408e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.9408e-05\n",
            "Epoch 97/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.5856e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.5856e-05 - val_loss: 3.9301e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.9301e-05\n",
            "Epoch 98/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.5520e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.5520e-05 - val_loss: 3.9173e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.9173e-05\n",
            "Epoch 99/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.5186e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.5186e-05 - val_loss: 3.9066e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.9066e-05\n",
            "Epoch 100/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.4861e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.4861e-05 - val_loss: 3.8969e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.8969e-05\n",
            "8/8 [==============================] - 0s 1ms/step\n",
            "tf.Tensor(2.4150486, shape=(), dtype=float32)\n",
            "[2.417001]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:10:42.877205: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(2.5594592, shape=(), dtype=float32)\n",
            "[2.550703]\n",
            "tf.Tensor(2.4490638, shape=(), dtype=float32)\n",
            "[2.4495628]\n",
            "tf.Tensor(2.5846996, shape=(), dtype=float32)\n",
            "[2.572866]\n",
            "0.0042963354 0.17922544\n",
            "41.71589\n",
            "2.6389473684210527\n",
            "/device:GPU:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:10:42.877467: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:10:42.877647: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:10:42.877845: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:10:42.878012: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:10:42.878146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n",
            "100%|██████████| 196/196 [01:54<00:00,  1.72it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rho (InputLayer)            [(None, 8, 8, 1)]         0         \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 64)                0         \n",
            "                                                                 \n",
            " concatenate_6 (Concatenate  (None, 64)                0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 16)                176       \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 32)                544       \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1403 (5.48 KB)\n",
            "Trainable params: 1403 (5.48 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "2024-03-12 17:12:37.119998: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:12:37.120306: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:12:37.120490: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:12:37.120724: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:12:37.120893: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:12:37.121032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    149/Unknown - 2s 3ms/step - loss: 1.0728 - accuracy: 0.0000e+00 - mean_squared_error: 1.0728"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:12:38.771024: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 17:12:38.771091: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n",
            "2024-03-12 17:12:38.771100: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 407123110838793230\n",
            "2024-03-12 17:12:38.771106: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 1353796229350139122\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 2s 6ms/step - loss: 1.0240 - accuracy: 0.0000e+00 - mean_squared_error: 1.0240 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 2/100\n",
            " 50/157 [========>.....................] - ETA: 0s - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:12:39.185684: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 17:12:39.185741: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 3/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 4/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 5/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 6/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 7/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 8/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 9/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 10/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 11/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 12/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 13/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 14/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 15/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 16/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 17/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 18/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0203 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0203\n",
            "Epoch 19/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0203 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0203\n",
            "Epoch 20/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0203 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0203\n",
            "Epoch 21/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0203 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0203\n",
            "Epoch 22/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0203 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0203\n",
            "Epoch 23/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0203 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0203\n",
            "Epoch 24/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0203 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0203\n",
            "Epoch 25/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0203 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0203\n",
            "Epoch 26/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0202 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0202\n",
            "Epoch 27/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0202 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0202\n",
            "Epoch 28/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0202 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0202\n",
            "Epoch 29/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0201 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0201\n",
            "Epoch 30/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0201 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0201\n",
            "Epoch 31/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0200 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0200\n",
            "Epoch 32/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0200 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0200\n",
            "Epoch 33/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - mean_squared_error: 0.0205 - val_loss: 0.0199 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0199\n",
            "Epoch 34/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - mean_squared_error: 0.0204 - val_loss: 0.0198 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0198\n",
            "Epoch 35/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0203 - accuracy: 0.0000e+00 - mean_squared_error: 0.0203 - val_loss: 0.0197 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0197\n",
            "Epoch 36/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0202 - accuracy: 0.0000e+00 - mean_squared_error: 0.0202 - val_loss: 0.0195 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0195\n",
            "Epoch 37/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0200 - accuracy: 0.0000e+00 - mean_squared_error: 0.0200 - val_loss: 0.0194 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0194\n",
            "Epoch 38/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0198 - accuracy: 0.0000e+00 - mean_squared_error: 0.0198 - val_loss: 0.0192 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0192\n",
            "Epoch 39/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0196 - accuracy: 0.0000e+00 - mean_squared_error: 0.0196 - val_loss: 0.0189 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0189\n",
            "Epoch 40/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0192 - accuracy: 0.0000e+00 - mean_squared_error: 0.0192 - val_loss: 0.0185 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0185\n",
            "Epoch 41/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0188 - accuracy: 0.0000e+00 - mean_squared_error: 0.0188 - val_loss: 0.0181 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0181\n",
            "Epoch 42/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0184 - accuracy: 0.0000e+00 - mean_squared_error: 0.0184 - val_loss: 0.0175 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0175\n",
            "Epoch 43/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0177 - accuracy: 0.0000e+00 - mean_squared_error: 0.0177 - val_loss: 0.0169 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0169\n",
            "Epoch 44/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0170 - accuracy: 0.0000e+00 - mean_squared_error: 0.0170 - val_loss: 0.0160 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0160\n",
            "Epoch 45/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0161 - accuracy: 0.0000e+00 - mean_squared_error: 0.0161 - val_loss: 0.0151 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0151\n",
            "Epoch 46/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0150 - accuracy: 0.0000e+00 - mean_squared_error: 0.0150 - val_loss: 0.0139 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0139\n",
            "Epoch 47/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0136 - accuracy: 0.0000e+00 - mean_squared_error: 0.0136 - val_loss: 0.0125 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0125\n",
            "Epoch 48/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0121 - accuracy: 0.0000e+00 - mean_squared_error: 0.0121 - val_loss: 0.0108 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0108\n",
            "Epoch 49/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0103 - accuracy: 0.0000e+00 - mean_squared_error: 0.0103 - val_loss: 0.0090 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0090\n",
            "Epoch 50/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0083 - accuracy: 0.0000e+00 - mean_squared_error: 0.0083 - val_loss: 0.0069 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0069\n",
            "Epoch 51/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0062 - accuracy: 0.0000e+00 - mean_squared_error: 0.0062 - val_loss: 0.0049 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0049\n",
            "Epoch 52/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0042 - accuracy: 0.0000e+00 - mean_squared_error: 0.0042 - val_loss: 0.0032 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0032\n",
            "Epoch 53/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0025 - accuracy: 0.0000e+00 - mean_squared_error: 0.0025 - val_loss: 0.0018 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0018\n",
            "Epoch 54/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0013 - accuracy: 0.0000e+00 - mean_squared_error: 0.0013 - val_loss: 8.4109e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 8.4109e-04\n",
            "Epoch 55/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 5.7842e-04 - accuracy: 0.0000e+00 - mean_squared_error: 5.7842e-04 - val_loss: 3.3753e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.3753e-04\n",
            "Epoch 56/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.2755e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.2755e-04 - val_loss: 1.2949e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.2949e-04\n",
            "Epoch 57/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 9.0944e-05 - accuracy: 0.0000e+00 - mean_squared_error: 9.0944e-05 - val_loss: 5.7931e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 5.7931e-05\n",
            "Epoch 58/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.8578e-05 - accuracy: 0.0000e+00 - mean_squared_error: 4.8578e-05 - val_loss: 3.8063e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.8063e-05\n",
            "Epoch 59/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.8004e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.8004e-05 - val_loss: 3.3482e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.3482e-05\n",
            "Epoch 60/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.6113e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.6113e-05 - val_loss: 3.5016e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.5016e-05\n",
            "Epoch 61/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.5340e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.5340e-05 - val_loss: 3.7480e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.7480e-05\n",
            "Epoch 62/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.5127e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.5127e-05 - val_loss: 3.4179e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.4179e-05\n",
            "Epoch 63/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.4471e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.4471e-05 - val_loss: 3.1722e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.1722e-05\n",
            "Epoch 64/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.4381e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.4381e-05 - val_loss: 3.1582e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.1582e-05\n",
            "Epoch 65/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.4421e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.4421e-05 - val_loss: 3.1688e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.1688e-05\n",
            "Epoch 66/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.4481e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.4481e-05 - val_loss: 3.1802e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.1802e-05\n",
            "Epoch 67/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.4810e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.4810e-05 - val_loss: 3.2879e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.2879e-05\n",
            "Epoch 68/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.4673e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.4673e-05 - val_loss: 3.1495e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.1495e-05\n",
            "Epoch 69/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.4278e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.4278e-05 - val_loss: 3.4036e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.4036e-05\n",
            "Epoch 70/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.4012e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.4012e-05 - val_loss: 3.9227e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.9227e-05\n",
            "Epoch 71/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.4487e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.4487e-05 - val_loss: 3.8591e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.8591e-05\n",
            "Epoch 72/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.5153e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.5153e-05 - val_loss: 3.7487e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.7487e-05\n",
            "Epoch 73/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.5472e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.5472e-05 - val_loss: 3.5963e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.5963e-05\n",
            "Epoch 74/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.5446e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.5446e-05 - val_loss: 3.4767e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.4767e-05\n",
            "Epoch 75/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.5331e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.5331e-05 - val_loss: 3.3831e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.3831e-05\n",
            "Epoch 76/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.5182e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.5182e-05 - val_loss: 3.3111e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.3111e-05\n",
            "Epoch 77/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.5015e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.5015e-05 - val_loss: 3.2573e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.2573e-05\n",
            "Epoch 78/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.4839e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.4839e-05 - val_loss: 3.2176e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.2176e-05\n",
            "Epoch 79/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.4652e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.4652e-05 - val_loss: 3.1873e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.1873e-05\n",
            "Epoch 80/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.4457e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.4457e-05 - val_loss: 3.1627e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.1627e-05\n",
            "Epoch 81/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.4254e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.4254e-05 - val_loss: 3.1415e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.1415e-05\n",
            "Epoch 82/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.4043e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.4043e-05 - val_loss: 3.1233e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.1233e-05\n",
            "Epoch 83/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.3829e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.3829e-05 - val_loss: 3.1060e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.1060e-05\n",
            "Epoch 84/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.3613e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.3613e-05 - val_loss: 3.0897e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.0897e-05\n",
            "Epoch 85/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.3396e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.3396e-05 - val_loss: 3.0744e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.0744e-05\n",
            "Epoch 86/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.3177e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.3177e-05 - val_loss: 3.0599e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.0599e-05\n",
            "Epoch 87/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.2959e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.2959e-05 - val_loss: 3.0455e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.0455e-05\n",
            "Epoch 88/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.2743e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.2743e-05 - val_loss: 3.0315e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.0315e-05\n",
            "Epoch 89/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.2526e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.2526e-05 - val_loss: 3.0188e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.0188e-05\n",
            "Epoch 90/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.2312e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.2312e-05 - val_loss: 3.0065e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.0065e-05\n",
            "Epoch 91/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.2100e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.2100e-05 - val_loss: 2.9946e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.9946e-05\n",
            "Epoch 92/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.1890e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.1890e-05 - val_loss: 2.9831e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.9831e-05\n",
            "Epoch 93/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.1681e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.1681e-05 - val_loss: 2.9724e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.9724e-05\n",
            "Epoch 94/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.1475e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.1475e-05 - val_loss: 2.9626e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.9626e-05\n",
            "Epoch 95/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.1271e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.1271e-05 - val_loss: 2.9534e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.9534e-05\n",
            "Epoch 96/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.1070e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.1070e-05 - val_loss: 2.9440e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.9440e-05\n",
            "Epoch 97/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.0871e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.0871e-05 - val_loss: 2.9355e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.9355e-05\n",
            "Epoch 98/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.0674e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.0674e-05 - val_loss: 2.9281e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.9281e-05\n",
            "Epoch 99/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.0480e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.0480e-05 - val_loss: 2.9202e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.9202e-05\n",
            "Epoch 100/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.0288e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.0288e-05 - val_loss: 2.9137e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.9137e-05\n",
            "8/8 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:13:57.512890: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(2.9414194, shape=(), dtype=float32)\n",
            "[2.9437094]\n",
            "tf.Tensor(2.682798, shape=(), dtype=float32)\n",
            "[2.6749654]\n",
            "tf.Tensor(2.980345, shape=(), dtype=float32)\n",
            "[2.981057]\n",
            "tf.Tensor(2.8586888, shape=(), dtype=float32)\n",
            "[2.861678]\n",
            "0.004561713 0.15951027\n",
            "34.967186\n",
            "3.1647368421052633\n",
            "/device:GPU:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:13:57.513196: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:13:57.513368: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:13:57.513576: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:13:57.513743: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:13:57.513884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n",
            "100%|██████████| 196/196 [01:53<00:00,  1.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rho (InputLayer)            [(None, 8, 8, 1)]         0         \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 64)                0         \n",
            "                                                                 \n",
            " concatenate_7 (Concatenate  (None, 64)                0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 16)                176       \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 32)                544       \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1403 (5.48 KB)\n",
            "Trainable params: 1403 (5.48 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "2024-03-12 17:15:51.170764: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:15:51.171067: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:15:51.171239: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:15:51.171450: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:15:51.171618: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:15:51.171760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    144/Unknown - 2s 3ms/step - loss: 0.8814 - accuracy: 0.0000e+00 - mean_squared_error: 0.8814"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:15:52.875333: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 2s 6ms/step - loss: 0.8139 - accuracy: 0.0000e+00 - mean_squared_error: 0.8139 - val_loss: 0.0205 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0205\n",
            "Epoch 2/100\n",
            " 50/157 [========>.....................] - ETA: 0s - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:15:53.284930: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 17:15:53.285006: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0205 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0205\n",
            "Epoch 3/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0205 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0205\n",
            "Epoch 4/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0205 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0205\n",
            "Epoch 5/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0205 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0205\n",
            "Epoch 6/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0205 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0205\n",
            "Epoch 7/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0205 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0205\n",
            "Epoch 8/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0205 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0205\n",
            "Epoch 9/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0205 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0205\n",
            "Epoch 10/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0205 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0205\n",
            "Epoch 11/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0205 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0205\n",
            "Epoch 12/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 13/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 14/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 15/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 16/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 17/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 18/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 19/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 20/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 21/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 22/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 23/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 24/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0205 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0205\n",
            "Epoch 25/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0205 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0205\n",
            "Epoch 26/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0205 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0205\n",
            "Epoch 27/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0205 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0205\n",
            "Epoch 28/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0205 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0205\n",
            "Epoch 29/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 30/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 31/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 32/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0203 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0203\n",
            "Epoch 33/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0203 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0203\n",
            "Epoch 34/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0203 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0203\n",
            "Epoch 35/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0202 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0202\n",
            "Epoch 36/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0202 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0202\n",
            "Epoch 37/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0201 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0201\n",
            "Epoch 38/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - mean_squared_error: 0.0205 - val_loss: 0.0201 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0201\n",
            "Epoch 39/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - mean_squared_error: 0.0205 - val_loss: 0.0200 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0200\n",
            "Epoch 40/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - mean_squared_error: 0.0204 - val_loss: 0.0200 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0200\n",
            "Epoch 41/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - mean_squared_error: 0.0204 - val_loss: 0.0199 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0199\n",
            "Epoch 42/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0203 - accuracy: 0.0000e+00 - mean_squared_error: 0.0203 - val_loss: 0.0198 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0198\n",
            "Epoch 43/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0203 - accuracy: 0.0000e+00 - mean_squared_error: 0.0203 - val_loss: 0.0198 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0198\n",
            "Epoch 44/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0202 - accuracy: 0.0000e+00 - mean_squared_error: 0.0202 - val_loss: 0.0197 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0197\n",
            "Epoch 45/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0201 - accuracy: 0.0000e+00 - mean_squared_error: 0.0201 - val_loss: 0.0196 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0196\n",
            "Epoch 46/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0200 - accuracy: 0.0000e+00 - mean_squared_error: 0.0200 - val_loss: 0.0195 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0195\n",
            "Epoch 47/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0199 - accuracy: 0.0000e+00 - mean_squared_error: 0.0199 - val_loss: 0.0194 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0194\n",
            "Epoch 48/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0198 - accuracy: 0.0000e+00 - mean_squared_error: 0.0198 - val_loss: 0.0193 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0193\n",
            "Epoch 49/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0197 - accuracy: 0.0000e+00 - mean_squared_error: 0.0197 - val_loss: 0.0191 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0191\n",
            "Epoch 50/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0196 - accuracy: 0.0000e+00 - mean_squared_error: 0.0196 - val_loss: 0.0190 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0190\n",
            "Epoch 51/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0194 - accuracy: 0.0000e+00 - mean_squared_error: 0.0194 - val_loss: 0.0188 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0188\n",
            "Epoch 52/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0192 - accuracy: 0.0000e+00 - mean_squared_error: 0.0192 - val_loss: 0.0187 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0187\n",
            "Epoch 53/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0190 - accuracy: 0.0000e+00 - mean_squared_error: 0.0190 - val_loss: 0.0185 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0185\n",
            "Epoch 54/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0188 - accuracy: 0.0000e+00 - mean_squared_error: 0.0188 - val_loss: 0.0182 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0182\n",
            "Epoch 55/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0186 - accuracy: 0.0000e+00 - mean_squared_error: 0.0186 - val_loss: 0.0180 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0180\n",
            "Epoch 56/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0183 - accuracy: 0.0000e+00 - mean_squared_error: 0.0183 - val_loss: 0.0177 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0177\n",
            "Epoch 57/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0180 - accuracy: 0.0000e+00 - mean_squared_error: 0.0180 - val_loss: 0.0174 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0174\n",
            "Epoch 58/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0177 - accuracy: 0.0000e+00 - mean_squared_error: 0.0177 - val_loss: 0.0170 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0170\n",
            "Epoch 59/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0173 - accuracy: 0.0000e+00 - mean_squared_error: 0.0173 - val_loss: 0.0166 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0166\n",
            "Epoch 60/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0168 - accuracy: 0.0000e+00 - mean_squared_error: 0.0168 - val_loss: 0.0162 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0162\n",
            "Epoch 61/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0163 - accuracy: 0.0000e+00 - mean_squared_error: 0.0163 - val_loss: 0.0156 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0156\n",
            "Epoch 62/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0156 - accuracy: 0.0000e+00 - mean_squared_error: 0.0156 - val_loss: 0.0149 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0149\n",
            "Epoch 63/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0149 - accuracy: 0.0000e+00 - mean_squared_error: 0.0149 - val_loss: 0.0141 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0141\n",
            "Epoch 64/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0140 - accuracy: 0.0000e+00 - mean_squared_error: 0.0140 - val_loss: 0.0132 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0132\n",
            "Epoch 65/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0130 - accuracy: 0.0000e+00 - mean_squared_error: 0.0130 - val_loss: 0.0121 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0121\n",
            "Epoch 66/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0118 - accuracy: 0.0000e+00 - mean_squared_error: 0.0118 - val_loss: 0.0109 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0109\n",
            "Epoch 67/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0105 - accuracy: 0.0000e+00 - mean_squared_error: 0.0105 - val_loss: 0.0096 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0096\n",
            "Epoch 68/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0091 - accuracy: 0.0000e+00 - mean_squared_error: 0.0091 - val_loss: 0.0081 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0081\n",
            "Epoch 69/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0075 - accuracy: 0.0000e+00 - mean_squared_error: 0.0075 - val_loss: 0.0065 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0065\n",
            "Epoch 70/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0060 - accuracy: 0.0000e+00 - mean_squared_error: 0.0060 - val_loss: 0.0051 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0051\n",
            "Epoch 71/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0045 - accuracy: 0.0000e+00 - mean_squared_error: 0.0045 - val_loss: 0.0038 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0038\n",
            "Epoch 72/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0032 - accuracy: 0.0000e+00 - mean_squared_error: 0.0032 - val_loss: 0.0027 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0027\n",
            "Epoch 73/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0021 - accuracy: 0.0000e+00 - mean_squared_error: 0.0021 - val_loss: 0.0016 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0016\n",
            "Epoch 74/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0013 - accuracy: 0.0000e+00 - mean_squared_error: 0.0013 - val_loss: 9.7543e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 9.7543e-04\n",
            "Epoch 75/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 7.7333e-04 - accuracy: 0.0000e+00 - mean_squared_error: 7.7333e-04 - val_loss: 5.7968e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 5.7968e-04\n",
            "Epoch 76/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.1487e-04 - accuracy: 0.0000e+00 - mean_squared_error: 4.1487e-04 - val_loss: 2.7477e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.7477e-04\n",
            "Epoch 77/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.1091e-04 - accuracy: 0.0000e+00 - mean_squared_error: 2.1091e-04 - val_loss: 1.5546e-04 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.5546e-04\n",
            "Epoch 78/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.0647e-04 - accuracy: 0.0000e+00 - mean_squared_error: 1.0647e-04 - val_loss: 9.2108e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 9.2108e-05\n",
            "Epoch 79/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 6.0080e-05 - accuracy: 0.0000e+00 - mean_squared_error: 6.0080e-05 - val_loss: 5.4000e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 5.4000e-05\n",
            "Epoch 80/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 4.1463e-05 - accuracy: 0.0000e+00 - mean_squared_error: 4.1463e-05 - val_loss: 3.2141e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.2141e-05\n",
            "Epoch 81/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.3659e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.3659e-05 - val_loss: 2.9464e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.9464e-05\n",
            "Epoch 82/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.1015e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.1015e-05 - val_loss: 3.0435e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.0435e-05\n",
            "Epoch 83/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.9483e-05 - accuracy: 0.0000e+00 - mean_squared_error: 2.9483e-05 - val_loss: 3.2066e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.2066e-05\n",
            "Epoch 84/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.9138e-05 - accuracy: 0.0000e+00 - mean_squared_error: 2.9138e-05 - val_loss: 2.7806e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.7806e-05\n",
            "Epoch 85/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.9518e-05 - accuracy: 0.0000e+00 - mean_squared_error: 2.9518e-05 - val_loss: 3.0547e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 3.0547e-05\n",
            "Epoch 86/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.9918e-05 - accuracy: 0.0000e+00 - mean_squared_error: 2.9918e-05 - val_loss: 2.7660e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.7660e-05\n",
            "Epoch 87/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.0400e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.0400e-05 - val_loss: 2.6999e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.6999e-05\n",
            "Epoch 88/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.1623e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.1623e-05 - val_loss: 2.7049e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.7049e-05\n",
            "Epoch 89/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.2626e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.2626e-05 - val_loss: 2.6857e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.6857e-05\n",
            "Epoch 90/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.3264e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.3264e-05 - val_loss: 2.7118e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.7118e-05\n",
            "Epoch 91/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.3849e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.3849e-05 - val_loss: 2.7195e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.7195e-05\n",
            "Epoch 92/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 3.4183e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.4183e-05 - val_loss: 2.7157e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.7157e-05\n",
            "Epoch 93/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.4274e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.4274e-05 - val_loss: 2.7106e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.7106e-05\n",
            "Epoch 94/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 3.4299e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.4299e-05 - val_loss: 2.7050e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.7050e-05\n",
            "Epoch 95/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.4297e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.4297e-05 - val_loss: 2.6990e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.6990e-05\n",
            "Epoch 96/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.4278e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.4278e-05 - val_loss: 2.6928e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.6928e-05\n",
            "Epoch 97/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.4253e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.4253e-05 - val_loss: 2.6867e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.6867e-05\n",
            "Epoch 98/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.4213e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.4213e-05 - val_loss: 2.6802e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.6802e-05\n",
            "Epoch 99/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.4168e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.4168e-05 - val_loss: 2.6740e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.6740e-05\n",
            "Epoch 100/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 3.4119e-05 - accuracy: 0.0000e+00 - mean_squared_error: 3.4119e-05 - val_loss: 2.6675e-05 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.6675e-05\n",
            "8/8 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:17:09.724174: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:17:09.724455: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:17:09.724646: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:17:09.724852: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(3.5211618, shape=(), dtype=float32)\n",
            "[3.5241098]\n",
            "tf.Tensor(3.468742, shape=(), dtype=float32)\n",
            "[3.4741335]\n",
            "tf.Tensor(3.4764898, shape=(), dtype=float32)\n",
            "[3.4816122]\n",
            "tf.Tensor(3.5439172, shape=(), dtype=float32)\n",
            "[3.54536]\n",
            "0.0047089783 0.16757843\n",
            "35.587\n",
            "3.690526315789474\n",
            "/device:GPU:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "L355\n",
            "2024-03-12 17:17:09.725026: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:17:09.725190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n",
            "100%|██████████| 196/196 [01:54<00:00,  1.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rho (InputLayer)            [(None, 8, 8, 1)]         0         \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 64)                0         \n",
            "                                                                 \n",
            " concatenate_8 (Concatenate  (None, 64)                0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 16)                176       \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 32)                544       \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1403 (5.48 KB)\n",
            "Trainable params: 1403 (5.48 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "2024-03-12 17:19:04.478016: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:19:04.478321: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:19:04.478541: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:19:04.478767: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:19:04.478948: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:19:04.479082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    147/Unknown - 2s 3ms/step - loss: 4.7921 - accuracy: 0.0000e+00 - mean_squared_error: 4.7921"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:19:06.132322: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 17:19:06.132400: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n",
            "2024-03-12 17:19:06.132410: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 407123110838793230\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 2s 6ms/step - loss: 4.5182 - accuracy: 0.0000e+00 - mean_squared_error: 4.5182 - val_loss: 0.1266 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.1266\n",
            "Epoch 2/100\n",
            " 50/157 [========>.....................] - ETA: 0s - loss: 0.0571 - accuracy: 0.0000e+00 - mean_squared_error: 0.0571"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:19:06.557695: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 17:19:06.557762: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0328 - accuracy: 0.0000e+00 - mean_squared_error: 0.0328 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 3/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 4/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 5/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 6/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 7/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 8/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 9/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 10/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 11/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 12/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 13/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 14/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 15/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 16/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 17/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 18/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 19/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 20/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 21/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 22/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 23/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 24/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 25/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 26/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 27/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 28/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 29/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 30/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 31/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 32/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 33/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 34/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 35/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 36/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 37/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 38/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 39/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 40/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 41/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 42/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 43/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 44/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 45/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 46/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 47/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 48/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 49/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 50/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - mean_squared_error: 0.0205 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 51/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - mean_squared_error: 0.0205 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 52/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - mean_squared_error: 0.0205 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 53/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - mean_squared_error: 0.0205 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 54/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - mean_squared_error: 0.0205 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 55/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - mean_squared_error: 0.0204 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 56/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - mean_squared_error: 0.0204 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 57/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - mean_squared_error: 0.0204 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 58/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0203 - accuracy: 0.0000e+00 - mean_squared_error: 0.0203 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 59/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0203 - accuracy: 0.0000e+00 - mean_squared_error: 0.0203 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 60/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0202 - accuracy: 0.0000e+00 - mean_squared_error: 0.0202 - val_loss: 0.0205 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0205\n",
            "Epoch 61/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0202 - accuracy: 0.0000e+00 - mean_squared_error: 0.0202 - val_loss: 0.0205 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0205\n",
            "Epoch 62/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0201 - accuracy: 0.0000e+00 - mean_squared_error: 0.0201 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 63/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0201 - accuracy: 0.0000e+00 - mean_squared_error: 0.0201 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 64/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0200 - accuracy: 0.0000e+00 - mean_squared_error: 0.0200 - val_loss: 0.0203 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0203\n",
            "Epoch 65/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0200 - accuracy: 0.0000e+00 - mean_squared_error: 0.0200 - val_loss: 0.0202 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0202\n",
            "Epoch 66/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0199 - accuracy: 0.0000e+00 - mean_squared_error: 0.0199 - val_loss: 0.0201 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0201\n",
            "Epoch 67/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0198 - accuracy: 0.0000e+00 - mean_squared_error: 0.0198 - val_loss: 0.0201 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0201\n",
            "Epoch 68/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0197 - accuracy: 0.0000e+00 - mean_squared_error: 0.0197 - val_loss: 0.0200 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0200\n",
            "Epoch 69/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0196 - accuracy: 0.0000e+00 - mean_squared_error: 0.0196 - val_loss: 0.0199 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0199\n",
            "Epoch 70/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.0195 - accuracy: 0.0000e+00 - mean_squared_error: 0.0195 - val_loss: 0.0198 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0198\n",
            "Epoch 71/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0194 - accuracy: 0.0000e+00 - mean_squared_error: 0.0194 - val_loss: 0.0196 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0196\n",
            "Epoch 72/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.0193 - accuracy: 0.0000e+00 - mean_squared_error: 0.0193 - val_loss: 0.0195 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0195\n",
            "Epoch 73/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0191 - accuracy: 0.0000e+00 - mean_squared_error: 0.0191 - val_loss: 0.0193 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0193\n",
            "Epoch 74/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0190 - accuracy: 0.0000e+00 - mean_squared_error: 0.0190 - val_loss: 0.0192 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0192\n",
            "Epoch 75/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0188 - accuracy: 0.0000e+00 - mean_squared_error: 0.0188 - val_loss: 0.0190 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0190\n",
            "Epoch 76/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0186 - accuracy: 0.0000e+00 - mean_squared_error: 0.0186 - val_loss: 0.0188 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0188\n",
            "Epoch 77/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0184 - accuracy: 0.0000e+00 - mean_squared_error: 0.0184 - val_loss: 0.0186 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0186\n",
            "Epoch 78/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0182 - accuracy: 0.0000e+00 - mean_squared_error: 0.0182 - val_loss: 0.0183 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0183\n",
            "Epoch 79/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0179 - accuracy: 0.0000e+00 - mean_squared_error: 0.0179 - val_loss: 0.0181 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0181\n",
            "Epoch 80/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.0176 - accuracy: 0.0000e+00 - mean_squared_error: 0.0176 - val_loss: 0.0178 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0178\n",
            "Epoch 81/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0172 - accuracy: 0.0000e+00 - mean_squared_error: 0.0172 - val_loss: 0.0174 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0174\n",
            "Epoch 82/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0169 - accuracy: 0.0000e+00 - mean_squared_error: 0.0169 - val_loss: 0.0170 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0170\n",
            "Epoch 83/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0164 - accuracy: 0.0000e+00 - mean_squared_error: 0.0164 - val_loss: 0.0166 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0166\n",
            "Epoch 84/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0159 - accuracy: 0.0000e+00 - mean_squared_error: 0.0159 - val_loss: 0.0161 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0161\n",
            "Epoch 85/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0154 - accuracy: 0.0000e+00 - mean_squared_error: 0.0154 - val_loss: 0.0156 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0156\n",
            "Epoch 86/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0148 - accuracy: 0.0000e+00 - mean_squared_error: 0.0148 - val_loss: 0.0149 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0149\n",
            "Epoch 87/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0141 - accuracy: 0.0000e+00 - mean_squared_error: 0.0141 - val_loss: 0.0142 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0142\n",
            "Epoch 88/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0133 - accuracy: 0.0000e+00 - mean_squared_error: 0.0133 - val_loss: 0.0134 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0134\n",
            "Epoch 89/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0125 - accuracy: 0.0000e+00 - mean_squared_error: 0.0125 - val_loss: 0.0126 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0126\n",
            "Epoch 90/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0115 - accuracy: 0.0000e+00 - mean_squared_error: 0.0115 - val_loss: 0.0117 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0117\n",
            "Epoch 91/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0106 - accuracy: 0.0000e+00 - mean_squared_error: 0.0106 - val_loss: 0.0107 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0107\n",
            "Epoch 92/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0095 - accuracy: 0.0000e+00 - mean_squared_error: 0.0095 - val_loss: 0.0095 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0095\n",
            "Epoch 93/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0085 - accuracy: 0.0000e+00 - mean_squared_error: 0.0085 - val_loss: 0.0083 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0083\n",
            "Epoch 94/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0075 - accuracy: 0.0000e+00 - mean_squared_error: 0.0075 - val_loss: 0.0071 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0071\n",
            "Epoch 95/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0065 - accuracy: 0.0000e+00 - mean_squared_error: 0.0065 - val_loss: 0.0061 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0061\n",
            "Epoch 96/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0056 - accuracy: 0.0000e+00 - mean_squared_error: 0.0056 - val_loss: 0.0051 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0051\n",
            "Epoch 97/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0046 - accuracy: 0.0000e+00 - mean_squared_error: 0.0046 - val_loss: 0.0042 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0042\n",
            "Epoch 98/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0038 - accuracy: 0.0000e+00 - mean_squared_error: 0.0038 - val_loss: 0.0034 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0034\n",
            "Epoch 99/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0030 - accuracy: 0.0000e+00 - mean_squared_error: 0.0030 - val_loss: 0.0026 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0026\n",
            "Epoch 100/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0023 - accuracy: 0.0000e+00 - mean_squared_error: 0.0023 - val_loss: 0.0020 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0020\n",
            "8/8 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:20:27.086160: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(3.74669, shape=(), dtype=float32)\n",
            "[3.8091178]\n",
            "tf.Tensor(3.9037993, shape=(), dtype=float32)\n",
            "[3.9243963]\n",
            "tf.Tensor(3.843815, shape=(), dtype=float32)\n",
            "[3.8814697]\n",
            "tf.Tensor(3.7606518, shape=(), dtype=float32)\n",
            "[3.8197384]\n",
            "0.04065907 0.17470759\n",
            "4.296891\n",
            "4.2163157894736845\n",
            "/device:GPU:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:20:27.086479: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:20:27.086801: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:20:27.087026: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:20:27.087196: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:20:27.087337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n",
            "100%|██████████| 196/196 [01:54<00:00,  1.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rho (InputLayer)            [(None, 8, 8, 1)]         0         \n",
            "                                                                 \n",
            " flatten_9 (Flatten)         (None, 64)                0         \n",
            "                                                                 \n",
            " concatenate_9 (Concatenate  (None, 64)                0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 16)                176       \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 32)                544       \n",
            "                                                                 \n",
            " dense_39 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1403 (5.48 KB)\n",
            "Trainable params: 1403 (5.48 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "2024-03-12 17:22:22.076484: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:22:22.076811: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:22:22.076991: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:22:22.077212: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:22:22.077388: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:22:22.077574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    152/Unknown - 2s 3ms/step - loss: 6.2705 - accuracy: 0.0000e+00 - mean_squared_error: 6.2705"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:22:23.673534: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 17:22:23.673588: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 2s 6ms/step - loss: 6.1069 - accuracy: 0.0000e+00 - mean_squared_error: 6.1069 - val_loss: 0.2283 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.2283\n",
            "Epoch 2/100\n",
            " 51/157 [========>.....................] - ETA: 0s - loss: 0.0947 - accuracy: 0.0000e+00 - mean_squared_error: 0.0947"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:22:24.080555: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0463 - accuracy: 0.0000e+00 - mean_squared_error: 0.0463 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 3/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 4/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 5/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 6/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 7/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 8/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 9/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 10/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 11/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 12/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 13/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 14/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 15/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 16/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 17/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 18/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 19/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 20/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 21/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 22/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 23/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 24/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 25/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 26/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 27/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 28/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 29/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 30/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 31/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 32/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 33/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 34/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 35/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 36/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 37/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 38/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 39/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 40/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 41/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 42/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 43/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 44/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 45/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 46/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 47/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 48/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 49/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 50/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 51/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 52/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 53/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 54/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 55/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 56/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 57/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 58/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 59/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 60/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 61/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 62/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 63/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 64/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 65/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 66/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 67/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 68/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 69/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 70/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 71/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 72/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 73/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 74/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 75/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 76/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - mean_squared_error: 0.0205 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 77/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - mean_squared_error: 0.0205 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 78/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - mean_squared_error: 0.0205 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 79/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - mean_squared_error: 0.0205 - val_loss: 0.0205 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0205\n",
            "Epoch 80/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - mean_squared_error: 0.0204 - val_loss: 0.0205 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0205\n",
            "Epoch 81/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - mean_squared_error: 0.0204 - val_loss: 0.0205 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0205\n",
            "Epoch 82/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0203 - accuracy: 0.0000e+00 - mean_squared_error: 0.0203 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 83/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0203 - accuracy: 0.0000e+00 - mean_squared_error: 0.0203 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 84/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0203 - accuracy: 0.0000e+00 - mean_squared_error: 0.0203 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 85/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0202 - accuracy: 0.0000e+00 - mean_squared_error: 0.0202 - val_loss: 0.0203 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0203\n",
            "Epoch 86/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0202 - accuracy: 0.0000e+00 - mean_squared_error: 0.0202 - val_loss: 0.0203 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0203\n",
            "Epoch 87/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0201 - accuracy: 0.0000e+00 - mean_squared_error: 0.0201 - val_loss: 0.0202 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0202\n",
            "Epoch 88/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0201 - accuracy: 0.0000e+00 - mean_squared_error: 0.0201 - val_loss: 0.0202 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0202\n",
            "Epoch 89/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0200 - accuracy: 0.0000e+00 - mean_squared_error: 0.0200 - val_loss: 0.0201 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0201\n",
            "Epoch 90/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0199 - accuracy: 0.0000e+00 - mean_squared_error: 0.0199 - val_loss: 0.0200 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0200\n",
            "Epoch 91/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0199 - accuracy: 0.0000e+00 - mean_squared_error: 0.0199 - val_loss: 0.0200 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0200\n",
            "Epoch 92/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0198 - accuracy: 0.0000e+00 - mean_squared_error: 0.0198 - val_loss: 0.0199 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0199\n",
            "Epoch 93/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0197 - accuracy: 0.0000e+00 - mean_squared_error: 0.0197 - val_loss: 0.0198 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0198\n",
            "Epoch 94/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0196 - accuracy: 0.0000e+00 - mean_squared_error: 0.0196 - val_loss: 0.0197 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0197\n",
            "Epoch 95/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0195 - accuracy: 0.0000e+00 - mean_squared_error: 0.0195 - val_loss: 0.0197 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0197\n",
            "Epoch 96/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0195 - accuracy: 0.0000e+00 - mean_squared_error: 0.0195 - val_loss: 0.0196 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0196\n",
            "Epoch 97/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0194 - accuracy: 0.0000e+00 - mean_squared_error: 0.0194 - val_loss: 0.0195 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0195\n",
            "Epoch 98/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0193 - accuracy: 0.0000e+00 - mean_squared_error: 0.0193 - val_loss: 0.0194 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0194\n",
            "Epoch 99/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0191 - accuracy: 0.0000e+00 - mean_squared_error: 0.0191 - val_loss: 0.0192 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0192\n",
            "Epoch 100/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0190 - accuracy: 0.0000e+00 - mean_squared_error: 0.0190 - val_loss: 0.0191 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0191\n",
            "8/8 [==============================] - 0s 1ms/step\n",
            "tf.Tensor(4.4759398, shape=(), dtype=float32)\n",
            "[4.4505234]\n",
            "tf.Tensor(4.5398, shape=(), dtype=float32)\n",
            "[4.4537225]\n",
            "tf.Tensor(4.385601, shape=(), dtype=float32)\n",
            "[4.4458385]\n",
            "tf.Tensor(4.3329344, shape=(), dtype=float32)\n",
            "[4.4430175]\n",
            "0.12462529 0.15320504\n",
            "1.2293254\n",
            "4.742105263157895\n",
            "/device:GPU:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:23:40.439055: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:23:40.439298: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:23:40.439459: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "  0%|          | 0/196 [00:00<?, ?it/s]L355l_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-\n",
            "2024-03-12 17:23:40.439822: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:23:40.439955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n",
            "100%|██████████| 196/196 [01:56<00:00,  1.69it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rho (InputLayer)            [(None, 8, 8, 1)]         0         \n",
            "                                                                 \n",
            " flatten_10 (Flatten)        (None, 64)                0         \n",
            "                                                                 \n",
            " concatenate_10 (Concatenat  (None, 64)                0         \n",
            " e)                                                              \n",
            "                                                                 \n",
            " dense_40 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            " dense_41 (Dense)            (None, 16)                176       \n",
            "                                                                 \n",
            " dense_42 (Dense)            (None, 32)                544       \n",
            "                                                                 \n",
            " dense_43 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1403 (5.48 KB)\n",
            "Trainable params: 1403 (5.48 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "2024-03-12 17:25:36.607492: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:25:36.607779: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:25:36.607941: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:25:36.608185: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:25:36.608365: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:25:36.608524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    153/Unknown - 2s 3ms/step - loss: 11.9421 - accuracy: 0.0000e+00 - mean_squared_error: 11.9421"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:25:38.212611: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 2s 6ms/step - loss: 11.7283 - accuracy: 0.0000e+00 - mean_squared_error: 11.7283 - val_loss: 1.5653 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.5653\n",
            "Epoch 2/100\n",
            " 50/157 [========>.....................] - ETA: 0s - loss: 0.8886 - accuracy: 0.0000e+00 - mean_squared_error: 0.8886"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:25:38.596607: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 17:25:38.596678: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 1s 5ms/step - loss: 0.3820 - accuracy: 0.0000e+00 - mean_squared_error: 0.3820 - val_loss: 0.0344 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0344\n",
            "Epoch 3/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0230 - accuracy: 0.0000e+00 - mean_squared_error: 0.0230 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 4/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 5/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 6/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 7/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 8/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 9/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 10/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 11/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 12/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 13/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 14/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 15/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 16/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 17/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 18/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 19/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 20/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 21/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 22/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 23/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 24/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 25/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 26/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 27/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 28/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 29/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 30/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 31/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 32/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 33/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 34/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 35/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 36/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 37/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 38/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 39/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 40/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 41/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 42/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 43/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 44/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 45/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 46/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 47/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 48/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 49/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 50/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 51/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 52/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 53/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 54/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 55/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 56/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 57/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 58/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 59/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 60/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 61/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 62/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 63/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 64/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 65/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 66/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 67/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 68/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 69/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 70/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 71/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 72/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 73/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 74/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 75/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 76/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 77/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 78/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 79/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 80/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 81/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 82/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 83/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0205 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0205\n",
            "Epoch 84/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - mean_squared_error: 0.0205 - val_loss: 0.0205 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0205\n",
            "Epoch 85/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - mean_squared_error: 0.0205 - val_loss: 0.0205 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0205\n",
            "Epoch 86/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - mean_squared_error: 0.0205 - val_loss: 0.0205 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0205\n",
            "Epoch 87/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - mean_squared_error: 0.0205 - val_loss: 0.0205 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0205\n",
            "Epoch 88/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - mean_squared_error: 0.0205 - val_loss: 0.0205 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0205\n",
            "Epoch 89/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - mean_squared_error: 0.0205 - val_loss: 0.0205 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0205\n",
            "Epoch 90/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - mean_squared_error: 0.0205 - val_loss: 0.0205 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0205\n",
            "Epoch 91/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - mean_squared_error: 0.0204 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 92/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - mean_squared_error: 0.0204 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 93/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - mean_squared_error: 0.0204 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 94/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - mean_squared_error: 0.0204 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 95/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - mean_squared_error: 0.0204 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 96/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - mean_squared_error: 0.0204 - val_loss: 0.0204 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0204\n",
            "Epoch 97/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0203 - accuracy: 0.0000e+00 - mean_squared_error: 0.0203 - val_loss: 0.0203 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0203\n",
            "Epoch 98/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0203 - accuracy: 0.0000e+00 - mean_squared_error: 0.0203 - val_loss: 0.0203 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0203\n",
            "Epoch 99/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0203 - accuracy: 0.0000e+00 - mean_squared_error: 0.0203 - val_loss: 0.0203 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0203\n",
            "Epoch 100/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0203 - accuracy: 0.0000e+00 - mean_squared_error: 0.0203 - val_loss: 0.0203 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0203\n",
            "8/8 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:26:57.540327: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(4.8363576, shape=(), dtype=float32)\n",
            "[4.984097]\n",
            "tf.Tensor(4.9154835, shape=(), dtype=float32)\n",
            "[4.9850907]\n",
            "tf.Tensor(5.098521, shape=(), dtype=float32)\n",
            "[4.9872684]\n",
            "tf.Tensor(5.0536017, shape=(), dtype=float32)\n",
            "[4.9867496]\n",
            "0.11576864 0.15836728\n",
            "1.3679636\n",
            "5.267894736842106\n",
            "/device:GPU:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:26:57.540636: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:26:57.540815: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "  0%|          | 0/196 [00:00<?, ?it/s]L355l_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-\n",
            "2024-03-12 17:26:57.541211: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:26:57.541358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n",
            "100%|██████████| 196/196 [01:55<00:00,  1.70it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rho (InputLayer)            [(None, 8, 8, 1)]         0         \n",
            "                                                                 \n",
            " flatten_11 (Flatten)        (None, 64)                0         \n",
            "                                                                 \n",
            " concatenate_11 (Concatenat  (None, 64)                0         \n",
            " e)                                                              \n",
            "                                                                 \n",
            " dense_44 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            " dense_45 (Dense)            (None, 16)                176       \n",
            "                                                                 \n",
            " dense_46 (Dense)            (None, 32)                544       \n",
            "                                                                 \n",
            " dense_47 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1403 (5.48 KB)\n",
            "Trainable params: 1403 (5.48 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "2024-03-12 17:28:53.186996: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:28:53.187275: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:28:53.187437: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:28:53.187648: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:28:53.187808: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:28:53.187943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    157/Unknown - 2s 3ms/step - loss: 7.8889 - accuracy: 0.0000e+00 - mean_squared_error: 7.8889"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:28:54.740316: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 17:28:54.740382: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n",
            "2024-03-12 17:28:54.740390: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 1353796229350139122\n",
            "2024-03-12 17:28:54.740396: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 407123110838793230\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 2s 6ms/step - loss: 7.8889 - accuracy: 0.0000e+00 - mean_squared_error: 7.8889 - val_loss: 0.5050 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.5050\n",
            "Epoch 2/100\n",
            " 52/157 [========>.....................] - ETA: 0s - loss: 0.2313 - accuracy: 0.0000e+00 - mean_squared_error: 0.2313"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:28:55.133828: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 17:28:55.133880: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0986 - accuracy: 0.0000e+00 - mean_squared_error: 0.0986 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 3/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 4/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 5/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 6/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 7/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 8/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 9/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 10/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 11/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 12/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 13/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 14/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 15/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 16/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 17/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 18/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 19/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 20/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 21/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 22/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 23/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 24/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 25/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 26/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 27/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 28/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 29/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 30/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 31/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 32/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 33/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 34/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 35/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 36/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 37/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 38/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 39/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 40/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 41/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 42/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 43/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 44/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 45/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 46/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 47/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 48/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 49/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 50/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 51/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 52/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 53/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 54/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 55/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 56/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 57/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 58/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 59/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 60/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 61/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 62/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 63/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 64/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 65/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 66/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 67/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 68/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 69/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 70/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 71/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 72/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 73/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 74/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 75/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 76/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 77/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 78/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 79/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 80/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 81/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 82/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 83/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 84/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 85/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 86/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 87/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 88/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 89/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 90/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 91/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 92/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 93/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 94/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 95/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 96/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 97/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 98/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - mean_squared_error: 0.0205 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 99/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - mean_squared_error: 0.0205 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 100/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - mean_squared_error: 0.0205 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "8/8 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:30:16.108889: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(5.609955, shape=(), dtype=float32)\n",
            "[5.496489]\n",
            "tf.Tensor(5.640685, shape=(), dtype=float32)\n",
            "[5.496824]\n",
            "tf.Tensor(5.30691, shape=(), dtype=float32)\n",
            "[5.492983]\n",
            "tf.Tensor(5.4209204, shape=(), dtype=float32)\n",
            "[5.4943485]\n",
            "0.12402874 0.17441797\n",
            "1.4062706\n",
            "5.793684210526316\n",
            "/device:GPU:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:30:16.109153: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:30:16.109322: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:30:16.109537: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:30:16.109706: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:30:16.109846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n",
            "100%|██████████| 196/196 [01:54<00:00,  1.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rho (InputLayer)            [(None, 8, 8, 1)]         0         \n",
            "                                                                 \n",
            " flatten_12 (Flatten)        (None, 64)                0         \n",
            "                                                                 \n",
            " concatenate_12 (Concatenat  (None, 64)                0         \n",
            " e)                                                              \n",
            "                                                                 \n",
            " dense_48 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            " dense_49 (Dense)            (None, 16)                176       \n",
            "                                                                 \n",
            " dense_50 (Dense)            (None, 32)                544       \n",
            "                                                                 \n",
            " dense_51 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1403 (5.48 KB)\n",
            "Trainable params: 1403 (5.48 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "2024-03-12 17:32:10.578492: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:32:10.578766: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:32:10.578929: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:32:10.579128: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:32:10.579287: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:32:10.579420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    156/Unknown - 2s 3ms/step - loss: 7.1659 - accuracy: 0.0000e+00 - mean_squared_error: 7.1659"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:32:12.137085: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 17:32:12.137157: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 2s 6ms/step - loss: 7.1549 - accuracy: 0.0000e+00 - mean_squared_error: 7.1549 - val_loss: 0.3171 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.3171\n",
            "Epoch 2/100\n",
            " 54/157 [=========>....................] - ETA: 0s - loss: 0.1286 - accuracy: 0.0000e+00 - mean_squared_error: 0.1286"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:32:12.558750: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 17:32:12.558799: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0599 - accuracy: 0.0000e+00 - mean_squared_error: 0.0599 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 3/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 4/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 5/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 6/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 7/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 8/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 9/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 10/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 11/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 12/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 13/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 14/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 15/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 16/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 17/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 18/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 19/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 20/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 21/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 22/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 23/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 24/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 25/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 26/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 27/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 28/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 29/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 30/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 31/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 32/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 33/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 34/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 35/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 36/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 37/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 38/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 39/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 40/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 41/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 42/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 43/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 44/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 45/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 46/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 47/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 48/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 49/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 50/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 51/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 52/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 53/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 54/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 55/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 56/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 57/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0211 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0211\n",
            "Epoch 58/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 59/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 60/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 61/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 62/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 63/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 64/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 65/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 66/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 67/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 68/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 69/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 70/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 71/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 72/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 73/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 74/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 75/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 76/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 77/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 78/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 79/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 80/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 81/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 82/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 83/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 84/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 85/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 86/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 87/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 88/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 89/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 90/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 91/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 92/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 93/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 94/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 95/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 96/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 97/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 98/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - mean_squared_error: 0.0206 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 99/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - mean_squared_error: 0.0205 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 100/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - mean_squared_error: 0.0205 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "8/8 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:33:31.311318: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(6.2049065, shape=(), dtype=float32)\n",
            "[6.0370817]\n",
            "tf.Tensor(5.8949494, shape=(), dtype=float32)\n",
            "[6.035226]\n",
            "tf.Tensor(6.067466, shape=(), dtype=float32)\n",
            "[6.036283]\n",
            "tf.Tensor(6.2751393, shape=(), dtype=float32)\n",
            "[6.0374765]\n",
            "0.12789711 0.17339928\n",
            "1.3557717\n",
            "6.319473684210527\n",
            "/device:GPU:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:33:31.311599: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:33:31.311777: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:33:31.311995: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:33:31.312180: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:33:31.312327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n",
            "100%|██████████| 196/196 [01:56<00:00,  1.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rho (InputLayer)            [(None, 8, 8, 1)]         0         \n",
            "                                                                 \n",
            " flatten_13 (Flatten)        (None, 64)                0         \n",
            "                                                                 \n",
            " concatenate_13 (Concatenat  (None, 64)                0         \n",
            " e)                                                              \n",
            "                                                                 \n",
            " dense_52 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            " dense_53 (Dense)            (None, 16)                176       \n",
            "                                                                 \n",
            " dense_54 (Dense)            (None, 32)                544       \n",
            "                                                                 \n",
            " dense_55 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1403 (5.48 KB)\n",
            "Trainable params: 1403 (5.48 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "2024-03-12 17:35:28.338809: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:35:28.339116: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:35:28.339295: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:35:28.339516: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:35:28.339711: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:35:28.339852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    156/Unknown - 2s 3ms/step - loss: 17.1290 - accuracy: 0.0000e+00 - mean_squared_error: 17.1290"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:35:30.046218: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 17:35:30.046270: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 2s 6ms/step - loss: 17.1062 - accuracy: 0.0000e+00 - mean_squared_error: 17.1062 - val_loss: 2.8362 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.8362\n",
            "Epoch 2/100\n",
            " 46/157 [=======>......................] - ETA: 0s - loss: 1.6647 - accuracy: 0.0000e+00 - mean_squared_error: 1.6647"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:35:30.463029: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 17:35:30.463097: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 1s 5ms/step - loss: 0.6640 - accuracy: 0.0000e+00 - mean_squared_error: 0.6640 - val_loss: 0.0398 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0398\n",
            "Epoch 3/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0238 - accuracy: 0.0000e+00 - mean_squared_error: 0.0238 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 4/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 5/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 6/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 7/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 8/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 9/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 10/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 11/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 12/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 13/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 14/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 15/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 16/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 17/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 18/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 19/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 20/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 21/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 22/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 23/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 24/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 25/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 26/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 27/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 28/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 29/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 30/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 31/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 32/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 33/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 34/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 35/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 36/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 37/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 38/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 39/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 40/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 41/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 42/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 43/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 44/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 45/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 46/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 47/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 48/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 49/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 50/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 51/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 52/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 53/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 54/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 55/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 56/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 57/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 58/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 59/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 60/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 61/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 62/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 63/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 64/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 65/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 66/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 67/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 68/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 69/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 70/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 71/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 72/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 73/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 74/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 75/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 76/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 77/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 78/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 79/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 80/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 81/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 82/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 83/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 84/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 85/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 86/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 87/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 88/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 89/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 90/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 91/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 92/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 93/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 94/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 95/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 96/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 97/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 98/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 99/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 100/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "8/8 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:36:47.298848: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(6.445969, shape=(), dtype=float32)\n",
            "[6.5655556]\n",
            "tf.Tensor(6.3836384, shape=(), dtype=float32)\n",
            "[6.5653377]\n",
            "tf.Tensor(6.3642907, shape=(), dtype=float32)\n",
            "[6.565268]\n",
            "tf.Tensor(6.584234, shape=(), dtype=float32)\n",
            "[6.566025]\n",
            "0.12648398 0.17750749\n",
            "1.403399\n",
            "6.845263157894737\n",
            "/device:GPU:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:36:47.299126: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:36:47.299296: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:36:47.299505: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:36:47.299679: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:36:47.299813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n",
            "100%|██████████| 196/196 [01:54<00:00,  1.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rho (InputLayer)            [(None, 8, 8, 1)]         0         \n",
            "                                                                 \n",
            " flatten_14 (Flatten)        (None, 64)                0         \n",
            "                                                                 \n",
            " concatenate_14 (Concatenat  (None, 64)                0         \n",
            " e)                                                              \n",
            "                                                                 \n",
            " dense_56 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            " dense_57 (Dense)            (None, 16)                176       \n",
            "                                                                 \n",
            " dense_58 (Dense)            (None, 32)                544       \n",
            "                                                                 \n",
            " dense_59 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1403 (5.48 KB)\n",
            "Trainable params: 1403 (5.48 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "2024-03-12 17:38:42.010171: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:38:42.010508: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:38:42.010683: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:38:42.010899: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:38:42.011068: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:38:42.011210: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 2s 6ms/step - loss: 8.0455 - accuracy: 0.0000e+00 - mean_squared_error: 8.0455 - val_loss: 0.2665 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.2665\n",
            "Epoch 2/100\n",
            " 51/157 [========>.....................] - ETA: 0s - loss: 0.1032 - accuracy: 0.0000e+00 - mean_squared_error: 0.1032"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:38:43.991961: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 17:38:43.992022: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0487 - accuracy: 0.0000e+00 - mean_squared_error: 0.0487 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 3/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 4/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 5/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 6/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 7/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 8/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 9/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 10/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 11/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 12/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 13/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 14/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 15/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 16/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 17/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 18/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 19/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 20/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 21/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 22/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 23/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 24/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 25/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 26/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 27/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 28/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 29/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 30/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 31/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 32/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 33/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 34/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 35/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 36/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 37/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 38/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 39/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 40/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 41/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 42/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 43/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 44/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 45/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 46/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 47/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 48/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 49/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 50/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 51/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 52/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 53/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 54/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 55/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 56/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 57/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 58/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 59/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 60/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 61/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 62/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 63/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 64/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 65/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 66/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 67/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 68/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 69/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 70/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 71/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 72/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 73/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 74/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 75/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 76/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 77/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 78/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 79/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 80/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 81/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 82/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 83/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 84/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 85/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 86/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 87/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 88/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 89/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 90/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 91/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 92/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 93/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 94/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 95/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 96/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 97/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 98/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 99/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 100/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "8/8 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:40:00.632613: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:40:00.632862: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:40:00.633024: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:40:00.633222: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:40:00.633382: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:40:00.633527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(7.2373815, shape=(), dtype=float32)\n",
            "[7.089455]\n",
            "tf.Tensor(6.887478, shape=(), dtype=float32)\n",
            "[7.0884066]\n",
            "tf.Tensor(6.9949045, shape=(), dtype=float32)\n",
            "[7.08874]\n",
            "tf.Tensor(6.9668975, shape=(), dtype=float32)\n",
            "[7.088654]\n",
            "0.12526333 0.17323682\n",
            "1.3829811\n",
            "7.371052631578948\n",
            "/device:GPU:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 196/196 [01:51<00:00,  1.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_15\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rho (InputLayer)            [(None, 8, 8, 1)]         0         \n",
            "                                                                 \n",
            " flatten_15 (Flatten)        (None, 64)                0         \n",
            "                                                                 \n",
            " concatenate_15 (Concatenat  (None, 64)                0         \n",
            " e)                                                              \n",
            "                                                                 \n",
            " dense_60 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            " dense_61 (Dense)            (None, 16)                176       \n",
            "                                                                 \n",
            " dense_62 (Dense)            (None, 32)                544       \n",
            "                                                                 \n",
            " dense_63 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1403 (5.48 KB)\n",
            "Trainable params: 1403 (5.48 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "2024-03-12 17:41:52.381766: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:41:52.382165: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:41:52.382472: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:41:52.382795: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:41:52.383054: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:41:52.383258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    142/Unknown - 2s 3ms/step - loss: 30.7106 - accuracy: 0.0000e+00 - mean_squared_error: 30.7106"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:41:54.008879: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 17:41:54.008955: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 2s 6ms/step - loss: 28.7729 - accuracy: 0.0000e+00 - mean_squared_error: 28.7729 - val_loss: 8.1448 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 8.1448\n",
            "Epoch 2/100\n",
            " 51/157 [========>.....................] - ETA: 0s - loss: 5.2773 - accuracy: 0.0000e+00 - mean_squared_error: 5.2773"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:41:54.419639: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 17:41:54.419692: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 1s 5ms/step - loss: 2.5322 - accuracy: 0.0000e+00 - mean_squared_error: 2.5322 - val_loss: 0.2803 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.2803\n",
            "Epoch 3/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0823 - accuracy: 0.0000e+00 - mean_squared_error: 0.0823 - val_loss: 0.0236 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0236\n",
            "Epoch 4/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0213 - accuracy: 0.0000e+00 - mean_squared_error: 0.0213 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 5/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 6/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 7/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 8/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 9/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 10/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 11/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 12/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 13/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 14/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 15/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 16/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 17/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 18/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 19/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 20/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 21/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 22/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 23/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 24/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 25/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 26/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 27/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 28/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 29/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 30/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 31/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 32/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 33/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 34/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 35/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 36/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 37/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 38/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 39/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 40/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 41/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 42/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 43/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 44/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 45/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 46/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 47/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 48/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 49/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 50/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 51/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 52/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 53/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 54/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 55/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 56/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 57/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 58/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 59/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 60/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 61/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 62/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 63/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 64/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 65/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 66/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 67/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 68/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 69/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 70/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 71/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 72/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 73/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 74/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 75/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 76/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 77/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 78/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 79/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 80/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 81/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 82/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 83/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 84/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 85/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 86/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 87/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 88/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 89/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 90/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 91/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 92/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 93/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 94/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 95/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 96/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 97/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 98/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 99/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 100/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "8/8 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:43:12.655686: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(7.6569147, shape=(), dtype=float32)\n",
            "[7.6229553]\n",
            "tf.Tensor(7.702554, shape=(), dtype=float32)\n",
            "[7.6229887]\n",
            "tf.Tensor(7.801806, shape=(), dtype=float32)\n",
            "[7.6230617]\n",
            "tf.Tensor(7.8065515, shape=(), dtype=float32)\n",
            "[7.6230645]\n",
            "0.12664992 0.15410654\n",
            "1.2167915\n",
            "7.8968421052631586\n",
            "/device:GPU:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/196 [00:00<?, ?it/s]2024-03-12 17:43:12.655943: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:43:12.656103: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:43:12.656298: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:43:12.656457: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:43:12.656598: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n",
            "100%|██████████| 196/196 [01:54<00:00,  1.72it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rho (InputLayer)            [(None, 8, 8, 1)]         0         \n",
            "                                                                 \n",
            " flatten_16 (Flatten)        (None, 64)                0         \n",
            "                                                                 \n",
            " concatenate_16 (Concatenat  (None, 64)                0         \n",
            " e)                                                              \n",
            "                                                                 \n",
            " dense_64 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            " dense_65 (Dense)            (None, 16)                176       \n",
            "                                                                 \n",
            " dense_66 (Dense)            (None, 32)                544       \n",
            "                                                                 \n",
            " dense_67 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1403 (5.48 KB)\n",
            "Trainable params: 1403 (5.48 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "2024-03-12 17:45:07.005579: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:45:07.005915: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:45:07.006097: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:45:07.006319: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:45:07.006556: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:45:07.006709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    149/Unknown - 2s 3ms/step - loss: 41.1668 - accuracy: 0.0000e+00 - mean_squared_error: 41.1668"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:45:08.783410: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 3s 8ms/step - loss: 40.0548 - accuracy: 0.0000e+00 - mean_squared_error: 40.0548 - val_loss: 16.2315 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 16.2315\n",
            "Epoch 2/100\n",
            " 50/157 [========>.....................] - ETA: 0s - loss: 12.3106 - accuracy: 0.0000e+00 - mean_squared_error: 12.3106"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:45:09.601195: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 17:45:09.601272: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 1s 5ms/step - loss: 7.1900 - accuracy: 0.0000e+00 - mean_squared_error: 7.1900 - val_loss: 1.9677 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 1.9677\n",
            "Epoch 3/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.6905 - accuracy: 0.0000e+00 - mean_squared_error: 0.6905 - val_loss: 0.1190 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.1190\n",
            "Epoch 4/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0479 - accuracy: 0.0000e+00 - mean_squared_error: 0.0479 - val_loss: 0.0227 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0227\n",
            "Epoch 5/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0214 - accuracy: 0.0000e+00 - mean_squared_error: 0.0214 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 6/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 7/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 8/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 9/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 10/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 11/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 12/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 13/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 14/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 15/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 16/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 17/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 18/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 19/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 20/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 21/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 22/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 23/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 24/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 25/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 26/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 27/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 28/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 29/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 30/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 31/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 32/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 33/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 34/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 35/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 36/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 37/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 38/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 39/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 40/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 41/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 42/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 43/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 44/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 45/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 46/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 47/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 48/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 49/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 50/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 51/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 52/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 53/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 54/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 55/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 56/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 57/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 58/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 59/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 60/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 61/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 62/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 63/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 64/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 65/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 66/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 67/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 68/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 69/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 70/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 71/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 72/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 73/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 74/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 75/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 76/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 77/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 78/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 79/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 80/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 81/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 82/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 83/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 84/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 85/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 86/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 87/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 88/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 89/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 90/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 91/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 92/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 93/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 94/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 95/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 96/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 97/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 98/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 99/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "Epoch 100/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - mean_squared_error: 0.0210 - val_loss: 0.0206 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0206\n",
            "8/8 [==============================] - 0s 1ms/step\n",
            "tf.Tensor(7.986399, shape=(), dtype=float32)\n",
            "[8.145678]\n",
            "tf.Tensor(8.021889, shape=(), dtype=float32)\n",
            "[8.145682]\n",
            "tf.Tensor(8.075761, shape=(), dtype=float32)\n",
            "[8.145691]\n",
            "tf.Tensor(7.9629903, shape=(), dtype=float32)\n",
            "[8.145674]\n",
            "0.11833882 0.1609611\n",
            "1.3601716\n",
            "8.42263157894737\n",
            "/device:GPU:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:46:28.372731: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:46:28.373011: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:46:28.373175: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "  0%|          | 0/196 [00:00<?, ?it/s]local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:46:28.373538: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:46:28.373692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n",
            "100%|██████████| 196/196 [01:55<00:00,  1.69it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_17\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rho (InputLayer)            [(None, 8, 8, 1)]         0         \n",
            "                                                                 \n",
            " flatten_17 (Flatten)        (None, 64)                0         \n",
            "                                                                 \n",
            " concatenate_17 (Concatenat  (None, 64)                0         \n",
            " e)                                                              \n",
            "                                                                 \n",
            " dense_68 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            " dense_69 (Dense)            (None, 16)                176       \n",
            "                                                                 \n",
            " dense_70 (Dense)            (None, 32)                544       \n",
            "                                                                 \n",
            " dense_71 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1403 (5.48 KB)\n",
            "Trainable params: 1403 (5.48 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "2024-03-12 17:48:24.155369: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:48:24.155711: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:48:24.155893: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:48:24.156117: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:48:24.156296: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:48:24.156445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    155/Unknown - 2s 3ms/step - loss: 35.0664 - accuracy: 0.0000e+00 - mean_squared_error: 35.0664"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:48:25.866262: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 17:48:25.866322: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n",
            "2024-03-12 17:48:25.866332: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 407123110838793230\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 2s 6ms/step - loss: 34.8636 - accuracy: 0.0000e+00 - mean_squared_error: 34.8636 - val_loss: 9.4415 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 9.4415\n",
            "Epoch 2/100\n",
            " 49/157 [========>.....................] - ETA: 0s - loss: 6.2509 - accuracy: 0.0000e+00 - mean_squared_error: 6.2509"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:48:26.297830: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 17:48:26.297913: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 1s 6ms/step - loss: 3.0195 - accuracy: 0.0000e+00 - mean_squared_error: 3.0195 - val_loss: 0.3961 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.3961\n",
            "Epoch 3/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.1183 - accuracy: 0.0000e+00 - mean_squared_error: 0.1183 - val_loss: 0.0270 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0270\n",
            "Epoch 4/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0221 - accuracy: 0.0000e+00 - mean_squared_error: 0.0221 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 5/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 6/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 7/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 8/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 9/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 10/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 11/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 12/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 13/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 14/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 15/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 16/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 17/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 18/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 19/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 20/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 21/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 22/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 23/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 24/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 25/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 26/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 27/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 28/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 29/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 30/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 31/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 32/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 33/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 34/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 35/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 36/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 37/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 38/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 39/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 40/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 41/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 42/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 43/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 44/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 45/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 46/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 47/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 48/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 49/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 50/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 51/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 52/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 53/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 54/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 55/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 56/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 57/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 58/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 59/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 60/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 61/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 62/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 63/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 64/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 65/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 66/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 67/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 68/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 69/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 70/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 71/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 72/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 73/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 74/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 75/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 76/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 77/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 78/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 79/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 80/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 81/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 82/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 83/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 84/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 85/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 86/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 87/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 88/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 89/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 90/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 91/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 92/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 93/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 94/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 95/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 96/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 97/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 98/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 99/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 100/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "8/8 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:49:45.562081: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(8.689343, shape=(), dtype=float32)\n",
            "[8.6772175]\n",
            "tf.Tensor(8.681929, shape=(), dtype=float32)\n",
            "[8.677216]\n",
            "tf.Tensor(8.618863, shape=(), dtype=float32)\n",
            "[8.677195]\n",
            "tf.Tensor(8.76074, shape=(), dtype=float32)\n",
            "[8.677241]\n",
            "0.12888433 0.17361465\n",
            "1.3470578\n",
            "8.948421052631579\n",
            "/device:GPU:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:49:45.562349: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:49:45.562568: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:49:45.562792: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:49:45.562960: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:49:45.563109: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n",
            "100%|██████████| 196/196 [01:56<00:00,  1.69it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_18\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rho (InputLayer)            [(None, 8, 8, 1)]         0         \n",
            "                                                                 \n",
            " flatten_18 (Flatten)        (None, 64)                0         \n",
            "                                                                 \n",
            " concatenate_18 (Concatenat  (None, 64)                0         \n",
            " e)                                                              \n",
            "                                                                 \n",
            " dense_72 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            " dense_73 (Dense)            (None, 16)                176       \n",
            "                                                                 \n",
            " dense_74 (Dense)            (None, 32)                544       \n",
            "                                                                 \n",
            " dense_75 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1403 (5.48 KB)\n",
            "Trainable params: 1403 (5.48 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "2024-03-12 17:51:41.829090: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:51:41.829400: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:51:41.829589: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:51:41.829810: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:51:41.829986: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:51:41.830133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    143/Unknown - 2s 3ms/step - loss: 62.5575 - accuracy: 0.0000e+00 - mean_squared_error: 62.5575"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:51:43.509460: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 17:51:43.509551: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n",
            "2024-03-12 17:51:43.509561: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 1353796229350139122\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 2s 6ms/step - loss: 59.6619 - accuracy: 0.0000e+00 - mean_squared_error: 59.6619 - val_loss: 26.0230 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 26.0230\n",
            "Epoch 2/100\n",
            " 46/157 [=======>......................] - ETA: 0s - loss: 20.4196 - accuracy: 0.0000e+00 - mean_squared_error: 20.4196"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:51:43.951379: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 17:51:43.951450: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 1s 5ms/step - loss: 12.3265 - accuracy: 0.0000e+00 - mean_squared_error: 12.3265 - val_loss: 4.4575 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 4.4575\n",
            "Epoch 3/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 2.0474 - accuracy: 0.0000e+00 - mean_squared_error: 2.0474 - val_loss: 0.6572 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.6572\n",
            "Epoch 4/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.2795 - accuracy: 0.0000e+00 - mean_squared_error: 0.2795 - val_loss: 0.0832 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0832\n",
            "Epoch 5/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0430 - accuracy: 0.0000e+00 - mean_squared_error: 0.0430 - val_loss: 0.0246 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0246\n",
            "Epoch 6/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0219 - accuracy: 0.0000e+00 - mean_squared_error: 0.0219 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 7/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 8/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 9/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 10/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 11/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 12/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 13/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 14/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 15/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 16/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 17/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 18/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 19/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 20/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 21/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 22/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 23/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 24/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 25/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 26/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 27/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 28/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 29/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 30/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 31/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 32/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 33/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 34/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 35/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 36/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 37/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 38/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 39/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 40/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 41/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 42/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 43/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 44/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 45/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 46/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 47/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 48/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 49/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 50/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 51/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 52/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 53/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 54/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 55/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 56/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 57/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 58/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 59/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 60/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 61/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 62/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 63/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 64/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 65/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 66/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 67/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 68/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 69/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 70/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 71/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 72/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 73/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 74/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 75/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 76/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 77/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 78/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 79/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 80/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 81/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 82/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 83/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 84/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 85/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 86/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 87/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 88/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 89/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 90/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 91/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 92/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 93/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 94/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 95/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 96/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 97/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 98/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 99/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "Epoch 100/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0207 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0207\n",
            "8/8 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:53:01.257149: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(9.149671, shape=(), dtype=float32)\n",
            "[9.196247]\n",
            "tf.Tensor(9.409496, shape=(), dtype=float32)\n",
            "[9.196278]\n",
            "tf.Tensor(9.362543, shape=(), dtype=float32)\n",
            "[9.196271]\n",
            "tf.Tensor(9.344836, shape=(), dtype=float32)\n",
            "[9.19627]\n",
            "0.122030675 0.16535588\n",
            "1.3550353\n",
            "9.47421052631579\n",
            "/device:GPU:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:53:01.257416: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:53:01.257578: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:53:01.257776: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:53:01.257936: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:53:01.258077: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n",
            "100%|██████████| 196/196 [01:53<00:00,  1.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_19\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rho (InputLayer)            [(None, 8, 8, 1)]         0         \n",
            "                                                                 \n",
            " flatten_19 (Flatten)        (None, 64)                0         \n",
            "                                                                 \n",
            " concatenate_19 (Concatenat  (None, 64)                0         \n",
            " e)                                                              \n",
            "                                                                 \n",
            " dense_76 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            " dense_77 (Dense)            (None, 16)                176       \n",
            "                                                                 \n",
            " dense_78 (Dense)            (None, 32)                544       \n",
            "                                                                 \n",
            " dense_79 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1403 (5.48 KB)\n",
            "Trainable params: 1403 (5.48 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "2024-03-12 17:54:54.929538: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:54:54.929824: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:54:54.929987: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:54:54.930188: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:54:54.930347: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:54:54.930532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    151/Unknown - 2s 3ms/step - loss: 54.4509 - accuracy: 0.0000e+00 - mean_squared_error: 54.4509"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:54:56.524429: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 17:54:56.524515: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n",
            "2024-03-12 17:54:56.524543: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 407123110838793230\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 2s 6ms/step - loss: 53.3396 - accuracy: 0.0000e+00 - mean_squared_error: 53.3396 - val_loss: 20.3363 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 20.3363\n",
            "Epoch 2/100\n",
            " 50/157 [========>.....................] - ETA: 0s - loss: 14.9854 - accuracy: 0.0000e+00 - mean_squared_error: 14.9854"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:54:56.941670: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 17:54:56.941738: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 1s 5ms/step - loss: 8.4868 - accuracy: 0.0000e+00 - mean_squared_error: 8.4868 - val_loss: 2.2061 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.2061\n",
            "Epoch 3/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.8121 - accuracy: 0.0000e+00 - mean_squared_error: 0.8121 - val_loss: 0.1580 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.1580\n",
            "Epoch 4/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0632 - accuracy: 0.0000e+00 - mean_squared_error: 0.0632 - val_loss: 0.0250 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0250\n",
            "Epoch 5/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0220 - accuracy: 0.0000e+00 - mean_squared_error: 0.0220 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 6/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 7/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 8/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 9/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 10/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 11/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 12/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 13/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 14/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 15/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 16/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 17/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 18/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 19/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 20/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 21/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 22/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 23/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 24/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 25/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 26/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 27/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 28/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 29/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 30/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 31/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 32/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 33/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 34/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 35/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 36/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 37/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 38/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 39/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 40/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 41/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 42/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 43/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 44/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 45/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 46/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 47/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 48/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 49/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 50/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 51/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 52/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 53/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 54/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 55/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 56/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 57/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 58/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 59/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 60/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 61/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 62/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 63/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 64/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 65/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 66/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 67/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 68/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 69/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 70/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 71/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 72/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 73/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 74/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 75/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 76/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 77/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 78/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 79/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 80/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 81/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 82/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 83/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 84/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 85/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 86/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 87/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 88/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 89/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 90/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 91/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 92/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 93/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 94/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 95/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 96/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 97/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 98/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 99/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 100/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - mean_squared_error: 0.0209 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "8/8 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:56:15.297463: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(9.704356, shape=(), dtype=float32)\n",
            "[9.715017]\n",
            "tf.Tensor(9.477821, shape=(), dtype=float32)\n",
            "[9.714995]\n",
            "tf.Tensor(9.790624, shape=(), dtype=float32)\n",
            "[9.715025]\n",
            "tf.Tensor(9.51896, shape=(), dtype=float32)\n",
            "[9.714999]\n",
            "0.12046434 0.16640495\n",
            "1.3813627\n",
            "10.0\n",
            "/device:GPU:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:56:15.297726: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:56:15.297895: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:56:15.298113: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:56:15.298288: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:56:15.298456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n",
            "100%|██████████| 196/196 [01:55<00:00,  1.70it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_20\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rho (InputLayer)            [(None, 8, 8, 1)]         0         \n",
            "                                                                 \n",
            " flatten_20 (Flatten)        (None, 64)                0         \n",
            "                                                                 \n",
            " concatenate_20 (Concatenat  (None, 64)                0         \n",
            " e)                                                              \n",
            "                                                                 \n",
            " dense_80 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            " dense_81 (Dense)            (None, 16)                176       \n",
            "                                                                 \n",
            " dense_82 (Dense)            (None, 32)                544       \n",
            "                                                                 \n",
            " dense_83 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1403 (5.48 KB)\n",
            "Trainable params: 1403 (5.48 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "2024-03-12 17:58:10.690875: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:58:10.691181: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:58:10.691365: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:58:10.691614: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:58:10.691787: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-03-12 17:58:10.691921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 14604 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    151/Unknown - 2s 3ms/step - loss: 51.6281 - accuracy: 0.0000e+00 - mean_squared_error: 51.6281"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:58:12.291016: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 17:58:12.291086: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n",
            "2024-03-12 17:58:12.291099: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 1353796229350139122\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 2s 6ms/step - loss: 50.5632 - accuracy: 0.0000e+00 - mean_squared_error: 50.5632 - val_loss: 19.0004 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 19.0004\n",
            "Epoch 2/100\n",
            " 51/157 [========>.....................] - ETA: 0s - loss: 14.1669 - accuracy: 0.0000e+00 - mean_squared_error: 14.1669"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 17:58:12.700292: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10624554673512270284\n",
            "2024-03-12 17:58:12.700363: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11954791201448340350\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 1s 5ms/step - loss: 8.3566 - accuracy: 0.0000e+00 - mean_squared_error: 8.3566 - val_loss: 2.5024 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 2.5024\n",
            "Epoch 3/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.9914 - accuracy: 0.0000e+00 - mean_squared_error: 0.9914 - val_loss: 0.2245 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.2245\n",
            "Epoch 4/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0866 - accuracy: 0.0000e+00 - mean_squared_error: 0.0866 - val_loss: 0.0291 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0291\n",
            "Epoch 5/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0230 - accuracy: 0.0000e+00 - mean_squared_error: 0.0230 - val_loss: 0.0210 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0210\n",
            "Epoch 6/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 7/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 8/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 9/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 10/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 11/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 12/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 13/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 14/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 15/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 16/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 17/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 18/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 19/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 20/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 21/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 22/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 23/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 24/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 25/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 26/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 27/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 28/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 29/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 30/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 31/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 32/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 33/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 34/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 35/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 36/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 37/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 38/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 39/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 40/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 41/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 42/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 43/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 44/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 45/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 46/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 47/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 48/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 49/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 50/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 51/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 52/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 53/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 54/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 55/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 56/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 57/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 58/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 59/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 60/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - mean_squared_error: 0.0207 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 61/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 62/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 63/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 64/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 65/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 66/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 67/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 68/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 69/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 70/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 71/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 72/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 73/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 74/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 75/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 76/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 77/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 78/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 79/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0209 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0209\n",
            "Epoch 80/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 81/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 82/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 83/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 84/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 85/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 86/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 87/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 88/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 89/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 90/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 91/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 92/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 93/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 94/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 95/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 96/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 97/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 98/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 99/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "Epoch 100/100\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - mean_squared_error: 0.0208 - val_loss: 0.0208 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 0.0208\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "tf.Tensor(10.033898, shape=(), dtype=float32)\n",
            "[10.255551]\n",
            "tf.Tensor(10.433544, shape=(), dtype=float32)\n",
            "[10.255572]\n",
            "tf.Tensor(10.499094, shape=(), dtype=float32)\n",
            "[10.255575]\n",
            "tf.Tensor(10.3738575, shape=(), dtype=float32)\n",
            "[10.255571]\n",
            "0.12570347 0.17100905\n",
            "1.3604163\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fdea476bcd0>]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Barrido en intervalos de G para G cte\n",
        "g_init_range = np.linspace(0.01,10,20)\n",
        "err_arr = []\n",
        "for g_init in g_init_range:\n",
        "    print(g_init)\n",
        "    dataset, label_size, input_type = gen_dataset('const', g_init, g_init+0.5, 'gs', 'rho1')\n",
        "    # DNN\n",
        "    model, val_dataset, history = dnn_fit(dataset, label_size, input_type)\n",
        "    err = dnn_error_coef(model, val_dataset)\n",
        "    # RF\n",
        "    #regressor, X_test, y_test, y_train = rf_fit(dataset)\n",
        "    #err = rf_error_coef(regressor, X_test, y_test, y_train)\n",
        "    err_arr.append(err)\n",
        "\n",
        "plt.plot(g_init_range,err_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fdea4669870>]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAGwCAYAAABPSaTdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABG10lEQVR4nO3deXyU5b3///c9k2SyJySQDQKExaIsAQRSqlU8UBX6xbXtV8UjVlu6oFZojzZd1NqjuPZYlaOltdoepWrPUWo9v68WEEFbdkwRF2QnkIUlJpMFJsnM/fsjuScZCJCEydyzvJ6PxzzM3LN9Zkg771z357ouwzRNUwAAAFHKYXcBAAAAfYmwAwAAohphBwAARDXCDgAAiGqEHQAAENUIOwAAIKoRdgAAQFSLs7uAcODz+VRRUaG0tDQZhmF3OQAAoBtM01R9fb0KCgrkcJx6/IawI6miokKFhYV2lwEAAHqhvLxcgwYNOuXthB1JaWlpkto+rPT0dJurAQAA3eF2u1VYWOj/Hj8Vwo7kP3WVnp5O2AEAIMKcqQWFBmUAABDVCDsAACCqEXYAAEBUI+wAAICoRtgBAABRjbADAACiGmEHAABENcIOAACIaoQdAAAQ1Qg7AAAgqhF2AABAVCPsAACAqEbYwSkda/baXQIAAGeNsIMulZXXatwv3tajb39qdykAAJwVwg66tHFPjVq8pt759LDdpQAAcFYIO+hSZd1xSdLuww3y+kybqwEAoPdsDTtr1qzR7NmzVVBQIMMwtGzZsoDbDcPo8vLoo4/67zN06NCTbn/ooYdC/E6iT2XdMUmSp9WnA5832VwNAAC9Z2vYaWxsVHFxsRYvXtzl7ZWVlQGX3//+9zIMQ9dee23A/e6///6A+91+++2hKD+qWSM7krTzUIONlQAAcHbi7HzxmTNnaubMmae8PS8vL+D6X/7yF11yySUaNmxYwPG0tLST7ns6Ho9HHo/Hf93tdnf7sbGiqlPY2XGoQdPPzbWxGgAAei9ienaqq6v1v//7v7r11ltPuu2hhx5Sdna2JkyYoEcffVStra2nfa5FixYpIyPDfyksLOyrsiNSq9enQ/Wdwk41IzsAgMhl68hOT/zhD39QWlqarrnmmoDjd9xxhyZOnKisrCz94x//UGlpqSorK/WrX/3qlM9VWlqqhQsX+q+73W4CTyeH6j3q3JO88zBhBwAQuSIm7Pz+97/XnDlzlJiYGHC8c2gZN26cEhIS9J3vfEeLFi2Sy+Xq8rlcLtcpb0NHv47TYcjrM7XrUINM05RhGDZXBgBAz0XEaaz33ntP27dv17e+9a0z3rekpEStra3au3dv3xcWpax+nTEDM+R0GGrwtKrKffwMjwIAIDxFRNh57rnndP7556u4uPiM9y0rK5PD4VBOTk4IKotO1rTzwVnJGpqdLIm+HQBA5LL1NFZDQ4N27tzpv75nzx6VlZUpKytLgwcPltTWT/PnP/9Zjz/++EmPX7t2rdavX69LLrlEaWlpWrt2rRYsWKAbb7xR/fr1C9n7iDbWaaz8jEQ1t3q163Cjdh5q0EXnDLC5MgAAes7WsLNp0yZdcskl/utW/83cuXP1wgsvSJJefvllmaap66+//qTHu1wuvfzyy7rvvvvk8XhUVFSkBQsWBPTxoOeqOoWdBKdDb39UrR2stQMAiFC2hp1p06bJNE+/FcG8efM0b968Lm+bOHGi1q1b1xelxTTrNFZ+RqKyUhIkSbsIOwCACBUxs7EQOtZprLyMJMU52mZg7ThUb2dJAAD0GmEHAdoWFGxbXTo/I1HpifEyDOnzphYdbfAoO5Up+wCAyBIRs7EQOkcamuX1mYpzGOqf6lJSglOD+iVJEn07AICIRNhBAKtfJzc9Uc72U1gjc9IkEXYAAJGJsIMAHf06HStVj8hJlUSTMgAgMhF2EOB0YYcmZQBAJCLsIEBV+2msgi7Czk5GdgAAEYiwgwCdp51brLBT7fbIfbzFlroAAOgtwg4CdN4qwpKeGK+89LbrjO4AACINYQcBqroIO1KnU1lsCAoAiDCEHfh5faaq3VbYSQq4zR92DhN2AACRhbADv6MNHrX6TDkdhgakBa6U7J+RVc2MLABAZCHswK+i/RRWTprLv6CgZaR/+jkjOwCAyELYgV9Vp93OT2SN7BysPaam5taQ1gUAwNkg7MCvYyZW0km3Zae6lJWSINOUdh9uDHVpAAD0GmEHflVdrJ7cGYsLAgAiEWEHfhWnmHZuYdsIAEAkIuzAr6Nn5+TTWFJHkzIjOwCASELYgV9Xm4B2NoIZWQCACETYgSTJF7CgYNdhZ2ROmiRp39EmNbf6QlYbAABng7ADSdKRRo9avKYcRts6O13JTXcp1RUnr8/U3qPMyAIARAbCDiR1zMTKSUtUnLPrXwvDMDqtpMypLABAZCDsQNKZ+3UsTD8HAEQawg4kSZW1p149ubORTD8HAEQYwg4kSZXu7o3sjMxlZAcAEFkIO5DU0bNTcIo1diwjBrTNyNp9pFFen9nndQEAcLYIO5DU/Z6dgf2SlBjvUHOrT+U1TaEoDQCAs0LYgSSp8jQ7nnfmdBga1p/FBQEAkYOwg7YFBes8ks48siPRtwMAiCyEHaimqVnNXp8MQ8pNP3PYGTGAGVkAgMhB2IG/OXlAqkvxp1hQsDNGdgAAkYSwA1V0c40dS+eFBU2TGVkAgPBG2IGq/BuAnn7auWVIdoriHIaamr2qaB8VAgAgXBF20O1p55Z4p0NF/VMkcSoLABD+CDvw9+x09zSWpE4bgtKkDAAIb4Qd+Ht2ujuyI3XskbXrMCM7AIDwZmvYWbNmjWbPnq2CggIZhqFly5YF3H7zzTfLMIyAy+WXXx5wn5qaGs2ZM0fp6enKzMzUrbfeqoYGvoB7wurZKcjsXs+OJA33j+zwWQMAwputYaexsVHFxcVavHjxKe9z+eWXq7Ky0n/505/+FHD7nDlz9NFHH2n58uV68803tWbNGs2bN6+vS48apml29Ox0Y40dy8ictj2ydjAjCwAQ5uLsfPGZM2dq5syZp72Py+VSXl5el7d98skneuutt7Rx40ZNmjRJkvTUU09p1qxZeuyxx1RQUBD0mqPN500tam71SeregoKWYQNSZBhS3bEWHWlo1oA0V1+VCADAWQn7np13331XOTk5+sIXvqDvfe97Onr0qP+2tWvXKjMz0x90JGnGjBlyOBxav379KZ/T4/HI7XYHXGKV1a/TP9WlhLju/zokxjs1OCtZEispAwDCW1iHncsvv1x//OMftXLlSj388MNavXq1Zs6cKa/XK0mqqqpSTk5OwGPi4uKUlZWlqqqqUz7vokWLlJGR4b8UFhb26fsIZ9ZMrILM7o/qWPxNykw/BwCEMVtPY53Jdddd5/957NixGjdunIYPH653331X06dP7/XzlpaWauHChf7rbrc7ZgNPpbvn/TqW4TmpWvHJIXY/BwCEtbAe2TnRsGHD1L9/f+3cuVOSlJeXp0OHDgXcp7W1VTU1Nafs85Ha+oDS09MDLrGqsodbRXRmNSmzsCAAIJxFVNg5cOCAjh49qvz8fEnS1KlTVVtbq82bN/vv884778jn86mkpMSuMiNKlX/15O5PO7f4FxYk7AAAwpitp7EaGhr8ozSStGfPHpWVlSkrK0tZWVn6xS9+oWuvvVZ5eXnatWuX7rrrLo0YMUKXXXaZJOncc8/V5Zdfrm9/+9t69tln1dLSottuu03XXXcdM7G6qfIsenassHO43qO6phZlJMcHtTYAAILB1pGdTZs2acKECZowYYIkaeHChZowYYLuueceOZ1Obd26VVdccYXOOecc3XrrrTr//PP13nvvyeXqmOb80ksvadSoUZo+fbpmzZqlCy+8UEuWLLHrLUWcqrPo2Ul1xflPf+08zIwsAEB4snVkZ9q0aaddkO7tt98+43NkZWVp6dKlwSwrZpim6Z963t0dz080IidVlXXHtfNQg84fkhXM8gAACIqI6tlBcNU2tchjLSiY0btFAUewbQQAIMwRdmKY1a/TPzVBrjhnr56j87YRAACEI8JODKty93y38xONzG0b2WH6OQAgXBF2YlhFrdWc3Lt+HUkaMaAt7BysPaZGT2tQ6gIAIJgIOzHMWmOnNwsKWvqlJKh/aoIkaffhxqDUBQBAMBF2YpjVs5PfizV2Ohs+wFpckOnnAIDwQ9iJYVbPztmM7Ej07QAAwhthJ4ZVBqFnR+ro22FGFgAgHBF2YpRpmh2nsc56ZKdt+vkuwg4AIAwRdmKU+1irjrV4JZ3d1HOpY2HBvUcb5Wn1nnVtAAAEE2EnRlW29+tkpSQoMb53CwpactJcSkuMk8+U9hxhRhYAILwQdmJUR7/O2Y3qSJJhGBqZQ5MyACA8EXZilNWvU3CW084t7JEFAAhXhJ0YVVV39ltFdGbtkbXzMGEHABBeCDsxqmMm1tlNO7dYIzs7GdkBAIQZwk6MssJOMHp2pI6ws+dIo1q9vqA8JwAAwUDYiVGV7aexznarCMvAzCQlxTvV7PVpf01TUJ4TAIBgIOzEoMAFBYNzGsvhMDQ8J0USM7IAAOGFsBOD3Mdb1dTcvqBgkE5jSWwbAQAIT4SdGFTVPqqTmRyvpISzW1CwM2vbCEZ2AADhhLATg/z9OkE6hWUZwcKCAIAwRNiJQVVB2gD0RJ3Djs9nBvW5AQDoLcJODKqwpp0HOewMyUpWvNPQsRavKtpHjwAAsBthJwZZqyfnB7E5WZLinA4V9W+bkUWTMgAgXBB2YpB/2nlmcHt2pI5tI3YRdgAAYYKwE4P6qmdHkoazISgAIMwQdmJQZR/17EjSSKtJmQ1BAQBhgrATY+qPt6jB0yqpb0Z2RuZaIzv1Mk1mZAEA7EfYiTHWKayMpHglJ8QF/fmL+qfIYbSt0ny43hP05wcAoKcIOzGmsg/7dSTJFefUkGz2yAIAhA/CToyxVk/ui34dy3D2yAIAhBHCTozp65EdqaNvh5EdAEA4IOzEmI5p58FfY8fSsft5fZ+9BgAA3UXYiTF9Oe3c0jGy09hnrwEAQHcRdmJMx47nfd+zc6TBo9qm5j57HQAAuoOwE2MqQ3AaK8UVp4HtW1HQtwMAsBthJ4Y0eFpVf7xtQcG+PI0lSSNymJEFAAgPtoadNWvWaPbs2SooKJBhGFq2bJn/tpaWFt19990aO3asUlJSVFBQoJtuukkVFRUBzzF06FAZhhFweeihh0L8TiKD1ZyclhinVFfwFxTsbAR7ZAEAwoStYaexsVHFxcVavHjxSbc1NTVpy5Yt+vnPf64tW7botdde0/bt23XFFVecdN/7779flZWV/svtt98eivIjTij6dSzskQUACBd9++f9GcycOVMzZ87s8raMjAwtX7484NjTTz+tKVOmaP/+/Ro8eLD/eFpamvLy8rr9uh6PRx5Px1YGbre7h5VHplD061iskZ2d1Uw/BwDYK6J6durq6mQYhjIzMwOOP/TQQ8rOztaECRP06KOPqrW19bTPs2jRImVkZPgvhYWFfVh1+KgKwYKCFivsVNQd9288CgCAHSIm7Bw/flx33323rr/+eqWnp/uP33HHHXr55Ze1atUqfec739GDDz6ou+6667TPVVpaqrq6Ov+lvLy8r8sPC6HYKsKSmZyg/qkuSdIumpQBADay9TRWd7W0tOgb3/iGTNPUM888E3DbwoUL/T+PGzdOCQkJ+s53vqNFixbJ5XJ1+Xwul+uUt0WzUGwV0dnInFQdafBo56EGFRdmhuQ1AQA4UdiP7FhBZ9++fVq+fHnAqE5XSkpK1Nraqr1794amwAgSiq0iOmP6OQAgHIT1yI4VdHbs2KFVq1YpOzv7jI8pKyuTw+FQTk5OCCqMLCEf2WFDUABAGLA17DQ0NGjnzp3+63v27FFZWZmysrKUn5+vr33ta9qyZYvefPNNeb1eVVVVSZKysrKUkJCgtWvXav369brkkkuUlpamtWvXasGCBbrxxhvVr18/u95WWGpqblXdsRZJoenZkTrNyGJDUACAjWwNO5s2bdIll1ziv27138ydO1f33Xef3njjDUnS+PHjAx63atUqTZs2TS6XSy+//LLuu+8+eTweFRUVacGCBQF9PGhjjeqkuuKUlhgfkte0ws7+miYdb/EqMd4ZktcFAKAzW8POtGnTZJrmKW8/3W2SNHHiRK1bty7YZUWlUE47twxIdSkjKV51x1q050ijzs0/fb8VAAB9IewblBEc1shOqE5hSZJhGDQpAwBsR9iJEZW1odsqojP/thGEHQCATQg7MaLSbY3shGbauYUmZQCA3Qg7McLq2SkI8cjOCEZ2AAA2I+zECDt6diRpZG6aJGnPkUa1en0hfW0AACTCTsyw9sUK1erJloKMRCUnONXiNbWvpimkrw0AgETYiQnHmr2qbQrtgoKWgBlZ1ZzKAgCEHmEnBlS1NyenJDiVnhj6pZVGDKBJGQBgH8JODLBOYeVlJMowjJC//gj2yAIA2IiwEwMqa0O72/mJrJEdFhYEANiBsBMDqtz2zMSyWDOydh1ukM93+i1AAAAINsJODLBOY4V6jR1LYb8kJTgdOt7i08H2lZwBAAgVwk4MqKqzZ/VkS5zToWEDUiTRtwMACD3CTgyoqA39jucn6tgQlBlZAIDQIuzEAKtnJz/T/rDDyA4AINQIO1HueItXNY3NkqT8dHtOY0nSyJy2JmVmZAEAQo2wE+Wsfp2keKfSk0K/oKDFP7JT3SDTZEYWACB0CDtRztoANN+mBQUtQ/sny+kwVO9p1aF6j211AABiD2EnylW52zcAtbFfR5JccU4NyUqWxB5ZAIDQIuxEOWtkJ8/Gfh1LR5MyM7IAAKFD2IlylWEw7dzSMf2ckR0AQOgQdqKcf2QnDMLOSDYEBQDYgLAT5ayenQKbe3akjunnhB0AQCgRdqJcVRj17FhbRhxtbPav/QMAQF8j7EQxT6tXRxraFxQMg9NYyQlxGtSvLXQxugMACBXCThSrrmtbz8YV51BmcrzN1bRhjywAQKgRdqJYZZ3Vr5Nk64KCnY1kjywAQIgRdqKYtQFoXrr9p7AsbAgKAAg1wk4UqwijNXYsI5iRBQAIsW6FnSeffFLHj7d9ce7fv5+NHCNEVftprHBYY8dijexU1h1X/fEWm6sBAMSCboWdhQsXyu12S5KKiop0+PDhPi0KweHfBDTT/mnnloykeOWkuSRJuw432lwNACAWxHXnTgUFBfqf//kfzZo1S6Zp6sCBA/6RnhMNHjw4qAWi96yenfww6tmR2lZSPlTv0Y7qeo0vzLS7HABAlOtW2PnZz36m22+/XbfddpsMw9DkyZNPuo9pmjIMQ16vN+hFonesnp1wOo0lSSMGpOrvO49q52H6dgAAfa9bYWfevHm6/vrrtW/fPo0bN04rVqxQdnZ2X9eGs9Dc6tORhrZ1dsKpQVmSRuS2NylXE3YAAH2vW2FHktLS0jRmzBg9//zzuuCCC+RyufqyLpyl6vZTWAlxDmWlJNhcTaARA9j9HAAQOj2eej537lwdO3ZMv/vd71RaWqqamhpJ0pYtW3Tw4MGgF4je8ffrZCSGzYKCFmv38/LPm3S8hdOeAIC+1eOws3XrVp1zzjl6+OGH9dhjj6m2tlaS9Nprr6m0tLRHz7VmzRrNnj1bBQUFMgxDy5YtC7jdNE3dc889ys/PV1JSkmbMmKEdO3YE3KempkZz5sxRenq6MjMzdeutt6qhgRGDitr2aedh1pwsSdkpCcpMjpdpSrvo2wEA9LEeh50FCxbo5ptv1o4dO5SY2PFFOmvWLK1Zs6ZHz9XY2Kji4mItXry4y9sfeeQRPfnkk3r22We1fv16paSk6LLLLguYCTZnzhx99NFHWr58ud58802tWbNG8+bN6+nbijrWbucFYTTt3GIYBttGAABCpts9O5ZNmzZpyZIlJx0fOHCgqqqqevRcM2fO1MyZM7u8zTRNPfHEE/rZz36mK6+8UpL0xz/+Ubm5uVq2bJmuu+46ffLJJ3rrrbe0ceNGTZo0SZL01FNPadasWXrsscdUUFDQw3cXPaw1dsJtJpblnNw0bdz7uT6ucOvK8QPtLgcAEMV6PLLjcrn8Cwx29tlnn2nAgAFBKUqS9uzZo6qqKs2YMcN/LCMjQyUlJVq7dq0kae3atcrMzPQHHUmaMWOGHA6H1q9ff8rn9ng8crvdAZdoY20CGm4zsSzF7evrlJXX2loHACD69TjsXHHFFbr//vvV0tK21L9hGNq/f7/uvvtuXXvttUErzBolys3NDTiem5vrv62qqko5OTkBt8fFxSkrK+u0o0yLFi1SRkaG/1JYWBi0usOFdRorHHt2JPkXE9x2sE5eH9uPAAD6To/DzuOPP66Ghgbl5OTo2LFjuvjiizVixAilpaXpgQce6Isag660tFR1dXX+S3l5ud0lBV1lGPfsSNLwAalKSXCqsdlLkzIAoE/1uGcnIyNDy5cv1/vvv6+tW7eqoaFBEydODDjdFAx5eXmSpOrqauXn5/uPV1dXa/z48f77HDp0KOBxra2tqqmp8T++Ky6XK6rXCWrx+nS4fUHBcO3ZcToMjR2UoXW7a1RWXqtz2hcaBAAg2Ho8smO58MIL9f3vf1933XVX0IOO1LbhaF5enlauXOk/5na7tX79ek2dOlWSNHXqVNXW1mrz5s3++7zzzjvy+XwqKSkJek2Rotp9XKYpJTgdykoOrwUFO7P6dv5J3w4AoA/1KuysXr1as2fP1ogRIzRixAhdccUVeu+993r8PA0NDSorK1NZWZmktqbksrIy7d+/X4Zh6M4779S///u/64033tCHH36om266SQUFBbrqqqskSeeee64uv/xyffvb39aGDRv097//Xbfddpuuu+66mJ6JZfXr5Ga45HCE14KCnRUPypQk/fNAra11AACiW4/DzosvvqgZM2YoOTlZd9xxh+644w4lJiZq+vTpWrp0aY+ea9OmTZowYYImTJggSVq4cKEmTJige+65R5J011136fbbb9e8efM0efJkNTQ06K233gpY3+ell17SqFGjNH36dM2aNUsXXnhhl1PjY4nVr5OfEZ79OhZrZOfTynpWUgYA9BnDNM0eTYU599xzNW/ePC1YsCDg+K9+9Sv99re/1SeffBLUAkPB7XYrIyNDdXV1Sk9Pt7ucs/bbNbv1wP/3ia4cX6BfXzfB7nJOyTRNTX5gpY40ePQ/3/uSzh/Sz+6SAAARpLvf3z0e2dm9e7dmz5590vErrrhCe/bs6enToQ9UtK+xE67NyRbDMDS+MEMSfTsAgL7T47BTWFgY0DRsWbFiRVSuVxOJrJ6d/DBdY6cz+nYAAH2tx1PPf/jDH+qOO+5QWVmZvvSlL0mS/v73v+uFF17Qr3/966AXiJ7z9+yE6Ro7nTEjCwDQ13ocdr73ve8pLy9Pjz/+uF599VVJbX08r7zyin8PK9jLP7IT5qexJGncoLbTWHuPNqm2qVmZYTxVHgAQmXocdiTp6quv1tVXXx3sWhAErV6fDtWH9yagnWUmJ6iof4r2HGnU1gN1uuic4O2vBgCA1IuenY0bN3a5yeb69eu1adOmoBSF3jtU75HPlOKdhvqnRMYq0cWDaFIGAPSdHoed+fPnd7mX1MGDBzV//vygFIXes/p1ctMTw3pBwc7G0aQMAOhDPQ47H3/8sSZOnHjS8QkTJujjjz8OSlHovUjq17FYTcpl5XXq4bJPAACcUY/DjsvlUnV19UnHKysrFRfXqxYgBFGlf42d8J+JZRldkK44h6EjDR5VtIc1AACCpcdh59JLL1Vpaanq6ur8x2pra/WTn/xEX/nKV4JaHHquMgJHdhLjnRqV37brOX07AIBg63HYeeyxx1ReXq4hQ4bokksu0SWXXKKioiJVVVXp8ccf74sa0QOReBpL6rS4IGEHABBkPQ47AwcO1NatW/XII4/ovPPO0/nnn69f//rX+vDDD1lBOQxYp7EiLuz4+3Zqba0DABB9etVkk5KSonnz5gW7FgSBdRorknp2JGl8e9jZdrBOXp8pZ4TMJAMAhL8ej+wgfLUtKOiRJBVE2MjO8AGpSklwqrHZq12HG+wuBwAQRQg7UeRIQ7O8PlNxDkPZqZGxoKDF6TA0tn1xQU5lAQCCibATRSra+3Vy0xMj8jQQTcoAgL5A2IkiVXWRsydWV/w7oLOSMgAgiHocdsrLy3XgwAH/9Q0bNujOO+/UkiVLgloYei4S19jpzAo7n1bW63iL195iAABRo8dh54YbbtCqVaskSVVVVfrKV76iDRs26Kc//anuv//+oBeI7quK0GnnloKMRPVPdanVZ+qjCrfd5QAAokSPw862bds0ZcoUSdKrr76qMWPG6B//+IdeeuklvfDCC8GuDz1QEaHTzi2GYWh8ITugAwCCq8dhp6WlRS5X20yfFStW6IorrpAkjRo1SpWVlcGtDj0Sqasnd1bMDugAgCDrcdgZPXq0nn32Wb333ntavny5Lr/8cklSRUWFsrOzg14gui8qwo7VpMzIDgAgSHocdh5++GH95je/0bRp03T99deruLhYkvTGG2/4T28h9Lw+U9VuK+xE5mksSRrXvtbO3qNNqm1qtrkaAEA06PF2EdOmTdORI0fkdrvVr18///F58+YpOTk5qMWh+440eNTavs3CgLTIWlCws8zkBBX1T9GeI43aeqBOF50zwO6SAAARrscjO8eOHZPH4/EHnX379umJJ57Q9u3blZOTE/QC0T3WtPOcNFdELijYmTW6w6ksAEAw9DjsXHnllfrjH/8oSaqtrVVJSYkef/xxXXXVVXrmmWeCXiC6J9KnnXdGkzIAIJh6HHa2bNmiL3/5y5Kk//7v/1Zubq727dunP/7xj3ryySeDXiC6p2NBwcjt17FYTcpl5XUyTdPeYgAAEa/HYaepqUlpaWmSpL/97W+65ppr5HA49MUvflH79u0LeoHonsoI3yqis9EF6YpzGDrS4PGvHQQAQG/1OOyMGDFCy5YtU3l5ud5++21deumlkqRDhw4pPT096AWieyJ9q4jOEuOdGpXfFqjp2wEAnK0eh5177rlHP/rRjzR06FBNmTJFU6dOldQ2yjNhwoSgF4ju6ejZifzTWBI7oAMAgqfHYedrX/ua9u/fr02bNuntt9/2H58+fbr+4z/+I6jFofui6TSW1Llvp9bWOgAAka/H6+xIUl5envLy8vy7nw8aNIgFBW3kC1hQMDrCzvj2sLPtYJ287esHAQDQGz0e2fH5fLr//vuVkZGhIUOGaMiQIcrMzNQvf/lL+Xy+vqgRZ3Ck0aMWrymH0bbOTjQYPiBVyQlONTZ7tetwg93lAAAiWI9Hdn7605/queee00MPPaQLLrhAkvT+++/rvvvu0/Hjx/XAAw8EvUicXpV/QcFExTl7nF/DktNhaOzADK3fU6Oy8lqdk5tmd0kAgAjV47Dzhz/8Qb/73e/8u51L0rhx4zRw4EB9//vfJ+zYINr6dSzjCzO1fk+N/lleq29MKrS7HABAhOrxMEBNTY1GjRp10vFRo0appqYmKEWhZypro2f15M78O6CzkjIA4Cz0OOwUFxfr6aefPun4008/7d8BHaFV6Y7OkR0r7HxaWa/jLV57iwEARKweh51HHnlEv//973Xeeefp1ltv1a233qrzzjtPL7zwgh599NGgFzh06FAZhnHSZf78+ZLadmE/8bbvfve7Qa8jnFk9OwVRssaOpSAjUf1TXWr1mfqowm13OQCACNXjsHPxxRfrs88+09VXX63a2lrV1tbqmmuu0fbt2/17ZgXTxo0bVVlZ6b8sX75ckvT1r3/df59vf/vbAfd55JFHgl5HOKusjc6RHcMwNL6QHdABAGenV+vsFBQUnNSIfODAAc2bN09LliwJSmGWAQMGBFx/6KGHNHz4cF188cX+Y8nJycrLywvq60aSSnd09uxIbSspr/jkEH07AIBeC9o85aNHj+q5554L1tN1qbm5WS+++KJuueUWGUbHInMvvfSS+vfvrzFjxqi0tFRNTU2nfR6PxyO32x1wiVQ+n6nqOo8kKT8zuk5jSZ2alBnZAQD0Uq9GduyybNky1dbW6uabb/Yfu+GGGzRkyBAVFBRo69atuvvuu7V9+3a99tprp3yeRYsW6Re/+EUIKu57NU3Navb6ZETRgoKdjRvUdhpr79Em1TY1KzM5weaKAACRJqLCznPPPaeZM2eqoKDAf2zevHn+n8eOHav8/HxNnz5du3bt0vDhw7t8ntLSUi1cuNB/3e12q7AwMtdxsfp1BqS6FB8lCwp2lpmcoKHZydp7tElbD9TponMGnPlBAAB0EjHfjvv27dOKFSv0rW9967T3KykpkSTt3LnzlPdxuVxKT08PuESqyrro7dexcCoLAHA2uj2yc80115z29tra2rOt5bSef/555eTk6Ktf/epp71dWViZJys/P79N6wkWVfwPQ6OvXsRQPytRfyipoUgYA9Eq3w05GRsYZb7/pppvOuqCu+Hw+Pf/885o7d67i4jpK3rVrl5YuXapZs2YpOztbW7du1YIFC3TRRRdp3LhxfVJLuInWrSI6s0Z2ysrrZJpmQHM6AABn0u2w8/zzz/dlHae1YsUK7d+/X7fcckvA8YSEBK1YsUJPPPGEGhsbVVhYqGuvvVY/+9nPbKo09KJ1q4jORhekK85h6EiDRxV1xzUwCmedAQD6TkQ0KF966aUyTfOk44WFhVq9erUNFYWPWBjZSYx3alR+mrYddOuf5bWEHQBAj0RMgzK6ZvXsFER5ACgelCmJJmUAQM8RdiKYaZodIzvp0TuyI3Xu26m1tQ4AQOQh7ESwmsZmNbf6JEm50R522kd2th2sk9d38ilNAABOhbATwaxRnf6pLiXERfc/5YicVCUnONXY7NWuww12lwMAiCDR/Q0Z5arqrH6d6B7VkSSnw9DYgW3LH3AqCwDQE4SdCFbpjo1+Hct4VlIGAPQCYSeCxcIaO535t41gJWUAQA8QdiJYlX+Nneiedm6xws6nlfU63uK1txgAQMQg7ESwyhjq2ZGkgoxE9U91qdVn6qMKt93lAAAiBGEnglXFWM+OYRgaX9jWpEzfDgCguwg7Eco0TVX4e3Zi4zSWJI2zVlKmbwcA0E2EnQj1eVOLPNaCghkum6sJnWJmZAEAeoiwE6GskY0h2clyxTntLSaEige1ncbae7RJtU3NNlcDAIgEhJ0ItWFPjSRpytAsmysJrczkBA3NTpYkbT1QZ3M1AIBIQNiJUButsFMUW2FH4lQWAKBnCDsR6HiL138aKybDDk3KAIAeIOxEoA/216rFayo33aXBWcl2lxNy1shOWXmdTJMd0AEAp0fYiUAb91qnsLJlGIbN1YTe6IJ0xTkMHWnwqKJ9YUUAAE6FsBOBOpqT+9lciT0S450alZ8mib4dAMCZEXYiTIvXpy37P5fUNrITq/yLCxJ2AABnQNiJMB9VuNXU7FVGUrxG5qTaXY5txreHnTLCDgDgDAg7EWbDnqOSpMlDs+RwxF6/jsVqUt52sE5eH03KAIBTI+xEmA172k5hlcTglPPORuSkKjnBqcZmr3YdbrC7HABAGCPsRBCfz/TPxJoc42HH6TA0dmDb1hGcygIAnA5hJ4J8dqhedcdalJzg1OiCdLvLsd14VlIGAHQDYSeCWFtEnD+kn+Kd/NP5t41gJWUAwGnwjRlB1reHnckxtvnnqVhh59PKeh1v8dpbDAAgbBF2IoRpmh2LCcZ4v46lICNR/VNdavWZ+qjCbXc5AIAwRdiJEPtrmnSo3qMEp8PfqxLrDMNQ8aC2JmX6dgAAp0LYiRDWKaxxgzKUGO+0uZrwQd8OAOBMCDsRglNYXStmRhYA4AwIOxGiY6dzwk5n1mmsvUebVNvUbHM1AIBwRNiJAFV1x7XvaJMcRtu0c3TITE7Q0OxkSdLWA3U2VwMACEeEnQiwoX1U57yCdKUlxttcTfjhVBYA4HQIOxHAWkxwytBsmysJT8XtO6DTpAwA6AphJwJ0NCdzCqsr1shOWXmdTJMd0AEAgcI67Nx3330yDCPgMmrUKP/tx48f1/z585Wdna3U1FRde+21qq6utrHi4Pu8sVnbq+slsXLyqYwuSFecw9CRBo8q6o7bXQ4AIMyEddiRpNGjR6uystJ/ef/99/23LViwQH/961/15z//WatXr1ZFRYWuueYaG6sNvk37PpckjchJVXaqy+ZqwlNivFNfyEuTRN8OAOBkcXYXcCZxcXHKy8s76XhdXZ2ee+45LV26VP/yL/8iSXr++ed17rnnat26dfriF78Y6lL7xIY9RyUxqnMmxYWZ+qjCrX+W12rW2Hy7ywEAhJGwH9nZsWOHCgoKNGzYMM2ZM0f79++XJG3evFktLS2aMWOG/76jRo3S4MGDtXbt2tM+p8fjkdvtDriEK6tfp4T1dU5rfHuTchkjOwCAE4R12CkpKdELL7ygt956S88884z27NmjL3/5y6qvr1dVVZUSEhKUmZkZ8Jjc3FxVVVWd9nkXLVqkjIwM/6WwsLAP30XvNXpata19g0sWEzw9q0l528E6eX00KQMAOoT1aayZM2f6fx43bpxKSko0ZMgQvfrqq0pKSur185aWlmrhwoX+6263OywDz5b9n8vrMzUwM0kFmb1/v7FgRE6qkhOcamz2atfhBp2Tm2Z3SQCAMBHWIzsnyszM1DnnnKOdO3cqLy9Pzc3Nqq2tDbhPdXV1lz0+nblcLqWnpwdcwhGnsLrP6TA0dmDb1hGcygIAdBZRYaehoUG7du1Sfn6+zj//fMXHx2vlypX+27dv3679+/dr6tSpNlYZPFbYmUzY6ZbxrKQMAOhCWJ/G+tGPfqTZs2dryJAhqqio0L333iun06nrr79eGRkZuvXWW7Vw4UJlZWUpPT1dt99+u6ZOnRoVM7E8rV590P6lTb9O9/i3jWAlZQBAJ2Eddg4cOKDrr79eR48e1YABA3ThhRdq3bp1GjBggCTpP/7jP+RwOHTttdfK4/Hosssu03/+53/aXHVwbD1Qp+ZWn/qnJmhY/xS7y4kI49p3QP+0sl7HW7xKjHfaXBEAIByEddh5+eWXT3t7YmKiFi9erMWLF4eootDxn8IamiXDMGyuJjIMzExS/9QEHWlo1kcVbnaIBwBIirCenVjSsR8Wp7C6yzCMjk1B6dsBALQj7IQhr8/U5vZtIgg7PUPfDgDgRISdMPRJpVsNnlalueI0Ki88p8WHq2JmZAEATkDYCUPr209hTRraT04H/To9UdzepLz3aJNqm5ptrgYAEA4IO2HI2vxzSlG2zZVEnszkBA3NTpbUNqMNAADCTpgxTVMb91r9Oswm6g1OZQEAOiPshJldhxtU09gsV5xDYwdm2l1ORPLPyKJJGQAgwk7Ysfp1Jg7up4Q4/nl6o7jQ2iOrTqbJDugAEOv4Ng0zG9kP66yNLsiQ02HoSINHP3n9Qx1yH7e7JACAjQg7YYadzs9eYrxT3/pykSTpTxvKdfGj7+qxt7er/niLzZUBAOxA2AkjBz5vUkXdccU5DE0YnGl3ORGtdOa5evU7UzVxcKaOtXj19KqduvjRd/X79/fI0+q1uzwAQAgRdsKINaozZmCGkhPCetuyiDClKEv/870v6dkbz9ewASmqaWzW/W9+rBm/Wq2/lB2Uz0c/DwDEAsJOGOEUVvAZhqHLx+Tpb3depAevHqsBaS6V1xzTD14u0xWL39f7O47YXSIAoI8RdsLIhr1s/tlX4pwO3VAyWKv/bZp+dOk5SnXFadtBt258br3+9bn12naQBQgBIFoRdsLE4XqPdh9ulGFIk4YQdvpKckKcbvuXkVpz1yX65gVDFe809N6OI/o/T72vH7z8gcprmuwuEQAQZISdMLGxfVTnC7lpykiOt7ma6JeVkqB7Z4/WOz+cpivHF0iS/lJWoX95/F394q8f6WiDx+YKAQDBQtgJE/Tr2KMwK1m/vm6C3rz9Qn15ZH+1eE09//e9uvjRd/X0OzvU1Nxqd4kAgLNE2AkTG1hM0FZjBmbov24t0X/dOkWjC9LV4GnVY3/7TNMefVdL1+9Xq9dnd4kAgF4i7ISBumMt+qTKLUmaMpSwY6cvjxygv952oX593XgN6pekQ/VtqzBf+sQavbWtiu0nACACsZhLGNiy73OZpjQ0O1k56Yl2lxPzHA5DV44fqMvH5Omldfv11Ds7tPtwo7774mZNHJyp0lnnanIvQqlpmvK0+tTgaVWjp1WNHq8am1v915s8Xv/PrT5T35hcqIGZSX3wDgEgthB2woC1+SdTzsOLK86pWy4s0tcmDdKS1bv1u/d3a8v+Wn392bWacW6upp+b0xZSmr1q9HSElsb2651/bmi/n7cHCxluPVCr5785pQ/fIQDEBsJOGNiw56gkaUpRts2VoCvpifH60WVf0L9OHaInVuzQq5vKteKTaq34pLrXz5mc4FSKK04p1n9dcUp1xSk5wanEeKf+e/MBvfvZYR2sPcboDgCcJcKOzY41e/Vh+4J29OuEt9z0RC26ZqxuvbBIv1m9SzWNzUp2xSnV5VRKQlx7aHH6g0tKQpySXc62nzuFmeSEODkdxmlfq6L2mP6x66he2ViuhV85J0TvEACiE2HHZh+Uf64Wr6m89EQVZvEXfCQYkZOqR79e3Kevcf2UwfrHrqN6dWO57viXEYpzMpcAAHqL/we12YZO/TqGcfq/9hE7Lh2dq6yUBFW5j+vd7YftLgcAIhphx2bWysmsr4POXHFOfe38QZKkP23Yb3M1ABDZCDs2am71afO+zyWxcjJOdt3kQknSqu2HVFF7zOZqACByEXZstK2iTsdbfOqXHK8RA1LtLgdhZtiAVH1xWJZ8pvTqpnK7ywGAiEXYsdHG9n6dSUOz5DjD7BzEpuunDJYkvbKxvEdr9AAAOhB2bMTmnziTy0bnqV9yvCrrjmv1Z4fsLgcAIhJhxyY+n+lvTmblZJxKYrxT105sa1Reup5TWQDQG4Qdm2yvrpf7eKtSEpw6Lz/d7nIQxq5rP5X1zqfVqqo7bnM1ABB5CDs2sU5hTRzSjwXjcFojclI1pYhGZQDoLb5lbUK/DnriBhqVAaDXCDs2ME1TG6zFBNkPC91w+Zg8ZSTF62DtMa3ZwYrKANAThB0b7D3apMP1HiU4HSouzLS7HESAzo3Kf1rPisoA0BOEHRts2HNUkjS+MFOJ8U6bq0GkuH5K24rKKz89pGo3jcoA0F1hHXYWLVqkyZMnKy0tTTk5Obrqqqu0ffv2gPtMmzZNhmEEXL773e/aVHH3bNjTtkXE5KJ+NleCSDIyN02Th/aT12fqzzQqA0C3hXXYWb16tebPn69169Zp+fLlamlp0aWXXqrGxsaA+337299WZWWl//LII4/YVHH3bNjbNrIzpSjb5koQaawVlf+0oVw+GpUBoFvi7C7gdN56662A6y+88IJycnK0efNmXXTRRf7jycnJysvLC3V5vVJZd0zlNcfkMKTzhzCyg56ZNTZf973xkQ7WHtN7O4/o4nMG2F0SAIS9sB7ZOVFdXZ0kKSsrcAbTSy+9pP79+2vMmDEqLS1VU1PTaZ/H4/HI7XYHXELFmnI+uiBDqa6wzpoIQ4nxTl1DozIA9EjEhB2fz6c777xTF1xwgcaMGeM/fsMNN+jFF1/UqlWrVFpaqv/6r//SjTfeeNrnWrRokTIyMvyXwsLCvi7fzwo7bBGB3rJOZa34pFqHaFQGgDOKmKGF+fPna9u2bXr//fcDjs+bN8//89ixY5Wfn6/p06dr165dGj58eJfPVVpaqoULF/qvu93ukAUeK+ywvg566wt5aTp/SD9t3ve5/rz5gOZfMsLukgAgrEXEyM5tt92mN998U6tWrdKgQYNOe9+SkhJJ0s6dO095H5fLpfT09IBLKNQ0NmvHoQZJ0uSh9Oug96zRnZc37qdRGQDOIKzDjmmauu222/T666/rnXfeUVFR0RkfU1ZWJknKz8/v4+p6ztrlfGROqrJTXTZXg0j21bH5SkuMU3nNMb2/84jd5QBAWAvrsDN//ny9+OKLWrp0qdLS0lRVVaWqqiodO3ZMkrRr1y798pe/1ObNm7V371698cYbuummm3TRRRdp3LhxNld/Mv8pLPp1cJaSEpy6ZsJASdKfNtCoDACnE9Zh55lnnlFdXZ2mTZum/Px8/+WVV16RJCUkJGjFihW69NJLNWrUKP3whz/Utddeq7/+9a82V941a2SHzT8RDNeXtJ3KWv5xtQ7V06gMAKcS1g3Kpnn6XoTCwkKtXr06RNWcnQZPq7YdbJs6T3MygmFUXromDM7UB/tr9d+bD+j702hUBoCuhPXITjTZsu9z+UxpUL8kFWQm2V0OooS/UZkVlQHglAg7IcL6OugL/2dcvtJccdpf06R/7DpqdzkAEJYIOyFihR36dRBMyQlxuopGZQA4LcJOCBxv8arsQK0k+nUQfNaprLc/qtLheo/N1QBA+CHshMDWA3VqbvWpf6pLRf1T7C4HUea8gnQVF2aq1Wfqf7YcsLscAAg7hJ0Q2LCnrZeipChLhmHYXA2i0Q1T2rY7eXkDKyoDwIkIOyGwYe/nktgiAn3n/4wrUKorTnuPNmndbhqVAaAzwk4fa/X6tHmvNRMr2+ZqEK1SXHG6cnyBJGkpjcoAEICw08c+rnSrsdmr9MQ4fSEvze5yEMVuKOloVD7aQKMyAFgIO33MmnI+aWiWnA76ddB3RhdkqHhQhlq8NCoDQGeEnT7GYoIIJWsa+p82lJ9xuxUAiBWEnT7k85n+zT8JOwiF2cUFSklwas+RRq3bXWN3OQAQFgg7fWjX4QZ93tSixHiHxhRk2F0OYkCKK05XsqIyAAQg7PSh9e2nsCYO7qeEOD5qhMYN7aey3tpWpZrGZpurAQD78Q3ch6x+HbaIQCiNGZihsQMz1Oz16TUalQGAsNNXTNNk80/YxmpUXrphP43KAGIeYaePmKZUOmuU5pQM1oTBrJyM0LpifIGSE5zafbjRH7oBIFYRdvqIw2HoyvED9cDVY5WU4LS7HMSY1E4rKtOoDCDWEXaAKGWdyvr/tlXpcxqVAcQwwg4QpcYOzNDognQ1t/r02gcH7S4HAGxD2AGilGEYnVZUplEZQOwi7ABR7MrxBUqKd2rnoQZt2ve53eUAgC0IO0AUS0uM1xXF7Y3K62lUBhCbCDtAlLu+pO1U1psfVqq2iUZlALGHsANEueJBGTo3v61R+XUalQHEIMIOEOUMw9ANUwol0agMIDYRdoAYcOWEgUqMd+iz6gZt2U+jMoDYQtgBYkB6Yrxmj2trVF66vtzmagAgtAg7QIzwNypvrVBdU4vN1QBA6BB2gBgxoTBTo/LS5Gn16fUPDthdDgCEDGEHiBGBKyqX06gMIGYQdoAYctWEgXLFObS9ul43PrdeT7+zQxv31sjT6rW7NADoM3F2FwAgdDKS4nVDyWA9//e9+vvOo/r7zqOSJFecQxMH91PJsCyVFGVrwuBMJcY7ba4WAILDMBnLltvtVkZGhurq6pSenm53OUCfMk1T26vrtX53jdbvOar1u2t0tDFwZeUEp0PjCzP94WfikEwlJ/C3EYDw0t3vb8KOCDuIbaZpatfhBq3bXaP1e2q0fvdRHar3BNwnzmFo3KAMlQzLVklRliYNzVKqi/ADwF6EnR4g7AAdTNPU3qNNWr/7qD/8VNQdD7iP02FoTEF6QPjJSIq3qWIAsYqw0wOEHeDUTNPUgc+PaZ0VfvYcVXnNsYD7GIZ0Xn66SoqyVTIsS8MHpMo0TXlNUz6f5DNNeX3W9baffeaZj/tv95kyTcnbft1sK0xm23/8M8v816WA2WZtx9qeI/B+Hcd0wmMMw/C/N0NG+39PuG4YnY61X2//WQG3BT6+o8aOGrp6T129nxMf09X76Pzv4v9ZRhfHTr6v0elo5/tanA6j42IYgddPOOZwGIo7xTGHYSjOecJthnHCv738P5/q+Mm/P22/Qyf+/vjMM39GJ77/Ez+kEz8Oo6sP6ASn+oo95RfvKW5o/60P+B22rp/qNutAwH27OHaijt+FzseMrm/r9BkYJxw68Xdu1tj8oP9RFHNhZ/HixXr00UdVVVWl4uJiPfXUU5oyZUq3HkvYAXqmovaYv99n/Z4a7TnSaHdJAMLcOz+8WMMGpAb1Obv7/R0VJ91feeUVLVy4UM8++6xKSkr0xBNP6LLLLtP27duVk5Njd3lA1CnITNLVEwbp6gmDJEnV7uP+U14b9tSoyn3c/1e+o9Nf9g6H5DACjzschpwOyWkYMgyj0+Pk/2vf+m/bz23PIXWMmsjo/Fdl4GhL52MyAv/a7Dzy4j9mjbycYgRFnUZafJ1+bhuZ6fSYLh6v9utGezFdjxYFvqcuR486vZeu3kf7K7X9t/PIlfVfM/A+nW/suI954k0yO4/G+TpGVQKu9+CYzzTVesL1gN8PR8e/eedjJ/7+OIzA35Wufn+s369AgX/rn/in/4kjAV19Jp0fe6qBnlON/5xqZOjU9w+8x4mjLKcaUTlp1O6kx3X6venqPQb8mnTxe9XF79NJo0ySrZMcomJkp6SkRJMnT9bTTz8tSfL5fCosLNTtt9+uH//4xyfd3+PxyOPpaMB0u90qLCxkZAcAgAjS3ZGdiF9UsLm5WZs3b9aMGTP8xxwOh2bMmKG1a9d2+ZhFixYpIyPDfyksLAxVuQAAIMQiPuwcOXJEXq9Xubm5Acdzc3NVVVXV5WNKS0tVV1fnv5SXsws0AADRKip6dnrK5XLJ5XLZXQYAAAiBiB/Z6d+/v5xOp6qrqwOOV1dXKy8vz6aqAABAuIj4sJOQkKDzzz9fK1eu9B/z+XxauXKlpk6damNlAAAgHETFaayFCxdq7ty5mjRpkqZMmaInnnhCjY2N+uY3v2l3aQAAwGZREXb+7//9vzp8+LDuueceVVVVafz48XrrrbdOaloGAACxJyrW2TlbrKAMAEDkiZl1dgAAAE6HsAMAAKIaYQcAAEQ1wg4AAIhqhB0AABDVCDsAACCqRcU6O2fLmn3vdrttrgQAAHSX9b19plV0CDuS6uvrJUmFhYU2VwIAAHqqvr5eGRkZp7ydRQXVtpdWRUWF0tLSZBhG0J7X7XarsLBQ5eXlLFbYh/icQ4PPOXT4rEODzzk0+vJzNk1T9fX1KigokMNx6s4cRnYkORwODRo0qM+ePz09nf8hhQCfc2jwOYcOn3Vo8DmHRl99zqcb0bHQoAwAAKIaYQcAAEQ1wk4fcrlcuvfee+VyuewuJarxOYcGn3Po8FmHBp9zaITD50yDMgAAiGqM7AAAgKhG2AEAAFGNsAMAAKIaYQcAAEQ1wk4fWbx4sYYOHarExESVlJRow4YNdpcUdRYtWqTJkycrLS1NOTk5uuqqq7R9+3a7y4p6Dz30kAzD0J133ml3KVHn4MGDuvHGG5Wdna2kpCSNHTtWmzZtsrusqOL1evXzn/9cRUVFSkpK0vDhw/XLX/7yjHsr4czWrFmj2bNnq6CgQIZhaNmyZQG3m6ape+65R/n5+UpKStKMGTO0Y8eOkNRG2OkDr7zyihYuXKh7771XW7ZsUXFxsS677DIdOnTI7tKiyurVqzV//nytW7dOy5cvV0tLiy699FI1NjbaXVrU2rhxo37zm99o3LhxdpcSdT7//HNdcMEFio+P1//7f/9PH3/8sR5//HH169fP7tKiysMPP6xnnnlGTz/9tD755BM9/PDDeuSRR/TUU0/ZXVrEa2xsVHFxsRYvXtzl7Y888oiefPJJPfvss1q/fr1SUlJ02WWX6fjx431fnImgmzJlijl//nz/da/XaxYUFJiLFi2ysarod+jQIVOSuXr1artLiUr19fXmyJEjzeXLl5sXX3yx+YMf/MDukqLK3XffbV544YV2lxH1vvrVr5q33HJLwLFrrrnGnDNnjk0VRSdJ5uuvv+6/7vP5zLy8PPPRRx/1H6utrTVdLpf5pz/9qc/rYWQnyJqbm7V582bNmDHDf8zhcGjGjBlau3atjZVFv7q6OklSVlaWzZVEp/nz5+urX/1qwO82gueNN97QpEmT9PWvf105OTmaMGGCfvvb39pdVtT50pe+pJUrV+qzzz6TJP3zn//U+++/r5kzZ9pcWXTbs2ePqqqqAv7/IyMjQyUlJSH5bmQj0CA7cuSIvF6vcnNzA47n5ubq008/tamq6Ofz+XTnnXfqggsu0JgxY+wuJ+q8/PLL2rJlizZu3Gh3KVFr9+7deuaZZ7Rw4UL95Cc/0caNG3XHHXcoISFBc+fOtbu8qPHjH/9Ybrdbo0aNktPplNfr1QMPPKA5c+bYXVpUq6qqkqQuvxut2/oSYQdRYf78+dq2bZvef/99u0uJOuXl5frBD36g5cuXKzEx0e5yopbP59OkSZP04IMPSpImTJigbdu26dlnnyXsBNGrr76ql156SUuXLtXo0aNVVlamO++8UwUFBXzOUYzTWEHWv39/OZ1OVVdXBxyvrq5WXl6eTVVFt9tuu01vvvmmVq1apUGDBtldTtTZvHmzDh06pIkTJyouLk5xcXFavXq1nnzyScXFxcnr9dpdYlTIz8/XeeedF3Ds3HPP1f79+22qKDr927/9m3784x/ruuuu09ixY/Wv//qvWrBggRYtWmR3aVHN+v6z67uRsBNkCQkJOv/887Vy5Ur/MZ/Pp5UrV2rq1Kk2VhZ9TNPUbbfdptdff13vvPOOioqK7C4pKk2fPl0ffvihysrK/JdJkyZpzpw5Kisrk9PptLvEqHDBBRectHTCZ599piFDhthUUXRqamqSwxH41ed0OuXz+WyqKDYUFRUpLy8v4LvR7XZr/fr1Iflu5DRWH1i4cKHmzp2rSZMmacqUKXriiSfU2Niob37zm3aXFlXmz5+vpUuX6i9/+YvS0tL8530zMjKUlJRkc3XRIy0t7aQ+qJSUFGVnZ9MfFUQLFizQl770JT344IP6xje+oQ0bNmjJkiVasmSJ3aVFldmzZ+uBBx7Q4MGDNXr0aH3wwQf61a9+pVtuucXu0iJeQ0ODdu7c6b++Z88elZWVKSsrS4MHD9add96pf//3f9fIkSNVVFSkn//85yooKNBVV13V98X1+XyvGPXUU0+ZgwcPNhMSEswpU6aY69ats7ukqCOpy8vzzz9vd2lRj6nnfeOvf/2rOWbMGNPlcpmjRo0ylyxZYndJUcftdps/+MEPzMGDB5uJiYnmsGHDzJ/+9Kemx+Oxu7SIt2rVqi7/P3nu3LmmabZNP//5z39u5ubmmi6Xy5w+fbq5ffv2kNRmmCbLRgIAgOhFzw4AAIhqhB0AABDVCDsAACCqEXYAAEBUI+wAAICoRtgBAABRjbADAACiGmEHAABENcIOgKh0880393gZesMwtGzZsj6pB4B9WEEZQFirqqrSokWL9L//+786cOCAMjIyNGLECN14442aO3eukpOTu3xcXV2dTNNUZmZmj16rX79+crlc2rt3r4qKivTBBx9o/PjxwXkzAGzBRqAAwtbu3bt1wQUXKDMzUw8++KDGjh0rl8ulDz/8UEuWLNHAgQN1xRVXdPnYjIyMHr9eXl7e2ZYMIAwxsgMgbF1++eX66KOP9OmnnyolJeWk203TlGEYXT725ptvVm1trf+01LRp0zRu3DglJibqd7/7nRISEvTd735X9913n/8xhmHo9ddf11VXXXXS81588cV69913g/XWAIQQPTsAwtLRo0f1t7/9TfPnz+8y6Eg6ZdA5lT/84Q9KSUnR+vXr9cgjj+j+++/X8uXLu7zvhg0bJEkrVqxQZWWlXnvttZ69AQBhg7ADICzt3LlTpmnqC1/4QsDx/v37KzU1Vampqbr77rt79Jzjxo3Tvffeq5EjR+qmm27SpEmTtHLlyi7vO2DAAElSdna28vLylJWV1bs3AsB2hB0AEWXDhg0qKyvT6NGj5fF4evTYcePGBVzPz8/XoUOHglkegDBEgzKAsDRixAgZhqHt27cHHB82bJgkKSkpqcfPGR8fH3DdMAz5fL7eFwkgIjCyAyAsZWdn6ytf+YqefvppNTY2hvz1ExISJElerzfkrw0guAg7AMLWf/7nf6q1tVWTJk3SK6+8ok8++UTbt2/Xiy++qE8//VROp7PPXjsnJ0dJSUl66623VF1drbq6uj57LQB9i7ADIGwNHz5cH3zwgWbMmKHS0lIVFxdr0qRJeuqpp/SjH/1Iv/zlL/vstePi4vTkk0/qN7/5jQoKCnTllVf22WsB6FusswMAAKIaIzsAACCqEXYAAEBUI+wAAICoRtgBAABRjbADAACiGmEHAABENcIOAACIaoQdAAAQ1Qg7AAAgqhF2AABAVCPsAACAqPb/A/ncLQplbfnEAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.xlabel('G init')\n",
        "plt.ylabel('Loss coef')\n",
        "plt.plot(g_init_range,err_arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Misc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "You must install graphviz to plot tree",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xgboost/plotting.py:205\u001b[0m, in \u001b[0;36mto_graphviz\u001b[0;34m(booster, fmap, num_trees, rankdir, yes_color, no_color, condition_node_params, leaf_node_params, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgraphviz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Source\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_tree\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \n\u001b[0;32m----> 3\u001b[0m \u001b[43mxgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregressor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_trees\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m fig \u001b[38;5;241m=\u001b[39m matplotlib\u001b[38;5;241m.\u001b[39mpyplot\u001b[38;5;241m.\u001b[39mgcf()\n\u001b[1;32m      5\u001b[0m fig\u001b[38;5;241m.\u001b[39mset_size_inches(\u001b[38;5;241m150\u001b[39m, \u001b[38;5;241m100\u001b[39m)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xgboost/plotting.py:286\u001b[0m, in \u001b[0;36mplot_tree\u001b[0;34m(booster, fmap, num_trees, rankdir, ax, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    284\u001b[0m     _, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 286\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[43mto_graphviz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbooster\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_trees\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_trees\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrankdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrankdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m s \u001b[38;5;241m=\u001b[39m BytesIO()\n\u001b[1;32m    289\u001b[0m s\u001b[38;5;241m.\u001b[39mwrite(g\u001b[38;5;241m.\u001b[39mpipe(\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xgboost/plotting.py:207\u001b[0m, in \u001b[0;36mto_graphviz\u001b[0;34m(booster, fmap, num_trees, rankdir, yes_color, no_color, condition_node_params, leaf_node_params, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgraphviz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Source\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must install graphviz to plot tree\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(booster, XGBModel):\n\u001b[1;32m    209\u001b[0m     booster \u001b[38;5;241m=\u001b[39m booster\u001b[38;5;241m.\u001b[39mget_booster()\n",
            "\u001b[0;31mImportError\u001b[0m: You must install graphviz to plot tree"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from xgboost import plot_tree\n",
        "import matplotlib \n",
        "xgb.plot_tree(regressor, num_trees=20)\n",
        "fig = matplotlib.pyplot.gcf()\n",
        "fig.set_size_inches(150, 100)\n",
        "fig.savefig('tree.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f07263e1780>]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/svg+xml": [
              "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
              "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
              "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
              "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"407.148125pt\" height=\"297.190125pt\" viewBox=\"0 0 407.148125 297.190125\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
              " <metadata>\n",
              "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
              "   <cc:Work>\n",
              "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
              "    <dc:date>2024-02-02T15:55:14.816300</dc:date>\n",
              "    <dc:format>image/svg+xml</dc:format>\n",
              "    <dc:creator>\n",
              "     <cc:Agent>\n",
              "      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n",
              "     </cc:Agent>\n",
              "    </dc:creator>\n",
              "   </cc:Work>\n",
              "  </rdf:RDF>\n",
              " </metadata>\n",
              " <defs>\n",
              "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
              " </defs>\n",
              " <g id=\"figure_1\">\n",
              "  <g id=\"patch_1\">\n",
              "   <path d=\"M 0 297.190125 \n",
              "L 407.148125 297.190125 \n",
              "L 407.148125 0 \n",
              "L 0 0 \n",
              "z\n",
              "\" style=\"fill: #ffffff\"/>\n",
              "  </g>\n",
              "  <g id=\"axes_1\">\n",
              "   <g id=\"patch_2\">\n",
              "    <path d=\"M 42.828125 273.312 \n",
              "L 399.948125 273.312 \n",
              "L 399.948125 7.2 \n",
              "L 42.828125 7.2 \n",
              "z\n",
              "\" style=\"fill: #ffffff\"/>\n",
              "   </g>\n",
              "   <g id=\"matplotlib.axis_1\">\n",
              "    <g id=\"xtick_1\">\n",
              "     <g id=\"line2d_1\">\n",
              "      <defs>\n",
              "       <path id=\"mfea5334537\" d=\"M 0 0 \n",
              "L 0 3.5 \n",
              "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </defs>\n",
              "      <g>\n",
              "       <use xlink:href=\"#mfea5334537\" x=\"52.214953\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_1\">\n",
              "      <!-- 0 -->\n",
              "      <g transform=\"translate(49.033703 287.910437) scale(0.1 -0.1)\">\n",
              "       <defs>\n",
              "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
              "Q 1547 4250 1301 3770 \n",
              "Q 1056 3291 1056 2328 \n",
              "Q 1056 1369 1301 889 \n",
              "Q 1547 409 2034 409 \n",
              "Q 2525 409 2770 889 \n",
              "Q 3016 1369 3016 2328 \n",
              "Q 3016 3291 2770 3770 \n",
              "Q 2525 4250 2034 4250 \n",
              "z\n",
              "M 2034 4750 \n",
              "Q 2819 4750 3233 4129 \n",
              "Q 3647 3509 3647 2328 \n",
              "Q 3647 1150 3233 529 \n",
              "Q 2819 -91 2034 -91 \n",
              "Q 1250 -91 836 529 \n",
              "Q 422 1150 422 2328 \n",
              "Q 422 3509 836 4129 \n",
              "Q 1250 4750 2034 4750 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       </defs>\n",
              "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"xtick_2\">\n",
              "     <g id=\"line2d_2\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#mfea5334537\" x=\"118.584096\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_2\">\n",
              "      <!-- 1 -->\n",
              "      <g transform=\"translate(115.402846 287.910437) scale(0.1 -0.1)\">\n",
              "       <defs>\n",
              "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
              "L 1825 531 \n",
              "L 1825 4091 \n",
              "L 703 3866 \n",
              "L 703 4441 \n",
              "L 1819 4666 \n",
              "L 2450 4666 \n",
              "L 2450 531 \n",
              "L 3481 531 \n",
              "L 3481 0 \n",
              "L 794 0 \n",
              "L 794 531 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       </defs>\n",
              "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"xtick_3\">\n",
              "     <g id=\"line2d_3\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#mfea5334537\" x=\"184.953239\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_3\">\n",
              "      <!-- 2 -->\n",
              "      <g transform=\"translate(181.771989 287.910437) scale(0.1 -0.1)\">\n",
              "       <defs>\n",
              "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
              "L 3431 531 \n",
              "L 3431 0 \n",
              "L 469 0 \n",
              "L 469 531 \n",
              "Q 828 903 1448 1529 \n",
              "Q 2069 2156 2228 2338 \n",
              "Q 2531 2678 2651 2914 \n",
              "Q 2772 3150 2772 3378 \n",
              "Q 2772 3750 2511 3984 \n",
              "Q 2250 4219 1831 4219 \n",
              "Q 1534 4219 1204 4116 \n",
              "Q 875 4013 500 3803 \n",
              "L 500 4441 \n",
              "Q 881 4594 1212 4672 \n",
              "Q 1544 4750 1819 4750 \n",
              "Q 2544 4750 2975 4387 \n",
              "Q 3406 4025 3406 3419 \n",
              "Q 3406 3131 3298 2873 \n",
              "Q 3191 2616 2906 2266 \n",
              "Q 2828 2175 2409 1742 \n",
              "Q 1991 1309 1228 531 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       </defs>\n",
              "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"xtick_4\">\n",
              "     <g id=\"line2d_4\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#mfea5334537\" x=\"251.322383\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_4\">\n",
              "      <!-- 3 -->\n",
              "      <g transform=\"translate(248.141133 287.910437) scale(0.1 -0.1)\">\n",
              "       <defs>\n",
              "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
              "Q 3050 2419 3304 2112 \n",
              "Q 3559 1806 3559 1356 \n",
              "Q 3559 666 3084 287 \n",
              "Q 2609 -91 1734 -91 \n",
              "Q 1441 -91 1130 -33 \n",
              "Q 819 25 488 141 \n",
              "L 488 750 \n",
              "Q 750 597 1062 519 \n",
              "Q 1375 441 1716 441 \n",
              "Q 2309 441 2620 675 \n",
              "Q 2931 909 2931 1356 \n",
              "Q 2931 1769 2642 2001 \n",
              "Q 2353 2234 1838 2234 \n",
              "L 1294 2234 \n",
              "L 1294 2753 \n",
              "L 1863 2753 \n",
              "Q 2328 2753 2575 2939 \n",
              "Q 2822 3125 2822 3475 \n",
              "Q 2822 3834 2567 4026 \n",
              "Q 2313 4219 1838 4219 \n",
              "Q 1578 4219 1281 4162 \n",
              "Q 984 4106 628 3988 \n",
              "L 628 4550 \n",
              "Q 988 4650 1302 4700 \n",
              "Q 1616 4750 1894 4750 \n",
              "Q 2613 4750 3031 4423 \n",
              "Q 3450 4097 3450 3541 \n",
              "Q 3450 3153 3228 2886 \n",
              "Q 3006 2619 2597 2516 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       </defs>\n",
              "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"xtick_5\">\n",
              "     <g id=\"line2d_5\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#mfea5334537\" x=\"317.691526\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_5\">\n",
              "      <!-- 4 -->\n",
              "      <g transform=\"translate(314.510276 287.910437) scale(0.1 -0.1)\">\n",
              "       <defs>\n",
              "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
              "L 825 1625 \n",
              "L 2419 1625 \n",
              "L 2419 4116 \n",
              "z\n",
              "M 2253 4666 \n",
              "L 3047 4666 \n",
              "L 3047 1625 \n",
              "L 3713 1625 \n",
              "L 3713 1100 \n",
              "L 3047 1100 \n",
              "L 3047 0 \n",
              "L 2419 0 \n",
              "L 2419 1100 \n",
              "L 313 1100 \n",
              "L 313 1709 \n",
              "L 2253 4666 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       </defs>\n",
              "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"xtick_6\">\n",
              "     <g id=\"line2d_6\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#mfea5334537\" x=\"384.060669\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_6\">\n",
              "      <!-- 5 -->\n",
              "      <g transform=\"translate(380.879419 287.910437) scale(0.1 -0.1)\">\n",
              "       <defs>\n",
              "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
              "L 3169 4666 \n",
              "L 3169 4134 \n",
              "L 1269 4134 \n",
              "L 1269 2991 \n",
              "Q 1406 3038 1543 3061 \n",
              "Q 1681 3084 1819 3084 \n",
              "Q 2600 3084 3056 2656 \n",
              "Q 3513 2228 3513 1497 \n",
              "Q 3513 744 3044 326 \n",
              "Q 2575 -91 1722 -91 \n",
              "Q 1428 -91 1123 -41 \n",
              "Q 819 9 494 109 \n",
              "L 494 744 \n",
              "Q 775 591 1075 516 \n",
              "Q 1375 441 1709 441 \n",
              "Q 2250 441 2565 725 \n",
              "Q 2881 1009 2881 1497 \n",
              "Q 2881 1984 2565 2268 \n",
              "Q 2250 2553 1709 2553 \n",
              "Q 1456 2553 1204 2497 \n",
              "Q 953 2441 691 2322 \n",
              "L 691 4666 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       </defs>\n",
              "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "   </g>\n",
              "   <g id=\"matplotlib.axis_2\">\n",
              "    <g id=\"ytick_1\">\n",
              "     <g id=\"line2d_7\">\n",
              "      <defs>\n",
              "       <path id=\"m931bffd412\" d=\"M 0 0 \n",
              "L -3.5 0 \n",
              "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </defs>\n",
              "      <g>\n",
              "       <use xlink:href=\"#m931bffd412\" x=\"42.828125\" y=\"261.216\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_7\">\n",
              "      <!-- 0.000 -->\n",
              "      <g transform=\"translate(7.2 265.015219) scale(0.1 -0.1)\">\n",
              "       <defs>\n",
              "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
              "L 1344 794 \n",
              "L 1344 0 \n",
              "L 684 0 \n",
              "L 684 794 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       </defs>\n",
              "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"ytick_2\">\n",
              "     <g id=\"line2d_8\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#m931bffd412\" x=\"42.828125\" y=\"207.208867\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_8\">\n",
              "      <!-- 0.002 -->\n",
              "      <g transform=\"translate(7.2 211.008085) scale(0.1 -0.1)\">\n",
              "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-32\" x=\"222.65625\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"ytick_3\">\n",
              "     <g id=\"line2d_9\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#m931bffd412\" x=\"42.828125\" y=\"153.201733\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_9\">\n",
              "      <!-- 0.004 -->\n",
              "      <g transform=\"translate(7.2 157.000952) scale(0.1 -0.1)\">\n",
              "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-34\" x=\"222.65625\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"ytick_4\">\n",
              "     <g id=\"line2d_10\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#m931bffd412\" x=\"42.828125\" y=\"99.1946\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_10\">\n",
              "      <!-- 0.006 -->\n",
              "      <g transform=\"translate(7.2 102.993818) scale(0.1 -0.1)\">\n",
              "       <defs>\n",
              "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
              "Q 1688 2584 1439 2293 \n",
              "Q 1191 2003 1191 1497 \n",
              "Q 1191 994 1439 701 \n",
              "Q 1688 409 2113 409 \n",
              "Q 2538 409 2786 701 \n",
              "Q 3034 994 3034 1497 \n",
              "Q 3034 2003 2786 2293 \n",
              "Q 2538 2584 2113 2584 \n",
              "z\n",
              "M 3366 4563 \n",
              "L 3366 3988 \n",
              "Q 3128 4100 2886 4159 \n",
              "Q 2644 4219 2406 4219 \n",
              "Q 1781 4219 1451 3797 \n",
              "Q 1122 3375 1075 2522 \n",
              "Q 1259 2794 1537 2939 \n",
              "Q 1816 3084 2150 3084 \n",
              "Q 2853 3084 3261 2657 \n",
              "Q 3669 2231 3669 1497 \n",
              "Q 3669 778 3244 343 \n",
              "Q 2819 -91 2113 -91 \n",
              "Q 1303 -91 875 529 \n",
              "Q 447 1150 447 2328 \n",
              "Q 447 3434 972 4092 \n",
              "Q 1497 4750 2381 4750 \n",
              "Q 2619 4750 2861 4703 \n",
              "Q 3103 4656 3366 4563 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       </defs>\n",
              "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-36\" x=\"222.65625\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"ytick_5\">\n",
              "     <g id=\"line2d_11\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#m931bffd412\" x=\"42.828125\" y=\"45.187466\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_11\">\n",
              "      <!-- 0.008 -->\n",
              "      <g transform=\"translate(7.2 48.986685) scale(0.1 -0.1)\">\n",
              "       <defs>\n",
              "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
              "Q 1584 2216 1326 1975 \n",
              "Q 1069 1734 1069 1313 \n",
              "Q 1069 891 1326 650 \n",
              "Q 1584 409 2034 409 \n",
              "Q 2484 409 2743 651 \n",
              "Q 3003 894 3003 1313 \n",
              "Q 3003 1734 2745 1975 \n",
              "Q 2488 2216 2034 2216 \n",
              "z\n",
              "M 1403 2484 \n",
              "Q 997 2584 770 2862 \n",
              "Q 544 3141 544 3541 \n",
              "Q 544 4100 942 4425 \n",
              "Q 1341 4750 2034 4750 \n",
              "Q 2731 4750 3128 4425 \n",
              "Q 3525 4100 3525 3541 \n",
              "Q 3525 3141 3298 2862 \n",
              "Q 3072 2584 2669 2484 \n",
              "Q 3125 2378 3379 2068 \n",
              "Q 3634 1759 3634 1313 \n",
              "Q 3634 634 3220 271 \n",
              "Q 2806 -91 2034 -91 \n",
              "Q 1263 -91 848 271 \n",
              "Q 434 634 434 1313 \n",
              "Q 434 1759 690 2068 \n",
              "Q 947 2378 1403 2484 \n",
              "z\n",
              "M 1172 3481 \n",
              "Q 1172 3119 1398 2916 \n",
              "Q 1625 2713 2034 2713 \n",
              "Q 2441 2713 2670 2916 \n",
              "Q 2900 3119 2900 3481 \n",
              "Q 2900 3844 2670 4047 \n",
              "Q 2441 4250 2034 4250 \n",
              "Q 1625 4250 1398 4047 \n",
              "Q 1172 3844 1172 3481 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       </defs>\n",
              "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-38\" x=\"222.65625\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "   </g>\n",
              "   <g id=\"line2d_12\">\n",
              "    <path d=\"M 59.060852 196.843735 \n",
              "L 59.060852 178.888332 \n",
              "L 59.060852 261.137334 \n",
              "L 59.060852 191.851753 \n",
              "L 59.550447 197.960754 \n",
              "L 59.550447 145.630851 \n",
              "L 59.550447 258.809741 \n",
              "L 59.550447 232.476492 \n",
              "L 59.996089 203.482274 \n",
              "L 59.996089 260.124331 \n",
              "L 59.996089 229.719957 \n",
              "L 60.150213 258.596879 \n",
              "L 60.274566 252.878996 \n",
              "L 60.274566 211.176268 \n",
              "L 60.274566 259.848899 \n",
              "L 60.274566 216.759554 \n",
              "L 60.671963 166.90492 \n",
              "L 60.671963 135.001663 \n",
              "L 60.671963 259.51834 \n",
              "L 60.671963 219.848459 \n",
              "L 61.204955 165.858921 \n",
              "L 61.204955 163.904942 \n",
              "L 61.204955 260.116686 \n",
              "L 61.540477 227.092185 \n",
              "L 61.540477 228.062736 \n",
              "L 61.556949 253.714748 \n",
              "L 61.556949 252.632334 \n",
              "L 61.822185 149.250501 \n",
              "L 61.822185 168.674003 \n",
              "L 61.822185 260.890471 \n",
              "L 61.822185 143.834808 \n",
              "L 61.822185 234.712542 \n",
              "L 62.382783 183.717148 \n",
              "L 62.382783 260.448653 \n",
              "L 62.382783 147.632513 \n",
              "L 62.382783 183.017401 \n",
              "L 62.628765 247.165538 \n",
              "L 62.762015 228.114644 \n",
              "L 62.953882 226.073347 \n",
              "L 62.953882 152.963303 \n",
              "L 62.953882 258.55825 \n",
              "L 62.953882 198.108429 \n",
              "L 63.516895 190.855851 \n",
              "L 63.516895 260.627714 \n",
              "L 63.516895 146.437028 \n",
              "L 63.516895 190.173407 \n",
              "L 63.91417 260.822066 \n",
              "L 63.91417 251.622751 \n",
              "L 64.111542 217.014063 \n",
              "L 64.111542 259.695791 \n",
              "L 64.111542 168.222528 \n",
              "L 64.111542 210.131678 \n",
              "L 64.642068 153.584182 \n",
              "L 64.642068 259.512304 \n",
              "L 64.642068 142.257863 \n",
              "L 64.642068 189.537639 \n",
              "L 65.07666 261.043377 \n",
              "L 65.07666 249.097386 \n",
              "L 65.07666 196.701291 \n",
              "L 65.07666 258.151037 \n",
              "L 65.07666 222.46597 \n",
              "L 65.251492 260.220903 \n",
              "L 65.362359 240.40065 \n",
              "L 65.362359 207.072751 \n",
              "L 65.362359 261.125463 \n",
              "L 65.362359 230.27223 \n",
              "L 65.79238 150.056075 \n",
              "L 65.79238 258.948161 \n",
              "L 65.79238 226.845523 \n",
              "L 66.261629 243.342885 \n",
              "L 66.261629 242.132915 \n",
              "L 66.261629 184.374644 \n",
              "L 66.261629 260.978593 \n",
              "L 66.261629 216.004882 \n",
              "L 66.538149 242.834271 \n",
              "L 66.538149 259.155389 \n",
              "L 66.538149 219.176075 \n",
              "L 66.538149 254.485717 \n",
              "L 66.919431 161.644346 \n",
              "L 66.919431 258.516805 \n",
              "L 66.919431 145.00977 \n",
              "L 66.919431 209.510799 \n",
              "L 67.486908 177.217431 \n",
              "L 67.486908 257.870575 \n",
              "L 67.486908 135.626164 \n",
              "L 67.486908 160.305613 \n",
              "L 67.752774 241.626715 \n",
              "L 67.909106 228.046641 \n",
              "L 67.909106 228.812379 \n",
              "L 68.08826 172.83828 \n",
              "L 68.08826 260.463541 \n",
              "L 68.08826 161.961425 \n",
              "L 68.08826 204.064122 \n",
              "L 68.621561 214.006641 \n",
              "L 68.621561 140.869636 \n",
              "L 68.621561 260.970143 \n",
              "L 68.621561 254.159785 \n",
              "L 68.971478 253.712334 \n",
              "L 68.971478 226.870069 \n",
              "L 68.971478 260.781425 \n",
              "L 69.021734 240.525389 \n",
              "L 69.315009 233.846611 \n",
              "L 69.315009 167.040322 \n",
              "L 69.315009 259.752125 \n",
              "L 69.315009 199.50068 \n",
              "L 69.801881 186.709681 \n",
              "L 69.801881 145.336507 \n",
              "L 69.801881 260.090128 \n",
              "L 69.801881 206.532751 \n",
              "L 70.176074 259.218563 \n",
              "L 70.176074 258.943332 \n",
              "L 70.176074 214.974778 \n",
              "L 70.176074 218.430456 \n",
              "L 70.454847 238.407237 \n",
              "L 70.454847 232.626581 \n",
              "L 70.454847 259.813288 \n",
              "L 70.454847 192.150724 \n",
              "L 70.454847 205.595195 \n",
              "L 70.937881 175.399458 \n",
              "L 70.937881 260.785449 \n",
              "L 70.937881 148.095255 \n",
              "L 70.937881 167.136089 \n",
              "L 71.369352 216.318742 \n",
              "L 71.369352 259.977461 \n",
              "L 71.369352 185.201544 \n",
              "L 71.369352 231.318027 \n",
              "L 71.564296 259.901007 \n",
              "L 71.564296 255.267148 \n",
              "L 71.564296 242.769085 \n",
              "L 71.735474 216.595583 \n",
              "L 71.735474 261.063094 \n",
              "L 71.735474 215.880948 \n",
              "L 71.735474 254.404435 \n",
              "L 72.09738 202.502468 \n",
              "L 72.09738 259.923541 \n",
              "L 72.09738 147.06676 \n",
              "L 72.09738 190.010844 \n",
              "L 72.618795 191.432066 \n",
              "L 72.618795 260.676 \n",
              "L 72.618795 153.99904 \n",
              "L 72.618795 209.950605 \n",
              "L 72.950019 260.938355 \n",
              "L 72.950019 258.687416 \n",
              "L 73.269047 146.654718 \n",
              "L 73.269047 169.736298 \n",
              "L 73.269047 259.722349 \n",
              "L 73.269047 144.00059 \n",
              "L 73.269047 243.766194 \n",
              "L 73.858751 261.122647 \n",
              "L 73.858751 143.316537 \n",
              "L 73.858751 196.685598 \n",
              "L 74.494859 198.254897 \n",
              "L 74.494859 260.878802 \n",
              "L 74.494859 139.160709 \n",
              "L 74.494859 191.741902 \n",
              "L 75.0792 155.58363 \n",
              "L 75.0792 260.157729 \n",
              "L 75.0792 134.483392 \n",
              "L 75.0792 134.583988 \n",
              "L 75.529598 211.078891 \n",
              "L 75.529598 241.371201 \n",
              "L 75.529598 210.340113 \n",
              "L 75.545839 235.873019 \n",
              "L 75.707003 248.773466 \n",
              "L 75.707003 248.064465 \n",
              "L 75.707003 169.341156 \n",
              "L 75.707003 257.480262 \n",
              "L 75.707003 184.64223 \n",
              "L 76.219689 222.91141 \n",
              "L 76.219689 142.192274 \n",
              "L 76.219689 260.111857 \n",
              "L 76.219689 163.724674 \n",
              "L 76.661621 241.336596 \n",
              "L 76.661621 212.680382 \n",
              "L 76.661621 259.199249 \n",
              "L 76.661621 245.490009 \n",
              "L 76.916014 216.663988 \n",
              "L 76.916014 228.362915 \n",
              "L 76.916014 258.145001 \n",
              "L 76.916014 196.751589 \n",
              "L 76.916014 229.698832 \n",
              "L 77.403089 200.151738 \n",
              "L 77.403089 136.833317 \n",
              "L 77.403089 260.376626 \n",
              "L 77.403089 146.031022 \n",
              "L 77.814347 259.165449 \n",
              "L 77.814347 220.597699 \n",
              "L 77.814347 200.720709 \n",
              "L 77.814347 256.77287 \n",
              "L 77.814347 232.710277 \n",
              "L 78.145759 219.283511 \n",
              "L 78.145759 257.579249 \n",
              "L 78.145759 208.857729 \n",
              "L 78.145759 233.110247 \n",
              "L 78.567576 230.504405 \n",
              "L 78.567576 140.355791 \n",
              "L 78.567576 258.972304 \n",
              "L 78.567576 208.217133 \n",
              "L 79.113227 247.716805 \n",
              "L 79.113227 244.100173 \n",
              "L 79.113227 148.769651 \n",
              "L 79.113227 260.165776 \n",
              "L 79.113227 153.026879 \n",
              "L 79.493589 240.831201 \n",
              "L 79.493589 240.710486 \n",
              "L 79.760829 158.387446 \n",
              "L 79.760829 261.120232 \n",
              "L 79.760829 141.383481 \n",
              "L 79.760829 236.805747 \n",
              "L 80.341902 141.397162 \n",
              "L 80.341902 261.022855 \n",
              "L 80.341902 138.157162 \n",
              "L 80.341902 202.637669 \n",
              "L 80.609832 240.620352 \n",
              "L 80.993164 204.051648 \n",
              "L 80.993164 259.882498 \n",
              "L 80.993164 149.081097 \n",
              "L 80.993164 214.679428 \n",
              "L 81.520437 250.781365 \n",
              "L 81.520437 133.921663 \n",
              "L 81.520437 258.600501 \n",
              "L 81.520437 155.365538 \n",
              "L 81.966052 237.819756 \n",
              "L 81.966052 220.196924 \n",
              "L 81.966052 258.600501 \n",
              "L 81.966052 209.46211 \n",
              "L 82.051854 251.371261 \n",
              "L 82.227858 245.266283 \n",
              "L 82.227858 166.813377 \n",
              "L 82.227858 257.278265 \n",
              "L 82.227858 250.353228 \n",
              "L 82.809389 234.344763 \n",
              "L 82.809389 122.658921 \n",
              "L 82.809389 260.051499 \n",
              "L 82.809389 151.084167 \n",
              "L 83.19038 258.873317 \n",
              "L 83.19038 233.332364 \n",
              "L 83.19038 215.544554 \n",
              "L 83.19038 259.524376 \n",
              "L 83.19038 247.382021 \n",
              "L 83.31696 261.161276 \n",
              "L 83.31696 257.973586 \n",
              "L 83.485254 241.461335 \n",
              "L 83.485254 245.164077 \n",
              "L 83.485254 194.334063 \n",
              "L 83.485254 257.851261 \n",
              "L 83.485254 215.548578 \n",
              "L 83.923675 201.710575 \n",
              "L 83.923675 258.201335 \n",
              "L 83.923675 134.853586 \n",
              "L 83.923675 238.545657 \n",
              "L 84.415787 203.882647 \n",
              "L 84.415787 205.425389 \n",
              "L 84.415787 174.46834 \n",
              "L 84.415787 260.724286 \n",
              "L 84.511624 209.124912 \n",
              "L 84.795918 201.299338 \n",
              "L 84.795918 259.282945 \n",
              "L 84.795918 205.365836 \n",
              "L 85.131409 205.530009 \n",
              "L 85.131409 258.229502 \n",
              "L 85.131409 143.289979 \n",
              "L 85.131409 217.779398 \n",
              "L 85.647991 212.30375 \n",
              "L 85.647991 260.802349 \n",
              "L 85.647991 166.530098 \n",
              "L 85.647991 194.175523 \n",
              "L 85.975726 257.003839 \n",
              "L 85.975726 254.758534 \n",
              "L 85.975726 232.456775 \n",
              "L 85.975726 256.641693 \n",
              "L 85.975726 250.877937 \n",
              "L 86.311061 251.005091 \n",
              "L 86.311061 147.986611 \n",
              "L 86.311061 258.803303 \n",
              "L 86.311061 243.258385 \n",
              "L 86.915528 188.416596 \n",
              "L 86.915528 260.791082 \n",
              "L 86.915528 144.71442 \n",
              "L 86.915528 202.743094 \n",
              "L 87.169457 258.753407 \n",
              "L 87.186946 253.989174 \n",
              "L 87.485839 196.079607 \n",
              "L 87.485839 133.236805 \n",
              "L 87.485839 259.889741 \n",
              "L 87.485839 226.93606 \n",
              "L 88.137496 247.551022 \n",
              "L 88.137496 259.867207 \n",
              "L 88.137496 131.046224 \n",
              "L 88.737176 259.833407 \n",
              "L 88.737176 243.889323 \n",
              "L 88.737176 259.730396 \n",
              "L 88.737176 139.794063 \n",
              "L 89.324332 165.036447 \n",
              "L 89.324332 255.967297 \n",
              "L 89.324332 140.7453 \n",
              "L 89.324332 203.095583 \n",
              "L 89.70724 256.504882 \n",
              "L 89.70724 244.63293 \n",
              "L 89.70724 229.120203 \n",
              "L 89.70724 257.275851 \n",
              "L 89.70724 231.677759 \n",
              "L 89.985262 250.943928 \n",
              "L 89.985262 153.045389 \n",
              "L 89.985262 257.634778 \n",
              "L 89.985262 225.772364 \n",
              "L 90.561883 253.742915 \n",
              "L 90.561883 143.869413 \n",
              "L 90.561883 260.802349 \n",
              "L 90.561883 198.809383 \n",
              "L 90.943126 223.477162 \n",
              "L 90.943126 256.310128 \n",
              "L 90.943126 222.955672 \n",
              "L 90.943126 246.176477 \n",
              "L 91.181228 223.839308 \n",
              "L 91.181228 257.784465 \n",
              "L 91.181228 166.126104 \n",
              "L 91.181228 233.993884 \n",
              "L 91.771295 196.425657 \n",
              "L 91.771295 132.132662 \n",
              "L 91.771295 260.90214 \n",
              "L 91.771295 241.767952 \n",
              "L 92.12671 251.232036 \n",
              "L 92.12671 193.367535 \n",
              "L 92.12671 259.181544 \n",
              "L 92.12671 251.946671 \n",
              "L 92.424819 210.842289 \n",
              "L 92.424819 260.781425 \n",
              "L 92.424819 173.222557 \n",
              "L 92.424819 191.976894 \n",
              "L 92.97928 179.881216 \n",
              "L 92.97928 258.025896 \n",
              "L 92.97928 131.530694 \n",
              "L 92.97928 193.07138 \n",
              "L 93.481008 194.183571 \n",
              "L 93.481008 260.34524 \n",
              "L 93.481008 181.60986 \n",
              "L 93.481008 248.365449 \n",
              "L 93.75715 245.703273 \n",
              "L 93.75715 222.408429 \n",
              "L 93.75715 259.693377 \n",
              "L 93.75715 257.195374 \n",
              "L 94.152049 167.827386 \n",
              "L 94.152049 173.406045 \n",
              "L 94.152049 260.998712 \n",
              "L 94.152049 131.578981 \n",
              "L 94.152049 165.987684 \n",
              "L 94.735035 200.467207 \n",
              "L 94.735035 156.755374 \n",
              "L 94.735035 258.029115 \n",
              "L 94.735035 250.263094 \n",
              "L 94.996489 258.248012 \n",
              "L 94.996489 235.210694 \n",
              "L 94.996489 258.293079 \n",
              "L 94.996489 255.564912 \n",
              "L 95.382752 225.580829 \n",
              "L 95.382752 227.322349 \n",
              "L 95.382752 135.870009 \n",
              "L 95.382752 260.879607 \n",
              "L 95.960366 221.273705 \n",
              "L 95.960366 134.062498 \n",
              "L 95.960366 260.433765 \n",
              "L 95.960366 230.353109 \n",
              "L 96.209877 225.975165 \n",
              "L 96.209877 217.612006 \n",
              "L 96.209877 223.232513 \n",
              "L 96.639378 144.661306 \n",
              "L 96.639378 260.697729 \n",
              "L 96.639378 136.288489 \n",
              "L 96.639378 169.906909 \n",
              "L 97.190982 212.134748 \n",
              "L 97.190982 260.842587 \n",
              "L 97.190982 146.312692 \n",
              "L 97.190982 255.891648 \n",
              "L 97.721786 191.626015 \n",
              "L 97.721786 197.050158 \n",
              "L 97.721786 193.842349 \n",
              "L 97.857901 204.56831 \n",
              "L 97.857901 261.072751 \n",
              "L 97.857901 143.180531 \n",
              "L 97.857901 247.127714 \n",
              "L 98.361033 183.087416 \n",
              "L 98.361033 136.121097 \n",
              "L 98.361033 259.772244 \n",
              "L 98.361033 170.901604 \n",
              "L 98.763116 259.574271 \n",
              "L 98.763116 248.67287 \n",
              "L 99.041125 231.381604 \n",
              "L 99.041125 162.108697 \n",
              "L 99.041125 260.23901 \n",
              "L 99.041125 209.429115 \n",
              "L 99.330907 237.145359 \n",
              "L 99.330907 259.353765 \n",
              "L 99.330907 227.798772 \n",
              "L 99.330907 246.075076 \n",
              "L 99.524569 225.807773 \n",
              "L 99.535879 233.83293 \n",
              "L 99.596321 249.770575 \n",
              "L 99.619993 245.419994 \n",
              "L 99.67146 244.603958 \n",
              "L 99.67146 241.400978 \n",
              "L 99.689293 259.54369 \n",
              "L 99.689293 246.052542 \n",
              "L 99.788135 240.179338 \n",
              "L 99.788135 246.253735 \n",
              "L 99.802788 260.604376 \n",
              "L 99.788135 239.987803 \n",
              "L 99.806475 253.684972 \n",
              "L 99.918835 259.992751 \n",
              "L 99.918835 235.164018 \n",
              "L 99.960043 244.098563 \n",
              "L 100.069424 246.081514 \n",
              "L 100.069424 241.586075 \n",
              "L 100.080991 260.6591 \n",
              "L 100.09225 260.581842 \n",
              "L 100.186594 237.980709 \n",
              "L 100.197643 254.637818 \n",
              "L 100.274376 260.737967 \n",
              "L 100.301248 256.092841 \n",
              "L 100.343786 259.672453 \n",
              "L 100.354123 242.625836 \n",
              "L 100.446027 240.361216 \n",
              "L 100.543318 261.199905 \n",
              "L 100.623172 236.944167 \n",
              "L 100.623172 250.966462 \n",
              "L 100.681083 254.621723 \n",
              "L 100.770957 221.956149 \n",
              "L 100.978247 220.739338 \n",
              "L 101.041684 259.308697 \n",
              "L 101.041684 255.255881 \n",
              "L 101.112332 249.897729 \n",
              "L 101.137112 260.697729 \n",
              "L 101.137112 259.094629 \n",
              "L 101.215364 243.729979 \n",
              "L 101.261268 244.449443 \n",
              "L 101.321564 260.974569 \n",
              "L 101.321564 254.982259 \n",
              "L 101.385931 255.951201 \n",
              "L 101.385931 261.009979 \n",
              "L 101.456821 235.431201 \n",
              "L 101.554904 259.447118 \n",
              "L 101.554904 248.637461 \n",
              "L 101.637155 238.943213 \n",
              "L 101.637155 250.650993 \n",
              "L 101.637155 241.539398 \n",
              "L 101.741318 259.525985 \n",
              "L 101.637155 239.446999 \n",
              "L 101.741318 240.94065 \n",
              "L 101.811104 258.544167 \n",
              "L 101.847712 252.497133 \n",
              "L 101.922922 213.507684 \n",
              "L 101.922922 234.700471 \n",
              "L 102.145026 240.559189 \n",
              "L 102.145026 222.558116 \n",
              "L 102.222317 260.548042 \n",
              "L 102.222317 244.201574 \n",
              "L 102.2834 242.833466 \n",
              "L 102.2834 259.838235 \n",
              "L 102.2834 240.100471 \n",
              "L 102.353515 246.675434 \n",
              "L 102.434955 260.523899 \n",
              "L 102.434955 245.144763 \n",
              "L 102.434955 249.495344 \n",
              "L 102.493222 243.48211 \n",
              "L 102.493222 249.934748 \n",
              "L 102.543497 249.105836 \n",
              "L 102.543497 255.584227 \n",
              "L 102.593049 216.882885 \n",
              "L 102.593049 223.303332 \n",
              "L 102.797265 234.631261 \n",
              "L 102.797265 234.328668 \n",
              "L 102.985538 260.451469 \n",
              "L 102.985538 257.303213 \n",
              "L 103.00732 232.793168 \n",
              "L 102.985538 260.078057 \n",
              "L 103.00732 234.441335 \n",
              "L 103.189019 223.884376 \n",
              "L 103.189019 226.783154 \n",
              "L 103.237194 260.961693 \n",
              "L 103.189019 223.528668 \n",
              "L 103.237194 253.466075 \n",
              "L 103.37842 249.756089 \n",
              "L 103.466146 259.886522 \n",
              "L 103.37842 238.713049 \n",
              "L 103.466146 252.580829 \n",
              "L 103.577164 222.234599 \n",
              "L 103.50921 257.576835 \n",
              "L 103.577164 225.802945 \n",
              "L 103.778813 225.200978 \n",
              "L 103.778813 218.918951 \n",
              "L 103.875978 261.193466 \n",
              "L 103.875978 248.133675 \n",
              "L 103.946152 242.593645 \n",
              "L 103.946152 261.209562 \n",
              "L 103.998006 235.946253 \n",
              "L 104.160764 260.771768 \n",
              "L 104.160764 253.86363 \n",
              "L 104.202993 246.483899 \n",
              "L 104.288651 260.016894 \n",
              "L 104.288651 244.958057 \n",
              "L 104.288651 254.45594 \n",
              "L 104.353037 248.859577 \n",
              "L 104.353037 260.849025 \n",
              "L 104.435221 237.221007 \n",
              "L 104.435221 255.770933 \n",
              "L 104.50704 225.033586 \n",
              "L 104.50704 258.455642 \n",
              "L 104.766489 228.716209 \n",
              "L 104.766489 260.601156 \n",
              "L 104.814751 252.26375 \n",
              "L 104.879256 238.106253 \n",
              "L 104.935371 260.30983 \n",
              "L 105.032512 258.402528 \n",
              "L 105.032512 236.186075 \n",
              "L 105.112314 260.93755 \n",
              "L 105.112314 247.31442 \n",
              "L 105.199594 260.398355 \n",
              "L 105.199594 246.969979 \n",
              "L 105.199594 254.906611 \n",
              "L 105.333687 254.375463 \n",
              "L 105.367194 234.201514 \n",
              "L 105.333687 260.956864 \n",
              "L 105.367194 237.221007 \n",
              "L 105.498561 252.492304 \n",
              "L 105.498561 237.245151 \n",
              "L 105.498561 259.319964 \n",
              "L 105.498561 246.823511 \n",
              "L 105.624644 246.221544 \n",
              "L 105.624644 260.44825 \n",
              "L 105.695558 233.947207 \n",
              "L 105.695558 253.89904 \n",
              "L 105.769727 225.442408 \n",
              "L 105.769727 259.923541 \n",
              "L 105.769727 248.886939 \n",
              "L 105.968773 260.222915 \n",
              "L 105.968773 223.72986 \n",
              "L 106.074162 260.586671 \n",
              "L 106.115074 244.669949 \n",
              "L 106.115074 256.689979 \n",
              "L 106.115074 233.568966 \n",
              "L 106.225812 258.460471 \n",
              "L 106.225812 247.465717 \n",
              "L 106.346922 244.785836 \n",
              "L 106.444146 259.678891 \n",
              "L 106.346922 233.107028 \n",
              "L 106.444146 254.296596 \n",
              "L 106.485509 259.245925 \n",
              "L 106.485509 242.477759 \n",
              "L 106.564897 257.235613 \n",
              "L 106.564897 230.839189 \n",
              "L 106.564897 260.795911 \n",
              "L 106.608325 238.585896 \n",
              "L 106.753024 235.909234 \n",
              "L 106.753024 225.361931 \n",
              "L 106.841707 261.191857 \n",
              "L 106.841707 241.81141 \n",
              "L 106.944775 252.501961 \n",
              "L 106.944775 235.407058 \n",
              "L 106.944775 257.427148 \n",
              "L 106.944775 240.668638 \n",
              "L 107.058475 237.258027 \n",
              "L 107.058475 238.098206 \n",
              "L 107.058475 261.003541 \n",
              "L 107.058475 239.374569 \n",
              "L 107.181132 240.943869 \n",
              "L 107.181132 258.703511 \n",
              "L 107.181132 235.493973 \n",
              "L 107.249637 252.772364 \n",
              "L 107.298326 232.616119 \n",
              "L 107.298326 259.949294 \n",
              "L 107.298326 255.173794 \n",
              "L 107.430283 237.806879 \n",
              "L 107.430283 255.806343 \n",
              "L 107.430283 237.14214 \n",
              "L 107.54326 237.613735 \n",
              "L 107.54326 260.034599 \n",
              "L 107.54326 229.656179 \n",
              "L 107.54326 257.367595 \n",
              "L 107.675989 233.652662 \n",
              "L 107.675989 243.910247 \n",
              "L 107.675989 260.932721 \n",
              "L 107.675989 231.391261 \n",
              "L 107.700784 238.29296 \n",
              "L 107.871146 227.31913 \n",
              "L 107.871146 258.785598 \n",
              "L 107.871146 220.023094 \n",
              "L 107.871146 232.271678 \n",
              "L 107.983177 260.847416 \n",
              "L 107.983177 255.60676 \n",
              "L 107.983177 232.274897 \n",
              "L 107.983177 260.478832 \n",
              "L 108.1053 255.563303 \n",
              "L 108.1053 234.259458 \n",
              "L 108.1053 259.992751 \n",
              "L 108.1053 240.770039 \n",
              "L 108.243788 260.6253 \n",
              "L 108.243788 247.579994 \n",
              "L 108.243788 260.055523 \n",
              "L 108.317174 198.733735 \n",
              "L 108.603677 242.944525 \n",
              "L 108.603677 242.226671 \n",
              "L 108.603677 260.800739 \n",
              "L 108.62568 198.36837 \n",
              "L 108.711678 245.74673 \n",
              "L 108.836423 245.075553 \n",
              "L 108.836423 258.351022 \n",
              "L 108.836423 223.631678 \n",
              "L 108.836423 248.156209 \n",
              "L 108.986194 247.182438 \n",
              "L 108.986194 222.743213 \n",
              "L 108.986194 252.440799 \n",
              "L 108.986194 242.698265 \n",
              "L 109.191078 258.634301 \n",
              "L 109.191078 249.014092 \n",
              "L 109.191078 251.597401 \n",
              "L 109.30777 257.80056 \n",
              "L 109.30777 230.612244 \n",
              "L 109.30777 258.658444 \n",
              "L 109.30777 248.698623 \n",
              "L 109.439604 242.466492 \n",
              "L 109.439604 260.565747 \n",
              "L 109.439604 221.15138 \n",
              "L 109.439604 253.216596 \n",
              "L 109.679147 260.76372 \n",
              "L 109.679147 207.547565 \n",
              "L 109.679147 261.10977 \n",
              "L 109.75309 245.448966 \n",
              "L 109.820523 257.533377 \n",
              "L 109.820523 239.482408 \n",
              "L 109.820523 241.603779 \n",
              "L 109.820523 236.017073 \n",
              "L 109.950878 241.278653 \n",
              "L 109.950878 226.276149 \n",
              "L 109.950878 260.784644 \n",
              "L 109.950878 228.579398 \n",
              "L 110.129345 231.101544 \n",
              "L 110.129345 259.704644 \n",
              "L 110.129345 225.186492 \n",
              "L 110.129345 238.53439 \n",
              "L 110.296711 254.293377 \n",
              "L 110.296711 227.159785 \n",
              "L 110.374635 260.034599 \n",
              "L 110.384663 259.620948 \n",
              "L 110.46009 254.042289 \n",
              "L 110.46009 259.350545 \n",
              "L 110.46009 223.789413 \n",
              "L 110.660877 233.462736 \n",
              "L 110.660877 256.168489 \n",
              "L 110.660877 225.873765 \n",
              "L 110.660877 244.595911 \n",
              "L 110.817281 238.037043 \n",
              "L 110.817281 239.400322 \n",
              "L 110.817281 258.11603 \n",
              "L 110.817281 222.675613 \n",
              "L 110.817281 223.493258 \n",
              "L 110.930207 254.826134 \n",
              "L 110.930207 241.84521 \n",
              "L 111.076465 228.065955 \n",
              "L 111.076465 238.276864 \n",
              "L 111.076465 249.670784 \n",
              "L 111.20824 255.57135 \n",
              "L 111.20824 260.993884 \n",
              "L 111.20824 225.29755 \n",
              "L 111.20824 225.516447 \n",
              "L 111.381026 254.890516 \n",
              "L 111.381026 223.016835 \n",
              "L 111.381026 259.035076 \n",
              "L 111.381026 223.821604 \n",
              "L 111.544199 234.708519 \n",
              "L 111.544199 260.129562 \n",
              "L 111.544199 230.600978 \n",
              "L 111.619283 248.759785 \n",
              "L 111.783156 257.015106 \n",
              "L 111.783156 220.237162 \n",
              "L 111.783156 260.737967 \n",
              "L 111.783156 256.577311 \n",
              "L 111.984203 240.24211 \n",
              "L 111.984203 260.269592 \n",
              "L 111.984203 217.003601 \n",
              "L 111.984203 254.66679 \n",
              "L 112.169364 255.175404 \n",
              "L 112.169364 219.210277 \n",
              "L 112.169364 257.850456 \n",
              "L 112.268459 248.763004 \n",
              "L 112.404661 255.160918 \n",
              "L 112.404661 221.632632 \n",
              "L 112.404661 261.095285 \n",
              "L 112.404661 227.7022 \n",
              "L 112.58335 235.526164 \n",
              "L 112.58335 221.624584 \n",
              "L 112.58335 260.34363 \n",
              "L 112.58335 233.499756 \n",
              "L 112.788911 249.608012 \n",
              "L 112.788911 255.065955 \n",
              "L 112.788911 211.936775 \n",
              "L 112.972358 250.502915 \n",
              "L 112.981354 249.725508 \n",
              "L 113.06438 237.472095 \n",
              "L 113.06438 258.038772 \n",
              "L 113.06438 220.341782 \n",
              "L 113.06438 253.372721 \n",
              "L 113.247503 251.384942 \n",
              "L 113.247503 213.969621 \n",
              "L 113.247503 260.963303 \n",
              "L 113.247503 234.534689 \n",
              "L 113.419728 236.330933 \n",
              "L 113.419728 260.581842 \n",
              "L 113.419728 217.146849 \n",
              "L 113.601134 249.670784 \n",
              "L 113.601134 247.748996 \n",
              "L 113.704506 225.440799 \n",
              "L 113.704506 232.608072 \n",
              "L 113.704506 259.303869 \n",
              "L 113.704506 214.061365 \n",
              "L 113.704506 218.424823 \n",
              "L 113.937426 251.660173 \n",
              "L 113.937426 237.901842 \n",
              "L 113.937426 210.84068 \n",
              "L 113.937426 261.032513 \n",
              "L 113.937426 246.588519 \n",
              "L 114.1023 253.567475 \n",
              "L 114.1023 249.632155 \n",
              "L 114.1023 234.069532 \n",
              "L 114.1023 260.261544 \n",
              "L 114.220249 241.560322 \n",
              "L 114.220249 252.237997 \n",
              "L 114.220249 260.464346 \n",
              "L 114.220249 242.691827 \n",
              "L 114.306939 254.616894 \n",
              "L 114.395314 210.362647 \n",
              "L 114.395314 258.539338 \n",
              "L 114.395314 244.233765 \n",
              "L 114.643488 223.370933 \n",
              "L 114.643488 256.314957 \n",
              "L 114.643488 208.614689 \n",
              "L 114.643488 230.477043 \n",
              "L 114.925121 259.949294 \n",
              "L 114.925121 204.314003 \n",
              "L 114.925121 260.338802 \n",
              "L 115.183418 256.430844 \n",
              "L 115.183418 202.712513 \n",
              "L 115.183418 258.130516 \n",
              "L 115.183418 214.964316 \n",
              "L 115.491821 260.654271 \n",
              "L 115.507827 248.806462 \n",
              "L 115.507827 258.557043 \n",
              "L 115.507827 242.379577 \n",
              "L 115.66666 256.429234 \n",
              "L 115.66666 207.053437 \n",
              "L 115.66666 260.539994 \n",
              "L 115.66666 250.876328 \n",
              "L 115.935872 248.423392 \n",
              "L 115.935872 260.367773 \n",
              "L 115.935872 207.001931 \n",
              "L 115.935872 214.040441 \n",
              "L 116.145025 231.888608 \n",
              "L 116.211128 261.034122 \n",
              "L 116.211128 214.302796 \n",
              "L 116.211128 259.50989 \n",
              "L 116.464611 237.554182 \n",
              "L 116.464611 206.174629 \n",
              "L 116.464611 258.928846 \n",
              "L 116.464611 246.837997 \n",
              "L 116.691063 231.600501 \n",
              "L 116.691063 259.128429 \n",
              "L 116.691063 222.466373 \n",
              "L 116.691063 253.866849 \n",
              "L 116.809511 232.136477 \n",
              "L 116.809511 259.591976 \n",
              "L 116.855581 249.599964 \n",
              "L 117.052075 239.796268 \n",
              "L 117.052075 260.62208 \n",
              "L 117.052075 200.051946 \n",
              "L 117.300581 261.04217 \n",
              "L 117.300581 227.707028 \n",
              "L 117.300581 213.03448 \n",
              "L 117.399004 248.251171 \n",
              "L 117.399004 247.668519 \n",
              "L 117.630065 231.524852 \n",
              "L 117.630065 193.517222 \n",
              "L 117.630065 259.038295 \n",
              "L 117.630065 248.725985 \n",
              "L 117.88981 239.970098 \n",
              "L 117.88981 203.116507 \n",
              "L 117.88981 257.615463 \n",
              "L 117.88981 253.750963 \n",
              "L 118.0296 255.15287 \n",
              "L 118.136853 241.177252 \n",
              "L 118.136853 244.674778 \n",
              "L 118.251819 234.698861 \n",
              "L 118.251819 202.144346 \n",
              "L 118.251819 259.64992 \n",
              "L 118.251819 214.346253 \n",
              "L 118.505425 259.326402 \n",
              "L 118.505425 200.303034 \n",
              "L 118.505425 261.092066 \n",
              "L 118.59018 231.227088 \n",
              "L 118.852773 237.369085 \n",
              "L 118.852773 257.085925 \n",
              "L 118.852773 190.769741 \n",
              "L 118.852773 236.908757 \n",
              "L 119.149791 250.052244 \n",
              "L 119.149791 191.349174 \n",
              "L 119.149791 259.921931 \n",
              "L 119.149791 216.287356 \n",
              "L 119.354011 259.938027 \n",
              "L 119.399251 256.351976 \n",
              "L 119.527153 219.831559 \n",
              "L 119.527153 240.987326 \n",
              "L 119.527153 221.022617 \n",
              "L 119.527153 259.139696 \n",
              "L 119.823134 232.395613 \n",
              "L 119.823134 260.568966 \n",
              "L 119.823134 190.415642 \n",
              "L 119.823134 202.596626 \n",
              "L 119.973989 257.510844 \n",
              "L 120.11853 240.060232 \n",
              "L 120.11853 199.628638 \n",
              "L 120.11853 260.353288 \n",
              "L 120.11853 229.817133 \n",
              "L 120.427977 225.490694 \n",
              "L 120.427977 260.237401 \n",
              "L 120.427977 197.623154 \n",
              "L 120.427977 198.810993 \n",
              "L 120.643701 261.190247 \n",
              "L 120.652926 251.095225 \n",
              "L 120.832366 244.364137 \n",
              "L 120.832366 247.406164 \n",
              "L 120.832366 259.825359 \n",
              "L 120.832366 205.75132 \n",
              "L 120.832366 218.073943 \n",
              "L 121.108639 258.611768 \n",
              "L 121.108639 258.579577 \n",
              "L 121.108639 204.312393 \n",
              "L 121.108639 260.72992 \n",
              "L 121.108639 212.916984 \n",
              "L 121.256795 258.689025 \n",
              "L 121.256795 256.20068 \n",
              "L 121.449614 211.146492 \n",
              "L 121.449614 258.128906 \n",
              "L 121.449614 173.982259 \n",
              "L 121.449614 249.678832 \n",
              "L 121.776024 238.74363 \n",
              "L 121.776024 217.14363 \n",
              "L 121.776024 260.910188 \n",
              "L 121.776024 258.170754 \n",
              "L 121.913001 244.013258 \n",
              "L 121.913001 260.926283 \n",
              "L 121.913001 236.181246 \n",
              "L 122.021219 238.547267 \n",
              "L 122.213484 217.162945 \n",
              "L 122.213484 259.043124 \n",
              "L 122.213484 191.735463 \n",
              "L 122.213484 193.631499 \n",
              "L 122.469368 240.826373 \n",
              "L 122.469368 258.814569 \n",
              "L 122.469368 228.175404 \n",
              "L 122.469368 245.680739 \n",
              "L 122.627557 222.593526 \n",
              "L 122.627557 236.76068 \n",
              "L 122.627557 259.85755 \n",
              "L 122.627557 210.232274 \n",
              "L 122.627557 212.707744 \n",
              "L 122.955083 252.40217 \n",
              "L 122.955083 181.775642 \n",
              "L 122.955083 260.86834 \n",
              "L 122.955083 223.314599 \n",
              "L 123.168552 246.565985 \n",
              "L 123.227313 260.230963 \n",
              "L 123.227313 240.250158 \n",
              "L 123.267236 254.964554 \n",
              "L 123.366909 255.289681 \n",
              "L 123.366909 240.839249 \n",
              "L 123.366909 210.206522 \n",
              "L 123.366909 260.746015 \n",
              "L 123.366909 248.626194 \n",
              "L 123.718653 207.943511 \n",
              "L 123.718653 260.807177 \n",
              "L 123.718653 175.430844 \n",
              "L 123.718653 252.189711 \n",
              "L 123.906265 260.572185 \n",
              "L 124.150242 258.235136 \n",
              "L 124.150242 172.346969 \n",
              "L 124.150242 206.424107 \n",
              "L 124.430811 234.449383 \n",
              "L 124.430811 260.131171 \n",
              "L 124.430811 227.28372 \n",
              "L 124.536766 249.331171 \n",
              "L 124.607079 261.193466 \n",
              "L 124.651337 257.83597 \n",
              "L 124.651337 224.730993 \n",
              "L 124.651337 261.122647 \n",
              "L 124.651337 244.711797 \n",
              "L 124.972731 239.915374 \n",
              "L 124.972731 260.826492 \n",
              "L 124.972731 170.438057 \n",
              "L 125.355188 255.750009 \n",
              "L 125.355188 187.106432 \n",
              "L 125.355188 258.74375 \n",
              "L 125.355188 257.501186 \n",
              "L 125.646422 219.738206 \n",
              "L 125.646422 193.596089 \n",
              "L 125.646422 257.198593 \n",
              "L 125.646422 209.102379 \n",
              "L 125.872376 253.351797 \n",
              "L 125.872376 245.571291 \n",
              "L 125.872376 257.938981 \n",
              "L 125.872376 230.644435 \n",
              "L 125.872376 253.326045 \n",
              "L 126.143902 223.488429 \n",
              "L 126.143902 234.333496 \n",
              "L 126.143902 258.885389 \n",
              "L 126.143902 182.248846 \n",
              "L 126.143902 202.609502 \n",
              "L 126.490123 228.832095 \n",
              "L 126.490123 226.604495 \n",
              "L 126.594282 190.940352 \n",
              "L 126.594282 252.077043 \n",
              "L 126.594282 228.909353 \n",
              "L 126.943304 248.561812 \n",
              "L 126.943304 187.576417 \n",
              "L 126.943304 253.27132 \n",
              "L 126.943304 235.077103 \n",
              "L 127.24976 245.577729 \n",
              "L 127.24976 250.464286 \n",
              "L 127.24976 221.228638 \n",
              "L 127.462588 238.061186 \n",
              "L 127.462588 171.062557 \n",
              "L 127.462588 260.961693 \n",
              "L 127.462588 217.510605 \n",
              "L 127.73845 257.604197 \n",
              "L 127.73845 241.248072 \n",
              "L 127.73845 254.851887 \n",
              "L 127.73845 233.399964 \n",
              "L 127.95051 235.176894 \n",
              "L 127.95051 192.369621 \n",
              "L 127.95051 261.080799 \n",
              "L 127.95051 236.847595 \n",
              "L 128.303346 193.422259 \n",
              "L 128.303346 204.550605 \n",
              "L 128.303346 260.984227 \n",
              "L 128.303346 192.437222 \n",
              "L 128.303346 203.858504 \n",
              "L 128.483632 260.97135 \n",
              "L 128.512629 235.180113 \n",
              "L 128.512629 225.918832 \n",
              "L 128.512629 260.704167 \n",
              "L 128.512629 253.06208 \n",
              "L 128.858961 203.053735 \n",
              "L 128.858961 259.979875 \n",
              "L 128.858961 159.184167 \n",
              "L 128.858961 185.09451 \n",
              "L 129.079281 260.668757 \n",
              "L 129.079281 260.539994 \n",
              "L 129.319009 182.181246 \n",
              "L 129.319009 260.739577 \n",
              "L 129.319009 178.791559 \n",
              "L 129.319009 198.318474 \n",
              "L 129.587639 258.975523 \n",
              "L 129.587639 213.805449 \n",
              "L 129.587639 259.567833 \n",
              "L 129.680089 241.32211 \n",
              "L 129.885915 201.659875 \n",
              "L 129.885915 257.938981 \n",
              "L 129.885915 255.936715 \n",
              "L 130.184064 237.890575 \n",
              "L 130.184064 260.266373 \n",
              "L 130.184064 184.25755 \n",
              "L 130.435114 258.981961 \n",
              "L 130.435114 239.632095 \n",
              "L 130.721371 202.54834 \n",
              "L 130.721371 173.467207 \n",
              "L 130.721371 260.97135 \n",
              "L 130.721371 253.834659 \n",
              "L 130.846829 228.320262 \n",
              "L 131.020462 247.312811 \n",
              "L 131.020462 258.866075 \n",
              "L 131.020462 235.263809 \n",
              "L 131.020462 253.612542 \n",
              "L 131.245893 191.619577 \n",
              "L 131.245893 201.495702 \n",
              "L 131.245893 257.394957 \n",
              "L 131.245893 180.375344 \n",
              "L 131.245893 219.496775 \n",
              "L 131.570642 235.691946 \n",
              "L 131.570642 259.616119 \n",
              "L 131.570642 200.446283 \n",
              "L 131.570642 227.254748 \n",
              "L 131.888657 216.870009 \n",
              "L 131.888657 197.314122 \n",
              "L 131.888657 260.092542 \n",
              "L 131.888657 241.965925 \n",
              "L 132.101113 231.413794 \n",
              "L 132.101113 257.700769 \n",
              "L 132.101113 210.818146 \n",
              "L 132.101113 247.631499 \n",
              "L 132.222924 255.141604 \n",
              "L 132.232291 255.057908 \n",
              "L 132.284976 234.349592 \n",
              "L 132.284976 258.763064 \n",
              "L 132.284976 253.087833 \n",
              "L 132.484132 251.449323 \n",
              "L 132.484132 219.660948 \n",
              "L 132.69219 201.679189 \n",
              "L 132.69219 146.098623 \n",
              "L 132.69219 255.95603 \n",
              "L 132.69219 207.933854 \n",
              "L 133.067732 233.969741 \n",
              "L 133.067732 244.879189 \n",
              "L 133.067732 229.388996 \n",
              "L 133.067732 235.646879 \n",
              "L 133.264523 173.586313 \n",
              "L 133.264523 175.488787 \n",
              "L 133.264523 260.897311 \n",
              "L 133.264523 165.963541 \n",
              "L 133.264523 168.158951 \n",
              "L 133.446937 253.30673 \n",
              "L 133.459391 241.711618 \n",
              "L 133.651624 245.725806 \n",
              "L 133.651624 260.671976 \n",
              "L 133.651624 217.117878 \n",
              "L 133.651624 255.743571 \n",
              "L 133.895917 214.011469 \n",
              "L 133.895917 218.994599 \n",
              "L 133.895917 260.86834 \n",
              "L 133.895917 178.884912 \n",
              "L 133.895917 228.857848 \n",
              "L 134.220088 260.198772 \n",
              "L 134.220088 260.150486 \n",
              "L 134.220088 208.078712 \n",
              "L 134.220088 247.048846 \n",
              "L 134.473259 242.75138 \n",
              "L 134.473259 259.757759 \n",
              "L 134.473259 201.766104 \n",
              "L 134.473259 223.304942 \n",
              "L 134.687645 249.704584 \n",
              "L 134.687645 222.175046 \n",
              "L 134.687645 260.13439 \n",
              "L 134.687645 257.832751 \n",
              "L 134.85824 232.514718 \n",
              "L 134.933442 235.688727 \n",
              "L 134.933442 261.048608 \n",
              "L 134.933442 210.779517 \n",
              "L 134.933442 242.004554 \n",
              "L 135.128476 243.41451 \n",
              "L 135.128476 257.378861 \n",
              "L 135.128476 209.736537 \n",
              "L 135.128476 235.247714 \n",
              "L 135.461959 175.955553 \n",
              "L 135.461959 191.674301 \n",
              "L 135.461959 256.02363 \n",
              "L 135.461959 161.076984 \n",
              "L 135.461959 183.291827 \n",
              "L 135.708887 261.190247 \n",
              "L 135.708887 250.129502 \n",
              "L 135.904593 253.532066 \n",
              "L 135.904593 195.965329 \n",
              "L 135.904593 261.009979 \n",
              "L 135.904593 232.727177 \n",
              "L 136.208368 215.051231 \n",
              "L 136.208368 260.97135 \n",
              "L 136.208368 213.676686 \n",
              "L 136.208368 249.630545 \n",
              "L 136.361738 254.70059 \n",
              "L 136.361738 244.254689 \n",
              "L 136.587344 207.898444 \n",
              "L 136.587344 221.386373 \n",
              "L 136.587344 256.403481 \n",
              "L 136.587344 142.560858 \n",
              "L 136.587344 218.923779 \n",
              "L 136.995048 226.540113 \n",
              "L 136.995048 225.213854 \n",
              "L 136.995048 260.192334 \n",
              "L 137.184662 214.140232 \n",
              "L 137.184662 260.182677 \n",
              "L 137.184662 194.529621 \n",
              "L 137.184662 200.584703 \n",
              "L 137.400473 237.542915 \n",
              "L 137.400473 230.712036 \n",
              "L 137.400473 258.206164 \n",
              "L 137.400473 226.971469 \n",
              "L 137.400473 258.099934 \n",
              "L 137.552728 249.727118 \n",
              "L 137.552728 249.9267 \n",
              "L 137.552728 211.909413 \n",
              "L 137.552728 260.060352 \n",
              "L 137.552728 257.823094 \n",
              "L 137.861795 205.815702 \n",
              "L 137.861795 187.763124 \n",
              "L 137.861795 260.913407 \n",
              "L 137.861795 241.982021 \n",
              "L 138.213523 225.78363 \n",
              "L 138.213523 260.974569 \n",
              "L 138.213523 193.026313 \n",
              "L 138.213523 254.14369 \n",
              "L 138.455411 254.240262 \n",
              "L 138.455411 261.203124 \n",
              "L 138.455411 200.150128 \n",
              "L 138.546025 227.254748 \n",
              "L 138.766108 208.236447 \n",
              "L 138.836436 211.281693 \n",
              "L 138.836436 261.032513 \n",
              "L 138.836436 181.659756 \n",
              "L 138.836436 229.253794 \n",
              "L 139.119806 232.260411 \n",
              "L 139.119806 253.931231 \n",
              "L 139.119806 218.186611 \n",
              "L 139.119806 242.629055 \n",
              "L 139.447656 176.567177 \n",
              "L 139.447656 259.445508 \n",
              "L 139.447656 169.977729 \n",
              "L 139.447656 196.509353 \n",
              "L 139.72525 261.061484 \n",
              "L 139.72525 254.38834 \n",
              "L 139.72525 227.566999 \n",
              "L 139.824338 256.857371 \n",
              "L 139.824338 244.830903 \n",
              "L 140.140431 167.444316 \n",
              "L 140.140431 254.829353 \n",
              "L 140.140431 164.392632 \n",
              "L 140.140431 249.624107 \n",
              "L 140.412439 254.578265 \n",
              "L 140.412439 254.140471 \n",
              "L 140.412439 261.187028 \n",
              "L 140.412439 247.599308 \n",
              "L 140.412439 250.496477 \n",
              "L 140.738976 200.542855 \n",
              "L 140.738976 260.813615 \n",
              "L 140.738976 134.493854 \n",
              "L 140.738976 243.916686 \n",
              "L 141.034498 254.253139 \n",
              "L 141.054206 221.206104 \n",
              "L 141.054206 257.858504 \n",
              "L 141.054206 229.643303 \n",
              "L 141.420484 192.047714 \n",
              "L 141.420484 194.684137 \n",
              "L 141.420484 260.977788 \n",
              "L 141.420484 163.037401 \n",
              "L 141.420484 260.562528 \n",
              "L 141.665125 260.932721 \n",
              "L 141.665125 252.962289 \n",
              "L 142.022004 139.744167 \n",
              "L 142.022004 258.032334 \n",
              "L 142.022004 121.16366 \n",
              "L 142.022004 214.919249 \n",
              "L 142.322597 216.416119 \n",
              "L 142.429921 258.161097 \n",
              "L 142.429921 257.159964 \n",
              "L 142.684658 237.591201 \n",
              "L 142.684658 243.269651 \n",
              "L 142.684658 163.265955 \n",
              "L 142.684658 258.666492 \n",
              "L 142.684658 173.332006 \n",
              "L 142.919837 259.262021 \n",
              "L 142.919837 230.132602 \n",
              "L 143.256113 158.933079 \n",
              "L 143.256113 253.940888 \n",
              "L 143.256113 157.458742 \n",
              "L 143.256113 228.230128 \n",
              "L 143.460135 244.595911 \n",
              "L 143.460135 214.520083 \n",
              "L 143.460135 231.510367 \n",
              "L 143.644203 252.36676 \n",
              "L 143.644203 248.56825 \n",
              "L 143.890411 174.95442 \n",
              "L 143.890411 256.644912 \n",
              "L 143.890411 151.242706 \n",
              "L 143.890411 213.042528 \n",
              "L 144.292624 215.836686 \n",
              "L 144.292624 258.190069 \n",
              "L 144.292624 209.8653 \n",
              "L 144.292624 257.752274 \n",
              "L 144.589096 177.806522 \n",
              "L 144.589096 181.176894 \n",
              "L 144.589096 259.979875 \n",
              "L 144.589096 167.920739 \n",
              "L 144.589096 237.214569 \n",
              "L 144.833358 256.619159 \n",
              "L 144.920388 220.913168 \n",
              "L 144.833358 260.163362 \n",
              "L 144.920388 255.659875 \n",
              "L 145.261688 243.324376 \n",
              "L 145.261688 175.89439 \n",
              "L 145.261688 260.62369 \n",
              "L 145.261688 207.895225 \n",
              "L 145.600321 235.833586 \n",
              "L 145.600321 260.153705 \n",
              "L 145.600321 218.376537 \n",
              "L 145.600321 222.851052 \n",
              "L 145.898003 193.946969 \n",
              "L 145.898003 260.424107 \n",
              "L 145.898003 171.802945 \n",
              "L 145.898003 235.81749 \n",
              "L 146.091487 248.909472 \n",
              "L 146.284053 223.401514 \n",
              "L 146.284053 259.99597 \n",
              "L 146.284053 200.21451 \n",
              "L 146.284053 220.97755 \n",
              "L 146.599244 200.150128 \n",
              "L 146.599244 260.385478 \n",
              "L 146.599244 174.39752 \n",
              "L 146.599244 214.053317 \n",
              "L 146.918573 242.625836 \n",
              "L 146.918573 259.487356 \n",
              "L 146.918573 211.70983 \n",
              "L 146.918573 226.874897 \n",
              "L 147.221627 255.930277 \n",
              "L 147.221627 191.964018 \n",
              "L 147.221627 259.455165 \n",
              "L 147.221627 249.060769 \n",
              "L 147.429494 247.911559 \n",
              "L 147.429494 259.19442 \n",
              "L 147.429494 240.852125 \n",
              "L 147.429494 252.846402 \n",
              "L 147.615311 237.159845 \n",
              "L 147.615311 207.106551 \n",
              "L 147.615311 259.641872 \n",
              "L 147.615311 228.458683 \n",
              "L 147.950416 180.246581 \n",
              "L 147.950416 182.403362 \n",
              "L 147.950416 260.720262 \n",
              "L 147.950416 161.775523 \n",
              "L 147.950416 225.567952 \n",
              "L 148.336513 205.001276 \n",
              "L 148.336513 260.816835 \n",
              "L 148.336513 209.852423 \n",
              "L 148.561644 247.364316 \n",
              "L 148.561644 211.713049 \n",
              "L 148.561644 259.258802 \n",
              "L 148.561644 230.196984 \n",
              "L 148.675431 260.469174 \n",
              "L 148.772533 254.076089 \n",
              "L 148.772533 209.23758 \n",
              "L 148.772533 257.845627 \n",
              "L 148.772533 253.361455 \n",
              "L 149.028172 231.413794 \n",
              "L 149.028172 260.62369 \n",
              "L 149.028172 211.661544 \n",
              "L 149.028172 248.935225 \n",
              "L 149.38293 212.47597 \n",
              "L 149.38293 152.987446 \n",
              "L 149.38293 261.000322 \n",
              "L 149.38293 170.885508 \n",
              "L 149.698343 257.597759 \n",
              "L 149.698343 244.418861 \n",
              "L 149.843477 233.136 \n",
              "L 149.843477 260.887654 \n",
              "L 149.843477 224.138683 \n",
              "L 149.927121 257.291946 \n",
              "L 150.106822 247.135762 \n",
              "L 150.106822 177.172364 \n",
              "L 150.106822 253.535285 \n",
              "L 150.106822 196.010396 \n",
              "L 150.403167 260.929502 \n",
              "L 150.403167 249.849443 \n",
              "L 150.403167 228.143213 \n",
              "L 150.403167 257.803779 \n",
              "L 150.403167 246.070247 \n",
              "L 150.747853 218.05141 \n",
              "L 150.747853 131.19752 \n",
              "L 150.747853 259.635434 \n",
              "L 150.747853 253.905478 \n",
              "L 151.112667 252.0191 \n",
              "L 151.112667 260.710605 \n",
              "L 151.158516 231.445985 \n",
              "L 151.158516 245.880322 \n",
              "L 151.282486 245.355613 \n",
              "L 151.456048 181.411887 \n",
              "L 151.456048 215.878534 \n",
              "L 151.456048 256.123422 \n",
              "L 151.456048 180.639308 \n",
              "L 151.456048 248.597222 \n",
              "L 151.665703 256.696417 \n",
              "L 151.665703 261.116209 \n",
              "L 151.743864 251.632811 \n",
              "L 151.764704 252.125329 \n",
              "L 151.831021 260.713824 \n",
              "L 151.840048 249.44062 \n",
              "L 151.908944 261.096894 \n",
              "L 151.908944 257.530158 \n",
              "L 151.961613 256.699636 \n",
              "L 152.047971 247.068161 \n",
              "L 151.961613 261.209562 \n",
              "L 152.047971 252.736954 \n",
              "L 152.083099 260.617252 \n",
              "L 152.083099 259.841455 \n",
              "L 152.225733 237.52682 \n",
              "L 152.297288 260.591499 \n",
              "L 152.328373 253.65439 \n",
              "L 152.369903 254.095404 \n",
              "L 152.369903 253.860411 \n",
              "L 152.430088 242.101127 \n",
              "L 152.430088 255.305776 \n",
              "L 152.444258 255.022498 \n",
              "L 152.444258 260.243839 \n",
              "L 152.536209 244.554063 \n",
              "L 152.536209 244.612006 \n",
              "L 152.598712 260.890873 \n",
              "L 152.628492 257.633168 \n",
              "L 152.667553 253.944107 \n",
              "L 152.667553 259.596805 \n",
              "L 152.667553 248.680918 \n",
              "L 152.777931 260.530337 \n",
              "L 152.892897 238.869174 \n",
              "L 152.892897 242.580769 \n",
              "L 152.940986 260.881216 \n",
              "L 152.976019 259.986313 \n",
              "L 153.025389 250.628459 \n",
              "L 153.025389 260.977788 \n",
              "L 153.052028 254.774629 \n",
              "L 153.052028 239.426075 \n",
              "L 153.129603 260.877997 \n",
              "L 153.154613 251.166045 \n",
              "L 153.179282 251.484733 \n",
              "L 153.209845 250.197103 \n",
              "L 153.209845 260.517461 \n",
              "L 153.293212 235.698385 \n",
              "L 153.293212 239.867088 \n",
              "L 153.455847 231.674539 \n",
              "L 153.515407 258.537729 \n",
              "L 153.562016 257.24366 \n",
              "L 153.631402 250.535106 \n",
              "L 153.738046 261.14518 \n",
              "L 153.738046 241.98524 \n",
              "L 153.752343 258.5667 \n",
              "L 153.81555 222.020531 \n",
              "L 153.81555 235.695165 \n",
              "L 154.049439 210.991976 \n",
              "L 154.130251 259.931589 \n",
              "L 154.049439 209.376 \n",
              "L 154.130251 245.101306 \n",
              "L 154.2044 260.147267 \n",
              "L 154.266033 251.00831 \n",
              "L 154.302032 208.667803 \n",
              "L 154.266033 260.76211 \n",
              "L 154.302032 213.113347 \n",
              "L 154.609287 200.922706 \n",
              "L 154.645373 260.932721 \n",
              "L 154.609287 200.723124 \n",
              "L 154.713779 258.46369 \n",
              "L 154.748678 226.672095 \n",
              "L 154.728542 260.758891 \n",
              "L 154.748678 237.848727 \n",
              "L 154.921116 243.945657 \n",
              "L 154.939558 260.543213 \n",
              "L 154.939558 230.13904 \n",
              "L 154.939558 233.824882 \n",
              "L 155.070285 260.173019 \n",
              "L 155.103879 245.931827 \n",
              "L 155.103879 233.287297 \n",
              "L 155.103879 260.79752 \n",
              "L 155.148604 249.978206 \n",
              "L 155.216393 245.99299 \n",
              "L 155.216393 247.853615 \n",
              "L 155.266831 258.814569 \n",
              "L 155.312474 221.003303 \n",
              "L 155.312474 229.102498 \n",
              "L 155.368553 224.219159 \n",
              "L 155.368553 208.47144 \n",
              "L 155.368553 240.604256 \n",
              "L 155.368553 209.755851 \n",
              "L 155.643845 208.642051 \n",
              "L 155.643845 204.238355 \n",
              "L 155.742228 260.858683 \n",
              "L 155.742228 243.231022 \n",
              "L 155.821869 260.945598 \n",
              "L 155.821869 240.34673 \n",
              "L 155.821869 258.418623 \n",
              "L 155.908598 250.837699 \n",
              "L 155.960594 232.669234 \n",
              "L 155.908598 258.000143 \n",
              "L 155.960594 249.376238 \n",
              "L 156.081629 247.299934 \n",
              "L 156.081629 248.333258 \n",
              "L 156.081629 241.116089 \n",
              "L 156.081629 259.19442 \n",
              "L 156.197601 199.944107 \n",
              "L 156.197601 242.294271 \n",
              "L 156.197601 260.952036 \n",
              "L 156.197601 206.089323 \n",
              "L 156.197601 241.862915 \n",
              "L 156.465661 258.048429 \n",
              "L 156.465661 214.536179 \n",
              "L 156.465661 259.918712 \n",
              "L 156.465661 257.984048 \n",
              "L 156.641921 247.679785 \n",
              "L 156.641921 214.368787 \n",
              "L 156.674857 258.618206 \n",
              "L 156.674857 249.572602 \n",
              "L 156.77802 243.771827 \n",
              "L 156.77802 259.954122 \n",
              "L 156.867985 230.229174 \n",
              "L 156.867985 241.772781 \n",
              "L 156.931469 246.260173 \n",
              "L 156.931469 226.48217 \n",
              "L 156.931469 205.909055 \n",
              "L 156.931469 246.063809 \n",
              "L 156.931469 238.843422 \n",
              "L 157.190186 238.489323 \n",
              "L 157.190186 212.54679 \n",
              "L 157.242238 260.884435 \n",
              "L 157.290895 244.45749 \n",
              "L 157.3551 236.538563 \n",
              "L 157.3551 260.1022 \n",
              "L 157.3551 234.723004 \n",
              "L 157.3551 250.615583 \n",
              "L 157.487472 230.354718 \n",
              "L 157.487472 260.7267 \n",
              "L 157.577469 248.101484 \n",
              "L 157.617598 248.333258 \n",
              "L 157.714739 261.129085 \n",
              "L 157.714739 236.670545 \n",
              "L 157.714739 238.846641 \n",
              "L 157.848149 239.966879 \n",
              "L 157.955472 261.135523 \n",
              "L 157.848149 233.737967 \n",
              "L 157.955472 256.731827 \n",
              "L 158.032715 211.185121 \n",
              "L 158.032715 258.248012 \n",
              "L 158.032715 255.289681 \n",
              "L 158.277191 217.35287 \n",
              "L 158.277191 261.167714 \n",
              "L 158.277191 236.287475 \n",
              "L 158.400045 259.725568 \n",
              "L 158.400045 222.43901 \n",
              "L 158.400045 257.845627 \n",
              "L 158.521769 232.981484 \n",
              "L 158.521769 253.200501 \n",
              "L 158.521769 260.079666 \n",
              "L 158.521769 234.101723 \n",
              "L 158.521769 244.84056 \n",
              "L 158.635999 254.542855 \n",
              "L 158.635999 250.454629 \n",
              "L 158.635999 259.596805 \n",
              "L 158.654964 231.919189 \n",
              "L 158.832157 251.626373 \n",
              "L 158.832157 227.966164 \n",
              "L 158.832157 259.506671 \n",
              "L 158.87098 249.096179 \n",
              "L 158.943271 250.10375 \n",
              "L 159.039115 260.456298 \n",
              "L 159.039115 216.425776 \n",
              "L 159.039115 253.023452 \n",
              "L 159.260899 257.584882 \n",
              "L 159.260899 256.603064 \n",
              "L 159.260899 221.540888 \n",
              "L 159.333933 245.403899 \n",
              "L 159.403754 258.315613 \n",
              "L 159.403754 240.060232 \n",
              "L 159.403754 242.609741 \n",
              "L 159.403754 224.090396 \n",
              "L 159.403754 261.141961 \n",
              "L 159.471519 259.056 \n",
              "L 159.528373 248.268876 \n",
              "L 159.528373 261.061484 \n",
              "L 159.528373 252.878593 \n",
              "L 159.528373 223.440143 \n",
              "L 159.528373 257.182498 \n",
              "L 159.528373 247.003779 \n",
              "L 159.738282 244.795493 \n",
              "L 159.738282 219.1298 \n",
              "L 159.738282 260.523899 \n",
              "L 159.738282 219.738206 \n",
              "L 159.862711 261.209562 \n",
              "L 159.862711 247.911559 \n",
              "L 160.028076 245.033705 \n",
              "L 160.028076 238.04831 \n",
              "L 160.131539 259.661186 \n",
              "L 160.131539 257.739398 \n",
              "L 160.197436 254.723124 \n",
              "L 160.197436 259.123601 \n",
              "L 160.246798 241.843601 \n",
              "L 160.246798 261.051827 \n",
              "L 160.246798 256.265061 \n",
              "L 160.368094 246.47907 \n",
              "L 160.368094 229.018802 \n",
              "L 160.368094 259.82214 \n",
              "L 160.368094 236.068578 \n",
              "L 160.518973 242.931648 \n",
              "L 160.518973 260.614033 \n",
              "L 160.518973 224.331827 \n",
              "L 160.518973 247.013437 \n",
              "L 160.713777 227.312692 \n",
              "L 160.778433 258.924018 \n",
              "L 160.778433 250.374152 \n",
              "L 160.920608 252.476209 \n",
              "L 160.920608 221.515136 \n",
              "L 160.920608 261.032513 \n",
              "L 160.920608 234.243362 \n",
              "L 161.05378 254.397997 \n",
              "L 161.05378 242.509949 \n",
              "L 161.05378 229.19907 \n",
              "L 161.05378 260.581842 \n",
              "L 161.05378 238.708221 \n",
              "L 161.190995 255.283243 \n",
              "L 161.190995 253.200501 \n",
              "L 161.272653 228.893258 \n",
              "L 161.272653 230.786075 \n",
              "L 161.272653 259.046343 \n",
              "L 161.272653 226.710724 \n",
              "L 161.272653 238.566581 \n",
              "L 161.404092 258.962647 \n",
              "L 161.404092 251.706849 \n",
              "L 161.473154 223.4498 \n",
              "L 161.473154 257.549472 \n",
              "L 161.473154 234.227267 \n",
              "L 161.473154 261.177371 \n",
              "L 161.473154 246.253735 \n",
              "L 161.663085 220.204972 \n",
              "L 161.663085 258.782379 \n",
              "L 161.899814 249.68527 \n",
              "L 161.899814 211.265598 \n",
              "L 161.899814 259.184763 \n",
              "L 161.899814 256.818742 \n",
              "L 162.037156 248.333258 \n",
              "L 162.037156 236.287475 \n",
              "L 162.131662 261.148399 \n",
              "L 162.131662 255.447416 \n",
              "L 162.26697 250.161693 \n",
              "L 162.26697 252.521276 \n",
              "L 162.26697 209.485449 \n",
              "L 162.26697 260.356507 \n",
              "L 162.498858 229.730218 \n",
              "L 162.498858 238.788697 \n",
              "L 162.498858 211.43299 \n",
              "L 162.498858 260.440203 \n",
              "L 162.498858 257.040858 \n",
              "L 162.692365 254.269234 \n",
              "L 162.692365 236.702736 \n",
              "L 162.748832 261.190247 \n",
              "L 162.779268 260.536775 \n",
              "L 162.943067 215.827028 \n",
              "L 162.943067 260.884435 \n",
              "L 162.943067 213.193824 \n",
              "L 162.943067 223.707326 \n",
              "L 163.173791 259.789949 \n",
              "L 163.173791 207.187028 \n",
              "L 163.173791 260.704167 \n",
              "L 163.233502 231.014629 \n",
              "L 163.413472 241.486283 \n",
              "L 163.413472 241.351082 \n",
              "L 163.413472 227.338444 \n",
              "L 163.523288 260.861902 \n",
              "L 163.523288 257.485091 \n",
              "L 163.597295 259.799607 \n",
              "L 163.597295 254.967773 \n",
              "L 163.597295 260.272811 \n",
              "L 163.597295 258.576358 \n",
              "L 163.723868 253.783154 \n",
              "L 163.828249 223.652602 \n",
              "L 163.828249 260.755672 \n",
              "L 163.828249 243.166641 \n",
              "L 163.994698 255.743571 \n",
              "L 163.994698 252.598534 \n",
              "L 164.048221 234.195076 \n",
              "L 163.994698 261.14518 \n",
              "L 164.048221 248.680918 \n",
              "L 164.20334 257.092364 \n",
              "L 164.20334 231.764674 \n",
              "L 164.20334 260.230963 \n",
              "L 164.259253 254.803601 \n",
              "L 164.354646 215.872095 \n",
              "L 164.354646 259.693377 \n",
              "L 164.354646 243.346909 \n",
              "L 164.354646 261.206343 \n",
              "L 164.354646 231.819398 \n",
              "L 164.450624 249.717461 \n",
              "L 164.668183 208.651708 \n",
              "L 164.668183 225.600143 \n",
              "L 164.668183 255.038593 \n",
              "L 164.668183 200.385121 \n",
              "L 164.668183 241.457311 \n",
              "L 164.789044 260.224525 \n",
              "L 164.789044 252.26375 \n",
              "L 164.95221 219.387326 \n",
              "L 164.95221 260.575404 \n",
              "L 164.95221 215.804495 \n",
              "L 164.95221 235.662975 \n",
              "L 165.067881 261.209562 \n",
              "L 165.067881 226.807297 \n",
              "L 165.067881 244.930694 \n",
              "L 165.262701 228.700113 \n",
              "L 165.296334 261.154838 \n",
              "L 165.262701 223.958414 \n",
              "L 165.296334 251.668221 \n",
              "L 165.386956 250.676745 \n",
              "L 165.386956 261.048608 \n",
              "L 165.386956 226.961812 \n",
              "L 165.386956 247.480203 \n",
              "L 165.530849 245.342736 \n",
              "L 165.530849 260.472393 \n",
              "L 165.530849 250.557639 \n",
              "L 165.688539 227.286939 \n",
              "L 165.688539 230.744227 \n",
              "L 165.688539 259.084972 \n",
              "L 165.688539 198.830307 \n",
              "L 165.688539 213.79901 \n",
              "L 165.999473 242.686999 \n",
              "L 165.999473 208.239666 \n",
              "L 165.999473 259.564614 \n",
              "L 165.999473 235.537431 \n",
              "L 166.121592 261.058265 \n",
              "L 166.121592 260.813615 \n",
              "L 166.264638 226.131291 \n",
              "L 166.264638 252.550247 \n",
              "L 166.264638 261.216 \n",
              "L 166.264638 216.828161 \n",
              "L 166.264638 253.96986 \n",
              "L 166.414314 249.163779 \n",
              "L 166.414314 260.173019 \n",
              "L 166.414314 248.124018 \n",
              "L 166.414314 255.595493 \n",
              "L 166.600424 220.398116 \n",
              "L 166.600424 260.395136 \n",
              "L 166.600424 206.044256 \n",
              "L 166.600424 227.277282 \n",
              "L 166.766493 259.909055 \n",
              "L 166.82106 247.679785 \n",
              "L 166.89794 250.374152 \n",
              "L 166.89794 216.62214 \n",
              "L 166.89794 256.744703 \n",
              "L 166.89794 219.277878 \n",
              "L 167.233116 232.356984 \n",
              "L 167.233116 185.220054 \n",
              "L 167.233116 260.13439 \n",
              "L 167.233116 240.20831 \n",
              "L 167.350575 242.950963 \n",
              "L 167.350575 220.459279 \n",
              "L 167.350575 255.88521 \n",
              "L 167.350575 227.773019 \n",
              "L 167.55985 252.772364 \n",
              "L 167.612045 212.041395 \n",
              "L 167.588269 260.826492 \n",
              "L 167.612045 218.048191 \n",
              "L 167.908239 250.061902 \n",
              "L 167.908239 195.273228 \n",
              "L 167.908239 261.00676 \n",
              "L 167.997279 258.0098 \n",
              "L 168.082497 243.494987 \n",
              "L 168.082497 249.199189 \n",
              "L 168.082497 259.19442 \n",
              "L 168.082497 228.252662 \n",
              "L 168.246295 231.111201 \n",
              "L 168.246295 258.209383 \n",
              "L 168.246295 237.031082 \n",
              "L 168.449977 243.195613 \n",
              "L 168.449977 203.903571 \n",
              "L 168.449977 260.791082 \n",
              "L 168.449977 225.654867 \n",
              "L 168.677213 251.974033 \n",
              "L 168.677213 239.50977 \n",
              "L 168.787662 208.468221 \n",
              "L 168.787662 259.133258 \n",
              "L 168.787662 253.200501 \n",
              "L 168.860063 255.450635 \n",
              "L 168.950115 248.732423 \n",
              "L 168.950115 256.72217 \n",
              "L 168.950115 238.556924 \n",
              "L 168.950115 260.7267 \n",
              "L 168.950115 245.12062 \n",
              "L 169.170752 253.631857 \n",
              "L 169.170752 186.037699 \n",
              "L 169.170752 261.003541 \n",
              "L 169.170752 201.534331 \n",
              "L 169.441732 260.713824 \n",
              "L 169.441732 244.28366 \n",
              "L 169.441732 230.277461 \n",
              "L 169.441732 260.314659 \n",
              "L 169.53199 255.025717 \n",
              "L 169.624123 259.57749 \n",
              "L 169.624123 217.288489 \n",
              "L 169.624123 260.140829 \n",
              "L 169.624123 258.016238 \n",
              "L 169.931536 215.40211 \n",
              "L 169.931536 187.714838 \n",
              "L 169.931536 259.43907 \n",
              "L 169.931536 227.679666 \n",
              "L 170.201306 250.229294 \n",
              "L 170.201306 260.317878 \n",
              "L 170.201306 229.797818 \n",
              "L 170.201306 251.465419 \n",
              "L 170.417741 225.410218 \n",
              "L 170.417741 229.092841 \n",
              "L 170.417741 259.709472 \n",
              "L 170.417741 219.232811 \n",
              "L 170.488323 258.804912 \n",
              "L 170.674575 207.917759 \n",
              "L 170.674575 227.480083 \n",
              "L 170.674575 261.038951 \n",
              "L 170.674575 183.130873 \n",
              "L 170.674575 228.262319 \n",
              "L 170.895505 223.733079 \n",
              "L 170.895505 258.508757 \n",
              "L 170.895505 219.442051 \n",
              "L 170.895505 228.011231 \n",
              "L 171.149695 235.910844 \n",
              "L 171.149695 259.793168 \n",
              "L 171.149695 209.594897 \n",
              "L 171.149695 231.136954 \n",
              "L 171.311722 257.359547 \n",
              "L 171.311722 248.420173 \n",
              "L 171.409527 259.854331 \n",
              "L 171.409527 228.04986 \n",
              "L 171.563302 255.109413 \n",
              "L 171.563302 244.296537 \n",
              "L 171.563302 232.714301 \n",
              "L 171.563302 259.448727 \n",
              "L 171.630845 251.661782 \n",
              "L 171.82421 191.133496 \n",
              "L 171.82421 212.987803 \n",
              "L 171.82421 260.758891 \n",
              "L 171.82421 185.599905 \n",
              "L 171.82421 257.671797 \n",
              "L 172.173152 259.789949 \n",
              "L 172.173152 201.756447 \n",
              "L 172.173152 261.067923 \n",
              "L 172.284029 247.07138 \n",
              "L 172.43107 236.709174 \n",
              "L 172.43107 260.498146 \n",
              "L 172.43107 235.624346 \n",
              "L 172.43107 239.181425 \n",
              "L 172.561069 210.412542 \n",
              "L 172.561069 260.852244 \n",
              "L 172.561069 202.397043 \n",
              "L 172.561069 243.633407 \n",
              "L 172.8231 258.035553 \n",
              "L 172.933668 213.06828 \n",
              "L 172.933668 260.098981 \n",
              "L 173.202346 227.187148 \n",
              "L 173.202346 244.602349 \n",
              "L 173.202346 256.435672 \n",
              "L 173.202346 227.080918 \n",
              "L 173.202346 241.380054 \n",
              "L 173.467075 257.910009 \n",
              "L 173.467075 168.278057 \n",
              "L 173.467075 260.008846 \n",
              "L 173.467075 200.681276 \n",
              "L 173.776206 236.866909 \n",
              "L 173.776206 230.911618 \n",
              "L 173.85636 228.71299 \n",
              "L 173.85636 259.909055 \n",
              "L 173.85636 211.313884 \n",
              "L 173.85636 247.496298 \n",
              "L 173.992902 248.207714 \n",
              "L 173.992902 261.10977 \n",
              "L 174.072717 218.315374 \n",
              "L 174.072717 235.559964 \n",
              "L 174.147934 248.684137 \n",
              "L 174.292254 236.738146 \n",
              "L 174.292254 259.818921 \n",
              "L 174.292254 219.313288 \n",
              "L 174.292254 245.143154 \n",
              "L 174.490864 242.661246 \n",
              "L 174.490864 243.910247 \n",
              "L 174.490864 260.585061 \n",
              "L 174.490864 207.473526 \n",
              "L 174.490864 231.967475 \n",
              "L 174.792123 235.846462 \n",
              "L 174.792123 259.674063 \n",
              "L 174.792123 199.235911 \n",
              "L 174.792123 236.960262 \n",
              "L 175.007767 238.733973 \n",
              "L 175.007767 226.260054 \n",
              "L 175.007767 260.871559 \n",
              "L 175.086102 257.806999 \n",
              "L 175.14241 257.17606 \n",
              "L 175.14241 259.149353 \n",
              "L 175.14241 244.373794 \n",
              "L 175.406333 203.655702 \n",
              "L 175.406333 258.801693 \n",
              "L 175.406333 193.567118 \n",
              "L 175.406333 199.509532 \n",
              "L 175.664013 250.148817 \n",
              "L 175.664013 242.790009 \n",
              "L 175.664013 217.195136 \n",
              "L 175.664013 261.061484 \n",
              "L 175.664013 233.734748 \n",
              "L 175.806236 261.196686 \n",
              "L 175.806236 257.221127 \n",
              "L 175.929146 219.879845 \n",
              "L 175.929146 242.85761 \n",
              "L 175.929146 256.61594 \n",
              "L 175.929146 220.613794 \n",
              "L 176.261094 219.622319 \n",
              "L 176.261094 260.758891 \n",
              "L 176.261094 172.787982 \n",
              "L 176.261094 229.260232 \n",
              "L 176.439624 258.67293 \n",
              "L 176.511076 239.448608 \n",
              "L 176.511076 259.47448 \n",
              "L 176.559346 259.500232 \n",
              "L 176.652555 239.96366 \n",
              "L 176.652555 245.371708 \n",
              "L 176.652555 225.329741 \n",
              "L 176.652555 259.390784 \n",
              "L 176.652555 251.986909 \n",
              "L 176.880779 242.979934 \n",
              "L 176.880779 260.147267 \n",
              "L 176.880779 196.927833 \n",
              "L 176.880779 249.746432 \n",
              "L 177.075608 243.034659 \n",
              "L 177.075608 258.489443 \n",
              "L 177.075608 230.66375 \n",
              "L 177.075608 250.663869 \n",
              "L 177.357862 190.322289 \n",
              "L 177.357862 211.365389 \n",
              "L 177.357862 259.590367 \n",
              "L 177.357862 178.952513 \n",
              "L 177.357862 205.319964 \n",
              "L 177.546852 261.04217 \n",
              "L 177.60767 251.629592 \n",
              "L 177.60767 243.4467 \n",
              "L 177.694589 259.690158 \n",
              "L 177.743911 229.002706 \n",
              "L 177.743911 237.723183 \n",
              "L 177.96382 260.807177 \n",
              "L 177.96382 248.95132 \n",
              "L 177.96382 211.323541 \n",
              "L 177.96382 255.663094 \n",
              "L 177.96382 220.410993 \n",
              "L 178.280064 176.747446 \n",
              "L 178.280064 260.820054 \n",
              "L 178.280064 164.035314 \n",
              "L 178.280064 219.599785 \n",
              "L 178.521675 254.858325 \n",
              "L 178.538748 247.786015 \n",
              "L 178.538748 231.021067 \n",
              "L 178.824784 213.863392 \n",
              "L 178.824784 179.094152 \n",
              "L 178.824784 257.526939 \n",
              "L 178.824784 209.47901 \n",
              "L 179.012832 260.346849 \n",
              "L 179.138045 238.160978 \n",
              "L 179.138045 239.149234 \n",
              "L 179.138045 259.329621 \n",
              "L 179.333791 228.436149 \n",
              "L 179.333791 260.465955 \n",
              "L 179.333791 206.292125 \n",
              "L 179.333791 236.078235 \n",
              "L 179.56086 247.1422 \n",
              "L 179.56086 257.710426 \n",
              "L 179.56086 234.156447 \n",
              "L 179.56086 245.822379 \n",
              "L 179.719627 234.034122 \n",
              "L 179.719627 191.854569 \n",
              "L 179.719627 255.730694 \n",
              "L 179.719627 200.204852 \n",
              "L 180.025181 224.814689 \n",
              "L 180.025181 260.678414 \n",
              "L 180.025181 211.436209 \n",
              "L 180.125867 257.420709 \n",
              "L 180.277418 230.287118 \n",
              "L 180.277418 232.810873 \n",
              "L 180.277418 260.93594 \n",
              "L 180.277418 211.004852 \n",
              "L 180.277418 249.855881 \n",
              "L 180.55881 226.108757 \n",
              "L 180.55881 261.019636 \n",
              "L 180.55881 199.870069 \n",
              "L 180.55881 201.173794 \n",
              "L 180.755497 214.526522 \n",
              "L 180.755497 234.751976 \n",
              "L 180.755497 210.155016 \n",
              "L 180.755497 219.319726 \n",
              "L 180.945476 171.303988 \n",
              "L 180.945476 260.37904 \n",
              "L 180.945476 154.545478 \n",
              "L 181.047523 192.900769 \n",
              "L 181.22576 258.093496 \n",
              "L 181.22576 227.245091 \n",
              "L 181.41153 229.440501 \n",
              "L 181.41153 227.299815 \n",
              "L 181.431903 260.562528 \n",
              "L 181.431903 249.179875 \n",
              "L 181.634872 202.110545 \n",
              "L 181.634872 202.207118 \n",
              "L 181.634872 260.482051 \n",
              "L 181.634872 192.141067 \n",
              "L 181.634872 206.881216 \n",
              "L 181.869521 253.252006 \n",
              "L 181.869521 250.293675 \n",
              "L 181.869521 211.725925 \n",
              "L 181.869521 258.557043 \n",
              "L 181.869521 236.303571 \n",
              "L 182.183082 214.384882 \n",
              "L 182.183082 260.028161 \n",
              "L 182.183082 193.10679 \n",
              "L 182.183082 237.938861 \n",
              "L 182.363385 242.043183 \n",
              "L 182.363385 238.756507 \n",
              "L 182.363385 260.697729 \n",
              "L 182.539185 233.197162 \n",
              "L 182.539185 260.401574 \n",
              "L 182.539185 197.388161 \n",
              "L 182.539185 256.387386 \n",
              "L 182.693434 254.568608 \n",
              "L 182.693434 250.348399 \n",
              "L 182.855278 243.218146 \n",
              "L 182.855278 211.091768 \n",
              "L 182.855278 261.022855 \n",
              "L 182.855278 243.131231 \n",
              "L 183.130776 251.265836 \n",
              "L 183.130776 198.942975 \n",
              "L 183.130776 259.915493 \n",
              "L 183.130776 219.20062 \n",
              "L 183.34551 260.713824 \n",
              "L 183.34551 255.160918 \n",
              "L 183.513035 214.046879 \n",
              "L 183.513035 233.895702 \n",
              "L 183.513035 260.095762 \n",
              "L 183.513035 212.900888 \n",
              "L 183.513035 233.783034 \n",
              "L 183.801089 256.049383 \n",
              "L 183.801089 175.875076 \n",
              "L 183.801089 258.521633 \n",
              "L 183.801089 180.980531 \n",
              "L 184.083374 260.646224 \n",
              "L 184.182549 254.787505 \n",
              "L 184.182549 230.80217 \n",
              "L 184.182549 261.074361 \n",
              "L 184.21445 249.99752 \n",
              "L 184.516151 230.596149 \n",
              "L 184.516151 146.880858 \n",
              "L 184.516151 259.899398 \n",
              "L 184.516151 207.73749 \n",
              "L 184.91675 224.341484 \n",
              "L 184.91675 221.367058 \n",
              "L 184.91675 238.379875 \n",
              "L 185.034494 253.715553 \n",
              "L 185.034494 187.499159 \n",
              "L 185.034494 260.507803 \n",
              "L 185.034494 225.078653 \n",
              "L 185.227685 259.986313 \n",
              "L 185.227685 258.492662 \n",
              "L 185.399418 248.944882 \n",
              "L 185.399418 211.661544 \n",
              "L 185.399418 256.767237 \n",
              "L 185.399418 215.215404 \n",
              "L 185.837417 175.620769 \n",
              "L 185.837417 141.820471 \n",
              "L 185.837417 260.964912 \n",
              "L 185.837417 170.064644 \n",
              "L 186.273643 245.165687 \n",
              "L 186.403033 204.959428 \n",
              "L 186.403033 223.10214 \n",
              "L 186.403033 261.100113 \n",
              "L 186.403033 193.293496 \n",
              "L 186.403033 217.011648 \n",
              "L 186.73036 219.065419 \n",
              "L 186.73036 257.636387 \n",
              "L 186.73036 207.933854 \n",
              "L 186.73036 231.825836 \n",
              "L 186.892679 257.668578 \n",
              "L 186.892679 253.973079 \n",
              "L 186.892679 227.254748 \n",
              "L 186.892679 258.808131 \n",
              "L 186.892679 240.774867 \n",
              "L 187.096045 231.008191 \n",
              "L 187.096045 207.985359 \n",
              "L 187.096045 259.226611 \n",
              "L 187.096045 227.254748 \n",
              "L 187.368575 226.128072 \n",
              "L 187.368575 260.720262 \n",
              "L 187.368575 204.11603 \n",
              "L 187.368575 251.204674 \n",
              "L 187.67894 198.772364 \n",
              "L 187.67894 209.433943 \n",
              "L 187.67894 258.518414 \n",
              "L 187.67894 178.749711 \n",
              "L 187.67894 246.717282 \n",
              "L 187.83811 235.244495 \n",
              "L 187.83811 237.472095 \n",
              "L 188.022487 234.645747 \n",
              "L 188.022487 261.196686 \n",
              "L 188.022487 202.854152 \n",
              "L 188.022487 213.271082 \n",
              "L 188.168492 260.475613 \n",
              "L 188.186341 256.207118 \n",
              "L 188.522799 246.105657 \n",
              "L 188.522799 133.206224 \n",
              "L 188.522799 255.621246 \n",
              "L 188.522799 152.668757 \n",
              "L 188.967253 248.932006 \n",
              "L 188.967253 236.635136 \n",
              "L 188.967253 197.883899 \n",
              "L 188.967253 255.260709 \n",
              "L 188.967253 245.397461 \n",
              "L 189.144383 258.145001 \n",
              "L 189.144383 258.067744 \n",
              "L 189.370803 178.775463 \n",
              "L 189.370803 223.089264 \n",
              "L 189.370803 260.27603 \n",
              "L 189.370803 176.374033 \n",
              "L 189.370803 182.470963 \n",
              "L 189.554531 258.653615 \n",
              "L 189.873536 142.72825 \n",
              "L 189.873536 261.203124 \n",
              "L 189.873536 126.716566 \n",
              "L 189.873536 215.769085 \n",
              "L 190.347501 254.191976 \n",
              "L 190.347501 215.498683 \n",
              "L 190.347501 259.741663 \n",
              "L 190.695289 223.700888 \n",
              "L 190.695289 261.190247 \n",
              "L 190.695289 177.494271 \n",
              "L 190.695289 208.551917 \n",
              "L 191.058695 250.676745 \n",
              "L 191.058695 198.71442 \n",
              "L 191.058695 253.599666 \n",
              "L 191.058695 227.364197 \n",
              "L 191.426262 250.715374 \n",
              "L 191.426262 175.910486 \n",
              "L 191.426262 259.007714 \n",
              "L 191.426262 227.763362 \n",
              "L 191.703144 258.788817 \n",
              "L 191.969361 192.520918 \n",
              "L 191.969361 259.014152 \n",
              "L 191.969361 154.265419 \n",
              "L 191.969361 236.178027 \n",
              "L 192.274029 253.27132 \n",
              "L 192.274029 227.525151 \n",
              "L 192.274029 259.664405 \n",
              "L 192.274029 228.883601 \n",
              "L 192.628162 158.900888 \n",
              "L 192.628162 260.237401 \n",
              "L 192.628162 157.793526 \n",
              "L 192.628162 176.580054 \n",
              "L 192.934239 235.347505 \n",
              "L 192.934239 260.964912 \n",
              "L 192.934239 220.514003 \n",
              "L 192.934239 233.158534 \n",
              "L 193.143807 259.130039 \n",
              "L 193.143807 250.226075 \n",
              "L 193.143807 221.376715 \n",
              "L 193.143807 260.752453 \n",
              "L 193.143807 228.413615 \n",
              "L 193.485977 232.154182 \n",
              "L 193.485977 153.183809 \n",
              "L 193.485977 260.295344 \n",
              "L 193.485977 228.902915 \n",
              "L 193.95442 241.013079 \n",
              "L 193.95442 182.142617 \n",
              "L 193.95442 250.773317 \n",
              "L 194.126566 241.74059 \n",
              "L 194.126566 243.388757 \n",
              "L 194.126566 248.571469 \n",
              "L 194.355534 206.910188 \n",
              "L 194.355534 258.911142 \n",
              "L 194.355534 189.301842 \n",
              "L 194.355534 245.796626 \n",
              "L 194.807188 186.494808 \n",
              "L 194.807188 123.658444 \n",
              "L 194.807188 260.012066 \n",
              "L 194.807188 147.582617 \n",
              "L 195.334985 259.16223 \n",
              "L 195.334985 254.642647 \n",
              "L 195.334985 223.224465 \n",
              "L 195.334985 253.606104 \n",
              "L 195.705654 190.467148 \n",
              "L 195.705654 257.06983 \n",
              "L 195.705654 137.178563 \n",
              "L 195.705654 199.441931 \n",
              "L 196.106688 257.874599 \n",
              "L 196.106688 218.743511 \n",
              "L 196.106688 245.004733 \n",
              "L 196.432797 221.524793 \n",
              "L 196.432797 155.153884 \n",
              "L 196.432797 260.849025 \n",
              "L 196.432797 188.316805 \n",
              "L 196.763068 259.110724 \n",
              "L 196.763068 249.672393 \n",
              "L 197.009015 169.279189 \n",
              "L 197.009015 175.685151 \n",
              "L 197.009015 258.943332 \n",
              "L 197.009015 163.085687 \n",
              "L 197.412138 214.333377 \n",
              "L 197.412138 211.017729 \n",
              "L 197.412138 260.69451 \n",
              "L 197.412138 187.730933 \n",
              "L 197.412138 202.210337 \n",
              "L 197.834297 216.000858 \n",
              "L 197.834297 260.675195 \n",
              "L 197.834297 166.414212 \n",
              "L 197.834297 260.256715 \n",
              "L 198.208415 236.216656 \n",
              "L 198.208415 202.577311 \n",
              "L 198.208415 258.711559 \n",
              "L 198.208415 251.082349 \n",
              "L 198.413379 244.760083 \n",
              "L 198.413379 256.496835 \n",
              "L 198.413379 244.624882 \n",
              "L 198.79408 246.717282 \n",
              "L 198.79408 260.758891 \n",
              "L 198.79408 107.601693 \n",
              "L 198.79408 253.387207 \n",
              "L 199.208247 253.496656 \n",
              "L 199.208247 234.684376 \n",
              "L 199.208247 256.001097 \n",
              "L 199.208247 248.977073 \n",
              "L 199.368493 247.296715 \n",
              "L 199.368493 231.349413 \n",
              "L 199.368493 250.13594 \n",
              "L 199.368493 244.586253 \n",
              "L 199.67669 177.719607 \n",
              "L 199.67669 257.185717 \n",
              "L 199.67669 125.293735 \n",
              "L 199.67669 236.635136 \n",
              "L 200.008196 260.572185 \n",
              "L 200.200184 250.457848 \n",
              "L 200.200184 180.706909 \n",
              "L 200.200184 260.874778 \n",
              "L 200.200184 212.85904 \n",
              "L 200.524695 260.700948 \n",
              "L 200.524695 215.685389 \n",
              "L 200.524695 260.501365 \n",
              "L 200.723171 257.320918 \n",
              "L 200.723171 215.550188 \n",
              "L 200.723171 260.514241 \n",
              "L 200.723171 259.078534 \n",
              "L 200.99404 194.529621 \n",
              "L 201.140788 242.674122 \n",
              "L 201.140788 128.58363 \n",
              "L 201.140788 260.939159 \n",
              "L 201.140788 238.482885 \n",
              "L 201.57252 215.376358 \n",
              "L 201.57252 259.033466 \n",
              "L 201.57252 210.721574 \n",
              "L 201.57252 228.111022 \n",
              "L 201.791061 233.364554 \n",
              "L 201.855748 239.300531 \n",
              "L 201.855748 260.089323 \n",
              "L 201.855748 210.708697 \n",
              "L 201.939787 258.15144 \n",
              "L 202.23599 182.155493 \n",
              "L 202.23599 204.70834 \n",
              "L 202.23599 259.992751 \n",
              "L 202.23599 148.419577 \n",
              "L 202.23599 169.530277 \n",
              "L 202.543016 252.05451 \n",
              "L 202.843317 225.155911 \n",
              "L 202.843317 116.570039 \n",
              "L 202.843317 260.127952 \n",
              "L 202.843317 149.617073 \n",
              "L 203.283989 254.94524 \n",
              "L 203.283989 231.864465 \n",
              "L 203.283989 260.836149 \n",
              "L 203.283989 232.559785 \n",
              "L 203.469679 231.510367 \n",
              "L 203.469679 261.080799 \n",
              "L 203.469679 219.986075 \n",
              "L 203.469679 224.376894 \n",
              "L 203.877344 189.13445 \n",
              "L 203.877344 260.089323 \n",
              "L 203.877344 122.692721 \n",
              "L 203.877344 149.378861 \n",
              "L 204.377165 214.127356 \n",
              "L 204.377165 257.166402 \n",
              "L 204.377165 201.978563 \n",
              "L 204.377165 206.820054 \n",
              "L 204.599772 260.443422 \n",
              "L 204.599772 245.571291 \n",
              "L 204.8714 181.131827 \n",
              "L 204.8714 261.125866 \n",
              "L 204.8714 175.730218 \n",
              "L 205.383659 147.95603 \n",
              "L 205.383659 260.237401 \n",
              "L 205.383659 146.488131 \n",
              "L 205.383659 206.478832 \n",
              "L 205.753552 243.787923 \n",
              "L 205.753552 234.94834 \n",
              "L 205.825265 215.524435 \n",
              "L 205.825265 259.889741 \n",
              "L 205.825265 222.92831 \n",
              "L 205.93842 260.964912 \n",
              "L 205.93842 256.928191 \n",
              "L 206.32947 156.905061 \n",
              "L 206.32947 259.548519 \n",
              "L 206.32947 114.046283 \n",
              "L 206.32947 211.951261 \n",
              "L 206.74427 260.861902 \n",
              "L 206.74427 259.297431 \n",
              "L 206.74427 246.955493 \n",
              "L 206.776171 259.19442 \n",
              "L 206.864134 251.204674 \n",
              "L 206.864134 217.050277 \n",
              "L 206.864134 258.737311 \n",
              "L 206.864134 223.018444 \n",
              "L 207.299569 258.132125 \n",
              "L 207.299569 133.212662 \n",
              "L 207.299569 260.024942 \n",
              "L 207.299569 173.63138 \n",
              "L 207.708215 260.44986 \n",
              "L 207.708215 237.76825 \n",
              "L 208.012123 248.301067 \n",
              "L 208.012123 160.001812 \n",
              "L 208.012123 255.601931 \n",
              "L 208.403474 218.228459 \n",
              "L 208.403474 261.132304 \n",
              "L 208.403474 209.536954 \n",
              "L 208.403474 239.596686 \n",
              "L 208.534984 260.13439 \n",
              "L 208.696749 189.301842 \n",
              "L 208.696749 185.58059 \n",
              "L 208.696749 259.497013 \n",
              "L 209.145365 234.323839 \n",
              "L 209.145365 151.78673 \n",
              "L 209.145365 259.226611 \n",
              "L 209.145365 238.843422 \n",
              "L 209.453024 245.545538 \n",
              "L 209.453024 260.385478 \n",
              "L 209.453024 239.770516 \n",
              "L 209.466126 240.478712 \n",
              "L 209.727801 177.700292 \n",
              "L 209.727801 261.022855 \n",
              "L 209.727801 256.026849 \n",
              "L 210.130418 198.398951 \n",
              "L 210.130418 220.932483 \n",
              "L 210.130418 259.844674 \n",
              "L 210.130418 184.775821 \n",
              "L 210.130418 202.216775 \n",
              "L 210.345556 259.909055 \n",
              "L 210.345556 253.23913 \n",
              "L 210.608197 212.0929 \n",
              "L 210.608197 213.17451 \n",
              "L 210.608197 160.813019 \n",
              "L 210.608197 260.836149 \n",
              "L 210.608197 202.603064 \n",
              "L 210.8146 244.554063 \n",
              "L 210.952044 249.028578 \n",
              "L 210.952044 229.675493 \n",
              "L 210.952044 260.778206 \n",
              "L 210.952044 240.510903 \n",
              "L 211.17329 230.776417 \n",
              "L 211.17329 205.628996 \n",
              "L 211.17329 259.110724 \n",
              "L 211.17329 223.964852 \n",
              "L 211.311937 259.310307 \n",
              "L 211.752815 123.722826 \n",
              "L 211.752815 148.715732 \n",
              "L 211.752815 259.593586 \n",
              "L 211.752815 94.841276 \n",
              "L 211.752815 96.302736 \n",
              "L 212.224027 260.668757 \n",
              "L 212.224027 252.485866 \n",
              "L 212.742315 138.350307 \n",
              "L 212.742315 161.321633 \n",
              "L 212.742315 74.020292 \n",
              "L 212.742315 260.758891 \n",
              "L 212.742315 76.215702 \n",
              "L 213.201311 256.780113 \n",
              "L 213.201311 256.078355 \n",
              "L 213.298247 223.404733 \n",
              "L 213.298247 259.535642 \n",
              "L 213.298247 251.957937 \n",
              "L 213.298247 260.185896 \n",
              "L 213.298247 249.356924 \n",
              "L 213.298247 251.526581 \n",
              "L 213.435248 244.87597 \n",
              "L 213.721513 196.152036 \n",
              "L 213.721513 132.053794 \n",
              "L 213.721513 260.823273 \n",
              "L 213.721513 200.922706 \n",
              "L 214.071959 258.248012 \n",
              "L 214.154986 232.849502 \n",
              "L 214.154986 260.1022 \n",
              "L 214.381596 231.246402 \n",
              "L 214.381596 260.340411 \n",
              "L 214.381596 194.059636 \n",
              "L 214.381596 198.463332 \n",
              "L 214.531873 252.704763 \n",
              "L 214.774386 180.674718 \n",
              "L 214.774386 258.84676 \n",
              "L 214.774386 178.698206 \n",
              "L 214.774386 254.533198 \n",
              "L 214.994477 256.593407 \n",
              "L 214.994477 259.587148 \n",
              "L 214.994477 258.557043 \n",
              "L 215.164455 199.828221 \n",
              "L 215.164455 231.111201 \n",
              "L 215.164455 258.679368 \n",
              "L 215.164455 199.461246 \n",
              "L 215.164455 234.401097 \n",
              "L 215.605222 161.012602 \n",
              "L 215.605222 259.664405 \n",
              "L 215.605222 147.286462 \n",
              "L 215.605222 251.005091 \n",
              "L 215.871961 260.597937 \n",
              "L 215.884082 258.788817 \n",
              "L 215.934433 247.270963 \n",
              "L 215.934433 261.216 \n",
              "L 215.934433 253.419398 \n",
              "L 216.21247 181.846462 \n",
              "L 216.21247 215.060888 \n",
              "L 216.21247 261.029294 \n",
              "L 216.21247 163.362528 \n",
              "L 216.21247 189.475672 \n",
              "L 216.689932 217.378623 \n",
              "L 216.689932 163.56211 \n",
              "L 216.689932 259.954122 \n",
              "L 216.689932 208.938206 \n",
              "L 216.920973 260.939159 \n",
              "L 216.935879 260.597937 \n",
              "L 217.025599 258.093496 \n",
              "L 217.025599 233.34524 \n",
              "L 217.025599 258.988399 \n",
              "L 217.295899 236.860471 \n",
              "L 217.295899 172.266492 \n",
              "L 217.295899 260.108638 \n",
              "L 217.295899 208.416715 \n",
              "L 217.704402 237.42059 \n",
              "L 217.704402 187.956268 \n",
              "L 217.704402 260.256715 \n",
              "L 217.704402 240.588161 \n",
              "L 217.89764 259.870426 \n",
              "L 217.89764 256.876686 \n",
              "L 218.126798 192.971589 \n",
              "L 218.126798 261.14518 \n",
              "L 218.126798 183.456 \n",
              "L 218.126798 232.591976 \n",
              "L 218.377159 260.198772 \n",
              "L 218.377159 233.918235 \n",
              "L 218.452084 261.029294 \n",
              "L 218.452084 259.001276 \n",
              "L 218.551077 256.136298 \n",
              "L 218.551077 226.392036 \n",
              "L 218.551077 260.057133 \n",
              "L 218.551077 247.747386 \n",
              "L 218.742701 227.499398 \n",
              "L 218.844289 260.62369 \n",
              "L 218.742701 221.215762 \n",
              "L 218.844289 259.432632 \n",
              "L 219.189355 184.447475 \n",
              "L 219.189355 260.533556 \n",
              "L 219.189355 108.116745 \n",
              "L 219.189355 205.770635 \n",
              "L 219.63911 245.64211 \n",
              "L 219.63911 242.480978 \n",
              "L 219.71845 259.632215 \n",
              "L 219.63911 236.416238 \n",
              "L 219.71845 257.675016 \n",
              "L 219.753389 254.449502 \n",
              "L 219.753389 259.844674 \n",
              "L 220.264634 178.498623 \n",
              "L 220.264634 62.0067 \n",
              "L 220.264634 258.351022 \n",
              "L 220.264634 254.043899 \n",
              "L 220.776735 260.314659 \n",
              "L 220.886962 231.948161 \n",
              "L 220.886962 261.203124 \n",
              "L 220.886962 239.004376 \n",
              "L 221.366244 159.815106 \n",
              "L 221.366244 259.175106 \n",
              "L 221.366244 102.843899 \n",
              "L 221.366244 121.089621 \n",
              "L 221.858961 244.888846 \n",
              "L 221.858961 241.553884 \n",
              "L 221.858961 260.765329 \n",
              "L 221.858961 217.069592 \n",
              "L 221.858961 235.720918 \n",
              "L 222.02302 256.953943 \n",
              "L 222.02302 253.316387 \n",
              "L 222.02302 240.182557 \n",
              "L 222.02302 253.329264 \n",
              "L 222.167696 231.375165 \n",
              "L 222.167696 231.896656 \n",
              "L 222.167696 256.619159 \n",
              "L 222.167696 238.090158 \n",
              "L 222.513062 161.225061 \n",
              "L 222.513062 260.417669 \n",
              "L 222.513062 155.694689 \n",
              "L 222.513062 195.40521 \n",
              "L 222.782776 261.196686 \n",
              "L 222.782776 260.533556 \n",
              "L 222.92636 210.032692 \n",
              "L 222.92636 216.277699 \n",
              "L 222.92636 259.754539 \n",
              "L 222.92636 205.468042 \n",
              "L 222.92636 228.323481 \n",
              "L 223.2547 234.124256 \n",
              "L 223.2547 260.610814 \n",
              "L 223.2547 191.098086 \n",
              "L 223.2547 213.59299 \n",
              "L 223.484539 239.950784 \n",
              "L 223.559068 261.216 \n",
              "L 223.484539 239.339159 \n",
              "L 223.559068 258.505538 \n",
              "L 223.662681 230.654092 \n",
              "L 223.662681 257.501186 \n",
              "L 223.662681 260.430545 \n",
              "L 223.662681 232.179934 \n",
              "L 223.662681 253.026671 \n",
              "L 223.9348 204.849979 \n",
              "L 223.9348 258.782379 \n",
              "L 223.9348 186.559189 \n",
              "L 223.9348 204.161097 \n",
              "L 224.400411 171.139815 \n",
              "L 224.400411 150.782379 \n",
              "L 224.400411 260.475613 \n",
              "L 224.400411 259.741663 \n",
              "L 224.68329 256.831618 \n",
              "L 224.68329 257.308042 \n",
              "L 224.68329 261.183809 \n",
              "L 224.751332 243.066849 \n",
              "L 224.751332 259.400441 \n",
              "L 225.028372 205.010933 \n",
              "L 225.028372 257.926104 \n",
              "L 225.028372 173.972602 \n",
              "L 225.028372 189.919905 \n",
              "L 225.44273 200.652304 \n",
              "L 225.44273 176.090754 \n",
              "L 225.44273 260.230963 \n",
              "L 225.44273 193.744167 \n",
              "L 225.653912 260.675195 \n",
              "L 225.653912 258.595672 \n",
              "L 225.779393 230.596149 \n",
              "L 225.779393 245.700054 \n",
              "L 225.779393 259.329621 \n",
              "L 225.779393 215.505121 \n",
              "L 225.828542 232.018981 \n",
              "L 225.981113 251.024405 \n",
              "L 225.981113 230.982438 \n",
              "L 225.981113 257.533377 \n",
              "L 225.981113 243.916686 \n",
              "L 226.208625 219.831559 \n",
              "L 226.208625 225.452066 \n",
              "L 226.208625 257.037639 \n",
              "L 226.208625 201.186671 \n",
              "L 226.208625 226.269711 \n",
              "L 226.519829 260.436984 \n",
              "L 226.519829 190.055106 \n",
              "L 226.519829 202.706075 \n",
              "L 226.915467 259.078534 \n",
              "L 226.915467 170.090396 \n",
              "L 226.915467 261.170933 \n",
              "L 226.915467 172.916745 \n",
              "L 227.228126 259.632215 \n",
              "L 227.228126 243.285747 \n",
              "L 227.342167 254.513884 \n",
              "L 227.342167 247.889025 \n",
              "L 227.342167 234.671499 \n",
              "L 227.342167 259.941246 \n",
              "L 227.342167 257.102021 \n",
              "L 227.536687 239.809145 \n",
              "L 227.536687 248.243124 \n",
              "L 227.536687 209.363124 \n",
              "L 227.536687 261.125866 \n",
              "L 227.536687 256.104107 \n",
              "L 227.67891 260.546432 \n",
              "L 227.67891 257.41749 \n",
              "L 228.055339 125.261544 \n",
              "L 228.055339 131.165329 \n",
              "L 228.055339 260.005627 \n",
              "L 228.055339 111.44527 \n",
              "L 228.055339 239.075195 \n",
              "L 228.456104 252.563124 \n",
              "L 228.456104 259.477699 \n",
              "L 228.456104 247.412602 \n",
              "L 228.456104 259.426194 \n",
              "L 228.5738 234.735881 \n",
              "L 228.5738 235.701604 \n",
              "L 228.5738 260.372602 \n",
              "L 228.5738 227.306253 \n",
              "L 228.68254 258.859636 \n",
              "L 229.157328 83.43287 \n",
              "L 229.157328 260.037818 \n",
              "L 229.157328 67.105717 \n",
              "L 229.157328 226.701067 \n",
              "L 229.625281 259.625776 \n",
              "L 229.642956 256.342319 \n",
              "L 229.73899 236.796089 \n",
              "L 229.73899 261.080799 \n",
              "L 229.73899 256.541902 \n",
              "L 229.790274 260.13439 \n",
              "L 230.246295 231.381604 \n",
              "L 230.246295 84.250516 \n",
              "L 230.246295 260.494927 \n",
              "L 230.246295 253.496656 \n",
              "L 230.689072 246.311678 \n",
              "L 230.689072 244.753645 \n",
              "L 230.689072 260.108638 \n",
              "L 230.810709 233.821663 \n",
              "L 230.810709 255.653437 \n",
              "L 230.893261 261.119428 \n",
              "L 230.912518 258.177192 \n",
              "L 231.340025 120.774152 \n",
              "L 231.340025 127.489145 \n",
              "L 231.340025 260.243839 \n",
              "L 231.340025 93.032155 \n",
              "L 231.340025 190.866313 \n",
              "L 231.753971 259.175106 \n",
              "L 231.837789 259.445508 \n",
              "L 232.104149 195.250694 \n",
              "L 232.104149 261.177371 \n",
              "L 232.104149 185.612781 \n",
              "L 232.104149 210.155016 \n",
              "L 232.345127 243.395195 \n",
              "L 232.568145 197.516924 \n",
              "L 232.568145 258.492662 \n",
              "L 232.568145 155.211827 \n",
              "L 232.568145 207.966045 \n",
              "L 232.767792 226.591618 \n",
              "L 232.977154 221.061246 \n",
              "L 232.977154 258.338146 \n",
              "L 232.977154 210.283779 \n",
              "L 232.977154 223.127893 \n",
              "L 233.338535 193.782796 \n",
              "L 233.338535 168.435791 \n",
              "L 233.338535 260.37904 \n",
              "L 233.338535 215.414987 \n",
              "L 233.626446 240.980888 \n",
              "L 233.746246 236.4098 \n",
              "L 233.746246 201.283243 \n",
              "L 233.746246 260.707386 \n",
              "L 233.746246 202.126641 \n",
              "L 234.065298 249.68527 \n",
              "L 234.065298 188.690218 \n",
              "L 234.065298 260.849025 \n",
              "L 234.065298 250.457848 \n",
              "L 234.53727 172.524018 \n",
              "L 234.53727 256.979696 \n",
              "L 234.53727 143.899994 \n",
              "L 234.53727 167.663213 \n",
              "L 234.891545 260.681633 \n",
              "L 234.895549 258.885389 \n",
              "L 235.17376 180.327058 \n",
              "L 235.17376 261.061484 \n",
              "L 235.17376 157.503809 \n",
              "L 235.17376 221.56986 \n",
              "L 235.715324 238.92068 \n",
              "L 235.715324 149.044077 \n",
              "L 235.715324 261.016417 \n",
              "L 235.715324 238.141663 \n",
              "L 236.111231 236.635136 \n",
              "L 236.111231 259.728787 \n",
              "L 236.111231 216.689741 \n",
              "L 236.111231 250.754003 \n",
              "L 236.415346 214.738981 \n",
              "L 236.415346 180.288429 \n",
              "L 236.415346 260.295344 \n",
              "L 236.415346 246.575642 \n",
              "L 236.835717 187.035613 \n",
              "L 236.835717 260.115076 \n",
              "L 236.835717 157.317103 \n",
              "L 236.835717 187.94983 \n",
              "L 237.289823 213.811887 \n",
              "L 237.289823 257.610635 \n",
              "L 237.289823 181.473049 \n",
              "L 237.289823 234.652185 \n",
              "L 237.638038 235.836805 \n",
              "L 237.638038 203.285508 \n",
              "L 237.638038 258.943332 \n",
              "L 237.956948 196.950367 \n",
              "L 237.956948 259.966999 \n",
              "L 237.956948 184.968966 \n",
              "L 237.956948 192.842826 \n",
              "L 238.164111 258.981961 \n",
              "L 238.417685 232.675672 \n",
              "L 238.417685 157.181902 \n",
              "L 238.417685 256.857371 \n",
              "L 238.417685 235.907624 \n",
              "L 238.718286 249.427744 \n",
              "L 238.718286 247.045627 \n",
              "L 238.780157 261.170933 \n",
              "L 238.780157 242.062498 \n",
              "L 238.780157 232.862379 \n",
              "L 238.780157 255.743571 \n",
              "L 238.780157 255.704942 \n",
              "L 239.031436 222.451887 \n",
              "L 239.031436 189.784703 \n",
              "L 239.031436 259.387565 \n",
              "L 239.602638 256.30369 \n",
              "L 239.602638 107.408548 \n",
              "L 239.602638 260.86834 \n",
              "L 239.602638 252.801335 \n",
              "L 239.979826 260.494927 \n",
              "L 239.984177 257.423928 \n",
              "L 240.004922 261.209562 \n",
              "L 240.004922 259.870426 \n",
              "L 240.137635 253.129681 \n",
              "L 240.137635 214.037222 \n",
              "L 240.137635 258.376775 \n",
              "L 240.137635 239.931469 \n",
              "L 240.268639 258.209383 \n",
              "L 240.699564 130.405627 \n",
              "L 240.699564 158.250635 \n",
              "L 240.699564 259.818921 \n",
              "L 240.699564 88.622021 \n",
              "L 240.699564 184.531171 \n",
              "L 241.165079 254.011708 \n",
              "L 241.165079 253.084614 \n",
              "L 241.165079 236.004197 \n",
              "L 241.165079 260.475613 \n",
              "L 241.180033 244.013258 \n",
              "L 241.299074 253.670486 \n",
              "L 241.299074 238.521514 \n",
              "L 241.344725 260.610814 \n",
              "L 241.344725 260.346849 \n",
              "L 241.814228 156.216179 \n",
              "L 241.814228 170.856537 \n",
              "L 241.814228 72.507326 \n",
              "L 241.814228 258.492662 \n",
              "L 241.814228 138.768787 \n",
              "L 242.330063 260.211648 \n",
              "L 242.460529 249.550069 \n",
              "L 242.460529 240.787744 \n",
              "L 242.460529 260.243839 \n",
              "L 242.476827 247.869711 \n",
              "L 243.00472 230.177669 \n",
              "L 243.00472 66.494092 \n",
              "L 243.00472 261.003541 \n",
              "L 243.00472 97.416537 \n",
              "L 243.466137 258.988399 \n",
              "L 243.466137 257.887475 \n",
              "L 243.672714 230.551082 \n",
              "L 243.672714 201.244614 \n",
              "L 243.672714 258.215821 \n",
              "L 243.672714 206.124733 \n",
              "L 244.193787 238.070844 \n",
              "L 244.193787 110.5053 \n",
              "L 244.193787 260.939159 \n",
              "L 244.193787 257.919666 \n",
              "L 244.564804 261.138742 \n",
              "L 244.795053 203.015106 \n",
              "L 244.795053 156.563839 \n",
              "L 244.795053 259.863988 \n",
              "L 244.795053 190.763303 \n",
              "L 245.368249 253.483779 \n",
              "L 245.368249 242.596864 \n",
              "L 245.368249 144.254092 \n",
              "L 245.368249 261.003541 \n",
              "L 245.368249 161.489025 \n",
              "L 245.663644 259.992751 \n",
              "L 245.690386 258.544167 \n",
              "L 245.966398 219.116924 \n",
              "L 245.966398 146.397997 \n",
              "L 245.966398 260.945598 \n",
              "L 245.966398 250.148817 \n",
              "L 246.181157 229.578921 \n",
              "L 246.181157 231.272155 \n",
              "L 246.478926 228.729085 \n",
              "L 246.478926 172.208548 \n",
              "L 246.478926 260.333973 \n",
              "L 246.478926 220.127714 \n",
              "L 246.752675 258.77594 \n",
              "L 246.752675 252.827088 \n",
              "L 246.752675 241.476626 \n",
              "L 246.752675 256.683541 \n",
              "L 246.752675 255.177013 \n",
              "L 247.129214 253.561037 \n",
              "L 247.129214 128.995672 \n",
              "L 247.129214 260.836149 \n",
              "L 247.129214 188.658027 \n",
              "L 247.501148 248.989949 \n",
              "L 247.625491 228.947982 \n",
              "L 247.625491 243.066849 \n",
              "L 247.625491 204.386432 \n",
              "L 247.625491 261.209562 \n",
              "L 247.731256 259.760978 \n",
              "L 247.823618 254.224167 \n",
              "L 247.823618 228.857848 \n",
              "L 247.823618 259.284554 \n",
              "L 247.823618 235.759547 \n",
              "L 248.31029 151.200858 \n",
              "L 248.31029 257.327356 \n",
              "L 248.31029 87.99752 \n",
              "L 248.31029 229.295642 \n",
              "L 248.788528 245.455404 \n",
              "L 248.805823 257.996924 \n",
              "L 248.805823 239.532304 \n",
              "L 248.805823 244.206402 \n",
              "L 248.95686 248.236686 \n",
              "L 248.95686 210.264465 \n",
              "L 248.95686 260.700948 \n",
              "L 248.95686 249.350486 \n",
              "L 249.472457 141.073645 \n",
              "L 249.472457 260.095762 \n",
              "L 249.472457 102.650754 \n",
              "L 249.472457 116.653735 \n",
              "L 249.879409 260.44986 \n",
              "L 249.879409 251.726164 \n",
              "L 250.034576 241.676209 \n",
              "L 250.034576 222.522706 \n",
              "L 250.034576 259.876864 \n",
              "L 250.034576 249.112274 \n",
              "L 250.569178 125.107028 \n",
              "L 250.569178 259.36825 \n",
              "L 250.569178 89.639249 \n",
              "L 250.569178 107.421425 \n",
              "L 251.098114 259.323183 \n",
              "L 251.098114 245.436089 \n",
              "L 251.221412 250.341961 \n",
              "L 251.221412 228.729085 \n",
              "L 251.221412 260.919845 \n",
              "L 251.221412 241.231976 \n",
              "L 251.720505 220.507565 \n",
              "L 251.720505 93.392692 \n",
              "L 251.720505 261.016417 \n",
              "L 251.720505 242.339338 \n",
              "L 252.152601 259.535642 \n",
              "L 252.152601 254.951678 \n",
              "L 252.152601 252.15752 \n",
              "L 252.315158 235.437639 \n",
              "L 252.315158 258.138563 \n",
              "L 252.315158 185.567714 \n",
              "L 252.315158 206.182677 \n",
              "L 252.561801 241.49594 \n",
              "L 252.884999 201.701723 \n",
              "L 252.884999 115.243779 \n",
              "L 252.884999 256.928191 \n",
              "L 252.884999 146.913049 \n",
              "L 253.447972 206.691291 \n",
              "L 253.447972 181.067446 \n",
              "L 253.447972 257.327356 \n",
              "L 253.447972 201.894867 \n",
              "L 253.685121 253.960203 \n",
              "L 253.685121 236.010635 \n",
              "L 253.685121 260.597937 \n",
              "L 253.685121 233.76372 \n",
              "L 253.685121 253.928012 \n",
              "L 253.889799 206.76211 \n",
              "L 254.079477 236.65445 \n",
              "L 254.079477 139.586432 \n",
              "L 254.079477 258.351022 \n",
              "L 254.079477 172.395255 \n",
              "L 254.403419 256.632036 \n",
              "L 254.403419 254.900173 \n",
              "L 254.616009 244.818027 \n",
              "L 254.616009 175.176537 \n",
              "L 254.616009 261.138742 \n",
              "L 254.616009 200.594361 \n",
              "L 254.945268 232.714301 \n",
              "L 254.945268 259.355374 \n",
              "L 254.945268 213.226015 \n",
              "L 254.945268 220.893854 \n",
              "L 255.254256 232.141306 \n",
              "L 255.254256 260.643004 \n",
              "L 255.254256 179.522289 \n",
              "L 255.254256 228.001574 \n",
              "L 255.413742 231.999666 \n",
              "L 255.413742 239.165329 \n",
              "L 255.413742 233.441812 \n",
              "L 255.718759 258.241574 \n",
              "L 255.718759 171.867326 \n",
              "L 255.718759 260.185896 \n",
              "L 255.718759 203.607416 \n",
              "L 256.054758 222.496954 \n",
              "L 256.054758 258.930456 \n",
              "L 256.054758 194.194838 \n",
              "L 256.054758 256.013973 \n",
              "L 256.37305 227.905001 \n",
              "L 256.37305 214.011469 \n",
              "L 256.37305 258.273765 \n",
              "L 256.487914 253.702677 \n",
              "L 256.487914 253.966641 \n",
              "L 256.757834 249.382677 \n",
              "L 256.757834 165.995732 \n",
              "L 256.757834 257.778027 \n",
              "L 256.757834 247.850396 \n",
              "L 257.231878 202.519368 \n",
              "L 257.231878 170.856537 \n",
              "L 257.231878 259.947684 \n",
              "L 257.231878 209.260113 \n",
              "L 257.567292 249.704584 \n",
              "L 257.567292 238.341246 \n",
              "L 257.567292 219.960322 \n",
              "L 257.567292 260.610814 \n",
              "L 257.567292 248.307505 \n",
              "L 257.895743 209.182855 \n",
              "L 257.895743 214.140232 \n",
              "L 257.895743 257.803779 \n",
              "L 257.895743 154.728966 \n",
              "L 257.895743 190.24825 \n",
              "L 258.420692 173.219338 \n",
              "L 258.420692 261.074361 \n",
              "L 258.420692 152.108638 \n",
              "L 258.420692 217.217669 \n",
              "L 258.645673 243.259994 \n",
              "L 258.645673 226.565866 \n",
              "L 258.645673 243.498206 \n",
              "L 258.645673 242.686999 \n",
              "L 259.049808 196.66065 \n",
              "L 259.049808 258.801693 \n",
              "L 259.049808 135.09904 \n",
              "L 259.049808 204.367118 \n",
              "L 259.64954 160.201395 \n",
              "L 259.64954 257.159964 \n",
              "L 259.64954 138.807416 \n",
              "L 259.957958 259.657967 \n",
              "L 259.957958 252.151082 \n",
              "L 260.28247 241.850039 \n",
              "L 260.28247 159.570456 \n",
              "L 260.28247 259.323183 \n",
              "L 260.28247 171.036805 \n",
              "L 260.841709 187.260948 \n",
              "L 260.841709 131.796268 \n",
              "L 260.841709 259.600024 \n",
              "L 260.841709 149.546253 \n",
              "L 261.236477 257.984048 \n",
              "L 261.236477 228.059517 \n",
              "L 261.236477 259.458385 \n",
              "L 261.236477 223.34679 \n",
              "L 261.32702 259.19442 \n",
              "L 261.552016 187.177252 \n",
              "L 261.552016 256.574092 \n",
              "L 261.552016 174.661484 \n",
              "L 261.552016 228.471559 \n",
              "L 262.013306 212.002766 \n",
              "L 262.013306 213.979279 \n",
              "L 262.013306 163.027744 \n",
              "L 262.013306 260.211648 \n",
              "L 262.013306 240.395016 \n",
              "L 262.424436 227.49296 \n",
              "L 262.424436 258.42828 \n",
              "L 262.424436 173.63138 \n",
              "L 262.424436 180.545955 \n",
              "L 262.792083 223.166522 \n",
              "L 262.792083 258.576358 \n",
              "L 262.792083 210.998414 \n",
              "L 262.792083 240.781306 \n",
              "L 263.16014 238.43138 \n",
              "L 263.16014 138.730158 \n",
              "L 263.16014 259.696596 \n",
              "L 263.16014 234.001931 \n",
              "L 263.708588 164.74673 \n",
              "L 263.708588 259.999189 \n",
              "L 263.708588 163.80676 \n",
              "L 263.708588 248.822557 \n",
              "L 264.025709 235.032036 \n",
              "L 264.025709 258.048429 \n",
              "L 264.025709 252.588876 \n",
              "L 264.296863 174.230128 \n",
              "L 264.296863 202.577311 \n",
              "L 264.296863 260.147267 \n",
              "L 264.296863 149.771589 \n",
              "L 264.296863 226.662438 \n",
              "L 264.830768 234.407535 \n",
              "L 264.830768 129.407714 \n",
              "L 264.830768 258.730873 \n",
              "L 264.830768 160.536179 \n",
              "L 265.070417 225.007833 \n",
              "L 265.252595 236.635136 \n",
              "L 265.252595 259.632215 \n",
              "L 265.252595 235.740232 \n",
              "L 265.491785 241.038832 \n",
              "L 265.491785 183.423809 \n",
              "L 265.491785 260.340411 \n",
              "L 265.491785 256.252185 \n",
              "L 266.067227 146.243481 \n",
              "L 266.067227 105.882706 \n",
              "L 266.067227 256.471082 \n",
              "L 266.067227 251.648906 \n",
              "L 266.639663 214.584465 \n",
              "L 266.639663 260.121514 \n",
              "L 266.639663 188.490635 \n",
              "L 266.639663 190.016477 \n",
              "L 266.875151 239.146015 \n",
              "L 267.18023 137.288012 \n",
              "L 267.18023 260.919845 \n",
              "L 267.18023 126.130694 \n",
              "L 267.18023 234.137133 \n",
              "L 267.615618 236.841156 \n",
              "L 267.615618 260.849025 \n",
              "L 267.615618 207.554003 \n",
              "L 267.615618 216.515911 \n",
              "L 267.802621 249.150903 \n",
              "L 267.915887 239.673943 \n",
              "L 267.915887 247.73451 \n",
              "L 267.915887 204.972304 \n",
              "L 267.915887 259.600024 \n",
              "L 267.915887 225.503571 \n",
              "L 268.399093 143.80986 \n",
              "L 268.399093 254.031022 \n",
              "L 268.399093 116.615106 \n",
              "L 268.399093 140.281753 \n",
              "L 268.887442 216.824942 \n",
              "L 268.887442 260.913407 \n",
              "L 268.887442 184.074063 \n",
              "L 268.887442 193.338563 \n",
              "L 269.173819 252.028757 \n",
              "L 269.173819 226.598057 \n",
              "L 269.173819 260.610814 \n",
              "L 269.200387 255.878772 \n",
              "L 269.576277 204.643958 \n",
              "L 269.576277 103.680858 \n",
              "L 269.576277 260.237401 \n",
              "L 269.576277 176.48992 \n",
              "L 270.184411 200.761753 \n",
              "L 270.184411 257.359547 \n",
              "L 270.184411 174.494092 \n",
              "L 270.184411 223.372542 \n",
              "L 270.393172 257.359547 \n",
              "L 270.393172 256.162051 \n",
              "L 270.804587 163.645806 \n",
              "L 270.804587 260.803958 \n",
              "L 270.804587 108.618921 \n",
              "L 270.804587 239.867088 \n",
              "L 271.313697 205.75132 \n",
              "L 271.313697 260.675195 \n",
              "L 271.313697 178.942855 \n",
              "L 271.313697 201.064346 \n",
              "L 271.619077 235.437639 \n",
              "L 271.619077 226.48217 \n",
              "L 271.619077 260.758891 \n",
              "L 271.679476 253.013794 \n",
              "L 271.974033 146.069651 \n",
              "L 271.974033 258.795255 \n",
              "L 271.974033 139.019875 \n",
              "L 271.974033 244.096954 \n",
              "L 272.45822 251.777669 \n",
              "L 272.45822 261.164495 \n",
              "L 272.45822 190.602349 \n",
              "L 272.561707 220.365925 \n",
              "L 272.821547 248.172304 \n",
              "L 272.821547 238.798355 \n",
              "L 272.821547 200.890516 \n",
              "L 272.920286 255.64056 \n",
              "L 273.176613 174.133556 \n",
              "L 273.176613 257.694331 \n",
              "L 273.176613 168.61606 \n",
              "L 273.176613 201.15448 \n",
              "L 273.544117 258.576358 \n",
              "L 273.544117 194.059636 \n",
              "L 273.544117 222.290933 \n",
              "L 273.902048 198.920441 \n",
              "L 273.902048 202.68676 \n",
              "L 273.902048 260.919845 \n",
              "L 273.902048 189.13445 \n",
              "L 273.902048 246.955493 \n",
              "L 274.138912 238.695344 \n",
              "L 274.333305 163.87758 \n",
              "L 274.333305 260.546432 \n",
              "L 274.333305 248.648727 \n",
              "L 274.733897 250.593049 \n",
              "L 274.733897 260.404793 \n",
              "L 274.733897 193.679785 \n",
              "L 274.733897 237.716745 \n",
              "L 274.939857 250.116626 \n",
              "L 274.939857 249.395553 \n",
              "L 275.133966 249.749651 \n",
              "L 275.133966 162.956924 \n",
              "L 275.133966 258.975523 \n",
              "L 275.133966 215.64676 \n",
              "L 275.566584 196.557639 \n",
              "L 275.566584 258.550605 \n",
              "L 275.566584 189.958534 \n",
              "L 275.566584 210.470486 \n",
              "L 275.894086 213.605866 \n",
              "L 275.894086 259.754539 \n",
              "L 275.894086 205.120382 \n",
              "L 275.894086 228.50375 \n",
              "L 276.017019 253.799249 \n",
              "L 276.357512 183.237103 \n",
              "L 276.357512 138.150724 \n",
              "L 276.357512 261.209562 \n",
              "L 276.357512 166.517222 \n",
              "L 276.807347 254.861544 \n",
              "L 276.807347 245.333079 \n",
              "L 276.807347 213.844077 \n",
              "L 276.807347 258.904703 \n",
              "L 276.807347 229.276328 \n",
              "L 277.088074 221.157818 \n",
              "L 277.088074 257.636387 \n",
              "L 277.088074 201.946373 \n",
              "L 277.088074 234.574927 \n",
              "L 277.566802 152.668757 \n",
              "L 277.566802 260.539994 \n",
              "L 277.566802 114.155732 \n",
              "L 277.566802 214.970754 \n",
              "L 277.99961 248.886939 \n",
              "L 277.99961 234.677937 \n",
              "L 277.99961 235.54065 \n",
              "L 278.134444 248.462021 \n",
              "L 278.134444 195.40521 \n",
              "L 278.134444 260.372602 \n",
              "L 278.134444 219.593347 \n",
              "L 278.306684 259.065657 \n",
              "L 278.306684 224.505657 \n",
              "L 278.306684 241.502379 \n",
              "L 278.843485 103.57141 \n",
              "L 278.843485 105.058623 \n",
              "L 278.843485 260.984227 \n",
              "L 278.843485 87.913824 \n",
              "L 278.843485 125.667148 \n",
              "L 279.34674 260.063571 \n",
              "L 279.34674 255.131946 \n",
              "L 279.470006 233.094152 \n",
              "L 279.470006 260.945598 \n",
              "L 279.470006 216.4129 \n",
              "L 279.470006 247.296715 \n",
              "L 279.982629 110.524614 \n",
              "L 279.982629 256.760799 \n",
              "L 279.982629 99.277162 \n",
              "L 279.982629 112.938921 \n",
              "L 280.426845 247.412602 \n",
              "L 280.505631 241.354301 \n",
              "L 280.505631 221.074122 \n",
              "L 280.505631 257.990486 \n",
              "L 280.580683 256.896 \n",
              "L 281.103496 206.491708 \n",
              "L 281.103496 53.463273 \n",
              "L 281.103496 260.243839 \n",
              "L 281.103496 61.601097 \n",
              "L 281.675267 261.048608 \n",
              "L 281.675267 232.450337 \n",
              "L 281.675267 225.175225 \n",
              "L 281.675267 261.106551 \n",
              "L 281.675267 243.86518 \n",
              "L 282.246152 231.439547 \n",
              "L 282.246152 54.332423 \n",
              "L 282.246152 260.713824 \n",
              "L 282.246152 195.688489 \n",
              "L 282.812053 255.376596 \n",
              "L 282.812053 253.290635 \n",
              "L 282.812053 260.810396 \n",
              "L 282.867452 231.39448 \n",
              "L 282.867452 242.92521 \n",
              "L 283.53282 38.70059 \n",
              "L 283.53282 260.887654 \n",
              "L 283.53282 23.133139 \n",
              "L 283.53282 125.409621 \n",
              "L 284.209771 260.758891 \n",
              "L 284.209771 257.681455 \n",
              "L 284.232747 240.968012 \n",
              "L 284.232747 260.829711 \n",
              "L 284.306865 253.464465 \n",
              "L 284.341803 248.983511 \n",
              "L 284.341803 252.782021 \n",
              "L 284.83463 89.413914 \n",
              "L 284.83463 257.411052 \n",
              "L 284.83463 55.793884 \n",
              "L 284.83463 201.791857 \n",
              "L 285.326049 243.76217 \n",
              "L 285.326049 259.580709 \n",
              "L 285.326049 224.119368 \n",
              "L 285.326049 257.185717 \n",
              "L 285.559274 241.573198 \n",
              "L 285.614024 258.576358 \n",
              "L 285.559274 209.421067 \n",
              "L 285.649374 253.805687 \n",
              "L 286.112943 124.031857 \n",
              "L 286.112943 260.237401 \n",
              "L 286.112943 69.886999 \n",
              "L 286.112943 258.486224 \n",
              "L 286.621182 257.855285 \n",
              "L 286.621182 245.629234 \n",
              "L 286.621182 259.394003 \n",
              "L 286.621182 249.891291 \n",
              "L 286.769671 219.09761 \n",
              "L 286.769671 261.074361 \n",
              "L 286.769671 202.905657 \n",
              "L 286.769671 248.893377 \n",
              "L 286.899583 249.421306 \n",
              "L 286.899583 256.657788 \n",
              "L 287.339449 226.804077 \n",
              "L 287.339449 230.467386 \n",
              "L 287.339449 99.727833 \n",
              "L 287.339449 260.668757 \n",
              "L 287.339449 177.629472 \n",
              "L 287.701114 243.285747 \n",
              "L 287.906568 234.491231 \n",
              "L 287.906568 198.740173 \n",
              "L 287.906568 256.432453 \n",
              "L 288.00901 232.315136 \n",
              "L 288.481425 237.858385 \n",
              "L 288.481425 110.151201 \n",
              "L 288.481425 258.614987 \n",
              "L 288.481425 209.826671 \n",
              "L 288.796378 229.656179 \n",
              "L 288.796378 235.984882 \n",
              "L 288.937003 247.476984 \n",
              "L 289.126459 161.68217 \n",
              "L 289.126459 175.562826 \n",
              "L 289.126459 257.642826 \n",
              "L 289.126459 162.139279 \n",
              "L 289.126459 198.823869 \n",
              "L 289.27286 218.576119 \n",
              "L 289.27286 215.447177 \n",
              "L 289.755085 235.057788 \n",
              "L 289.755085 121.160441 \n",
              "L 289.755085 259.728787 \n",
              "L 289.755085 133.734152 \n",
              "L 289.923116 191.696835 \n",
              "L 289.923116 189.55293 \n",
              "L 290.367112 152.643004 \n",
              "L 290.367112 260.758891 \n",
              "L 290.367112 152.385478 \n",
              "L 290.367112 177.88056 \n",
              "L 290.665498 253.335702 \n",
              "L 290.935244 168.847833 \n",
              "L 290.935244 191.683958 \n",
              "L 290.935244 261.11299 \n",
              "L 290.935244 148.091231 \n",
              "L 290.935244 241.46375 \n",
              "L 291.551258 204.322051 \n",
              "L 291.551258 255.511797 \n",
              "L 291.551258 135.729979 \n",
              "L 291.551258 244.77296 \n",
              "L 291.811889 243.028221 \n",
              "L 291.811889 244.399547 \n",
              "L 291.811889 236.113645 \n",
              "L 292.140609 229.154003 \n",
              "L 292.140609 155.926462 \n",
              "L 292.140609 255.106194 \n",
              "L 292.140609 228.581007 \n",
              "L 292.455294 241.437997 \n",
              "L 292.794853 241.766343 \n",
              "L 292.794853 115.443362 \n",
              "L 292.794853 259.780292 \n",
              "L 292.794853 202.184584 \n",
              "L 293.185839 245.867446 \n",
              "L 293.185839 243.459577 \n",
              "L 293.390281 189.353347 \n",
              "L 293.390281 260.900531 \n",
              "L 293.390281 176.451291 \n",
              "L 293.390281 177.159487 \n",
              "L 293.604422 260.861902 \n",
              "L 293.956546 228.877162 \n",
              "L 293.956546 260.430545 \n",
              "L 293.956546 114.760918 \n",
              "L 293.956546 214.616656 \n",
              "L 294.32147 257.893914 \n",
              "L 294.32147 257.102021 \n",
              "L 294.513284 220.687833 \n",
              "L 294.513284 230.164793 \n",
              "L 294.513284 259.348936 \n",
              "L 294.513284 181.859338 \n",
              "L 294.513284 238.0129 \n",
              "L 294.784248 240.240501 \n",
              "L 294.784248 239.944346 \n",
              "L 294.784248 237.755374 \n",
              "L 294.784248 238.141663 \n",
              "L 295.19033 179.251887 \n",
              "L 295.19033 88.557639 \n",
              "L 295.19033 259.896179 \n",
              "L 295.19033 185.200739 \n",
              "L 295.631493 259.419756 \n",
              "L 295.75006 257.610635 \n",
              "L 295.75006 208.815881 \n",
              "L 295.75006 260.527118 \n",
              "L 295.830523 228.70977 \n",
              "L 296.39511 65.534808 \n",
              "L 296.39511 259.400441 \n",
              "L 296.39511 221.91752 \n",
              "L 296.863949 256.084793 \n",
              "L 296.960014 255.074003 \n",
              "L 296.960014 204.193288 \n",
              "L 296.960014 259.754539 \n",
              "L 296.960014 224.692364 \n",
              "L 297.253717 199.6222 \n",
              "L 297.591963 119.66679 \n",
              "L 297.591963 259.690158 \n",
              "L 297.591963 58.884197 \n",
              "L 297.591963 200.793943 \n",
              "L 298.156613 235.939815 \n",
              "L 298.187739 259.226611 \n",
              "L 298.259277 249.54363 \n",
              "L 298.80611 161.115613 \n",
              "L 298.80611 259.690158 \n",
              "L 298.80611 44.913407 \n",
              "L 298.80611 142.541544 \n",
              "L 299.365429 233.351678 \n",
              "L 299.365429 260.791082 \n",
              "L 299.365429 230.840799 \n",
              "L 299.480593 260.675195 \n",
              "L 299.480593 252.50518 \n",
              "L 300.099978 117.207416 \n",
              "L 300.099978 31.367535 \n",
              "L 300.099978 260.585061 \n",
              "L 300.099978 45.975702 \n",
              "L 300.73949 246.994122 \n",
              "L 300.73949 239.480799 \n",
              "L 301.297352 142.432095 \n",
              "L 301.297352 19.296 \n",
              "L 301.297352 260.977788 \n",
              "L 301.586751 136.489681 \n",
              "L 301.980648 259.934808 \n",
              "L 301.980648 253.728429 \n",
              "L 302.484505 209.215046 \n",
              "L 302.484505 56.411946 \n",
              "L 302.484505 258.0098 \n",
              "L 302.484505 134.384405 \n",
              "L 303.151787 259.722349 \n",
              "L 303.151787 228.947982 \n",
              "L 303.192834 260.874778 \n",
              "L 303.671546 196.126283 \n",
              "L 303.671546 259.883303 \n",
              "L 303.671546 63.26214 \n",
              "L 303.671546 193.390069 \n",
              "L 304.254173 260.494927 \n",
              "L 304.254173 247.670128 \n",
              "L 304.254173 228.020888 \n",
              "L 304.315062 258.441156 \n",
              "L 304.315062 241.965925 \n",
              "L 304.896122 205.352155 \n",
              "L 304.896122 52.594122 \n",
              "L 304.896122 259.902617 \n",
              "L 304.896122 139.856835 \n",
              "L 305.43392 259.297431 \n",
              "L 305.53489 257.102021 \n",
              "L 305.53489 208.423154 \n",
              "L 305.53489 260.533556 \n",
              "L 305.53489 236.493496 \n",
              "L 306.109716 144.369979 \n",
              "L 306.109716 260.256715 \n",
              "L 306.109716 91.036328 \n",
              "L 306.109716 96.553824 \n",
              "L 306.510022 253.039547 \n",
              "L 306.776555 259.773854 \n",
              "L 306.776555 192.546671 \n",
              "L 306.776555 213.914897 \n",
              "L 306.896087 261.067923 \n",
              "L 306.918921 257.501186 \n",
              "L 307.350052 147.878772 \n",
              "L 307.350052 260.514241 \n",
              "L 307.350052 82.07442 \n",
              "L 307.350052 253.387207 \n",
              "L 307.780739 249.163779 \n",
              "L 307.814143 259.954122 \n",
              "L 307.814143 231.027505 \n",
              "L 307.814143 243.247118 \n",
              "L 308.109634 227.827744 \n",
              "L 308.109634 260.301782 \n",
              "L 308.109634 172.755791 \n",
              "L 308.109634 214.346253 \n",
              "L 308.629124 126.568489 \n",
              "L 308.629124 260.758891 \n",
              "L 308.629124 138.20223 \n",
              "L 308.935089 256.323004 \n",
              "L 308.935089 251.21755 \n",
              "L 309.21031 222.001216 \n",
              "L 309.21031 170.232036 \n",
              "L 309.21031 260.050694 \n",
              "L 309.21031 196.982557 \n",
              "L 309.331155 218.659815 \n",
              "L 309.331155 217.970933 \n",
              "L 309.762903 246.298802 \n",
              "L 309.762903 138.839607 \n",
              "L 309.762903 259.213735 \n",
              "L 309.762903 183.546134 \n",
              "L 310.182878 223.501306 \n",
              "L 310.182878 212.517818 \n",
              "L 310.28934 259.638653 \n",
              "L 310.536742 238.785478 \n",
              "L 310.536742 155.089502 \n",
              "L 310.536742 260.829711 \n",
              "L 310.536742 224.698802 \n",
              "L 310.760314 239.506551 \n",
              "L 310.760314 235.154361 \n",
              "L 311.108719 245.854569 \n",
              "L 311.108719 148.007535 \n",
              "L 311.108719 260.810396 \n",
              "L 311.108719 169.388638 \n",
              "L 311.390586 258.402528 \n",
              "L 311.390586 239.44217 \n",
              "L 311.390586 222.612841 \n",
              "L 311.390586 261.093675 \n",
              "L 311.390586 225.664525 \n",
              "L 311.78923 161.469711 \n",
              "L 311.78923 260.69451 \n",
              "L 311.78923 145.116805 \n",
              "L 311.78923 171.184882 \n",
              "L 312.289669 243.562587 \n",
              "L 312.289669 238.862736 \n",
              "L 312.289669 175.698027 \n",
              "L 312.289669 260.37904 \n",
              "L 312.289669 258.466909 \n",
              "L 312.566218 252.891469 \n",
              "L 312.566218 258.241574 \n",
              "L 312.566218 234.961216 \n",
              "L 312.566218 252.131768 \n",
              "L 312.686494 250.226075 \n",
              "L 312.686494 256.033288 \n",
              "L 313.00485 139.940531 \n",
              "L 313.00485 189.301842 \n",
              "L 313.00485 260.7267 \n",
              "L 313.00485 123.156268 \n",
              "L 313.00485 236.641574 \n",
              "L 313.24904 220.642766 \n",
              "L 313.534641 224.87907 \n",
              "L 313.534641 186.089204 \n",
              "L 313.534641 260.173019 \n",
              "L 313.534641 256.47752 \n",
              "L 313.684443 260.739577 \n",
              "L 313.684443 254.507446 \n",
              "L 313.810415 225.619458 \n",
              "L 313.810415 232.270069 \n",
              "L 313.810415 219.915255 \n",
              "L 313.810415 255.086879 \n",
              "L 313.810415 235.617908 \n",
              "L 314.219029 188.323243 \n",
              "L 314.219029 260.803958 \n",
              "L 314.219029 106.964316 \n",
              "L 314.219029 146.732781 \n",
              "L 314.451858 199.042766 \n",
              "L 314.451858 193.003779 \n",
              "L 314.85707 253.734867 \n",
              "L 314.85707 166.852006 \n",
              "L 314.85707 259.889741 \n",
              "L 314.85707 250.670307 \n",
              "L 315.052903 259.934808 \n",
              "L 315.07216 229.160441 \n",
              "L 315.052903 260.160143 \n",
              "L 315.07216 245.416775 \n",
              "L 315.545081 240.543094 \n",
              "L 315.545081 104.324674 \n",
              "L 315.545081 260.005627 \n",
              "L 315.545081 255.807952 \n",
              "L 316.100839 256.754361 \n",
              "L 316.100839 199.615762 \n",
              "L 316.100839 256.902438 \n",
              "L 316.202712 253.812125 \n",
              "L 316.298381 244.071201 \n",
              "L 316.298381 244.238593 \n",
              "L 316.298381 260.887654 \n",
              "L 316.298381 224.557162 \n",
              "L 316.298381 227.119547 \n",
              "L 316.850168 132.942259 \n",
              "L 316.850168 260.224525 \n",
              "L 316.850168 76.730754 \n",
              "L 316.850168 234.684376 \n",
              "L 317.271187 244.405985 \n",
              "L 317.271187 243.556149 \n",
              "L 317.480391 225.027148 \n",
              "L 317.480391 195.12837 \n",
              "L 317.582849 260.900531 \n",
              "L 317.700229 248.043541 \n",
              "L 317.700229 255.576179 \n",
              "L 317.700229 221.67287 \n",
              "L 317.700229 260.263154 \n",
              "L 317.700229 243.253556 \n",
              "L 318.151013 249.653079 \n",
              "L 318.151013 90.592095 \n",
              "L 318.151013 259.760978 \n",
              "L 318.151013 246.459756 \n",
              "L 318.688035 214.925687 \n",
              "L 318.688035 215.080203 \n",
              "L 318.719145 209.762289 \n",
              "L 318.719145 258.318832 \n",
              "L 318.719145 216.290575 \n",
              "L 318.830986 259.516328 \n",
              "L 318.830986 258.859636 \n",
              "L 318.943239 257.546253 \n",
              "L 318.943239 228.883601 \n",
              "L 318.943239 258.821007 \n",
              "L 318.943239 235.296 \n",
              "L 319.428645 132.388578 \n",
              "L 319.428645 81.256775 \n",
              "L 319.428645 260.469174 \n",
              "L 319.428645 126.079189 \n",
              "L 319.731256 202.564435 \n",
              "L 320.042001 220.372364 \n",
              "L 320.042001 219.265001 \n",
              "L 320.142797 261.151618 \n",
              "L 320.142797 261.138742 \n",
              "L 320.638077 216.960143 \n",
              "L 320.638077 260.572185 \n",
              "L 320.638077 73.981663 \n",
              "L 320.638077 93.527893 \n",
              "L 321.111204 260.185896 \n",
              "L 321.163738 255.7822 \n",
              "L 321.232603 209.195732 \n",
              "L 321.163738 260.700948 \n",
              "L 321.232603 227.364197 \n",
              "L 321.832572 101.498325 \n",
              "L 321.832572 261.009979 \n",
              "L 321.832572 73.183332 \n",
              "L 321.832572 255.936715 \n",
              "L 322.241391 248.107923 \n",
              "L 322.241391 257.391738 \n",
              "L 322.241391 237.278951 \n",
              "L 322.457922 209.427505 \n",
              "L 322.457922 214.912811 \n",
              "L 322.457922 261.022855 \n",
              "L 322.457922 209.891052 \n",
              "L 322.457922 219.226373 \n",
              "L 322.99466 114.258742 \n",
              "L 322.99466 131.809145 \n",
              "L 322.99466 259.014152 \n",
              "L 322.99466 62.457371 \n",
              "L 322.99466 147.71138 \n",
              "L 323.231255 156.802051 \n",
              "L 323.594787 240.111738 \n",
              "L 323.683336 257.803779 \n",
              "L 323.683336 225.188101 \n",
              "L 323.683336 243.897371 \n",
              "L 324.200706 215.621007 \n",
              "L 324.200706 70.002885 \n",
              "L 324.200706 260.932721 \n",
              "L 324.200706 256.12986 \n",
              "L 324.750071 246.446879 \n",
              "L 324.750071 260.533556 \n",
              "L 324.750071 228.819219 \n",
              "L 324.856627 249.627326 \n",
              "L 324.870552 252.640382 \n",
              "L 324.870552 236.82828 \n",
              "L 324.870552 221.286581 \n",
              "L 325.444475 116.306075 \n",
              "L 325.444475 260.559308 \n",
              "L 325.444475 70.041514 \n",
              "L 325.444475 228.304167 \n",
              "L 325.888107 257.430367 \n",
              "L 325.957604 201.843362 \n",
              "L 325.957604 258.872513 \n",
              "L 325.957604 240.292006 \n",
              "L 326.219992 230.274241 \n",
              "L 326.219992 238.399189 \n",
              "L 326.219992 218.646939 \n",
              "L 326.638305 177.545776 \n",
              "L 326.638305 260.816835 \n",
              "L 326.638305 69.590844 \n",
              "L 326.638305 107.71758 \n",
              "L 327.118774 228.226909 \n",
              "L 327.118774 216.406462 \n",
              "L 327.118774 257.456119 \n",
              "L 327.118774 253.387207 \n",
              "L 327.300335 249.8591 \n",
              "L 327.300335 250.876328 \n",
              "L 327.300335 260.958474 \n",
              "L 327.300335 239.223273 \n",
              "L 327.300335 246.588519 \n",
              "L 327.450248 218.273526 \n",
              "L 327.791247 101.781604 \n",
              "L 327.791247 257.134212 \n",
              "L 327.791247 85.621842 \n",
              "L 327.791247 141.492125 \n",
              "L 328.223992 258.460471 \n",
              "L 328.369854 211.371827 \n",
              "L 328.369854 216.83138 \n",
              "L 328.369854 259.966999 \n",
              "L 328.369854 205.397222 \n",
              "L 328.455713 228.149651 \n",
              "L 328.998306 170.000262 \n",
              "L 328.998306 258.756626 \n",
              "L 328.998306 74.599726 \n",
              "L 328.998306 203.234003 \n",
              "L 329.473205 260.108638 \n",
              "L 329.473205 259.632215 \n",
              "L 329.60663 246.575642 \n",
              "L 329.60663 260.005627 \n",
              "L 329.60663 218.943094 \n",
              "L 329.719737 227.686104 \n",
              "L 329.719737 224.518534 \n",
              "L 329.719737 260.404793 \n",
              "L 329.719737 221.67287 \n",
              "L 329.719737 259.220173 \n",
              "L 330.268184 130.212483 \n",
              "L 330.268184 260.03138 \n",
              "L 330.268184 97.558176 \n",
              "L 330.268184 183.971052 \n",
              "L 330.65273 260.906969 \n",
              "L 330.771566 254.880858 \n",
              "L 330.771566 211.526343 \n",
              "L 330.771566 258.692244 \n",
              "L 330.771566 238.270426 \n",
              "L 330.963791 260.62369 \n",
              "L 330.963791 253.026671 \n",
              "L 330.963791 221.943273 \n",
              "L 330.963791 259.451946 \n",
              "L 331.007117 235.695165 \n",
              "L 331.505245 235.617908 \n",
              "L 331.505245 89.626373 \n",
              "L 331.505245 259.426194 \n",
              "L 331.505245 206.118295 \n",
              "L 332.006379 226.037937 \n",
              "L 332.006379 222.728727 \n",
              "L 332.006379 260.62369 \n",
              "L 332.006379 204.663273 \n",
              "L 332.006379 258.627863 \n",
              "L 332.268387 257.932542 \n",
              "L 332.268387 215.2991 \n",
              "L 332.268387 260.958474 \n",
              "L 332.268387 227.827744 \n",
              "L 332.76104 251.468638 \n",
              "L 332.76104 108.129621 \n",
              "L 332.76104 260.700948 \n",
              "L 332.76104 148.329443 \n",
              "L 333.127009 256.580531 \n",
              "L 333.127009 256.387386 \n",
              "L 333.259738 201.096537 \n",
              "L 333.259738 224.98208 \n",
              "L 333.259738 260.559308 \n",
              "L 333.259738 202.397043 \n",
              "L 333.259738 221.041931 \n",
              "L 333.509087 234.368906 \n",
              "L 333.509087 213.76682 \n",
              "L 333.509087 260.713824 \n",
              "L 333.509087 235.914063 \n",
              "L 333.75622 221.183571 \n",
              "L 333.75622 227.570218 \n",
              "L 334.011962 248.043541 \n",
              "L 334.011962 247.618623 \n",
              "L 334.011962 124.85594 \n",
              "L 334.011962 260.507803 \n",
              "L 334.011962 140.15299 \n",
              "L 334.303243 253.194063 \n",
              "L 334.303243 249.730337 \n",
              "L 334.474012 231.948161 \n",
              "L 334.474012 260.700948 \n",
              "L 334.474012 203.234003 \n",
              "L 334.474012 226.836268 \n",
              "L 334.724564 217.809979 \n",
              "L 334.724564 260.649443 \n",
              "L 334.724564 187.318891 \n",
              "L 334.822733 213.470665 \n",
              "L 335.240288 256.258623 \n",
              "L 335.240288 143.706849 \n",
              "L 335.240288 261.11299 \n",
              "L 335.240288 247.399726 \n",
              "L 335.654867 235.99132 \n",
              "L 335.654867 260.894092 \n",
              "L 335.654867 207.676328 \n",
              "L 335.654867 241.631142 \n",
              "L 335.908836 257.159964 \n",
              "L 335.908836 191.774092 \n",
              "L 335.908836 259.567833 \n",
              "L 335.908836 198.392513 \n",
              "L 336.078244 258.975523 \n",
              "L 336.361899 213.135881 \n",
              "L 336.361899 146.076089 \n",
              "L 336.361899 260.340411 \n",
              "L 336.361899 169.897252 \n",
              "L 336.725589 236.802528 \n",
              "L 336.844235 194.864405 \n",
              "L 336.844235 206.092542 \n",
              "L 336.844235 191.246164 \n",
              "L 336.844235 258.666492 \n",
              "L 337.196184 227.325568 \n",
              "L 337.196184 174.867505 \n",
              "L 337.196184 259.014152 \n",
              "L 337.196184 200.787505 \n",
              "L 337.687888 200.967773 \n",
              "L 337.687888 150.351022 \n",
              "L 337.687888 260.945598 \n",
              "L 337.687888 197.18214 \n",
              "L 337.961921 258.305955 \n",
              "L 338.119905 199.963422 \n",
              "L 338.119905 257.855285 \n",
              "L 338.119905 243.974629 \n",
              "L 338.4422 234.072751 \n",
              "L 338.4422 259.310307 \n",
              "L 338.4422 171.712811 \n",
              "L 338.72 240.163243 \n",
              "L 338.907574 202.435672 \n",
              "L 338.907574 260.675195 \n",
              "L 338.907574 169.395076 \n",
              "L 338.907574 195.727118 \n",
              "L 339.265536 214.552274 \n",
              "L 339.265536 259.33606 \n",
              "L 339.265536 251.417133 \n",
              "L 339.627328 203.221127 \n",
              "L 339.627328 164.811112 \n",
              "L 339.627328 260.597937 \n",
              "L 339.627328 182.928072 \n",
              "L 339.873417 247.927654 \n",
              "L 340.054281 184.331589 \n",
              "L 340.054281 260.97135 \n",
              "L 340.054281 168.081693 \n",
              "L 340.054281 183.146969 \n",
              "L 340.223847 235.914063 \n",
              "L 340.450568 235.759547 \n",
              "L 340.450568 202.229651 \n",
              "L 340.450568 258.94977 \n",
              "L 340.450568 211.320322 \n",
              "L 340.788941 172.562647 \n",
              "L 340.788941 258.821007 \n",
              "L 340.788941 144.492304 \n",
              "L 340.788941 239.596686 \n",
              "L 341.292259 257.597759 \n",
              "L 341.292259 177.468519 \n",
              "L 341.292259 260.906969 \n",
              "L 341.371029 226.720382 \n",
              "L 341.61389 232.733615 \n",
              "L 341.61389 261.009979 \n",
              "L 341.61389 200.105061 \n",
              "L 341.61389 236.454867 \n",
              "L 341.968308 158.282826 \n",
              "L 341.968308 260.675195 \n",
              "L 341.968308 142.238951 \n",
              "L 341.968308 159.441693 \n",
              "L 342.507166 258.074182 \n",
              "L 342.507166 235.463392 \n",
              "L 342.507166 181.601812 \n",
              "L 342.507166 260.675195 \n",
              "L 342.507166 217.294927 \n",
              "L 342.662997 251.120978 \n",
              "L 342.786517 222.213675 \n",
              "L 342.786517 260.288906 \n",
              "L 342.786517 203.633168 \n",
              "L 343.163499 165.635195 \n",
              "L 343.163499 258.911142 \n",
              "L 343.163499 144.621067 \n",
              "L 343.163499 153.840501 \n",
              "L 343.589724 179.129562 \n",
              "L 343.589724 258.550605 \n",
              "L 343.589724 178.36986 \n",
              "L 343.589724 227.686104 \n",
              "L 343.871037 230.518891 \n",
              "L 343.949427 255.756447 \n",
              "L 343.949427 225.973556 \n",
              "L 343.949427 250.824823 \n",
              "L 343.949427 203.581663 \n",
              "L 343.949427 259.760978 \n",
              "L 343.949427 245.571291 \n",
              "L 344.400749 247.425478 \n",
              "L 344.400749 125.705776 \n",
              "L 344.400749 260.7267 \n",
              "L 344.400749 156.261246 \n",
              "L 344.862926 260.224525 \n",
              "L 344.862926 203.736179 \n",
              "L 344.953247 230.634778 \n",
              "L 345.158416 210.393228 \n",
              "L 345.158416 215.852781 \n",
              "L 345.158416 260.340411 \n",
              "L 345.158416 210.238712 \n",
              "L 345.158416 240.111738 \n",
              "L 345.665881 239.712572 \n",
              "L 345.665881 111.86375 \n",
              "L 345.665881 259.863988 \n",
              "L 345.665881 185.82524 \n",
              "L 346.0065 261.035732 \n",
              "L 346.0065 259.966999 \n",
              "L 346.08296 232.488966 \n",
              "L 346.08296 260.263154 \n",
              "L 346.08296 245.159249 \n",
              "L 346.08296 218.801455 \n",
              "L 346.08296 260.250277 \n",
              "L 346.08296 235.939815 \n",
              "L 346.203979 260.070009 \n",
              "L 346.203979 258.94977 \n",
              "L 346.330853 211.861127 \n",
              "L 346.330853 222.844614 \n",
              "L 346.330853 260.095762 \n",
              "L 346.330853 208.333019 \n",
              "L 346.330853 241.141842 \n",
              "L 346.85778 172.884554 \n",
              "L 346.85778 258.962647 \n",
              "L 346.85778 101.292304 \n",
              "L 346.85778 201.405568 \n",
              "L 347.314482 261.203124 \n",
              "L 347.314482 261.035732 \n",
              "L 347.314482 234.420411 \n",
              "L 347.334863 242.468101 \n",
              "L 347.564717 247.1422 \n",
              "L 347.564717 192.817073 \n",
              "L 347.564717 259.670844 \n",
              "L 347.564717 205.255583 \n",
              "L 348.122025 126.04056 \n",
              "L 348.122025 257.726522 \n",
              "L 348.122025 97.854331 \n",
              "L 348.122025 151.471261 \n",
              "L 348.568284 258.885389 \n",
              "L 348.610723 231.239964 \n",
              "L 348.610723 260.237401 \n",
              "L 348.610723 242.699875 \n",
              "L 348.862255 198.096358 \n",
              "L 348.862255 260.597937 \n",
              "L 348.862255 195.482468 \n",
              "L 348.862255 255.318653 \n",
              "L 349.42871 257.237222 \n",
              "L 349.42871 93.901306 \n",
              "L 349.42871 260.005627 \n",
              "L 349.42871 171.056119 \n",
              "L 349.819933 259.554957 \n",
              "L 349.890981 235.630784 \n",
              "L 349.890981 259.902617 \n",
              "L 349.890981 258.0098 \n",
              "L 350.068681 229.102498 \n",
              "L 350.068681 260.237401 \n",
              "L 350.068681 201.006402 \n",
              "L 350.068681 231.703511 \n",
              "L 350.642161 244.940352 \n",
              "L 350.642161 90.489085 \n",
              "L 350.642161 259.645091 \n",
              "L 350.642161 157.111082 \n",
              "L 351.113104 257.494748 \n",
              "L 351.113104 253.889383 \n",
              "L 351.262416 200.285329 \n",
              "L 351.262416 223.784584 \n",
              "L 351.262416 261.125866 \n",
              "L 351.262416 193.357878 \n",
              "L 351.262416 257.391738 \n",
              "L 351.912926 148.329443 \n",
              "L 351.912926 258.318832 \n",
              "L 351.912926 77.046224 \n",
              "L 351.912926 239.249025 \n",
              "L 352.321777 236.957043 \n",
              "L 352.405611 261.151618 \n",
              "L 352.545302 203.337013 \n",
              "L 352.545302 231.858027 \n",
              "L 352.545302 252.910784 \n",
              "L 352.545302 197.465419 \n",
              "L 352.545302 229.3729 \n",
              "L 353.12489 239.931469 \n",
              "L 353.12489 84.61749 \n",
              "L 353.12489 260.494927 \n",
              "L 353.12489 149.449681 \n",
              "L 353.591054 259.696596 \n",
              "L 353.688211 259.516328 \n",
              "L 353.688211 206.929502 \n",
              "L 353.688211 260.791082 \n",
              "L 353.688211 240.060232 \n",
              "L 354.322739 210.393228 \n",
              "L 354.322739 60.937967 \n",
              "L 354.322739 256.297252 \n",
              "L 354.322739 155.630307 \n",
              "L 354.804031 260.185896 \n",
              "L 354.830646 256.644912 \n",
              "L 354.923974 217.925866 \n",
              "L 354.923974 259.696596 \n",
              "L 354.923974 242.699875 \n",
              "L 355.475143 223.256656 \n",
              "L 355.475143 86.381544 \n",
              "L 355.475143 261.125866 \n",
              "L 355.475143 221.003303 \n",
              "L 355.927414 258.048429 \n",
              "L 355.927414 255.795076 \n",
              "L 355.927414 241.437997 \n",
              "L 355.927414 259.941246 \n",
              "L 355.981594 256.619159 \n",
              "L 356.045395 246.640024 \n",
              "L 356.045395 255.125508 \n",
              "L 356.045395 219.741425 \n",
              "L 356.045395 258.074182 \n",
              "L 356.608685 256.001097 \n",
              "L 356.608685 76.106253 \n",
              "L 356.608685 258.94977 \n",
              "L 356.608685 232.952513 \n",
              "L 357.103522 251.790545 \n",
              "L 357.103522 260.803958 \n",
              "L 357.103522 245.983332 \n",
              "L 357.177228 257.584882 \n",
              "L 357.235269 259.735225 \n",
              "L 357.235269 233.905359 \n",
              "L 357.814193 72.230486 \n",
              "L 357.814193 260.791082 \n",
              "L 357.814193 50.958832 \n",
              "L 357.814193 198.237997 \n",
              "L 358.319663 254.610456 \n",
              "L 358.396819 246.588519 \n",
              "L 358.396819 260.288906 \n",
              "L 358.396819 226.269711 \n",
              "L 358.49385 254.378683 \n",
              "L 358.511667 229.61755 \n",
              "L 358.511667 241.579636 \n",
              "L 359.091382 193.241991 \n",
              "L 359.091382 59.53445 \n",
              "L 359.091382 260.842587 \n",
              "L 359.091382 59.573079 \n",
              "L 359.801958 261.100113 \n",
              "L 359.801958 260.456298 \n",
              "L 359.801958 254.043899 \n",
              "L 360.246191 251.069472 \n",
              "L 360.246191 62.302855 \n",
              "L 360.246191 261.100113 \n",
              "L 360.246191 89.974033 \n",
              "L 360.889833 249.382677 \n",
              "L 360.889833 247.901902 \n",
              "L 360.889833 261.048608 \n",
              "L 360.889833 205.500232 \n",
              "L 360.970977 257.893914 \n",
              "L 361.457838 173.425359 \n",
              "L 361.457838 73.234838 \n",
              "L 361.457838 259.490575 \n",
              "L 361.457838 98.820054 \n",
              "L 361.878684 253.219815 \n",
              "L 361.95853 251.249741 \n",
              "L 361.95853 203.285508 \n",
              "L 361.95853 260.739577 \n",
              "L 362.055909 230.660531 \n",
              "L 362.605748 110.421604 \n",
              "L 362.605748 256.773675 \n",
              "L 362.605748 74.213437 \n",
              "L 362.605748 122.885866 \n",
              "L 363.061944 260.52068 \n",
              "L 363.17407 245.996209 \n",
              "L 363.17407 214.384882 \n",
              "L 363.17407 252.537371 \n",
              "L 363.281734 234.175762 \n",
              "L 363.469846 231.291469 \n",
              "L 363.469846 232.488966 \n",
              "L 363.469846 233.042647 \n",
              "L 363.469846 225.47138 \n",
              "L 363.904742 229.604674 \n",
              "L 363.904742 260.327535 \n",
              "L 363.904742 66.513407 \n",
              "L 363.904742 257.430367 \n",
              "L 364.52332 213.419159 \n",
              "L 364.52332 261.100113 \n",
              "L 364.52332 208.307267 \n",
              "L 364.52332 234.124256 \n",
              "L 364.663676 245.01761 \n",
              "L 364.663676 261.035732 \n",
              "L 364.663676 234.420411 \n",
              "L 364.751813 256.838057 \n",
              "L 365.174146 139.457669 \n",
              "L 365.174146 154.136656 \n",
              "L 365.174146 257.790903 \n",
              "L 365.174146 78.810277 \n",
              "L 365.174146 189.533615 \n",
              "L 365.763798 215.64676 \n",
              "L 365.763798 260.894092 \n",
              "L 365.763798 211.474838 \n",
              "L 365.763798 223.797461 \n",
              "L 365.907952 237.935642 \n",
              "L 365.907952 261.074361 \n",
              "L 365.907952 234.034122 \n",
              "L 366.006659 254.327177 \n",
              "L 366.428771 171.068996 \n",
              "L 366.428771 175.06065 \n",
              "L 366.428771 259.33606 \n",
              "L 366.428771 97.313526 \n",
              "L 366.428771 196.190665 \n",
              "L 366.830026 260.353288 \n",
              "L 366.830026 252.369979 \n",
              "L 366.936868 237.433466 \n",
              "L 366.936868 260.752453 \n",
              "L 366.936868 217.964495 \n",
              "L 366.936868 226.076566 \n",
              "L 367.233149 245.71293 \n",
              "L 367.233149 258.975523 \n",
              "L 367.233149 203.710426 \n",
              "L 367.233149 231.626253 \n",
              "L 367.724916 161.231499 \n",
              "L 367.724916 259.310307 \n",
              "L 367.724916 93.450635 \n",
              "L 367.724916 113.421782 \n",
              "L 368.261274 221.81451 \n",
              "L 368.261274 260.688072 \n",
              "L 368.261274 212.195911 \n",
              "L 368.261274 235.811052 \n",
              "L 368.509516 238.540829 \n",
              "L 368.509516 206.877997 \n",
              "L 368.509516 254.893735 \n",
              "L 368.509516 211.513466 \n",
              "L 368.680284 257.945419 \n",
              "L 369.027423 124.611291 \n",
              "L 369.027423 261.087237 \n",
              "L 369.027423 115.250218 \n",
              "L 369.027423 171.764316 \n",
              "L 369.479568 224.840441 \n",
              "L 369.515962 259.33606 \n",
              "L 369.515962 211.951261 \n",
              "L 369.515962 251.996566 \n",
              "L 369.731607 205.319964 \n",
              "L 369.731607 257.829532 \n",
              "L 369.731607 216.818504 \n",
              "L 369.865095 256.735046 \n",
              "L 369.865095 256.490396 \n",
              "L 370.231096 159.403064 \n",
              "L 370.231096 256.979696 \n",
              "L 370.231096 110.923779 \n",
              "L 370.231096 229.488787 \n",
              "L 370.591685 256.361633 \n",
              "L 370.707925 240.807058 \n",
              "L 370.707925 206.156924 \n",
              "L 370.707925 257.623511 \n",
              "L 370.707925 207.187028 \n",
              "L 371.025474 228.368548 \n",
              "L 371.025474 259.812483 \n",
              "L 371.025474 195.443839 \n",
              "L 371.025474 197.877461 \n",
              "L 371.17276 256.207118 \n",
              "L 371.177412 255.524674 \n",
              "L 371.532495 235.463392 \n",
              "L 371.532495 114.876805 \n",
              "L 371.532495 259.413317 \n",
              "L 371.532495 116.344703 \n",
              "L 371.89916 261.009979 \n",
              "L 372.027648 225.342617 \n",
              "L 372.027648 227.84062 \n",
              "L 372.027648 260.057133 \n",
              "L 372.027648 212.994241 \n",
              "L 372.027648 254.558951 \n",
              "L 372.293675 214.616656 \n",
              "L 372.293675 260.662319 \n",
              "L 372.293675 194.490993 \n",
              "L 372.293675 248.004912 \n",
              "L 372.445962 241.901544 \n",
              "L 372.445962 245.416775 \n",
              "L 372.844084 133.264167 \n",
              "L 372.844084 260.314659 \n",
              "L 372.844084 119.962945 \n",
              "L 372.844084 173.451112 \n",
              "L 373.190844 259.748101 \n",
              "L 373.300755 250.953586 \n",
              "L 373.300755 261.035732 \n",
              "L 373.300755 200.49135 \n",
              "L 373.673116 251.88068 \n",
              "L 373.673116 171.15913 \n",
              "L 373.673116 260.160143 \n",
              "L 373.673116 217.32068 \n",
              "L 374.153712 207.457431 \n",
              "L 374.153712 138.788101 \n",
              "L 374.153712 261.164495 \n",
              "L 374.153712 165.081514 \n",
              "L 374.613895 227.879249 \n",
              "L 374.613895 216.535225 \n",
              "L 374.613895 259.43907 \n",
              "L 374.613895 202.319785 \n",
              "L 374.613895 236.055702 \n",
              "L 374.92888 174.146432 \n",
              "L 374.92888 186.996984 \n",
              "L 374.92888 258.280203 \n",
              "L 374.92888 178.511499 \n",
              "L 374.92888 190.022915 \n",
              "L 375.174305 258.357461 \n",
              "L 375.174305 252.511618 \n",
              "L 375.495556 124.40527 \n",
              "L 375.495556 147.131946 \n",
              "L 375.495556 256.915314 \n",
              "L 375.495556 128.499934 \n",
              "L 375.495556 164.901246 \n",
              "L 375.981912 255.395911 \n",
              "L 375.981912 215.080203 \n",
              "L 375.981912 257.185717 \n",
              "L 375.981912 222.805985 \n",
              "L 376.273636 210.56062 \n",
              "L 376.273636 261.125866 \n",
              "L 376.273636 169.948757 \n",
              "L 376.273636 256.116984 \n",
              "L 376.494598 250.361276 \n",
              "L 376.494598 252.22834 \n",
              "L 376.803443 218.95597 \n",
              "L 376.803443 257.533377 \n",
              "L 376.803443 140.204495 \n",
              "L 376.803443 244.463928 \n",
              "L 377.204351 260.713824 \n",
              "L 377.204351 224.376894 \n",
              "L 377.204351 228.342796 \n",
              "L 377.515317 171.584048 \n",
              "L 377.515317 259.78673 \n",
              "L 377.515317 158.12831 \n",
              "L 377.515317 224.505657 \n",
              "L 377.805934 255.138385 \n",
              "L 377.805934 255.009621 \n",
              "L 378.050947 243.562587 \n",
              "L 378.050947 143.101663 \n",
              "L 378.050947 261.11299 \n",
              "L 378.050947 226.656 \n",
              "L 378.353558 260.816835 \n",
              "L 378.467742 259.735225 \n",
              "L 378.467742 217.719845 \n",
              "L 378.467742 261.009979 \n",
              "L 378.467742 231.961037 \n",
              "L 378.795386 178.138086 \n",
              "L 378.795386 259.645091 \n",
              "L 378.795386 152.771768 \n",
              "L 378.795386 187.087118 \n",
              "L 379.329244 260.456298 \n",
              "L 379.329244 159.003899 \n",
              "L 379.329244 184.344465 \n",
              "L 379.586156 257.996924 \n",
              "L 379.586156 256.001097 \n",
              "L 379.698346 241.064584 \n",
              "L 379.698346 249.550069 \n",
              "L 379.698346 260.636566 \n",
              "L 379.698346 225.020709 \n",
              "L 379.698346 255.254271 \n",
              "L 380.045928 181.872215 \n",
              "L 380.045928 260.37904 \n",
              "L 380.045928 150.685806 \n",
              "L 380.045928 258.112811 \n",
              "L 380.546904 186.211529 \n",
              "L 380.546904 257.919666 \n",
              "L 380.546904 178.833407 \n",
              "L 380.546904 207.727833 \n",
              "L 380.740016 259.413317 \n",
              "L 380.837141 217.346432 \n",
              "L 380.837141 261.074361 \n",
              "L 380.837141 255.112632 \n",
              "L 381.24767 184.305836 \n",
              "L 381.24767 142.663869 \n",
              "L 381.24767 260.340411 \n",
              "L 381.24767 215.492244 \n",
              "L 381.555123 253.528846 \n",
              "L 381.555123 252.022319 \n",
              "L 381.767318 175.627207 \n",
              "L 381.767318 259.477699 \n",
              "L 381.767318 168.004435 \n",
              "L 381.767318 177.77755 \n",
              "L 382.111672 260.456298 \n",
              "L 382.111672 250.013615 \n",
              "L 382.111672 215.311976 \n",
              "L 382.111672 260.340411 \n",
              "L 382.111672 233.943988 \n",
              "L 382.514574 149.604197 \n",
              "L 382.514574 261.151618 \n",
              "L 382.514574 135.388757 \n",
              "L 382.514574 152.527118 \n",
              "L 382.883771 232.707863 \n",
              "L 382.997321 210.625001 \n",
              "L 382.997321 260.919845 \n",
              "L 382.997321 189.469234 \n",
              "L 382.997321 248.442706 \n",
              "L 383.12967 240.742677 \n",
              "L 383.12967 242.686999 \n",
              "L 383.31256 243.974629 \n",
              "L 383.31256 214.050098 \n",
              "L 383.31256 261.177371 \n",
              "L 383.31256 221.763004 \n",
              "L 383.715398 259.722349 \n",
              "L 383.715398 123.993228 \n",
              "L 383.715398 259.451946 \n",
              "\" clip-path=\"url(#p315ed9965a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
              "   </g>\n",
              "   <g id=\"patch_3\">\n",
              "    <path d=\"M 42.828125 273.312 \n",
              "L 42.828125 7.2 \n",
              "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
              "   </g>\n",
              "   <g id=\"patch_4\">\n",
              "    <path d=\"M 399.948125 273.312 \n",
              "L 399.948125 7.2 \n",
              "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
              "   </g>\n",
              "   <g id=\"patch_5\">\n",
              "    <path d=\"M 42.828125 273.312 \n",
              "L 399.948125 273.312 \n",
              "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
              "   </g>\n",
              "   <g id=\"patch_6\">\n",
              "    <path d=\"M 42.828125 7.2 \n",
              "L 399.948125 7.2 \n",
              "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
              "   </g>\n",
              "  </g>\n",
              " </g>\n",
              " <defs>\n",
              "  <clipPath id=\"p315ed9965a\">\n",
              "   <rect x=\"42.828125\" y=\"7.2\" width=\"357.12\" height=\"266.112\"/>\n",
              "  </clipPath>\n",
              " </defs>\n",
              "</svg>\n"
            ],
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Para G cte, error en función de G. Sí, es cualquier cosa\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pred_ids = predictions.T.argsort()\n",
        "predictions_sort = predictions[pred_ids]\n",
        "G_true_sorted = y_test.to_numpy()[pred_ids].T[0]\n",
        "G_err = np.abs(predictions_sort-G_true_sorted)\n",
        "plt.plot(predictions_sort,G_err)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Use /tmp/tmpdvj0ejth as temporary training directory\n",
            "Reading training dataset...\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function CoreModel._consumes_training_examples_until_eof at 0x7f8ac84b9d80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function CoreModel._consumes_training_examples_until_eof at 0x7f8ac84b9d80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training dataset read in 0:00:01.264825. Found 140371 examples.\n",
            "Training model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[INFO 24-02-01 17:50:14.1314 -03 kernel.cc:1233] Loading model from path /tmp/tmpdvj0ejth/model/ with prefix 6dd0df30e9134b5f\n",
            "[INFO 24-02-01 17:50:35.1878 -03 decision_forest.cc:660] Model loaded with 300 root(s), 12267916 node(s), and 16 input feature(s).\n",
            "[INFO 24-02-01 17:50:35.1878 -03 abstract_model.cc:1344] Engine \"RandomForestOptPred\" built\n",
            "[INFO 24-02-01 17:50:35.1879 -03 kernel.cc:1061] Use fast generic engine\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model trained in 0:01:11.346276\n",
            "Compiling model...\n",
            "WARNING:tensorflow:5 out of the last 45 calls to <function InferenceCoreModel.make_predict_function.<locals>.predict_function_trained at 0x7f8a733b29e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 45 calls to <function InferenceCoreModel.make_predict_function.<locals>.predict_function_trained at 0x7f8a733b29e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model compiled.\n",
            "WARNING:tensorflow:5 out of the last 6 calls to <function InferenceCoreModel.yggdrasil_model_path_tensor at 0x7f8a733b13f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 6 calls to <function InferenceCoreModel.yggdrasil_model_path_tensor at 0x7f8a733b13f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f8a733360b0>"
            ]
          },
          "execution_count": 140,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Spliteo de DataFrames y generacion de Datasets\n",
        "label = 'h_labels'\n",
        "\n",
        "def split_dataset(dataset, test_ratio=0.30):\n",
        "  \"\"\"Splits a panda dataframe in two.\"\"\"\n",
        "  test_indices = np.random.rand(len(dataset)) < test_ratio\n",
        "  return dataset[~test_indices], dataset[test_indices]\n",
        "\n",
        "\n",
        "train_ds_pd, test_ds_pd = split_dataset(df)\n",
        "print(\"{} examples in training, {} examples for testing.\".format(\n",
        "    len(train_ds_pd), len(test_ds_pd)))\n",
        "\n",
        "train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label=label, task=tfdf.keras.Task.REGRESSION)\n",
        "test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_ds_pd, label=label, task=tfdf.keras.Task.REGRESSION)\n",
        "\n",
        "# Entrenamiento\n",
        "model = tfdf.keras.RandomForestModel(task = tfdf.keras.Task.REGRESSION)\n",
        "model.compile(metrics=[\"mse\"]) \n",
        "model.fit(x=train_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<script src=\"https://d3js.org/d3.v6.min.js\"></script>\n",
              "<div id=\"tree_plot_57aacd18076b4ccc85b2f413ec60fcb9\"></div>\n",
              "<script>\n",
              "/*\n",
              " * Copyright 2021 Google LLC.\n",
              " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              " * you may not use this file except in compliance with the License.\n",
              " * You may obtain a copy of the License at\n",
              " *\n",
              " *     https://www.apache.org/licenses/LICENSE-2.0\n",
              " *\n",
              " * Unless required by applicable law or agreed to in writing, software\n",
              " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              " * See the License for the specific language governing permissions and\n",
              " * limitations under the License.\n",
              " */\n",
              "\n",
              "/**\n",
              " *  Plotting of decision trees generated by TF-DF.\n",
              " *\n",
              " *  A tree is a recursive structure of node objects.\n",
              " *  A node contains one or more of the following components:\n",
              " *\n",
              " *    - A value: Representing the output of the node. If the node is not a leaf,\n",
              " *      the value is only present for analysis i.e. it is not used for\n",
              " *      predictions.\n",
              " *\n",
              " *    - A condition : For non-leaf nodes, the condition (also known as split)\n",
              " *      defines a binary test to branch to the positive or negative child.\n",
              " *\n",
              " *    - An explanation: Generally a plot showing the relation between the label\n",
              " *      and the condition to give insights about the effect of the condition.\n",
              " *\n",
              " *    - Two children : For non-leaf nodes, the children nodes. The first\n",
              " *      children (i.e. \"node.children[0]\") is the negative children (drawn in\n",
              " *      red). The second children is the positive one (drawn in green).\n",
              " *\n",
              " */\n",
              "\n",
              "/**\n",
              " * Plots a single decision tree into a DOM element.\n",
              " * @param {!options} options Dictionary of configurations.\n",
              " * @param {!tree} raw_tree Recursive tree structure.\n",
              " * @param {string} canvas_id Id of the output dom element.\n",
              " */\n",
              "function display_tree(options, raw_tree, canvas_id) {\n",
              "  console.log(options);\n",
              "\n",
              "  // Determine the node placement.\n",
              "  const tree_struct = d3.tree().nodeSize(\n",
              "      [options.node_y_offset, options.node_x_offset])(d3.hierarchy(raw_tree));\n",
              "\n",
              "  // Boundaries of the node placement.\n",
              "  let x_min = Infinity;\n",
              "  let x_max = -x_min;\n",
              "  let y_min = Infinity;\n",
              "  let y_max = -x_min;\n",
              "\n",
              "  tree_struct.each(d => {\n",
              "    if (d.x > x_max) x_max = d.x;\n",
              "    if (d.x < x_min) x_min = d.x;\n",
              "    if (d.y > y_max) y_max = d.y;\n",
              "    if (d.y < y_min) y_min = d.y;\n",
              "  });\n",
              "\n",
              "  // Size of the plot.\n",
              "  const width = y_max - y_min + options.node_x_size + options.margin * 2;\n",
              "  const height = x_max - x_min + options.node_y_size + options.margin * 2 +\n",
              "      options.node_y_offset - options.node_y_size;\n",
              "\n",
              "  const plot = d3.select(canvas_id);\n",
              "\n",
              "  // Tool tip\n",
              "  options.tooltip = plot.append('div')\n",
              "                        .attr('width', 100)\n",
              "                        .attr('height', 100)\n",
              "                        .style('padding', '4px')\n",
              "                        .style('background', '#fff')\n",
              "                        .style('box-shadow', '4px 4px 0px rgba(0,0,0,0.1)')\n",
              "                        .style('border', '1px solid black')\n",
              "                        .style('font-family', 'sans-serif')\n",
              "                        .style('font-size', options.font_size)\n",
              "                        .style('position', 'absolute')\n",
              "                        .style('z-index', '10')\n",
              "                        .attr('pointer-events', 'none')\n",
              "                        .style('display', 'none');\n",
              "\n",
              "  // Create canvas\n",
              "  const svg = plot.append('svg').attr('width', width).attr('height', height);\n",
              "  const graph =\n",
              "      svg.style('overflow', 'visible')\n",
              "          .append('g')\n",
              "          .attr('font-family', 'sans-serif')\n",
              "          .attr('font-size', options.font_size)\n",
              "          .attr(\n",
              "              'transform',\n",
              "              () => `translate(${options.margin},${\n",
              "                  - x_min + options.node_y_offset / 2 + options.margin})`);\n",
              "\n",
              "  // Plot bounding box.\n",
              "  if (options.show_plot_bounding_box) {\n",
              "    svg.append('rect')\n",
              "        .attr('width', width)\n",
              "        .attr('height', height)\n",
              "        .attr('fill', 'none')\n",
              "        .attr('stroke-width', 1.0)\n",
              "        .attr('stroke', 'black');\n",
              "  }\n",
              "\n",
              "  // Draw the edges.\n",
              "  display_edges(options, graph, tree_struct);\n",
              "\n",
              "  // Draw the nodes.\n",
              "  display_nodes(options, graph, tree_struct);\n",
              "}\n",
              "\n",
              "/**\n",
              " * Draw the nodes of the tree.\n",
              " * @param {!options} options Dictionary of configurations.\n",
              " * @param {!graph} graph D3 search handle containing the graph.\n",
              " * @param {!tree_struct} tree_struct Structure of the tree (node placement,\n",
              " *     data, etc.).\n",
              " */\n",
              "function display_nodes(options, graph, tree_struct) {\n",
              "  const nodes = graph.append('g')\n",
              "                    .selectAll('g')\n",
              "                    .data(tree_struct.descendants())\n",
              "                    .join('g')\n",
              "                    .attr('transform', d => `translate(${d.y},${d.x})`);\n",
              "\n",
              "  nodes.append('rect')\n",
              "      .attr('x', 0.5)\n",
              "      .attr('y', 0.5)\n",
              "      .attr('width', options.node_x_size)\n",
              "      .attr('height', options.node_y_size)\n",
              "      .attr('stroke', 'lightgrey')\n",
              "      .attr('stroke-width', 1)\n",
              "      .attr('fill', 'white')\n",
              "      .attr('y', -options.node_y_size / 2);\n",
              "\n",
              "  // Brackets on the right of condition nodes without children.\n",
              "  non_leaf_node_without_children =\n",
              "      nodes.filter(node => node.data.condition != null && node.children == null)\n",
              "          .append('g')\n",
              "          .attr('transform', `translate(${options.node_x_size},0)`);\n",
              "\n",
              "  non_leaf_node_without_children.append('path')\n",
              "      .attr('d', 'M0,0 C 10,0 0,10 10,10')\n",
              "      .attr('fill', 'none')\n",
              "      .attr('stroke-width', 1.0)\n",
              "      .attr('stroke', '#F00');\n",
              "\n",
              "  non_leaf_node_without_children.append('path')\n",
              "      .attr('d', 'M0,0 C 10,0 0,-10 10,-10')\n",
              "      .attr('fill', 'none')\n",
              "      .attr('stroke-width', 1.0)\n",
              "      .attr('stroke', '#0F0');\n",
              "\n",
              "  const node_content = nodes.append('g').attr(\n",
              "      'transform',\n",
              "      `translate(0,${options.node_padding - options.node_y_size / 2})`);\n",
              "\n",
              "  node_content.append(node => create_node_element(options, node));\n",
              "}\n",
              "\n",
              "/**\n",
              " * Creates the D3 content for a single node.\n",
              " * @param {!options} options Dictionary of configurations.\n",
              " * @param {!node} node Node to draw.\n",
              " * @return {!d3} D3 content.\n",
              " */\n",
              "function create_node_element(options, node) {\n",
              "  // Output accumulator.\n",
              "  let output = {\n",
              "    // Content to draw.\n",
              "    content: d3.create('svg:g'),\n",
              "    // Vertical offset to the next element to draw.\n",
              "    vertical_offset: 0\n",
              "  };\n",
              "\n",
              "  // Conditions.\n",
              "  if (node.data.condition != null) {\n",
              "    display_condition(options, node.data.condition, output);\n",
              "  }\n",
              "\n",
              "  // Values.\n",
              "  if (node.data.value != null) {\n",
              "    display_value(options, node.data.value, output);\n",
              "  }\n",
              "\n",
              "  // Explanations.\n",
              "  if (node.data.explanation != null) {\n",
              "    display_explanation(options, node.data.explanation, output);\n",
              "  }\n",
              "\n",
              "  return output.content.node();\n",
              "}\n",
              "\n",
              "\n",
              "/**\n",
              " * Adds a single line of text inside of a node.\n",
              " * @param {!options} options Dictionary of configurations.\n",
              " * @param {string} text Text to display.\n",
              " * @param {!output} output Output display accumulator.\n",
              " */\n",
              "function display_node_text(options, text, output) {\n",
              "  output.content.append('text')\n",
              "      .attr('x', options.node_padding)\n",
              "      .attr('y', output.vertical_offset)\n",
              "      .attr('alignment-baseline', 'hanging')\n",
              "      .text(text);\n",
              "  output.vertical_offset += 10;\n",
              "}\n",
              "\n",
              "/**\n",
              " * Adds a single line of text inside of a node with a tooltip.\n",
              " * @param {!options} options Dictionary of configurations.\n",
              " * @param {string} text Text to display.\n",
              " * @param {string} tooltip Text in the Tooltip.\n",
              " * @param {!output} output Output display accumulator.\n",
              " */\n",
              "function display_node_text_with_tooltip(options, text, tooltip, output) {\n",
              "  const item = output.content.append('text')\n",
              "                   .attr('x', options.node_padding)\n",
              "                   .attr('alignment-baseline', 'hanging')\n",
              "                   .text(text);\n",
              "\n",
              "  add_tooltip(options, item, () => tooltip);\n",
              "  output.vertical_offset += 10;\n",
              "}\n",
              "\n",
              "/**\n",
              " * Adds a tooltip to a dom element.\n",
              " * @param {!options} options Dictionary of configurations.\n",
              " * @param {!dom} target Dom element to equip with a tooltip.\n",
              " * @param {!func} get_content Generates the html content of the tooltip.\n",
              " */\n",
              "function add_tooltip(options, target, get_content) {\n",
              "  function show(d) {\n",
              "    options.tooltip.style('display', 'block');\n",
              "    options.tooltip.html(get_content());\n",
              "  }\n",
              "\n",
              "  function hide(d) {\n",
              "    options.tooltip.style('display', 'none');\n",
              "  }\n",
              "\n",
              "  function move(d) {\n",
              "    options.tooltip.style('display', 'block');\n",
              "    options.tooltip.style('left', (d.pageX + 5) + 'px');\n",
              "    options.tooltip.style('top', d.pageY + 'px');\n",
              "  }\n",
              "\n",
              "  target.on('mouseover', show);\n",
              "  target.on('mouseout', hide);\n",
              "  target.on('mousemove', move);\n",
              "}\n",
              "\n",
              "/**\n",
              " * Adds a condition inside of a node.\n",
              " * @param {!options} options Dictionary of configurations.\n",
              " * @param {!condition} condition Condition to display.\n",
              " * @param {!output} output Output display accumulator.\n",
              " */\n",
              "function display_condition(options, condition, output) {\n",
              "  threshold_format = d3.format('r');\n",
              "\n",
              "  if (condition.type === 'IS_MISSING') {\n",
              "    display_node_text(options, `${condition.attribute} is missing`, output);\n",
              "    return;\n",
              "  }\n",
              "\n",
              "  if (condition.type === 'IS_TRUE') {\n",
              "    display_node_text(options, `${condition.attribute} is true`, output);\n",
              "    return;\n",
              "  }\n",
              "\n",
              "  if (condition.type === 'NUMERICAL_IS_HIGHER_THAN') {\n",
              "    format = d3.format('r');\n",
              "    display_node_text(\n",
              "        options,\n",
              "        `${condition.attribute} >= ${threshold_format(condition.threshold)}`,\n",
              "        output);\n",
              "    return;\n",
              "  }\n",
              "\n",
              "  if (condition.type === 'CATEGORICAL_IS_IN') {\n",
              "    display_node_text_with_tooltip(\n",
              "        options, `${condition.attribute} in [...]`,\n",
              "        `${condition.attribute} in [${condition.mask}]`, output);\n",
              "    return;\n",
              "  }\n",
              "\n",
              "  if (condition.type === 'CATEGORICAL_SET_CONTAINS') {\n",
              "    display_node_text_with_tooltip(\n",
              "        options, `${condition.attribute} intersect [...]`,\n",
              "        `${condition.attribute} intersect [${condition.mask}]`, output);\n",
              "    return;\n",
              "  }\n",
              "\n",
              "  if (condition.type === 'NUMERICAL_SPARSE_OBLIQUE') {\n",
              "    display_node_text_with_tooltip(\n",
              "        options, `Sparse oblique split...`,\n",
              "        `[${condition.attributes}]*[${condition.weights}]>=${\n",
              "            threshold_format(condition.threshold)}`,\n",
              "        output);\n",
              "    return;\n",
              "  }\n",
              "\n",
              "  display_node_text(\n",
              "      options, `Non supported condition ${condition.type}`, output);\n",
              "}\n",
              "\n",
              "/**\n",
              " * Adds a value inside of a node.\n",
              " * @param {!options} options Dictionary of configurations.\n",
              " * @param {!value} value Value to display.\n",
              " * @param {!output} output Output display accumulator.\n",
              " */\n",
              "function display_value(options, value, output) {\n",
              "  if (value.type === 'PROBABILITY') {\n",
              "    const left_margin = 0;\n",
              "    const right_margin = 50;\n",
              "    const plot_width = options.node_x_size - options.node_padding * 2 -\n",
              "        left_margin - right_margin;\n",
              "\n",
              "    let cusum = Array.from(d3.cumsum(value.distribution));\n",
              "    cusum.unshift(0);\n",
              "    const distribution_plot = output.content.append('g').attr(\n",
              "        'transform', `translate(0,${output.vertical_offset + 0.5})`);\n",
              "\n",
              "    distribution_plot.selectAll('rect')\n",
              "        .data(value.distribution)\n",
              "        .join('rect')\n",
              "        .attr('height', 10)\n",
              "        .attr(\n",
              "            'x',\n",
              "            (d, i) =>\n",
              "                (cusum[i] * plot_width + left_margin + options.node_padding))\n",
              "        .attr('width', (d, i) => d * plot_width)\n",
              "        .style('fill', (d, i) => d3.schemeSet1[i]);\n",
              "\n",
              "    const num_examples =\n",
              "        output.content.append('g')\n",
              "            .attr('transform', `translate(0,${output.vertical_offset})`)\n",
              "            .append('text')\n",
              "            .attr('x', options.node_x_size - options.node_padding)\n",
              "            .attr('alignment-baseline', 'hanging')\n",
              "            .attr('text-anchor', 'end')\n",
              "            .text(`(${value.num_examples})`);\n",
              "\n",
              "    const distribution_details = d3.create('ul');\n",
              "    distribution_details.selectAll('li')\n",
              "        .data(value.distribution)\n",
              "        .join('li')\n",
              "        .append('span')\n",
              "        .text(\n",
              "            (d, i) =>\n",
              "                'class ' + i + ': ' + d3.format('.3%')(value.distribution[i]));\n",
              "\n",
              "    add_tooltip(options, distribution_plot, () => distribution_details.html());\n",
              "    add_tooltip(options, num_examples, () => 'Number of examples');\n",
              "\n",
              "    output.vertical_offset += 10;\n",
              "    return;\n",
              "  }\n",
              "\n",
              "  if (value.type === 'REGRESSION') {\n",
              "    display_node_text(\n",
              "        options,\n",
              "        'value: ' + d3.format('r')(value.value) + ` (` +\n",
              "            d3.format('.6')(value.num_examples) + `)`,\n",
              "        output);\n",
              "    return;\n",
              "  }\n",
              "\n",
              "  if (value.type === 'UPLIFT') {\n",
              "    display_node_text(\n",
              "        options,\n",
              "        'effect: ' + d3.format('r')(value.treatment_effect) + ` (` +\n",
              "            d3.format('.6')(value.num_examples) + `)`,\n",
              "        output);\n",
              "    return;\n",
              "  }\n",
              "\n",
              "  display_node_text(options, `Non supported value ${value.type}`, output);\n",
              "}\n",
              "\n",
              "/**\n",
              " * Adds an explanation inside of a node.\n",
              " * @param {!options} options Dictionary of configurations.\n",
              " * @param {!explanation} explanation Explanation to display.\n",
              " * @param {!output} output Output display accumulator.\n",
              " */\n",
              "function display_explanation(options, explanation, output) {\n",
              "  // Margin before the explanation.\n",
              "  output.vertical_offset += 10;\n",
              "\n",
              "  display_node_text(\n",
              "      options, `Non supported explanation ${explanation.type}`, output);\n",
              "}\n",
              "\n",
              "\n",
              "/**\n",
              " * Draw the edges of the tree.\n",
              " * @param {!options} options Dictionary of configurations.\n",
              " * @param {!graph} graph D3 search handle containing the graph.\n",
              " * @param {!tree_struct} tree_struct Structure of the tree (node placement,\n",
              " *     data, etc.).\n",
              " */\n",
              "function display_edges(options, graph, tree_struct) {\n",
              "  // Draw an edge between a parent and a child node with a bezier.\n",
              "  function draw_single_edge(d) {\n",
              "    return 'M' + (d.source.y + options.node_x_size) + ',' + d.source.x + ' C' +\n",
              "        (d.source.y + options.node_x_size + options.edge_rounding) + ',' +\n",
              "        d.source.x + ' ' + (d.target.y - options.edge_rounding) + ',' +\n",
              "        d.target.x + ' ' + d.target.y + ',' + d.target.x;\n",
              "  }\n",
              "\n",
              "  graph.append('g')\n",
              "      .attr('fill', 'none')\n",
              "      .attr('stroke-width', 1.2)\n",
              "      .selectAll('path')\n",
              "      .data(tree_struct.links())\n",
              "      .join('path')\n",
              "      .attr('d', draw_single_edge)\n",
              "      .attr(\n",
              "          'stroke', d => (d.target === d.source.children[0]) ? '#0F0' : '#F00');\n",
              "}\n",
              "\n",
              "display_tree({\"margin\": 10, \"node_x_size\": 160, \"node_y_size\": 28, \"node_x_offset\": 180, \"node_y_offset\": 33, \"font_size\": 10, \"edge_rounding\": 20, \"node_padding\": 2, \"show_plot_bounding_box\": false}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 2.5538034439086914, \"num_examples\": 140371.0, \"standard_deviation\": 1.4113065049525542}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"1\", \"threshold\": 0.2952420115470886}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 3.76804780960083, \"num_examples\": 70560.0, \"standard_deviation\": 0.707528166868187}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"0\", \"threshold\": 0.6289284825325012}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 3.16363263130188, \"num_examples\": 35735.0, \"standard_deviation\": 0.35597495719450023}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"0\", \"threshold\": 0.652271568775177}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 2.8527774810791016, \"num_examples\": 17758.0, \"standard_deviation\": 0.17672090370788532}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"0\", \"threshold\": 0.6673444509506226}}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 3.470700740814209, \"num_examples\": 17977.0, \"standard_deviation\": 0.1769132284826652}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"4\", \"threshold\": 0.31205785274505615}}]}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 4.388256549835205, \"num_examples\": 34825.0, \"standard_deviation\": 0.3531588564797567}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"3\", \"threshold\": 0.32263997197151184}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 4.6950483322143555, \"num_examples\": 17348.0, \"standard_deviation\": 0.17644523810783141}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"12\", \"threshold\": 0.32394617795944214}}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 4.083729267120361, \"num_examples\": 17477.0, \"standard_deviation\": 0.17735631115048014}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"2\", \"threshold\": 0.3272981643676758}}]}]}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 1.3265317678451538, \"num_examples\": 69811.0, \"standard_deviation\": 0.7089225861171137}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"15\", \"threshold\": 0.17479097843170166}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 1.9402440786361694, \"num_examples\": 34904.0, \"standard_deviation\": 0.35381385076872296}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"8\", \"threshold\": 0.3047623336315155}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 2.2452738285064697, \"num_examples\": 17565.0, \"standard_deviation\": 0.17556944781164668}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"11\", \"threshold\": 0.2849483788013458}}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 1.631238579750061, \"num_examples\": 17339.0, \"standard_deviation\": 0.1761632148916807}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"6\", \"threshold\": 0.34497541189193726}}]}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 0.7128720879554749, \"num_examples\": 34907.0, \"standard_deviation\": 0.356001564411327}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"4\", \"threshold\": 0.06811541318893433}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 1.0194621086120605, \"num_examples\": 17547.0, \"standard_deviation\": 0.17901706563055533}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"7\", \"threshold\": 0.19422610104084015}}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 0.4029795527458191, \"num_examples\": 17360.0, \"standard_deviation\": 0.17721111154260225}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"15\", \"threshold\": 0.023727431893348694}}]}]}]}, \"#tree_plot_57aacd18076b4ccc85b2f413ec60fcb9\")\n",
              "</script>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 159,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tfdf.model_plotter.plot_model_in_colab(model, tree_idx=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60/60 [==============================] - 5s 79ms/step - loss: 0.0000e+00 - mse: 7.6438e-10\n",
            "\n",
            "loss: 0.0000\n",
            "mse: 0.0000\n",
            "60/60 [==============================] - 5s 79ms/step\n",
            "tf.Tensor(1.1815647, shape=(), dtype=float32)\n",
            "[1.181533]\n",
            "tf.Tensor(2.9702878, shape=(), dtype=float32)\n",
            "[2.970294]\n",
            "tf.Tensor(3.161747, shape=(), dtype=float32)\n",
            "[3.1617692]\n",
            "tf.Tensor(0.8606368, shape=(), dtype=float32)\n",
            "[0.8606794]\n",
            "tf.Tensor(3.8257158, shape=(), dtype=float32)\n",
            "[3.825732]\n",
            "tf.Tensor(0.23398145, shape=(), dtype=float32)\n",
            "[0.23398799]\n",
            "tf.Tensor(4.3534102, shape=(), dtype=float32)\n",
            "[4.353419]\n",
            "tf.Tensor(1.7687318, shape=(), dtype=float32)\n",
            "[1.7687343]\n",
            "tf.Tensor(2.2264657, shape=(), dtype=float32)\n",
            "[2.226471]\n",
            "tf.Tensor(3.0126956, shape=(), dtype=float32)\n",
            "[3.012716]\n"
          ]
        }
      ],
      "source": [
        "model.compile(metrics=[\"mse\"])\n",
        "evaluation = model.evaluate(test_ds, return_dict=True)\n",
        "print()\n",
        "\n",
        "for name, value in evaluation.items():\n",
        "  print(f\"{name}: {value:.4f}\")\n",
        "\n",
        "predictions = model.predict(test_ds)\n",
        "\n",
        "for e in test_ds:\n",
        "    for i in range(0, 10):\n",
        "        print(e[1][i])\n",
        "        print(predictions[i])\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/svg+xml": [
              "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
              "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
              "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
              "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"433.55125pt\" height=\"310.86825pt\" viewBox=\"0 0 433.55125 310.86825\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
              " <metadata>\n",
              "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
              "   <cc:Work>\n",
              "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
              "    <dc:date>2024-02-01T17:57:25.797042</dc:date>\n",
              "    <dc:format>image/svg+xml</dc:format>\n",
              "    <dc:creator>\n",
              "     <cc:Agent>\n",
              "      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n",
              "     </cc:Agent>\n",
              "    </dc:creator>\n",
              "   </cc:Work>\n",
              "  </rdf:RDF>\n",
              " </metadata>\n",
              " <defs>\n",
              "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
              " </defs>\n",
              " <g id=\"figure_1\">\n",
              "  <g id=\"patch_1\">\n",
              "   <path d=\"M 0 310.86825 \n",
              "L 433.55125 310.86825 \n",
              "L 433.55125 0 \n",
              "L 0 0 \n",
              "z\n",
              "\" style=\"fill: #ffffff\"/>\n",
              "  </g>\n",
              "  <g id=\"axes_1\">\n",
              "   <g id=\"patch_2\">\n",
              "    <path d=\"M 69.23125 273.312 \n",
              "L 426.35125 273.312 \n",
              "L 426.35125 7.2 \n",
              "L 69.23125 7.2 \n",
              "z\n",
              "\" style=\"fill: #ffffff\"/>\n",
              "   </g>\n",
              "   <g id=\"matplotlib.axis_1\">\n",
              "    <g id=\"xtick_1\">\n",
              "     <g id=\"line2d_1\">\n",
              "      <defs>\n",
              "       <path id=\"m2adc57c624\" d=\"M 0 0 \n",
              "L 0 3.5 \n",
              "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </defs>\n",
              "      <g>\n",
              "       <use xlink:href=\"#m2adc57c624\" x=\"84.378176\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_1\">\n",
              "      <!-- 0 -->\n",
              "      <g transform=\"translate(81.196926 287.910437) scale(0.1 -0.1)\">\n",
              "       <defs>\n",
              "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
              "Q 1547 4250 1301 3770 \n",
              "Q 1056 3291 1056 2328 \n",
              "Q 1056 1369 1301 889 \n",
              "Q 1547 409 2034 409 \n",
              "Q 2525 409 2770 889 \n",
              "Q 3016 1369 3016 2328 \n",
              "Q 3016 3291 2770 3770 \n",
              "Q 2525 4250 2034 4250 \n",
              "z\n",
              "M 2034 4750 \n",
              "Q 2819 4750 3233 4129 \n",
              "Q 3647 3509 3647 2328 \n",
              "Q 3647 1150 3233 529 \n",
              "Q 2819 -91 2034 -91 \n",
              "Q 1250 -91 836 529 \n",
              "Q 422 1150 422 2328 \n",
              "Q 422 3509 836 4129 \n",
              "Q 1250 4750 2034 4750 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       </defs>\n",
              "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"xtick_2\">\n",
              "     <g id=\"line2d_2\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#m2adc57c624\" x=\"138.668234\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_2\">\n",
              "      <!-- 50 -->\n",
              "      <g transform=\"translate(132.305734 287.910437) scale(0.1 -0.1)\">\n",
              "       <defs>\n",
              "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
              "L 3169 4666 \n",
              "L 3169 4134 \n",
              "L 1269 4134 \n",
              "L 1269 2991 \n",
              "Q 1406 3038 1543 3061 \n",
              "Q 1681 3084 1819 3084 \n",
              "Q 2600 3084 3056 2656 \n",
              "Q 3513 2228 3513 1497 \n",
              "Q 3513 744 3044 326 \n",
              "Q 2575 -91 1722 -91 \n",
              "Q 1428 -91 1123 -41 \n",
              "Q 819 9 494 109 \n",
              "L 494 744 \n",
              "Q 775 591 1075 516 \n",
              "Q 1375 441 1709 441 \n",
              "Q 2250 441 2565 725 \n",
              "Q 2881 1009 2881 1497 \n",
              "Q 2881 1984 2565 2268 \n",
              "Q 2250 2553 1709 2553 \n",
              "Q 1456 2553 1204 2497 \n",
              "Q 953 2441 691 2322 \n",
              "L 691 4666 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       </defs>\n",
              "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"xtick_3\">\n",
              "     <g id=\"line2d_3\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#m2adc57c624\" x=\"192.958292\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_3\">\n",
              "      <!-- 100 -->\n",
              "      <g transform=\"translate(183.414542 287.910437) scale(0.1 -0.1)\">\n",
              "       <defs>\n",
              "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
              "L 1825 531 \n",
              "L 1825 4091 \n",
              "L 703 3866 \n",
              "L 703 4441 \n",
              "L 1819 4666 \n",
              "L 2450 4666 \n",
              "L 2450 531 \n",
              "L 3481 531 \n",
              "L 3481 0 \n",
              "L 794 0 \n",
              "L 794 531 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       </defs>\n",
              "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"xtick_4\">\n",
              "     <g id=\"line2d_4\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#m2adc57c624\" x=\"247.248349\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_4\">\n",
              "      <!-- 150 -->\n",
              "      <g transform=\"translate(237.704599 287.910437) scale(0.1 -0.1)\">\n",
              "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"xtick_5\">\n",
              "     <g id=\"line2d_5\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#m2adc57c624\" x=\"301.538407\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_5\">\n",
              "      <!-- 200 -->\n",
              "      <g transform=\"translate(291.994657 287.910437) scale(0.1 -0.1)\">\n",
              "       <defs>\n",
              "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
              "L 3431 531 \n",
              "L 3431 0 \n",
              "L 469 0 \n",
              "L 469 531 \n",
              "Q 828 903 1448 1529 \n",
              "Q 2069 2156 2228 2338 \n",
              "Q 2531 2678 2651 2914 \n",
              "Q 2772 3150 2772 3378 \n",
              "Q 2772 3750 2511 3984 \n",
              "Q 2250 4219 1831 4219 \n",
              "Q 1534 4219 1204 4116 \n",
              "Q 875 4013 500 3803 \n",
              "L 500 4441 \n",
              "Q 881 4594 1212 4672 \n",
              "Q 1544 4750 1819 4750 \n",
              "Q 2544 4750 2975 4387 \n",
              "Q 3406 4025 3406 3419 \n",
              "Q 3406 3131 3298 2873 \n",
              "Q 3191 2616 2906 2266 \n",
              "Q 2828 2175 2409 1742 \n",
              "Q 1991 1309 1228 531 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       </defs>\n",
              "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"xtick_6\">\n",
              "     <g id=\"line2d_6\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#m2adc57c624\" x=\"355.828465\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_6\">\n",
              "      <!-- 250 -->\n",
              "      <g transform=\"translate(346.284715 287.910437) scale(0.1 -0.1)\">\n",
              "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"xtick_7\">\n",
              "     <g id=\"line2d_7\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#m2adc57c624\" x=\"410.118523\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_7\">\n",
              "      <!-- 300 -->\n",
              "      <g transform=\"translate(400.574773 287.910437) scale(0.1 -0.1)\">\n",
              "       <defs>\n",
              "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
              "Q 3050 2419 3304 2112 \n",
              "Q 3559 1806 3559 1356 \n",
              "Q 3559 666 3084 287 \n",
              "Q 2609 -91 1734 -91 \n",
              "Q 1441 -91 1130 -33 \n",
              "Q 819 25 488 141 \n",
              "L 488 750 \n",
              "Q 750 597 1062 519 \n",
              "Q 1375 441 1716 441 \n",
              "Q 2309 441 2620 675 \n",
              "Q 2931 909 2931 1356 \n",
              "Q 2931 1769 2642 2001 \n",
              "Q 2353 2234 1838 2234 \n",
              "L 1294 2234 \n",
              "L 1294 2753 \n",
              "L 1863 2753 \n",
              "Q 2328 2753 2575 2939 \n",
              "Q 2822 3125 2822 3475 \n",
              "Q 2822 3834 2567 4026 \n",
              "Q 2313 4219 1838 4219 \n",
              "Q 1578 4219 1281 4162 \n",
              "Q 984 4106 628 3988 \n",
              "L 628 4550 \n",
              "Q 988 4650 1302 4700 \n",
              "Q 1616 4750 1894 4750 \n",
              "Q 2613 4750 3031 4423 \n",
              "Q 3450 4097 3450 3541 \n",
              "Q 3450 3153 3228 2886 \n",
              "Q 3006 2619 2597 2516 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       </defs>\n",
              "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"text_8\">\n",
              "     <!-- Number of trees -->\n",
              "     <g transform=\"translate(207.046719 301.588562) scale(0.1 -0.1)\">\n",
              "      <defs>\n",
              "       <path id=\"DejaVuSans-4e\" d=\"M 628 4666 \n",
              "L 1478 4666 \n",
              "L 3547 763 \n",
              "L 3547 4666 \n",
              "L 4159 4666 \n",
              "L 4159 0 \n",
              "L 3309 0 \n",
              "L 1241 3903 \n",
              "L 1241 0 \n",
              "L 628 0 \n",
              "L 628 4666 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
              "L 544 3500 \n",
              "L 1119 3500 \n",
              "L 1119 1403 \n",
              "Q 1119 906 1312 657 \n",
              "Q 1506 409 1894 409 \n",
              "Q 2359 409 2629 706 \n",
              "Q 2900 1003 2900 1516 \n",
              "L 2900 3500 \n",
              "L 3475 3500 \n",
              "L 3475 0 \n",
              "L 2900 0 \n",
              "L 2900 538 \n",
              "Q 2691 219 2414 64 \n",
              "Q 2138 -91 1772 -91 \n",
              "Q 1169 -91 856 284 \n",
              "Q 544 659 544 1381 \n",
              "z\n",
              "M 1991 3584 \n",
              "L 1991 3584 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \n",
              "Q 3544 3216 3844 3400 \n",
              "Q 4144 3584 4550 3584 \n",
              "Q 5097 3584 5394 3201 \n",
              "Q 5691 2819 5691 2113 \n",
              "L 5691 0 \n",
              "L 5113 0 \n",
              "L 5113 2094 \n",
              "Q 5113 2597 4934 2840 \n",
              "Q 4756 3084 4391 3084 \n",
              "Q 3944 3084 3684 2787 \n",
              "Q 3425 2491 3425 1978 \n",
              "L 3425 0 \n",
              "L 2847 0 \n",
              "L 2847 2094 \n",
              "Q 2847 2600 2669 2842 \n",
              "Q 2491 3084 2119 3084 \n",
              "Q 1678 3084 1418 2786 \n",
              "Q 1159 2488 1159 1978 \n",
              "L 1159 0 \n",
              "L 581 0 \n",
              "L 581 3500 \n",
              "L 1159 3500 \n",
              "L 1159 2956 \n",
              "Q 1356 3278 1631 3431 \n",
              "Q 1906 3584 2284 3584 \n",
              "Q 2666 3584 2933 3390 \n",
              "Q 3200 3197 3328 2828 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \n",
              "Q 3116 2381 2855 2742 \n",
              "Q 2594 3103 2138 3103 \n",
              "Q 1681 3103 1420 2742 \n",
              "Q 1159 2381 1159 1747 \n",
              "Q 1159 1113 1420 752 \n",
              "Q 1681 391 2138 391 \n",
              "Q 2594 391 2855 752 \n",
              "Q 3116 1113 3116 1747 \n",
              "z\n",
              "M 1159 2969 \n",
              "Q 1341 3281 1617 3432 \n",
              "Q 1894 3584 2278 3584 \n",
              "Q 2916 3584 3314 3078 \n",
              "Q 3713 2572 3713 1747 \n",
              "Q 3713 922 3314 415 \n",
              "Q 2916 -91 2278 -91 \n",
              "Q 1894 -91 1617 61 \n",
              "Q 1341 213 1159 525 \n",
              "L 1159 0 \n",
              "L 581 0 \n",
              "L 581 4863 \n",
              "L 1159 4863 \n",
              "L 1159 2969 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
              "L 3597 1613 \n",
              "L 953 1613 \n",
              "Q 991 1019 1311 708 \n",
              "Q 1631 397 2203 397 \n",
              "Q 2534 397 2845 478 \n",
              "Q 3156 559 3463 722 \n",
              "L 3463 178 \n",
              "Q 3153 47 2828 -22 \n",
              "Q 2503 -91 2169 -91 \n",
              "Q 1331 -91 842 396 \n",
              "Q 353 884 353 1716 \n",
              "Q 353 2575 817 3079 \n",
              "Q 1281 3584 2069 3584 \n",
              "Q 2775 3584 3186 3129 \n",
              "Q 3597 2675 3597 1894 \n",
              "z\n",
              "M 3022 2063 \n",
              "Q 3016 2534 2758 2815 \n",
              "Q 2500 3097 2075 3097 \n",
              "Q 1594 3097 1305 2825 \n",
              "Q 1016 2553 972 2059 \n",
              "L 3022 2063 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
              "Q 2534 3019 2420 3045 \n",
              "Q 2306 3072 2169 3072 \n",
              "Q 1681 3072 1420 2755 \n",
              "Q 1159 2438 1159 1844 \n",
              "L 1159 0 \n",
              "L 581 0 \n",
              "L 581 3500 \n",
              "L 1159 3500 \n",
              "L 1159 2956 \n",
              "Q 1341 3275 1631 3429 \n",
              "Q 1922 3584 2338 3584 \n",
              "Q 2397 3584 2469 3576 \n",
              "Q 2541 3569 2628 3553 \n",
              "L 2631 2963 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
              "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
              "Q 1497 3097 1228 2736 \n",
              "Q 959 2375 959 1747 \n",
              "Q 959 1119 1226 758 \n",
              "Q 1494 397 1959 397 \n",
              "Q 2419 397 2687 759 \n",
              "Q 2956 1122 2956 1747 \n",
              "Q 2956 2369 2687 2733 \n",
              "Q 2419 3097 1959 3097 \n",
              "z\n",
              "M 1959 3584 \n",
              "Q 2709 3584 3137 3096 \n",
              "Q 3566 2609 3566 1747 \n",
              "Q 3566 888 3137 398 \n",
              "Q 2709 -91 1959 -91 \n",
              "Q 1206 -91 779 398 \n",
              "Q 353 888 353 1747 \n",
              "Q 353 2609 779 3096 \n",
              "Q 1206 3584 1959 3584 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \n",
              "L 2375 4384 \n",
              "L 1825 4384 \n",
              "Q 1516 4384 1395 4259 \n",
              "Q 1275 4134 1275 3809 \n",
              "L 1275 3500 \n",
              "L 2222 3500 \n",
              "L 2222 3053 \n",
              "L 1275 3053 \n",
              "L 1275 0 \n",
              "L 697 0 \n",
              "L 697 3053 \n",
              "L 147 3053 \n",
              "L 147 3500 \n",
              "L 697 3500 \n",
              "L 697 3744 \n",
              "Q 697 4328 969 4595 \n",
              "Q 1241 4863 1831 4863 \n",
              "L 2375 4863 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
              "L 1172 3500 \n",
              "L 2356 3500 \n",
              "L 2356 3053 \n",
              "L 1172 3053 \n",
              "L 1172 1153 \n",
              "Q 1172 725 1289 603 \n",
              "Q 1406 481 1766 481 \n",
              "L 2356 481 \n",
              "L 2356 0 \n",
              "L 1766 0 \n",
              "Q 1100 0 847 248 \n",
              "Q 594 497 594 1153 \n",
              "L 594 3053 \n",
              "L 172 3053 \n",
              "L 172 3500 \n",
              "L 594 3500 \n",
              "L 594 4494 \n",
              "L 1172 4494 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
              "L 2834 2853 \n",
              "Q 2591 2978 2328 3040 \n",
              "Q 2066 3103 1784 3103 \n",
              "Q 1356 3103 1142 2972 \n",
              "Q 928 2841 928 2578 \n",
              "Q 928 2378 1081 2264 \n",
              "Q 1234 2150 1697 2047 \n",
              "L 1894 2003 \n",
              "Q 2506 1872 2764 1633 \n",
              "Q 3022 1394 3022 966 \n",
              "Q 3022 478 2636 193 \n",
              "Q 2250 -91 1575 -91 \n",
              "Q 1294 -91 989 -36 \n",
              "Q 684 19 347 128 \n",
              "L 347 722 \n",
              "Q 666 556 975 473 \n",
              "Q 1284 391 1588 391 \n",
              "Q 1994 391 2212 530 \n",
              "Q 2431 669 2431 922 \n",
              "Q 2431 1156 2273 1281 \n",
              "Q 2116 1406 1581 1522 \n",
              "L 1381 1569 \n",
              "Q 847 1681 609 1914 \n",
              "Q 372 2147 372 2553 \n",
              "Q 372 3047 722 3315 \n",
              "Q 1072 3584 1716 3584 \n",
              "Q 2034 3584 2315 3537 \n",
              "Q 2597 3491 2834 3397 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "      </defs>\n",
              "      <use xlink:href=\"#DejaVuSans-4e\"/>\n",
              "      <use xlink:href=\"#DejaVuSans-75\" x=\"74.804688\"/>\n",
              "      <use xlink:href=\"#DejaVuSans-6d\" x=\"138.183594\"/>\n",
              "      <use xlink:href=\"#DejaVuSans-62\" x=\"235.595703\"/>\n",
              "      <use xlink:href=\"#DejaVuSans-65\" x=\"299.072266\"/>\n",
              "      <use xlink:href=\"#DejaVuSans-72\" x=\"360.595703\"/>\n",
              "      <use xlink:href=\"#DejaVuSans-20\" x=\"401.708984\"/>\n",
              "      <use xlink:href=\"#DejaVuSans-6f\" x=\"433.496094\"/>\n",
              "      <use xlink:href=\"#DejaVuSans-66\" x=\"494.677734\"/>\n",
              "      <use xlink:href=\"#DejaVuSans-20\" x=\"529.882812\"/>\n",
              "      <use xlink:href=\"#DejaVuSans-74\" x=\"561.669922\"/>\n",
              "      <use xlink:href=\"#DejaVuSans-72\" x=\"600.878906\"/>\n",
              "      <use xlink:href=\"#DejaVuSans-65\" x=\"639.742188\"/>\n",
              "      <use xlink:href=\"#DejaVuSans-65\" x=\"701.265625\"/>\n",
              "      <use xlink:href=\"#DejaVuSans-73\" x=\"762.789062\"/>\n",
              "     </g>\n",
              "    </g>\n",
              "   </g>\n",
              "   <g id=\"matplotlib.axis_2\">\n",
              "    <g id=\"ytick_1\">\n",
              "     <g id=\"line2d_8\">\n",
              "      <defs>\n",
              "       <path id=\"m26bf4b6d31\" d=\"M 0 0 \n",
              "L -3.5 0 \n",
              "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </defs>\n",
              "      <g>\n",
              "       <use xlink:href=\"#m26bf4b6d31\" x=\"69.23125\" y=\"255.925466\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_9\">\n",
              "      <!-- 0.00003 -->\n",
              "      <g transform=\"translate(20.878125 259.724685) scale(0.1 -0.1)\">\n",
              "       <defs>\n",
              "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
              "L 1344 794 \n",
              "L 1344 0 \n",
              "L 684 0 \n",
              "L 684 794 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       </defs>\n",
              "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"286.279297\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-33\" x=\"349.902344\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"ytick_2\">\n",
              "     <g id=\"line2d_9\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#m26bf4b6d31\" x=\"69.23125\" y=\"222.146591\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_10\">\n",
              "      <!-- 0.00004 -->\n",
              "      <g transform=\"translate(20.878125 225.94581) scale(0.1 -0.1)\">\n",
              "       <defs>\n",
              "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
              "L 825 1625 \n",
              "L 2419 1625 \n",
              "L 2419 4116 \n",
              "z\n",
              "M 2253 4666 \n",
              "L 3047 4666 \n",
              "L 3047 1625 \n",
              "L 3713 1625 \n",
              "L 3713 1100 \n",
              "L 3047 1100 \n",
              "L 3047 0 \n",
              "L 2419 0 \n",
              "L 2419 1100 \n",
              "L 313 1100 \n",
              "L 313 1709 \n",
              "L 2253 4666 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       </defs>\n",
              "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"286.279297\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-34\" x=\"349.902344\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"ytick_3\">\n",
              "     <g id=\"line2d_10\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#m26bf4b6d31\" x=\"69.23125\" y=\"188.367716\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_11\">\n",
              "      <!-- 0.00005 -->\n",
              "      <g transform=\"translate(20.878125 192.166934) scale(0.1 -0.1)\">\n",
              "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"286.279297\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-35\" x=\"349.902344\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"ytick_4\">\n",
              "     <g id=\"line2d_11\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#m26bf4b6d31\" x=\"69.23125\" y=\"154.58884\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_12\">\n",
              "      <!-- 0.00006 -->\n",
              "      <g transform=\"translate(20.878125 158.388059) scale(0.1 -0.1)\">\n",
              "       <defs>\n",
              "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
              "Q 1688 2584 1439 2293 \n",
              "Q 1191 2003 1191 1497 \n",
              "Q 1191 994 1439 701 \n",
              "Q 1688 409 2113 409 \n",
              "Q 2538 409 2786 701 \n",
              "Q 3034 994 3034 1497 \n",
              "Q 3034 2003 2786 2293 \n",
              "Q 2538 2584 2113 2584 \n",
              "z\n",
              "M 3366 4563 \n",
              "L 3366 3988 \n",
              "Q 3128 4100 2886 4159 \n",
              "Q 2644 4219 2406 4219 \n",
              "Q 1781 4219 1451 3797 \n",
              "Q 1122 3375 1075 2522 \n",
              "Q 1259 2794 1537 2939 \n",
              "Q 1816 3084 2150 3084 \n",
              "Q 2853 3084 3261 2657 \n",
              "Q 3669 2231 3669 1497 \n",
              "Q 3669 778 3244 343 \n",
              "Q 2819 -91 2113 -91 \n",
              "Q 1303 -91 875 529 \n",
              "Q 447 1150 447 2328 \n",
              "Q 447 3434 972 4092 \n",
              "Q 1497 4750 2381 4750 \n",
              "Q 2619 4750 2861 4703 \n",
              "Q 3103 4656 3366 4563 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       </defs>\n",
              "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"286.279297\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-36\" x=\"349.902344\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"ytick_5\">\n",
              "     <g id=\"line2d_12\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#m26bf4b6d31\" x=\"69.23125\" y=\"120.809965\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_13\">\n",
              "      <!-- 0.00007 -->\n",
              "      <g transform=\"translate(20.878125 124.609184) scale(0.1 -0.1)\">\n",
              "       <defs>\n",
              "        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \n",
              "L 3525 4666 \n",
              "L 3525 4397 \n",
              "L 1831 0 \n",
              "L 1172 0 \n",
              "L 2766 4134 \n",
              "L 525 4134 \n",
              "L 525 4666 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       </defs>\n",
              "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"286.279297\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-37\" x=\"349.902344\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"ytick_6\">\n",
              "     <g id=\"line2d_13\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#m26bf4b6d31\" x=\"69.23125\" y=\"87.03109\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_14\">\n",
              "      <!-- 0.00008 -->\n",
              "      <g transform=\"translate(20.878125 90.830309) scale(0.1 -0.1)\">\n",
              "       <defs>\n",
              "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
              "Q 1584 2216 1326 1975 \n",
              "Q 1069 1734 1069 1313 \n",
              "Q 1069 891 1326 650 \n",
              "Q 1584 409 2034 409 \n",
              "Q 2484 409 2743 651 \n",
              "Q 3003 894 3003 1313 \n",
              "Q 3003 1734 2745 1975 \n",
              "Q 2488 2216 2034 2216 \n",
              "z\n",
              "M 1403 2484 \n",
              "Q 997 2584 770 2862 \n",
              "Q 544 3141 544 3541 \n",
              "Q 544 4100 942 4425 \n",
              "Q 1341 4750 2034 4750 \n",
              "Q 2731 4750 3128 4425 \n",
              "Q 3525 4100 3525 3541 \n",
              "Q 3525 3141 3298 2862 \n",
              "Q 3072 2584 2669 2484 \n",
              "Q 3125 2378 3379 2068 \n",
              "Q 3634 1759 3634 1313 \n",
              "Q 3634 634 3220 271 \n",
              "Q 2806 -91 2034 -91 \n",
              "Q 1263 -91 848 271 \n",
              "Q 434 634 434 1313 \n",
              "Q 434 1759 690 2068 \n",
              "Q 947 2378 1403 2484 \n",
              "z\n",
              "M 1172 3481 \n",
              "Q 1172 3119 1398 2916 \n",
              "Q 1625 2713 2034 2713 \n",
              "Q 2441 2713 2670 2916 \n",
              "Q 2900 3119 2900 3481 \n",
              "Q 2900 3844 2670 4047 \n",
              "Q 2441 4250 2034 4250 \n",
              "Q 1625 4250 1398 4047 \n",
              "Q 1172 3844 1172 3481 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       </defs>\n",
              "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"286.279297\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-38\" x=\"349.902344\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"ytick_7\">\n",
              "     <g id=\"line2d_14\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#m26bf4b6d31\" x=\"69.23125\" y=\"53.252215\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_15\">\n",
              "      <!-- 0.00009 -->\n",
              "      <g transform=\"translate(20.878125 57.051433) scale(0.1 -0.1)\">\n",
              "       <defs>\n",
              "        <path id=\"DejaVuSans-39\" d=\"M 703 97 \n",
              "L 703 672 \n",
              "Q 941 559 1184 500 \n",
              "Q 1428 441 1663 441 \n",
              "Q 2288 441 2617 861 \n",
              "Q 2947 1281 2994 2138 \n",
              "Q 2813 1869 2534 1725 \n",
              "Q 2256 1581 1919 1581 \n",
              "Q 1219 1581 811 2004 \n",
              "Q 403 2428 403 3163 \n",
              "Q 403 3881 828 4315 \n",
              "Q 1253 4750 1959 4750 \n",
              "Q 2769 4750 3195 4129 \n",
              "Q 3622 3509 3622 2328 \n",
              "Q 3622 1225 3098 567 \n",
              "Q 2575 -91 1691 -91 \n",
              "Q 1453 -91 1209 -44 \n",
              "Q 966 3 703 97 \n",
              "z\n",
              "M 1959 2075 \n",
              "Q 2384 2075 2632 2365 \n",
              "Q 2881 2656 2881 3163 \n",
              "Q 2881 3666 2632 3958 \n",
              "Q 2384 4250 1959 4250 \n",
              "Q 1534 4250 1286 3958 \n",
              "Q 1038 3666 1038 3163 \n",
              "Q 1038 2656 1286 2365 \n",
              "Q 1534 2075 1959 2075 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       </defs>\n",
              "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"286.279297\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-39\" x=\"349.902344\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"ytick_8\">\n",
              "     <g id=\"line2d_15\">\n",
              "      <g>\n",
              "       <use xlink:href=\"#m26bf4b6d31\" x=\"69.23125\" y=\"19.473339\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "     <g id=\"text_16\">\n",
              "      <!-- 0.00010 -->\n",
              "      <g transform=\"translate(20.878125 23.272558) scale(0.1 -0.1)\">\n",
              "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-31\" x=\"286.279297\"/>\n",
              "       <use xlink:href=\"#DejaVuSans-30\" x=\"349.902344\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"text_17\">\n",
              "     <!-- RMSE (out-of-bag) -->\n",
              "     <g transform=\"translate(14.798438 185.715375) rotate(-90) scale(0.1 -0.1)\">\n",
              "      <defs>\n",
              "       <path id=\"DejaVuSans-52\" d=\"M 2841 2188 \n",
              "Q 3044 2119 3236 1894 \n",
              "Q 3428 1669 3622 1275 \n",
              "L 4263 0 \n",
              "L 3584 0 \n",
              "L 2988 1197 \n",
              "Q 2756 1666 2539 1819 \n",
              "Q 2322 1972 1947 1972 \n",
              "L 1259 1972 \n",
              "L 1259 0 \n",
              "L 628 0 \n",
              "L 628 4666 \n",
              "L 2053 4666 \n",
              "Q 2853 4666 3247 4331 \n",
              "Q 3641 3997 3641 3322 \n",
              "Q 3641 2881 3436 2590 \n",
              "Q 3231 2300 2841 2188 \n",
              "z\n",
              "M 1259 4147 \n",
              "L 1259 2491 \n",
              "L 2053 2491 \n",
              "Q 2509 2491 2742 2702 \n",
              "Q 2975 2913 2975 3322 \n",
              "Q 2975 3731 2742 3939 \n",
              "Q 2509 4147 2053 4147 \n",
              "L 1259 4147 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       <path id=\"DejaVuSans-4d\" d=\"M 628 4666 \n",
              "L 1569 4666 \n",
              "L 2759 1491 \n",
              "L 3956 4666 \n",
              "L 4897 4666 \n",
              "L 4897 0 \n",
              "L 4281 0 \n",
              "L 4281 4097 \n",
              "L 3078 897 \n",
              "L 2444 897 \n",
              "L 1241 4097 \n",
              "L 1241 0 \n",
              "L 628 0 \n",
              "L 628 4666 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       <path id=\"DejaVuSans-53\" d=\"M 3425 4513 \n",
              "L 3425 3897 \n",
              "Q 3066 4069 2747 4153 \n",
              "Q 2428 4238 2131 4238 \n",
              "Q 1616 4238 1336 4038 \n",
              "Q 1056 3838 1056 3469 \n",
              "Q 1056 3159 1242 3001 \n",
              "Q 1428 2844 1947 2747 \n",
              "L 2328 2669 \n",
              "Q 3034 2534 3370 2195 \n",
              "Q 3706 1856 3706 1288 \n",
              "Q 3706 609 3251 259 \n",
              "Q 2797 -91 1919 -91 \n",
              "Q 1588 -91 1214 -16 \n",
              "Q 841 59 441 206 \n",
              "L 441 856 \n",
              "Q 825 641 1194 531 \n",
              "Q 1563 422 1919 422 \n",
              "Q 2459 422 2753 634 \n",
              "Q 3047 847 3047 1241 \n",
              "Q 3047 1584 2836 1778 \n",
              "Q 2625 1972 2144 2069 \n",
              "L 1759 2144 \n",
              "Q 1053 2284 737 2584 \n",
              "Q 422 2884 422 3419 \n",
              "Q 422 4038 858 4394 \n",
              "Q 1294 4750 2059 4750 \n",
              "Q 2388 4750 2728 4690 \n",
              "Q 3069 4631 3425 4513 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       <path id=\"DejaVuSans-45\" d=\"M 628 4666 \n",
              "L 3578 4666 \n",
              "L 3578 4134 \n",
              "L 1259 4134 \n",
              "L 1259 2753 \n",
              "L 3481 2753 \n",
              "L 3481 2222 \n",
              "L 1259 2222 \n",
              "L 1259 531 \n",
              "L 3634 531 \n",
              "L 3634 0 \n",
              "L 628 0 \n",
              "L 628 4666 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \n",
              "Q 1566 4138 1362 3434 \n",
              "Q 1159 2731 1159 2009 \n",
              "Q 1159 1288 1364 580 \n",
              "Q 1569 -128 1984 -844 \n",
              "L 1484 -844 \n",
              "Q 1016 -109 783 600 \n",
              "Q 550 1309 550 2009 \n",
              "Q 550 2706 781 3412 \n",
              "Q 1013 4119 1484 4856 \n",
              "L 1984 4856 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       <path id=\"DejaVuSans-2d\" d=\"M 313 2009 \n",
              "L 1997 2009 \n",
              "L 1997 1497 \n",
              "L 313 1497 \n",
              "L 313 2009 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
              "Q 1497 1759 1228 1600 \n",
              "Q 959 1441 959 1056 \n",
              "Q 959 750 1161 570 \n",
              "Q 1363 391 1709 391 \n",
              "Q 2188 391 2477 730 \n",
              "Q 2766 1069 2766 1631 \n",
              "L 2766 1759 \n",
              "L 2194 1759 \n",
              "z\n",
              "M 3341 1997 \n",
              "L 3341 0 \n",
              "L 2766 0 \n",
              "L 2766 531 \n",
              "Q 2569 213 2275 61 \n",
              "Q 1981 -91 1556 -91 \n",
              "Q 1019 -91 701 211 \n",
              "Q 384 513 384 1019 \n",
              "Q 384 1609 779 1909 \n",
              "Q 1175 2209 1959 2209 \n",
              "L 2766 2209 \n",
              "L 2766 2266 \n",
              "Q 2766 2663 2505 2880 \n",
              "Q 2244 3097 1772 3097 \n",
              "Q 1472 3097 1187 3025 \n",
              "Q 903 2953 641 2809 \n",
              "L 641 3341 \n",
              "Q 956 3463 1253 3523 \n",
              "Q 1550 3584 1831 3584 \n",
              "Q 2591 3584 2966 3190 \n",
              "Q 3341 2797 3341 1997 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \n",
              "Q 2906 2416 2648 2759 \n",
              "Q 2391 3103 1925 3103 \n",
              "Q 1463 3103 1205 2759 \n",
              "Q 947 2416 947 1791 \n",
              "Q 947 1169 1205 825 \n",
              "Q 1463 481 1925 481 \n",
              "Q 2391 481 2648 825 \n",
              "Q 2906 1169 2906 1791 \n",
              "z\n",
              "M 3481 434 \n",
              "Q 3481 -459 3084 -895 \n",
              "Q 2688 -1331 1869 -1331 \n",
              "Q 1566 -1331 1297 -1286 \n",
              "Q 1028 -1241 775 -1147 \n",
              "L 775 -588 \n",
              "Q 1028 -725 1275 -790 \n",
              "Q 1522 -856 1778 -856 \n",
              "Q 2344 -856 2625 -561 \n",
              "Q 2906 -266 2906 331 \n",
              "L 2906 616 \n",
              "Q 2728 306 2450 153 \n",
              "Q 2172 0 1784 0 \n",
              "Q 1141 0 747 490 \n",
              "Q 353 981 353 1791 \n",
              "Q 353 2603 747 3093 \n",
              "Q 1141 3584 1784 3584 \n",
              "Q 2172 3584 2450 3431 \n",
              "Q 2728 3278 2906 2969 \n",
              "L 2906 3500 \n",
              "L 3481 3500 \n",
              "L 3481 434 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \n",
              "L 1013 4856 \n",
              "Q 1481 4119 1714 3412 \n",
              "Q 1947 2706 1947 2009 \n",
              "Q 1947 1309 1714 600 \n",
              "Q 1481 -109 1013 -844 \n",
              "L 513 -844 \n",
              "Q 928 -128 1133 580 \n",
              "Q 1338 1288 1338 2009 \n",
              "Q 1338 2731 1133 3434 \n",
              "Q 928 4138 513 4856 \n",
              "z\n",
              "\" transform=\"scale(0.015625)\"/>\n",
              "      </defs>\n",
              "      <use xlink:href=\"#DejaVuSans-52\"/>\n",
              "      <use xlink:href=\"#DejaVuSans-4d\" x=\"69.482422\"/>\n",
              "      <use xlink:href=\"#DejaVuSans-53\" x=\"155.761719\"/>\n",
              "      <use xlink:href=\"#DejaVuSans-45\" x=\"219.238281\"/>\n",
              "      <use xlink:href=\"#DejaVuSans-20\" x=\"282.421875\"/>\n",
              "      <use xlink:href=\"#DejaVuSans-28\" x=\"314.208984\"/>\n",
              "      <use xlink:href=\"#DejaVuSans-6f\" x=\"353.222656\"/>\n",
              "      <use xlink:href=\"#DejaVuSans-75\" x=\"414.404297\"/>\n",
              "      <use xlink:href=\"#DejaVuSans-74\" x=\"477.783203\"/>\n",
              "      <use xlink:href=\"#DejaVuSans-2d\" x=\"516.992188\"/>\n",
              "      <use xlink:href=\"#DejaVuSans-6f\" x=\"554.951172\"/>\n",
              "      <use xlink:href=\"#DejaVuSans-66\" x=\"616.132812\"/>\n",
              "      <use xlink:href=\"#DejaVuSans-2d\" x=\"645.837891\"/>\n",
              "      <use xlink:href=\"#DejaVuSans-62\" x=\"681.921875\"/>\n",
              "      <use xlink:href=\"#DejaVuSans-61\" x=\"745.398438\"/>\n",
              "      <use xlink:href=\"#DejaVuSans-67\" x=\"806.677734\"/>\n",
              "      <use xlink:href=\"#DejaVuSans-29\" x=\"870.154297\"/>\n",
              "     </g>\n",
              "    </g>\n",
              "   </g>\n",
              "   <g id=\"line2d_16\">\n",
              "    <path d=\"M 85.463977 19.296 \n",
              "L 96.321989 155.580289 \n",
              "L 107.18 203.872973 \n",
              "L 118.038012 222.205333 \n",
              "L 128.896023 231.914647 \n",
              "L 139.754035 238.108615 \n",
              "L 150.612047 242.456209 \n",
              "L 161.470058 245.672559 \n",
              "L 172.32807 248.147456 \n",
              "L 183.186081 250.093271 \n",
              "L 194.044093 251.658401 \n",
              "L 204.902104 252.857767 \n",
              "L 215.760116 253.921432 \n",
              "L 226.618127 254.742537 \n",
              "L 237.476139 255.549671 \n",
              "L 248.334151 256.25479 \n",
              "L 259.192162 256.913927 \n",
              "L 270.050174 257.406046 \n",
              "L 280.908185 257.885511 \n",
              "L 291.766197 258.320827 \n",
              "L 302.624208 258.724079 \n",
              "L 313.48222 259.102241 \n",
              "L 324.340231 259.400843 \n",
              "L 335.198243 259.752551 \n",
              "L 346.056255 259.978681 \n",
              "L 356.914266 260.255786 \n",
              "L 367.772278 260.477936 \n",
              "L 378.630289 260.675465 \n",
              "L 389.488301 260.910804 \n",
              "L 400.346312 261.07041 \n",
              "L 410.118523 261.216 \n",
              "\" clip-path=\"url(#p2028e0a696)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
              "   </g>\n",
              "   <g id=\"patch_3\">\n",
              "    <path d=\"M 69.23125 273.312 \n",
              "L 69.23125 7.2 \n",
              "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
              "   </g>\n",
              "   <g id=\"patch_4\">\n",
              "    <path d=\"M 426.35125 273.312 \n",
              "L 426.35125 7.2 \n",
              "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
              "   </g>\n",
              "   <g id=\"patch_5\">\n",
              "    <path d=\"M 69.23125 273.312 \n",
              "L 426.35125 273.312 \n",
              "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
              "   </g>\n",
              "   <g id=\"patch_6\">\n",
              "    <path d=\"M 69.23125 7.2 \n",
              "L 426.35125 7.2 \n",
              "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
              "   </g>\n",
              "  </g>\n",
              " </g>\n",
              " <defs>\n",
              "  <clipPath id=\"p2028e0a696\">\n",
              "   <rect x=\"69.23125\" y=\"7.2\" width=\"357.12\" height=\"266.112\"/>\n",
              "  </clipPath>\n",
              " </defs>\n",
              "</svg>\n"
            ],
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "logs = model.make_inspector().training_logs()\n",
        "plt.plot([log.num_trees for log in logs], [log.evaluation.rmse for log in logs])\n",
        "plt.xlabel(\"Number of trees\")\n",
        "plt.ylabel(\"RMSE (out-of-bag)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testeo barrido en G código anterior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num = 50\n",
        "g_range = np.linspace(0.01,5,num)\n",
        "rho_range= {}\n",
        "gpu_batch_size = 2\n",
        "\n",
        "# Construccion de parametros y matrices auxiliares\n",
        "#rho1_size = m1_basis.size\n",
        "rho2_size = m2_basis.size\n",
        "rho2kkbar_size = basis.m\n",
        "fund_size = basis.size\n",
        "hamil_base_size = basis.d*(basis.d+1)//2\n",
        "rho_1_arrays = rho_1_gen(basis)\n",
        "rho_1_arrays_tf = tf.constant(rho_1_arrays, dtype=tf.float32)\n",
        "rho_2_arrays = rho_2_gen(basis, nm2_basis, m2_basis)\n",
        "rho_2_arrays_tf = tf.constant(rho_2_arrays, dtype=tf.float32)\n",
        "rho_2_arrays_kkbar = rho_2_kkbar_gen(t_basis, rho_2_arrays)\n",
        "rho_2_arrays_kkbar_tf = tf.constant(rho_2_arrays_kkbar, dtype=tf.float32)\n",
        "k_indices = get_kkbar_indices(t_basis)\n",
        "k_indices_tf = gen_update_indices(t_basis, gpu_batch_size)\n",
        "\n",
        "batch_size = 2\n",
        "indices = tf.constant(get_kkbar_indices(t_basis))\n",
        "indices_tf = gen_update_indices(t_basis, batch_size)\n",
        "en_batch = [np.arange(0, basis.m) for _ in range(0,batch_size)]\n",
        "G_batched = [np.ones((basis.m,basis.m)) for _ in range(0, batch_size)]\n",
        "\n",
        "t = two_body_hamiltonian_tf(t_basis, basis.m, en_batch, G_batched, rho_1_arrays, rho_2_arrays, indices_tf)\n",
        "(h0, hi) = (t[0][0].numpy(), t[1][0].numpy())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.01\n",
            "0.11183673469387755\n",
            "0.2136734693877551\n",
            "0.31551020408163266\n",
            "0.4173469387755102\n",
            "0.5191836734693878\n",
            "0.6210204081632653\n",
            "0.7228571428571429\n",
            "0.8246938775510204\n",
            "0.926530612244898\n",
            "1.0283673469387755\n",
            "1.130204081632653\n",
            "1.2320408163265306\n",
            "1.3338775510204082\n",
            "1.4357142857142857\n",
            "1.5375510204081633\n",
            "1.6393877551020408\n",
            "1.7412244897959184\n",
            "1.843061224489796\n",
            "1.9448979591836735\n",
            "2.046734693877551\n",
            "2.1485714285714286\n",
            "2.250408163265306\n",
            "2.3522448979591832\n",
            "2.454081632653061\n",
            "2.555918367346939\n",
            "2.657755102040816\n",
            "2.7595918367346934\n",
            "2.861428571428571\n",
            "2.963265306122449\n",
            "3.0651020408163263\n",
            "3.1669387755102036\n",
            "3.2687755102040814\n",
            "3.370612244897959\n",
            "3.4724489795918365\n",
            "3.574285714285714\n",
            "3.6761224489795916\n",
            "3.7779591836734694\n",
            "3.8797959183673467\n",
            "3.981632653061224\n",
            "4.083469387755102\n",
            "4.18530612244898\n",
            "4.287142857142857\n",
            "4.388979591836734\n",
            "4.490816326530612\n",
            "4.59265306122449\n",
            "4.694489795918367\n",
            "4.7963265306122445\n",
            "4.898163265306122\n",
            "5.0\n"
          ]
        }
      ],
      "source": [
        "def compute_g(g):\n",
        "    #print(g)\n",
        "    G_batched = [g * np.ones((basis.m,basis.m)) for _ in range(0, batch_size)]\n",
        "    t = two_body_hamiltonian_tf(t_basis, basis.m, en_batch, G_batched, rho_1_arrays, rho_2_arrays, indices_tf)\n",
        "    state = pure_state(t)\n",
        "    #print(fund)\n",
        "    #print('rho')\n",
        "    #Toda la matriz\n",
        "    rho = rho_2_tf(state, rho_2_arrays_kkbar_tf)\n",
        "    #Solo el bloque kkbar\n",
        "    #rho = rho_2_kkbar(basis, fund, ml_basis, mll_basis, t_basis)\n",
        "    #Rho1\n",
        "    #rho = rho_1(basis, fund).todense()\n",
        "    r = np.sort(linalg_d.eigvals(rho[0]).real)\n",
        "    #print(r)\n",
        "    return (g, r)\n",
        "\n",
        "# Version sincrónica\n",
        "rho_range = {}\n",
        "\n",
        "for g in g_range:\n",
        "    print(g)\n",
        "    rho_range[g] = compute_g(g)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Ploteamos\n",
        "rho_range = dict(rho_range)\n",
        "rho_range = dict(sorted(rho_range.items()))\n",
        "x_axis = list(g_range)\n",
        "values = list(rho_range.items())\n",
        "size = len(values[0][1])\n",
        "num = 50\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "\n",
        "# Plot using matplotlib\n",
        "# Use LaTeX to format all text\n",
        "\n",
        "plt.rcParams['text.usetex'] = True\n",
        "plt.rcParams['axes.labelsize'] = 30\n",
        "plt.rcParams['xtick.labelsize'] = 20\n",
        "plt.rcParams['ytick.labelsize'] = 20\n",
        "plt.rcParams['legend.fontsize'] = 20\n",
        "plt.rcParams['axes.linewidth'] = 1.5\n",
        "\n",
        "plt.cla()\n",
        "plt.figure(figsize=(8, 5))\n",
        "#%matplotlib qt\n",
        "%matplotlib inline \n",
        "for k in range(1,size):\n",
        "    plt.plot(x_axis, [values[j][1][k] for j in range(0,num)], linewidth=2)\n",
        "\n",
        "plt.xlabel(r'$G/\\epsilon$', fontsize=18)\n",
        "plt.ylabel(r'$\\lambda^{(2)}$', fontsize=18)\n",
        "plt.xlim(0, 5)  # Set x-axis limits from 0 to 6\n",
        "plt.ylim(0, 2)  # Set y-axis limits from 5 to 12\n",
        "\n",
        "#matplotlib.use('Agg')\n",
        "#matplotlib.use('GTK3Agg')\n",
        "\n",
        "plt.tick_params(axis='x', which='both', bottom=True, top=True, labelbottom=True)\n",
        "\n",
        "# Enable minor ticks on the x-axis\n",
        "plt.minorticks_on()\n",
        "\n",
        "# Customize the appearance of minor ticks on the x-axis\n",
        "plt.tick_params(axis='x', which='minor', width=1.5)\n",
        "plt.tick_params(axis='x', which='major', width=1.5)\n",
        "plt.tick_params(axis='y', which='major', width=1.5)\n",
        "\n",
        "plt.show()\n",
        "matplotlib.pyplot.savefig('filename.png')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "oapxWkD16fHg"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
