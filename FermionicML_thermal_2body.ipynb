{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aguschanchu/FermionicML/blob/main/FermionicML_thermal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXz5cOlVwrzZ"
      },
      "source": [
        "# FermionicML:\n",
        "\n",
        "Code based on aguschanchu/Bosonic.py\n",
        "\n",
        "A diferencia del código anterior, este modelo trabaja sobre estados térmicos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLD2Yai55rMm"
      },
      "source": [
        "## Código base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "num_gpus = tf.config.list_physical_devices('GPU')\n",
        "#tf.config.experimental.set_memory_growth(num_gpus[0],True)\n",
        "import sys\n",
        "import platform\n",
        "print(sys.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgf9ExZN4jA7"
      },
      "source": [
        "Cargamos el código de Bosonic.py básico, branch fermionic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {
        "id": "Gydz4kCH4l5w"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4036/1952354829.py:326: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
            "  def gamma_lamba_inv(x):\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.special import binom\n",
        "from scipy.sparse import dok_matrix, linalg\n",
        "from scipy import linalg as linalg_d\n",
        "from joblib import Memory\n",
        "import random\n",
        "import plotly.graph_objects as go\n",
        "from joblib import Parallel, delayed\n",
        "from numba import jit, prange, njit\n",
        "import numba as nb\n",
        "import pickle\n",
        "import math\n",
        "import tensorflow_probability as tfp\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from itertools import combinations\n",
        "import scipy\n",
        "\n",
        "# Funciones auxiliares optimiadas\n",
        "@nb.jit(nopython=True, parallel=True)\n",
        "def int_to_tuple_arr(ni,nf, b, digits=None):\n",
        "    sol = np.zeros((nf-ni, digits), dtype=np.int64)\n",
        "    for n in prange(ni, nf):\n",
        "        r = np.zeros(digits, dtype=np.int64)\n",
        "        ncop = n\n",
        "        idx = 0\n",
        "        while n != 0:\n",
        "            r[idx] = n % b\n",
        "            n = n // b\n",
        "            idx += 1\n",
        "        if digits is not None:\n",
        "            if idx < digits:\n",
        "                for i in range(idx, digits):\n",
        "                    r[i] = 0\n",
        "                idx = digits\n",
        "        sol[ncop-ni,:] = r[:idx]\n",
        "    return sol\n",
        "\n",
        "def tuple_to_int(t, d):\n",
        "    b = d-1\n",
        "    l = len(t)\n",
        "    s = [t[k]*b**(l-k-1) for k in range(0,l)]\n",
        "    return sum(s)\n",
        "\n",
        "def create_basis_(m, d, size):\n",
        "    base = []\n",
        "    index = 0\n",
        "    chunk_size = 1000000\n",
        "    for x in range(0,(m+1)**d, chunk_size):\n",
        "        start_index = x\n",
        "        end_index = min(x + chunk_size, (m+1)**d)\n",
        "        arr = int_to_tuple_arr(start_index, end_index, m+1, d)\n",
        "        sums = np.sum(arr, axis=1)\n",
        "        rows = np.where(sums == m)[0]\n",
        "        for row in [arr[i] for i in rows]:\n",
        "            if np.all(np.logical_or(row == 0, row == 1)):\n",
        "                base.append(row)\n",
        "\n",
        "    # Como consecuencia de la paralelizacion, es necesario reordenar la base\n",
        "    sorted_base = sorted(base, key=lambda x: tuple_to_int(x, d), reverse=True)\n",
        "    assert len(base) == size\n",
        "\n",
        "    return sorted_base\n",
        "\n",
        "def custom_base_representation_tf(n_min, n_max, base, num_digits):\n",
        "    # Generate a range of numbers from n_min to n_max\n",
        "    numbers = tf.range(n_min, n_max + 1, dtype=tf.int64)\n",
        "    \n",
        "    # Calculate the digits in the custom base using broadcasting\n",
        "    digits = tf.pow(tf.cast(base, dtype=tf.float64), tf.cast(tf.range(num_digits), dtype=tf.float64))\n",
        "    \n",
        "    # Reshape the digits to [1, num_digits] for broadcasting\n",
        "    digits = tf.reshape(digits, [1, -1])\n",
        "    \n",
        "    # Reshape numbers to [batch_size, 1]\n",
        "    numbers = tf.reshape(tf.cast(numbers, dtype=tf.float64), [-1, 1])\n",
        "    \n",
        "    # Calculate the digits in the custom base for each number using broadcasting\n",
        "    result = tf.cast(tf.math.floormod(tf.math.floordiv(numbers, digits), base), dtype=tf.int32)\n",
        "    \n",
        "    # Pad the result to have exactly num_digits columns\n",
        "    result = tf.pad(result, paddings=[[0, 0], [0, num_digits - tf.shape(result)[1]]], constant_values=0)\n",
        "    \n",
        "    # Reverse the order of columns\n",
        "    #result = tf.reverse(result, axis=[1])\n",
        "\n",
        "    return result\n",
        "\n",
        "def select_rows_with_sum(arr, m):\n",
        "    # Create a mask based on the criteria\n",
        "    mask = tf.reduce_all(tf.math.logical_or(tf.equal(arr, 0), tf.equal(arr, 1)), axis=1) & (tf.reduce_sum(arr, axis=1) == m)\n",
        "    \n",
        "    # Use the mask to select the rows\n",
        "    result = tf.boolean_mask(arr, mask, axis=0)\n",
        "    \n",
        "    return result\n",
        "\n",
        "def create_basis_tf_(m, d):\n",
        "    base = []\n",
        "    index = 0\n",
        "    chunk_size = 10000000\n",
        "    for x in tqdm(range(0,(m+1)**d, chunk_size)):\n",
        "        start_index = x\n",
        "        end_index = min(x + chunk_size, (m+1)**d)\n",
        "        res = custom_base_representation_tf(start_index, end_index, m+1, d)\n",
        "        arr = select_rows_with_sum(res, m)\n",
        "        base.append(arr.numpy())\n",
        "\n",
        "    return np.concatenate(base)\n",
        "\n",
        "def create_fermionic_base_(m, d):\n",
        "    indices = list(range(d))\n",
        "    combinations_list = list(combinations(indices, m))\n",
        "    \n",
        "    vectors = []\n",
        "    for combo in combinations_list:\n",
        "        vector = [1 if i in combo else 0 for i in indices]\n",
        "        vectors.append(vector)\n",
        "    \n",
        "    return vectors\n",
        "\n",
        "# Dada una base, devuelve los vectores que estan dados de a pares\n",
        "def get_kkbar_indices_(base):\n",
        "    indices = []\n",
        "    for i, v in enumerate(base):\n",
        "        if np.all(v[::2] == v[1::2]):\n",
        "            indices.append(i)\n",
        "    return indices\n",
        "\n",
        "class fixed_basis:\n",
        "\n",
        "    # Convierte a un enterno n a su escritura en base b\n",
        "    def _int_to_tuple(self, n, b, digits = None):\n",
        "        rep = np.base_repr(n, b)\n",
        "        rep_int = [int(x,b) for x in rep]\n",
        "        if digits is not None:\n",
        "            zeros = [0 for i in range(0,digits-len(rep))]\n",
        "            return zeros + rep_int\n",
        "        else:\n",
        "            return rep_int\n",
        "\n",
        "    # Revierte la transformacion anterior\n",
        "    def tuple_to_int(self, t):\n",
        "        b = self.d-1\n",
        "        l = len(t)\n",
        "        s = [t[k]*b**(l-k-1) for k in range(0,l)]\n",
        "        return sum(s)\n",
        "\n",
        "    # Convierte el vector en su representacion\n",
        "    def vect_to_repr(self, vect):\n",
        "        for i, k in enumerate(vect):\n",
        "            if k == 1. or k == 1:\n",
        "                break\n",
        "        else:\n",
        "            return 0\n",
        "        return self.base[i,:]\n",
        "\n",
        "    def rep_to_vect(self, rep):\n",
        "        rep = list(rep)\n",
        "        for i, r in [(j, self.base[j,:]) for j in range(0,self.size)]:\n",
        "            if list(r) == rep:\n",
        "                return self.canonicals[:,i]\n",
        "        else:\n",
        "            None\n",
        "\n",
        "    def rep_to_index(self, rep):\n",
        "        try:\n",
        "            return self.base.tolist().index(list(rep))\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    @staticmethod\n",
        "    def rep_to_exi(rep):\n",
        "        r = []\n",
        "        for i, k in enumerate(rep):\n",
        "            r += [i for x in range(0,k)]\n",
        "        return r\n",
        "\n",
        "    # Crea base de M particulas en D estados (repr y base canonica)\n",
        "    def create_basis(self, m, d, pairs = False):\n",
        "        #print(\"Creating basis: \", m, d)\n",
        "        #base = np.array(create_basis_tf_(m, d)) CASO GENERICO\n",
        "        base = np.array(create_fermionic_base_(m,d)) # UNICAMENTE FERMIONICO\n",
        "        if pairs:\n",
        "            base = base[get_kkbar_indices_(base)]\n",
        "        length = base.shape[0]\n",
        "        # Asignamos a cada uno de ellos un canónico\n",
        "        canonicals = np.eye(length)\n",
        "        return base, canonicals\n",
        "    \n",
        "    def __init__(self, m, d, pairs = False):\n",
        "        self.m = m\n",
        "        self.d = d\n",
        "        (self.base, self.canonicals) = self.create_basis(m, d, pairs)\n",
        "        self.size = self.base.shape[0]\n",
        "\n",
        "# Matrices de aniquilación y creación endomórficas. Estan fuera de la clase para poder ser cacheadas\n",
        "#@memory.cache\n",
        "def bdb(basis, i, j):\n",
        "    mat = dok_matrix((basis.size, basis.size), dtype=np.float32)\n",
        "    if i != j:\n",
        "        for k, v in enumerate(basis.base):\n",
        "            if v[j] != 0 and v[i] != 1:\n",
        "                #print(v)\n",
        "                dest = list(v.copy())\n",
        "                dest[j] -= 1\n",
        "                dest[i] += 1\n",
        "                tar = basis.rep_to_index(dest)\n",
        "                if tar is None:\n",
        "                    pass\n",
        "                else:\n",
        "                    mat[tar, k] = np.sqrt(v[i]+1)*np.sqrt(v[j])\n",
        "    else:\n",
        "        for k, v in enumerate(basis.base):\n",
        "            if v[j] != 0:\n",
        "                mat[k, k] = v[i] \n",
        "    return mat\n",
        "\n",
        "#@memory.cache\n",
        "def bbd(basis, i, j):\n",
        "    mat = dok_matrix((basis.size, basis.size), dtype=np.float32)\n",
        "    if i != j:\n",
        "        for k, v in enumerate(basis.base):\n",
        "            if v[i] != 0 and v[j] != 1:\n",
        "                dest = list(v.copy())\n",
        "                dest[i] -= 1\n",
        "                dest[j] += 1\n",
        "                tar = basis.rep_to_index(dest)\n",
        "                mat[tar, k] = np.sqrt(v[j]+1)*np.sqrt(v[i])\n",
        "    else:\n",
        "        for k, v in enumerate(basis.base):\n",
        "            if v[i] != 1:\n",
        "                mat[k, k] = v[i]+1\n",
        "    return mat\n",
        "\n",
        "# Matrices de aniquilación y creación.Toman la base de origen y destino (basis_o, basis_d) resp\n",
        "#@nb.jit(nopython=True, parallel=True)\n",
        "@nb.jit(nopython=True)\n",
        "def b_aux(basis_o, basis_d, i):\n",
        "    mat = np.zeros((len(basis_d), len(basis_o)), dtype=np.float32)\n",
        "    for k in prange(len(basis_o)):\n",
        "        if basis_o[k][i] != 0:\n",
        "            dest = list(basis_o[k].copy())\n",
        "            dest[i] -= 1\n",
        "            for j in prange(len(basis_d)):\n",
        "                if list(basis_d[j]) == dest:\n",
        "                    tar = j\n",
        "                    mat[tar, k] = np.sqrt(basis_o[k][i])\n",
        "    return mat\n",
        "\n",
        "def b(basis_o, basis_d, i):\n",
        "    return b_aux(basis_o.base, basis_d.base, i)\n",
        "\n",
        "#@nb.jit(nopython=True, parallel=True)\n",
        "@nb.jit(nopython=True)\n",
        "def bd_aux(basis_o, basis_d, i):\n",
        "    mat = np.zeros((len(basis_d), len(basis_o)), dtype=np.float32)\n",
        "    for k in prange(len(basis_o)):\n",
        "        if basis_o[k][i] != 1:\n",
        "            dest = list(basis_o[k].copy())\n",
        "            dest[i] += 1\n",
        "            for j in prange(len(basis_d)):\n",
        "                if list(basis_d[j]) == dest:\n",
        "                    tar = j\n",
        "                    mat[tar, k] = np.sqrt(basis_o[k][i]+1)\n",
        "    return mat\n",
        "\n",
        "def bd(basis_o, basis_d, i):\n",
        "    return bd_aux(basis_o.base, basis_d.base, i)\n",
        "\n",
        "\n",
        "# Acepta una lista de indices a crear\n",
        "@nb.jit(nopython=True, parallel=True)\n",
        "def bd_gen_aux(basis_o, basis_d, gen_list):\n",
        "    mat = np.zeros((len(basis_d), len(basis_o)), dtype=np.float32)\n",
        "    for k in prange(len(basis_o)):\n",
        "        conds = np.zeros(len(gen_list), dtype=np.int64)\n",
        "        for i in range(len(gen_list)):\n",
        "            if basis_o[k][gen_list[i]] != 1:\n",
        "                conds[i] = 1\n",
        "        if np.all(conds):\n",
        "            dest = list(basis_o[k].copy())\n",
        "            for i in gen_list:\n",
        "                dest[i] += 1\n",
        "            for j in prange(len(basis_d)):\n",
        "                if list(basis_d[j]) == dest:\n",
        "                    tar = j\n",
        "                    mat[tar, k] = np.sqrt(basis_o[k][i]+1)\n",
        "    return mat\n",
        "\n",
        "def bd_gen(basis_o, basis_d, i):\n",
        "    return bd_gen_aux(basis_o.base, basis_d.base, np.array(i))\n",
        "\n",
        "def b_gen(basis_o, basis_d, i):\n",
        "    return np.transpose(bd_gen(basis_d, basis_o, i))\n",
        "\n",
        "# Volvemos a definir la función para compilarla\n",
        "@nb.jit(forceobj=True)\n",
        "def _rep_to_index(base, rep):\n",
        "    return base.tolist().index(list(rep))\n",
        "\n",
        "# Funciones auxiliares para calcular rho2kkbar y gamma_p\n",
        "@nb.jit(nopython=True)\n",
        "def rep_to_exi(rep):\n",
        "    r = []\n",
        "    for i in range(len(rep)):\n",
        "        for j in range(rep[i]):\n",
        "            r.append(i)\n",
        "    return r\n",
        "\n",
        "@nb.njit\n",
        "def factorial(n):\n",
        "    result = 1\n",
        "    for i in range(1, n + 1):\n",
        "        result *= i\n",
        "    return result\n",
        "\n",
        "@nb.njit\n",
        "def gamma_lamba(x):\n",
        "    res = 1.0\n",
        "    for o in x:\n",
        "        res *= math.sqrt(factorial(o))\n",
        "    return res\n",
        "\n",
        "@nb.jit\n",
        "def gamma_lamba_inv(x):\n",
        "    res = 1.0\n",
        "    for o in x:\n",
        "        res *= 1.0 / np.sqrt(factorial(o))\n",
        "    return res\n",
        "\n",
        "@nb.njit\n",
        "def rep_to_index_np(base, rep):\n",
        "    for i in range(len(base)):\n",
        "        if np.all(base[i] == rep):\n",
        "            return i\n",
        "    return -1\n",
        "\n",
        "\n",
        "def gamma_p(basis, m, vect, m_basis = None, nm_basis = None):\n",
        "    d = basis.d\n",
        "    if not m_basis or not nm_basis:\n",
        "        m_basis = fixed_basis(m, d)\n",
        "        nm_basis = fixed_basis(basis.m-m,d)\n",
        "    return gamma_p_aux(basis.base, vect, m_basis.base, nm_basis.base)\n",
        "\n",
        "@nb.njit()\n",
        "def gamma_p_aux(basis, vect, m_basis, nm_basis):\n",
        "    mat = np.zeros((len(m_basis), len(nm_basis)), dtype=np.float32)\n",
        "    for i in prange(len(m_basis)):\n",
        "        v = m_basis[i]\n",
        "        for j in prange(len(nm_basis)):\n",
        "            w = nm_basis[j]\n",
        "            targ = v + w\n",
        "            index = rep_to_index_np(basis, targ)\n",
        "            if index != -1:\n",
        "                coef = vect[index]\n",
        "                if coef != 0:\n",
        "                    coef = coef * gamma_lamba_inv(v) * gamma_lamba_inv(w) * gamma_lamba(targ)\n",
        "                mat[i, j] = coef\n",
        "    return mat\n",
        "# Devuelve la matriz rho M asociada al vector\n",
        "def rho_m(basis, m, vect, m_basis = None, nm_basis = None):\n",
        "    g = gamma_p(basis, m, vect, m_basis, nm_basis)\n",
        "    return np.dot(g,np.transpose(g))\n",
        "\n",
        "# Devuelve la matriz gamma asociada a la descomposición (M,N-M) del vector\n",
        "@jit(forceobj=True)\n",
        "def gamma(basis, m, vect, m_basis = None, nm_basis = None):\n",
        "    d = basis.d\n",
        "    if not m_basis or not nm_basis:\n",
        "        m_basis = fixed_basis(m, d)\n",
        "        nm_basis = fixed_basis(basis.m-m,d)\n",
        "    mat = dok_matrix((m_basis.size, nm_basis.size), dtype=np.float32)\n",
        "    for i, v in enumerate(m_basis.base):\n",
        "        for j, w in enumerate(nm_basis.base):\n",
        "            targ = v+w\n",
        "            # Revisamos que sea un estado fermionico valido\n",
        "            arr = np.asarray(targ)\n",
        "            if not np.all(np.logical_or(arr == 0, arr == 1)):\n",
        "                continue\n",
        "            index = _rep_to_index(basis.base, targ)\n",
        "            coef = vect[index]\n",
        "            if coef != 0:\n",
        "                aux = lambda x: np.prod(np.reciprocal(np.sqrt([np.math.factorial(o) for o in x])))\n",
        "                aux_inv = lambda x: np.prod(np.sqrt([np.math.factorial(o) for o in x]))\n",
        "                coef = coef * aux(v) * aux(w) * aux_inv(targ)\n",
        "                #coef = coef\n",
        "                #print(v,w,coef)\n",
        "            mat[i,j] = coef\n",
        "    return mat\n",
        "\n",
        "# Genera las matrices de rho1\n",
        "def rho_1_gen(basis):\n",
        "    d = basis.d\n",
        "    s = basis.size\n",
        "    mat = np.empty((d,d,s,s), dtype=np.float32)\n",
        "    for i in range(0, d):\n",
        "        for j in range(0, d):\n",
        "            mat[i,j,:,:] = np.array(bdb(basis,j, i).todense())\n",
        "    return mat\n",
        "\n",
        "#@jit(parallel=True, nopython=True)\n",
        "def rho_1(d, state, rho_1_arrays):\n",
        "    state_expanded = state[np.newaxis, np.newaxis, :, :]\n",
        "    product = state_expanded * rho_1_arrays\n",
        "    mat = np.sum(product, axis=(-2, -1))\n",
        "\n",
        "    return mat\n",
        "\n",
        "\n",
        "# Genera las matrices de rho2\n",
        "def rho_2_gen(basis, mll_basis, t_basis):\n",
        "    size = t_basis.size\n",
        "    s = basis.size\n",
        "    # La entrada i, j contiene C_j^\\dag C_i    i, j \\in t_basis\n",
        "    mat = np.empty((size, size, s, s), dtype=np.float32)\n",
        "    for i, v in enumerate(t_basis.base):\n",
        "        for j, w in enumerate(t_basis.base):\n",
        "            c_i = b_gen(basis, mll_basis, rep_to_exi(v))\n",
        "            cdag_j = bd_gen(mll_basis, basis, rep_to_exi(w))\n",
        "            mat[i, j, :, :] = np.dot(cdag_j, c_i)\n",
        "\n",
        "    return mat\n",
        "\n",
        "def rho_2(size, state, rho_2_arrays):\n",
        "    state_expanded = np.expand_dims(state, axis=1)\n",
        "    state_expanded = np.expand_dims(state_expanded, axis=1)\n",
        "    rho_2_arrays = rho_2_arrays[np.newaxis, :, :, :, :]\n",
        "    print(state_expanded.shape, rho_2_arrays.shape)\n",
        "    product = state_expanded * rho_2_arrays\n",
        "    mat = np.sum(product, axis=(-2, -1))\n",
        "    return mat\n",
        "\n",
        "def get_kkbar_indices(t_basis):\n",
        "    indices = []\n",
        "    for i, v in enumerate(t_basis.base):\n",
        "        if np.all(v[::2] == v[1::2]):\n",
        "            indices.append(i)\n",
        "    return indices\n",
        "\n",
        "def rho_2_kkbar_gen(t_basis, rho_2_arrays):\n",
        "    indices = get_kkbar_indices(t_basis)\n",
        "    i, j = np.meshgrid(indices, indices, indexing='ij') # Lo usamos para rellenar la mat deseada\n",
        "\n",
        "    rho_2_arrays_kkbar = rho_2_arrays[i, j, :, :]\n",
        "\n",
        "    return rho_2_arrays_kkbar\n",
        "\n",
        "# Devuelve la matriz rho 2 asociada al bloque kkbar\n",
        "def rho_2_kkbar(basis, vect, ml_basis = None, mll_basis = None, t_basis = None):\n",
        "    d = basis.d\n",
        "    # Creo las bases si no están dadas\n",
        "    if ml_basis == None or mll_basis == None or t_basis == None:\n",
        "        ml_basis = fixed_basis(m-1,d)\n",
        "        mll_basis = fixed_basis(m-2,d)\n",
        "        t_basis = fixed_basis(2,d)\n",
        "    diag = []\n",
        "    for v in t_basis.base:\n",
        "        for j in range(0, d, 2):\n",
        "            if v[j] == v[j+1]:\n",
        "                continue\n",
        "            else:\n",
        "                break\n",
        "        else:\n",
        "            diag.append(v)\n",
        "    diag = np.array(diag)\n",
        "    return rho_2_kkbar_aux(diag, vect, basis.base, ml_basis.base, mll_basis.base, t_basis.base)\n",
        "\n",
        "@nb.njit\n",
        "def rho_2_kkbar_lambda(x):\n",
        "    res = 1.0\n",
        "    for o in x:\n",
        "        res *= 1.0 / math.sqrt(factorial(o))\n",
        "    return res\n",
        "\n",
        "#@nb.njit(parallel=True)\n",
        "def rho_2_kkbar_aux(diag, vect, basis, ml_basis, mll_basis, t_basis):\n",
        "    mat = np.zeros((len(diag), len(diag)), dtype=np.float32)\n",
        "    for i in prange(len(diag)):\n",
        "        for j in prange(len(diag)):\n",
        "            v = diag[i]\n",
        "            w = diag[j]\n",
        "            # Creacion de los a\n",
        "            i_set = rep_to_exi(v)\n",
        "            b_m = b_aux(ml_basis, mll_basis, i_set[1]) @ b_aux(basis, ml_basis, i_set[0])\n",
        "            # Creacion de los ad\n",
        "            i_set = rep_to_exi(w)\n",
        "            bd_m = bd_aux(ml_basis, basis, i_set[1]) @ bd_aux(mll_basis, ml_basis, i_set[0])\n",
        "            # v1 = vect @ bd_m @ b_m @ vect Para estados puros\n",
        "            # Mult de b's y filleo de mat\n",
        "            coef = np.trace(vect @ bd_m @ b_m)\n",
        "            mat[i,j] = coef * rho_2_kkbar_lambda(v) * rho_2_kkbar_lambda(w)\n",
        "    return mat\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dga5Xx_5vDf"
      },
      "source": [
        "## Definicion de Hamiltoniano"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myiTq53L5E1U"
      },
      "source": [
        "Cargamos el código de creación y resolución de Hamiltonianos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h5FXWv849Mq",
        "outputId": "49dd47b5-8c16-4ad4-92e7-e172462229b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "70\n"
          ]
        }
      ],
      "source": [
        "m = 4\n",
        "d = 2*m\n",
        "pairs = False # Usar solo para estados puros\n",
        "# Creo las bases para no tener que recrearlas luego\n",
        "basis = fixed_basis(m, d, pairs = pairs)\n",
        "#basis_m1 = fixed_basis(m-1, d, pairs = True)\n",
        "basis_m2 = fixed_basis(m-2, d, pairs = pairs)\n",
        "print(basis.size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {
        "id": "PToiSs915TXw"
      },
      "outputs": [],
      "source": [
        "## Usamos este approach si queremos guardar los generadores\n",
        "# Dados 1/2 (d^2+d) elementos, genera una mat de dxd:\n",
        "eps = 0.00001\n",
        "\n",
        "def sym_mat_gen(vect, d):\n",
        "    matrix = fill_matrix(vect, d)\n",
        "    return matrix + matrix.T - np.diag(matrix.diagonal())\n",
        "\n",
        "@jit(nopython=True)\n",
        "def fill_matrix(vect, d):\n",
        "    matrix = np.zeros((d, d))\n",
        "    idx = 0\n",
        "    for i in prange(d):\n",
        "        for j in prange(i, d):\n",
        "            matrix[i, j] = vect[idx]\n",
        "            idx += 1\n",
        "    return matrix\n",
        "\n",
        "# Generamos una matrix aleatoria. Cuidado con la distribución, ver https://stackoverflow.com/questions/56605189/is-there-an-efficient-way-to-generate-a-symmetric-random-matrix\n",
        "def hamil_base_gen(d):\n",
        "    U = np.random.uniform(low=0, high=1.0, size=(d, d))\n",
        "    hamil_base = np.tril(U) + np.tril(U, -1).T\n",
        "    return hamil_base\n",
        "\n",
        "# Dada un a mat dxd simetrica, contruye el hamiltoniano de un cuerpo a_{ij} c^{dag}_i c_j\n",
        "# Alternativamente podemos construirlo a partir de rho_1_gen\n",
        "def base_hamiltonian_aux(mat, size, d, rho_1_gen):\n",
        "    # Construccion de H\n",
        "    rho_1_gen_transposed = rho_1_gen.transpose(1, 0, 2, 3)\n",
        "    mat_expanded = mat[:, :, np.newaxis, np.newaxis]\n",
        "    h = np.sum(mat_expanded * rho_1_gen_transposed[:, :, :, :], axis=(0, 1))\n",
        "    return h.astype(np.float32)\n",
        "\n",
        "def base_hamiltonian(mat, basis, rho_1_gen):\n",
        "    return base_hamiltonian_aux(mat, basis.size, basis.d, rho_1_gen)\n",
        "\n",
        "def two_body_hamiltonian(t_basis_size, m, energy, G, rho_1_arrays, rho_2_arrays, indices):\n",
        "    # Creamos la mat diagonal de d*d con los elementos de energy\n",
        "    # cada uno de estos, se contraen con los elementos de rho_1_arrays\n",
        "    # la mat energy contiene las energias de cada termino c^\\dag_k c_k para k kbar (iguales)\n",
        "    # por ello los elementos se repiten \n",
        "    energy_matrix = np.diagflat(np.kron(energy, np.ones(2))) + eps * np.random.random((2*m,2*m))\n",
        "    \n",
        "    # Construimos la mat de energía\n",
        "    rho_1_arrays_t = tf.transpose(rho_1_arrays,perm=[1, 0, 2, 3])\n",
        "    h0 = np.sum(energy_matrix[:, :, np.newaxis, np.newaxis] * rho_1_arrays_t[:, :, :, :], axis=(0, 1))\n",
        "\n",
        "    # Pasamos ahora a la matrix de interacción con la misma estrategia\n",
        "    # dada G que indica la interacción entre los pares k' k'bar k kbar \n",
        "    # (que son elementos particulares de t_basis)\n",
        "    # transladamos estos coeficientes a una matriz en t_basis\n",
        "    # y multiplicamos por rho_2_arrays\n",
        "    \n",
        "    # Primero determinamos, dada t_basis, cuales son los indices de pares kkbar\n",
        "    i, j = np.meshgrid(indices, indices, indexing='ij') # Lo usamos para rellenar la mat deseada\n",
        "    rho_2_arrays_t = tf.transpose(rho_2_arrays,perm=[1, 0, 2, 3])\n",
        "\n",
        "    # Contruimos la mat que contraeremos con rho_2_arrays\n",
        "    mat = np.zeros((t_basis_size, t_basis_size))\n",
        "    mat[i, j] = G\n",
        "    hi = np.sum(mat[:, :, np.newaxis, np.newaxis] * rho_2_arrays_t[:, :, :, :], axis=(0, 1))\n",
        "    return (h0, hi)\n",
        "\n",
        "def solve(h, last_step = None):\n",
        "    sol = linalg.eigsh(h, which='SA',k=19)\n",
        "    eigenspace_tol = 0.0001\n",
        "    if type(last_step) != type(None):\n",
        "        # Seleccionamos todos los autovects que difieren sus autovalores menos que tol (mismo autoespacio)\n",
        "        # y tomamos la proyección en el autoespacio de la solución del paso anterior (last_step)\n",
        "        eig = sol[0].real\n",
        "        eigv = sol[1]\n",
        "        cand = [eigv[:,i].real  for (i, x) in enumerate(eig) if abs(x-min(eig)) < eigenspace_tol]\n",
        "        cand_norm = [x/np.linalg.norm(x) for x in cand]\n",
        "        fund = np.zeros(len(cand[0]))\n",
        "        for x in cand_norm:\n",
        "            fund += np.dot(last_step,x) * x\n",
        "    else:\n",
        "        argmin = np.argmin(sol[0].real)\n",
        "        fund = sol[1][:,argmin]\n",
        "    fund = fund.real / np.linalg.norm(fund)\n",
        "    return fund\n",
        "\n",
        "# Generacion de H basada en TF\n",
        "\n",
        "# Funciones auxiliares de gen de H basado en TF\n",
        "## Dada matrix de indices, genera los indices de updates de TF\n",
        "def gen_update_indices(t_basis, batch_size):\n",
        "    # Calculamos los indices de kkbar en t_basis\n",
        "    indices = tf.constant(get_kkbar_indices(t_basis))\n",
        "    # Creamos el array de indices x indices\n",
        "    i, j = tf.meshgrid(indices, indices, indexing='ij')\n",
        "    matrix = tf.reshape(tf.stack([i, j], axis=-1), (-1, 2))\n",
        "\n",
        "    # Repeat the matrix along the first axis (axis=0) 'b' times\n",
        "    repeated_matrix = tf.repeat(tf.expand_dims(matrix, axis=0), repeats=batch_size, axis=0)\n",
        "\n",
        "    # Create an index array from 0 to b-1\n",
        "    indices = tf.range(batch_size, dtype=tf.int32)\n",
        "\n",
        "    # Expand the index array to have the same shape as the repeated matrix\n",
        "    indices = tf.expand_dims(indices, axis=-1)\n",
        "    indices = tf.expand_dims(indices, axis=-1)\n",
        "    indices = tf.tile(indices, multiples=[1,matrix.shape[0],1]) \n",
        "\n",
        "    # Concatenate the index array to the repeated matrix along a new axis\n",
        "    tiled_matrix = tf.concat([indices, repeated_matrix], axis=-1)\n",
        "    tiled_matrix = tf.reshape(tiled_matrix, [-1,3])\n",
        "    return tiled_matrix\n",
        "\n",
        "\n",
        "def two_body_hamiltonian_tf(t_basis, m, energy_batch, G_batched, rho_1_arrays, rho_2_arrays, indices):\n",
        "    # SECCIÓN ENERGIAS\n",
        "    ## Dado un batch de niveles, lo pasamos a TF\n",
        "    #energy_matrix = tf.constant(energy_batch, dtype=tf.float32)\n",
        "    energy_matrix = energy_batch  \n",
        "    energy_matrix = tf.repeat(energy_matrix, repeats=2, axis=1) ## Repetimos los niveles para cada uno de los pares (por el nivel k y kbar)\n",
        "    ## Generamos la matrix diagonal y expandimos\n",
        "    energy_matrix_expanded = tf.linalg.diag(energy_matrix)\n",
        "    energy_matrix_expanded = energy_matrix_expanded[:, :, :, np.newaxis, np.newaxis]\n",
        "    rho_1_gen_transposed = tf.transpose(rho_1_arrays, perm=[1, 0, 2, 3])\n",
        "    # Multiplicamos por los operadores C^dag C\n",
        "    h0_arr = tf.reduce_sum(energy_matrix_expanded * rho_1_gen_transposed[np.newaxis,:,:,:,:], axis=[1,2])\n",
        "\n",
        "    # SECCIÓN INTERACCIÓN\n",
        "    # Ya tenemos los indices de updates, ahora tomamos la mat en t_basis (una de zeros)\n",
        "    # y updateamos de acuerdo a la lista de G's cada uno flatteneados\n",
        "    # G_flatten = np.ndarray.flatten(np.array([np.ndarray.flatten(G) for G in G_batched])) cambiado por una versión compatible con TF\n",
        "\n",
        "    G_flatten = tf.reshape(G_batched, [-1])\n",
        "    G_flatten = tf.cast(G_flatten, tf.float32)\n",
        "    indices = tf.slice(indices, [0, 0], [tf.size(G_flatten), -1]) # puede que nos pasen menos updates en el loop de entrenamiento!\n",
        "\n",
        "    # Creamos la mat de t_basis y updateamos a partir de los indices de kkbar\n",
        "    mat = tf.zeros((len(energy_batch), t_basis.size, t_basis.size), dtype=tf.float32)\n",
        "    \n",
        "    mat = tf.tensor_scatter_nd_update(mat, indices, G_flatten)\n",
        "    # Preparamos las dimensiones y multiplicamos\n",
        "    mat_expanded = mat[:, :, :, np.newaxis, np.newaxis]\n",
        "    rho_2_gen_transposed = tf.transpose(rho_2_arrays, perm=[1, 0, 2, 3])\n",
        "    hi_arr = tf.reduce_sum(mat_expanded * rho_2_gen_transposed[np.newaxis,:,:,:,:], axis=[1,2])\n",
        "\n",
        "    return h0_arr - hi_arr\n",
        "\n",
        "def two_body_hamiltonian_tf_arb(t_basis, m, energy_seed, G_batched, rho_1_arrays, rho_2_arrays, indices):\n",
        "    # SECCIÓN ENERGIAS\n",
        "    ## Dado un seed de niveles diagonal construimos la mat simétrica dxd que multiplicara a c^dag_i c_j\n",
        "    diagonal = np.zeros((gpu_batch_size, 2 * m, 2 * m))\n",
        "    diagonal[:, np.arange(2 * m), np.arange(2 * m)] = energy_seed\n",
        "    energy_matrix_expanded = diagonal\n",
        "    \n",
        "    # HARDODEADO SOLO CASO 2C!\n",
        "    #energy_matrix_expanded = np.array([np.eye(2*m)*4 for _ in range(gpu_batch_size)])\n",
        "\n",
        "    ## Generamos la matriz y expandimos\n",
        "    energy_matrix_expanded = energy_matrix_expanded[:, :, :, np.newaxis, np.newaxis]\n",
        "    rho_1_gen_transposed = tf.transpose(rho_1_arrays, perm=[1, 0, 2, 3])\n",
        "\n",
        "    # Multiplicamos por los operadores C^dag C\n",
        "    h0_arr = tf.reduce_sum(energy_matrix_expanded * rho_1_gen_transposed[np.newaxis,:,:,:,:], axis=[1,2])\n",
        "\n",
        "    # SECCIÓN INTERACCIÓN\n",
        "    # Ya tenemos los indices de updates, ahora tomamos la mat en t_basis (una de zeros)\n",
        "    # y updateamos de acuerdo a la lista de G's cada uno flatteneados\n",
        "    \n",
        "    # Creamos la mat de t_basis, nada más que hacer! los coeficientes están dados. Bueno, y simetrizar\n",
        "    int_mat = np.zeros((gpu_batch_size, t_basis.size, t_basis.size))\n",
        "\n",
        "    idx = np.triu_indices(t_basis.size)\n",
        "    int_mat[:, idx[0], idx[1]] = G_batched\n",
        "    diagonal = np.einsum('ijk,ijk->ijk', int_mat, np.eye(t_basis.size)[np.newaxis,::])\n",
        "    int_mat = int_mat + np.transpose(int_mat, axes=(0,2,1)) - diagonal\n",
        "\n",
        "    # Preparamos las dimensiones y multiplicamos\n",
        "    int_mat_expanded = int_mat[:, :, :, np.newaxis, np.newaxis]\n",
        "    rho_2_gen_transposed = tf.transpose(rho_2_arrays, perm=[1, 0, 2, 3])\n",
        "    hi_arr = tf.reduce_sum(int_mat_expanded * rho_2_gen_transposed[np.newaxis,:,:,:,:], axis=[1,2])\n",
        "\n",
        "    return h0_arr - hi_arr\n",
        "\n",
        "def state_energy(state, h_arr):\n",
        "    return tf.linalg.trace(tf.matmul(state, h_arr))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[[4., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 4., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 4., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 4., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 4., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 4., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 4., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 4.]],\n",
              "\n",
              "       [[4., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 4., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 4., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 4., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 4., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 4., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 4., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 4.]]])"
            ]
          },
          "execution_count": 246,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.array([np.eye(2*m)*4 for _ in range(2)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emVBTg2QD-Fg"
      },
      "source": [
        "## Modelo de ML\n",
        "Basado en matrices densidad de 1 y 2 cuerpos como input, con hamiltoniano como salida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {
        "id": "aF_Ec_mCGX96"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1726001646.854929    4036 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1726001646.855230    4036 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1726001646.855393    4036 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1726001646.855618    4036 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1726001646.855783    4036 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-09-10 20:54:06.855927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /device:GPU:0 with 14785 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "execution_count": 247,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tf.test.gpu_device_name()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJDoa6LUJJ8O",
        "outputId": "73481454-fbcb-469f-d72f-cd0f8d534808"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "28\n",
            "[[1 1 0 0 0 0 0 0]\n",
            " [1 0 1 0 0 0 0 0]\n",
            " [1 0 0 1 0 0 0 0]\n",
            " [1 0 0 0 1 0 0 0]\n",
            " [1 0 0 0 0 1 0 0]\n",
            " [1 0 0 0 0 0 1 0]\n",
            " [1 0 0 0 0 0 0 1]\n",
            " [0 1 1 0 0 0 0 0]\n",
            " [0 1 0 1 0 0 0 0]\n",
            " [0 1 0 0 1 0 0 0]\n",
            " [0 1 0 0 0 1 0 0]\n",
            " [0 1 0 0 0 0 1 0]\n",
            " [0 1 0 0 0 0 0 1]\n",
            " [0 0 1 1 0 0 0 0]\n",
            " [0 0 1 0 1 0 0 0]\n",
            " [0 0 1 0 0 1 0 0]\n",
            " [0 0 1 0 0 0 1 0]\n",
            " [0 0 1 0 0 0 0 1]\n",
            " [0 0 0 1 1 0 0 0]\n",
            " [0 0 0 1 0 1 0 0]\n",
            " [0 0 0 1 0 0 1 0]\n",
            " [0 0 0 1 0 0 0 1]\n",
            " [0 0 0 0 1 1 0 0]\n",
            " [0 0 0 0 1 0 1 0]\n",
            " [0 0 0 0 1 0 0 1]\n",
            " [0 0 0 0 0 1 1 0]\n",
            " [0 0 0 0 0 1 0 1]\n",
            " [0 0 0 0 0 0 1 1]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\nm = 1\\nm1_basis = fixed_basis(m, d)\\nprint(m1_basis.size)\\nprint(m1_basis.base)\\nnm1_basis = fixed_basis(basis.m-m, d)\\n'"
            ]
          },
          "execution_count": 248,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Construccion de bases para calculo de rho1 y rho2\n",
        "# rho2\n",
        "m = 2\n",
        "m2_basis = fixed_basis(m, d, pairs=pairs)\n",
        "print(m2_basis.size)\n",
        "nm2_basis = fixed_basis(basis.m-m, d, pairs=pairs)\n",
        "print(nm2_basis.base)\n",
        "t_basis = fixed_basis(2, basis.d, pairs=pairs)\n",
        "# rho1\n",
        "\"\"\"\n",
        "m = 1\n",
        "m1_basis = fixed_basis(m, d)\n",
        "print(m1_basis.size)\n",
        "print(m1_basis.base)\n",
        "nm1_basis = fixed_basis(basis.m-m, d)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oapxWkD16fHg"
      },
      "source": [
        "### Algunos benchmarks y funciones auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {
        "id": "umCIrxCZKXQd"
      },
      "outputs": [],
      "source": [
        "# Given h calculo en rho2 y rho1 máximo\n",
        "def rho1_rho2(h, beta):\n",
        "    fund = thermal_state(h, beta)\n",
        "    rho2 = np.array(rho_2(basis, m2_basis.size, state, rho_2_arrays))\n",
        "    r = np.sort(linalg_d.eigvals(rho2).real)\n",
        "    rho_2_max = r[0]\n",
        "    rho1 = np.array(rho_1(basis, state, rho_1_arrays))\n",
        "    r = np.sort(linalg_d.eigvals(rho1).real)\n",
        "    rho_1_max = r[0]\n",
        "\n",
        "    return (rho_1_max, rho_2_max)\n",
        "\n",
        "def fill_triangular_np(x):\n",
        "    m = x.shape[0]\n",
        "    n = np.int32(np.sqrt(.25 + 2 * m) - .5)\n",
        "    x_tail = x[(m - (n**2 - m)):]\n",
        "    return np.triu(np.concatenate([x, x_tail[::-1]], 0).reshape(n, n))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "id": "QaNnIIc5bZux"
      },
      "outputs": [],
      "source": [
        "# TEST: Las funciones de TF y comunes coinciden\n",
        "\n",
        "# Dado h, \\beta, construyo el estado térmico\n",
        "from scipy.linalg import expm\n",
        "\n",
        "def thermal_state(h, beta):\n",
        "    quotient = expm(-beta*h)\n",
        "    return quotient / np.trace(quotient)\n",
        "\n",
        "## NO usar para mat no hermiticas\n",
        "@nb.jit(nopython=True)\n",
        "def thermal_state_eig(h, beta):\n",
        "    w, v = np.linalg.eigh(-beta*h)\n",
        "    D = np.diag(np.exp(w))\n",
        "    mat = v @ D @ v.T\n",
        "    mat = mat / np.trace(mat)\n",
        "    return mat\n",
        "    \n",
        "def gen_to_h(base, rho_1_arrays):\n",
        "    triag = fill_triangular_np(base)\n",
        "    body_gen = triag + np.transpose(triag)-np.diag(np.diag(triag))\n",
        "    h = np.array(base_hamiltonian(body_gen, basis, rho_1_arrays))  \n",
        "    return h \n",
        "\n",
        "def gen_to_h_1b(hamil_base):\n",
        "    triag = tfp.math.fill_triangular(hamil_base, upper=True)\n",
        "    body_gen = triag + tf.transpose(triag, perm=[0,2,1])-tf.linalg.diag(tf.linalg.diag_part(triag))\n",
        "    return body_gen\n",
        "\n",
        "def gen_to_h_tf(hamil_base, rho_1_arrays):\n",
        "    triag = tfp.math.fill_triangular(hamil_base, upper=True)\n",
        "    body_gen = triag + tf.transpose(triag, perm=[0,2,1])-tf.linalg.diag(tf.linalg.diag_part(triag)) # Simetrizamos y generamos la matriz de h\n",
        "    hamil_expanded = body_gen[:, :, :, np.newaxis, np.newaxis]\n",
        "    rho_1_gen_transposed = tf.transpose(rho_1_arrays, perm=[1, 0, 2, 3])\n",
        "    h_arr = tf.reduce_sum(hamil_expanded * rho_1_gen_transposed[np.newaxis,:,:,:,:], axis=[1,2])\n",
        "    return h_arr\n",
        "\n",
        "def thermal_state_tf(h):\n",
        "    # Assume beta=1\n",
        "    exp_hamiltonian = tf.linalg.expm(-h)\n",
        "    partition_function = tf.linalg.trace(exp_hamiltonian)\n",
        "    partition_function = tf.expand_dims(partition_function, axis=1)\n",
        "    partition_function = tf.expand_dims(partition_function, axis=1)\n",
        "    \n",
        "    rho = exp_hamiltonian / partition_function\n",
        "\n",
        "    return rho\n",
        "\n",
        "def rho_1_tf(state, rho_1_arrays):\n",
        "    state = tf.expand_dims(state, axis=1)  # Shape: (5120, 10, 1, 10)\n",
        "    state_expanded = tf.expand_dims(state, axis=1)\n",
        "    rho_1_arrays_expanded = tf.expand_dims(rho_1_arrays, axis=0)  # Shape: (1, 5, 5, 10, 10)\n",
        "    product = state_expanded * rho_1_arrays_expanded  # Shape: (5120, 10, 5, 10, 10)\n",
        "    mat = tf.reduce_sum(product, axis=[-2, -1])  # Shape: (5120, 5, 5)\n",
        "    \n",
        "    return mat\n",
        "\n",
        "def rho_2_tf(state, rho_2_arrays):\n",
        "    state = tf.expand_dims(state, axis=1)  # Shape: (5120, 10, 1, 10)\n",
        "    state_expanded = tf.expand_dims(state, axis=1)\n",
        "    rho_2_arrays_expanded = tf.expand_dims(rho_2_arrays, axis=0)  # Shape: (1, 5, 5, 10, 10)\n",
        "    product = state_expanded * rho_2_arrays_expanded  # Shape: (5120, 10, 5, 10, 10)\n",
        "    mat = tf.reduce_sum(product, axis=[-2, -1])  # Shape: (5120, 5, 5)\n",
        "    \n",
        "    return mat\n",
        "\n",
        "# NOTA: para calcular el bloque rho2kkbar, utilizar en lugar\n",
        "\n",
        "def rho_1_gc_tf(hamil_base):\n",
        "    e, v = tf.linalg.eigh(gen_to_h_1b(hamil_base))\n",
        "    result = 1 / (1 + tf.exp(e))\n",
        "    result = tf.linalg.diag(result)\n",
        "    res = tf.linalg.matmul(v,result)\n",
        "    res = tf.linalg.matmul(res,v,adjoint_b=True)\n",
        "    \n",
        "    return tf.cast(res, tf.float32)\n",
        "\n",
        "# Aux function\n",
        "def outer_product(vector):\n",
        "    return tf.einsum('i,j->ij', vector, vector)\n",
        "\n",
        "def pure_state(h):\n",
        "    e, v = tf.linalg.eigh(h)\n",
        "    fund = v[:,:,0]\n",
        "    d = tf.map_fn(outer_product, fund)\n",
        "    return d\n",
        "\n",
        "# Casos de entrenamiento tipo mat gaussianas\n",
        "def gen_gauss_mat(G, sigma_sq, size):\n",
        "    indices = np.arange(size)\n",
        "    mat = G * np.exp(-((indices - indices[:, np.newaxis])**2) / (2 * sigma_sq))\n",
        "    return mat\n",
        "\n",
        "def gen_gauss_mat_np(G_values, sigma_sq_values, size):\n",
        "    indices = np.arange(size, dtype=np.float32)\n",
        "    indices_diff = indices - indices[:, np.newaxis]\n",
        "\n",
        "    mat = G_values[:, np.newaxis, np.newaxis] * np.exp(-np.square(indices_diff) / (2 * sigma_sq_values[:, np.newaxis, np.newaxis]))\n",
        "\n",
        "    return mat\n",
        "\n",
        "# Casos de entrenamiento tipo matriz vectorial\n",
        "def gen_vect_mat(size, g_init, g_stop, sym = True):\n",
        "    if sym:\n",
        "        vect = np.sort(np.random.uniform(g_init, g_stop, size // 2))[::-1]\n",
        "        vect = np.repeat(vect, 2)\n",
        "        if size % 2 != 0: # TODO: Agregar tipo en el medio\n",
        "            raise ValueError\n",
        "    else:\n",
        "        vect = np.sort(np.random.uniform(g_init, g_stop, size))[::-1]\n",
        "    indices = np.abs(np.arange(size)[:, np.newaxis] - np.arange(size))\n",
        "    mat = vect[indices]\n",
        "\n",
        "    return vect, mat\n",
        "\n",
        "def gen_gauss_plus_vect(G_values, sigma_sq_values, size):\n",
        "    indices = np.arange(size//2, dtype=np.float32)\n",
        "    vect = G_values[:, np.newaxis] * np.exp(-np.square(indices) / (2 * sigma_sq_values[:, np.newaxis]))\n",
        "    return vect\n",
        "\n",
        "def gen_random_arr(h_labels):\n",
        "    matrices = np.zeros((gpu_batch_size, basis.m, basis.m))\n",
        "    up_idx = np.triu_indices(basis.m, 1)\n",
        "    matrices[:, up_idx[0], up_idx[1]] = h_labels\n",
        "    matrices += matrices.transpose(0, 2, 1)\n",
        "\n",
        "    return matrices\n",
        "\n",
        "def random_fermi_arr(g_init, g_stop, s = basis.m):\n",
        "    # En primer lugar, construimos la mat simétrica con respecto a nivel de Fermi\n",
        "    seed = np.round(np.random.uniform(g_init, g_stop,(s//2, s//2)), 2)\n",
        "    mat = np.zeros((s, s))\n",
        "    for i in range(s):\n",
        "        for j in range(s):\n",
        "            conv = lambda x: s//2 - 1 - np.min([x,s-1-x]) \n",
        "            mat[i,j] = seed[conv(i), conv(j)]\n",
        "\n",
        "    # Simetrizamos\n",
        "    mat = (mat + mat.T)/2\n",
        "    # Volamos la diagonal + antidiagonal\n",
        "    mat = mat - np.diag(np.diag(mat)) - np.diag(np.diag(mat))[::-1]\n",
        " \n",
        "    # Los labels se buscan de la siguiente manera\n",
        "    #up_idx = np.triu_indices(basis.m//2, 1)\n",
        "    #mat[up_idx]\n",
        "\n",
        "    return mat\n",
        "\n",
        "def random_fermi_arr_inv(seed, s=basis.m, obj = False):\n",
        "    up_idx = np.triu_indices(s//2, 1)\n",
        "    reb = np.zeros((s//2,s//2))\n",
        "    if obj:\n",
        "        reb = np.zeros((s//2,s//2), dtype=object)\n",
        "    reb[up_idx] = seed\n",
        "    reb = reb + reb.T\n",
        "    rebsymm = reb[::-1]\n",
        "    res = np.block([[reb,np.flip(rebsymm)],[rebsymm,np.flip(reb)]])\n",
        "\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylpy_BCw6jxF"
      },
      "source": [
        "### Construccion de dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Version sincrónica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2is_Eo_qGpEz",
        "outputId": "9a968190-59f2-4695-ef18-b99ff5b4a212"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "from typing import Literal\n",
        "\n",
        "# Config\n",
        "#num_samples = 1500\n",
        "gpu_batch_size = 256 # 256\n",
        "en_batch = [np.arange(0, basis.m) - basis.m//2 + 1/2 for _ in range(0,gpu_batch_size)] \n",
        "en_batch = tf.constant(en_batch, dtype=tf.float32)\n",
        "energy_batch = en_batch\n",
        "\n",
        "# Beta\n",
        "beta = 1\n",
        "\n",
        "# Construccion de parametros y matrices auxiliares\n",
        "#rho1_size = m1_basis.size\n",
        "rho2_size = m2_basis.size\n",
        "rho2kkbar_size = basis.m\n",
        "input_shape = (basis.m,basis.m, 1) # Usando rho2kkbar como input batcheado\n",
        "fund_size = basis.size\n",
        "hamil_base_size = basis.d*(basis.d+1)//2\n",
        "rho_1_arrays = rho_1_gen(basis)\n",
        "rho_1_arrays_tf = tf.constant(rho_1_arrays, dtype=tf.float32)\n",
        "rho_2_arrays = rho_2_gen(basis, nm2_basis, m2_basis)\n",
        "rho_2_arrays_tf = tf.constant(rho_2_arrays, dtype=tf.float32)\n",
        "rho_2_arrays_kkbar = rho_2_kkbar_gen(t_basis, rho_2_arrays)\n",
        "rho_2_arrays_kkbar_tf = tf.constant(rho_2_arrays_kkbar, dtype=tf.float32)\n",
        "k_indices = get_kkbar_indices(t_basis)\n",
        "k_indices_tf = gen_update_indices(t_basis, gpu_batch_size)\n",
        "\n",
        "\n",
        "# Generación de dataset (params)\n",
        "# h_type = {const, gaussian, random}: const = proporcional a ones, gaussian = proporcional a mat gaussiana, random = full random \n",
        "# g_init, g_stop: rango de Gs (aplica a los 3 casos)\n",
        "# state_type = {thermal, gs}: tipo de estado (térmico o funalmental)\n",
        "# input_type = {rho2, rho1}: tipo de feature a calcular\n",
        "valid_h_type = Literal['const', 'gaussian', 'vect', 'gaussvect', 'random', 'vectnosymm', 'randomsymm', 'randomenerg']\n",
        "valid_state_type = Literal['thermal', 'gs']\n",
        "valid_input_type = Literal['rho2', 'rho1', 'rho1+rho2']\n",
        "\n",
        "# Funciones auxiliares\n",
        "indices = tf.abs(tf.range(basis.m)[:, tf.newaxis] - tf.range(basis.m))\n",
        "def gather_elements(x):\n",
        "    return tf.gather(x, indices)\n",
        "\n",
        "def gen_dataset(h_type: valid_h_type, g_init: float, g_stop: float, state_type: valid_state_type, input_type: valid_input_type, include_energy: bool, en_batch=en_batch, num_samples = 100000, arb = False):\n",
        "    print(tf.test.gpu_device_name())\n",
        "    datasets = []\n",
        "    for i in tqdm(range(num_samples//gpu_batch_size+1)):\n",
        "        size = basis.m*(basis.m+1)//2\n",
        "        # En una primera versión vamos a pasar una mat proporcional a range(0,m) para energias (DEFINIDO EN CONFIG)\n",
        "\n",
        "        ## Caso G proporcional a ones\n",
        "        if h_type == 'const':\n",
        "            label_size = 1 \n",
        "            h_labels = [np.random.uniform(g_init, g_stop) for _ in range(0,gpu_batch_size)]\n",
        "            g_arr = [np.ones((basis.m, basis.m))*g_seed for g_seed in h_labels]\n",
        "            g_arr = tf.constant(g_arr, dtype=tf.float32)\n",
        "\n",
        "\n",
        "        ## Caso generico\n",
        "        elif h_type == 'random':\n",
        "            label_size = basis.m*(basis.m-1)// 2  # CASO GENERICO elementos independientes de una mat de m x m sin diagonal\n",
        "            h_labels = [np.random.uniform(g_init, g_stop, label_size) for _ in range(0,gpu_batch_size)] \n",
        "            # Construimos la mat G\n",
        "            g_arr = gen_random_arr(h_labels)\n",
        "            h_labels = tf.constant(h_labels, dtype=tf.float32)\n",
        "            \n",
        "        elif h_type == 'randomsymm':\n",
        "            label_size = len(np.triu_indices(basis.m//2, 1)[0])\n",
        "            g_arr = [random_fermi_arr(g_init, g_stop) for _ in range(gpu_batch_size)]\n",
        "            # Ahora extraemos los labels\n",
        "            up_idx = np.triu_indices(basis.m//2, 1)\n",
        "            h_labels = [mat[up_idx] for mat in g_arr]\n",
        "            h_labels = tf.constant(h_labels, dtype=tf.float32)\n",
        "\n",
        "        elif h_type == 'vect':\n",
        "            label_size = basis.m - 1\n",
        "            #labels_gen = lambda x: np.sort(np.random.uniform(g_init, g_stop, basis.m // 2 - 1))[::-1]\n",
        "            labels_gen = lambda x: np.random.uniform(g_init, g_stop, basis.m - 1)\n",
        "            h_labels = [np.insert(labels_gen(0), 0, 0) for _ in range(0, gpu_batch_size)] # OJO CON EL DIAGONAL!\n",
        "            indices = np.abs(np.arange(basis.m)[:, np.newaxis] - np.arange(basis.m))\n",
        "            g_arr = [x[indices] for x in h_labels] \n",
        "            h_labels = [x[1:] for x in h_labels] # removemos el 0 agregado por el termino diagonal\n",
        "            h_labels = tf.constant(h_labels, dtype=tf.float32)\n",
        "            g_arr[0]\n",
        "            g_arr = tf.constant(g_arr, dtype=tf.float32)\n",
        "\n",
        "        elif h_type == 'vectnosymm':\n",
        "            label_size = basis.m - 1 \n",
        "            labels_gen = lambda x: np.sort(np.random.uniform(g_init, g_stop, basis.m - 1))[::-1]\n",
        "            h_labels = [np.insert(labels_gen(0), 0, 0) for _ in range(0, gpu_batch_size)] # OJO CON EL DIAGONAL!\n",
        "            indices = np.abs(np.arange(basis.m)[:, np.newaxis] - np.arange(basis.m))\n",
        "            g_arr = [x[indices] for x in h_labels] \n",
        "            h_labels = [x[1:] for x in h_labels] # removemos el 0 agregado por el termino diagonal\n",
        "            h_labels = tf.constant(h_labels, dtype=tf.float32)\n",
        "            g_arr = tf.constant(g_arr, dtype=tf.float32)\n",
        "\n",
        "        ## Caso reducido\n",
        "        elif h_type == 'gaussian':\n",
        "            label_size = 2\n",
        "            h_labels = np.array([[np.random.uniform(g_init, g_stop), np.random.random()*10 + 0.1] for _ in range(0, gpu_batch_size)])\n",
        "            g_arr = gen_gauss_mat_np(h_labels[:,0], h_labels[:,1], basis.m)\n",
        "            h_labels = tf.constant(h_labels, dtype=tf.float32)\n",
        "            g_arr = tf.constant(g_arr, dtype=tf.float32)\n",
        "\n",
        "        elif h_type == 'gaussvect':\n",
        "            label_size = 2\n",
        "            h_labels = np.array([[np.random.uniform(g_init, g_stop), np.random.random()*2 + 0.1] for _ in range(0, gpu_batch_size)])\n",
        "            vect_arr = gen_gauss_plus_vect(h_labels[:,0], h_labels[:,1], basis.m)\n",
        "            indices = np.abs(np.arange(basis.m)[:, np.newaxis] - np.arange(basis.m))\n",
        "            g_arr = [np.repeat(x,2)[indices] for x in vect_arr]\n",
        "            g_arr = [g_arr[k] - np.diag(np.diag(g_arr[k])) for k in range(gpu_batch_size)]\n",
        "            h_labels = tf.constant(h_labels, dtype=tf.float32)\n",
        "            g_arr = tf.constant(g_arr, dtype=tf.float32)\n",
        "\n",
        "\n",
        "        # HAMILTONIANOS GENERALES\n",
        "        elif h_type == 'randomenerg':\n",
        "            # Energias\n",
        "            label_size_en = 2*basis.m\n",
        "            en_batch = np.random.uniform(g_init*4, g_stop*4,(gpu_batch_size, label_size_en))\n",
        "            en_batch = tf.constant(en_batch, dtype=tf.float32)\n",
        "            h_labels_en = en_batch\n",
        "            # Interacción\n",
        "            label_size_int = t_basis.size * (t_basis.size + 1)//2\n",
        "            h_labels_int = np.random.uniform(g_init, g_stop,(gpu_batch_size, label_size_int))\n",
        "            g_arr = tf.constant(h_labels_int, dtype=tf.float32)\n",
        "            # Combinamos\n",
        "            label_size = label_size_en + label_size_int\n",
        "            h_labels = tf.concat([h_labels_en, h_labels_int], axis=-1)\n",
        "            #h_labels = h_labels_int\n",
        "            #label_size = label_size_int\n",
        "\n",
        "        else:\n",
        "            raise ValueError\n",
        "        \n",
        "        # Construimos los hamiltonianos basados en g_arr\n",
        "        if not arb:\n",
        "            h_arr = two_body_hamiltonian_tf(t_basis, basis.m, en_batch, g_arr, rho_1_arrays, rho_2_arrays, k_indices_tf)\n",
        "        else:\n",
        "            h_arr = two_body_hamiltonian_tf_arb(t_basis, basis.m, en_batch, g_arr, rho_1_arrays, rho_2_arrays, k_indices_tf)\n",
        "\n",
        "        # Calculamos los estados\n",
        "        if state_type == 'thermal':\n",
        "            state = thermal_state_tf(h_arr*beta) \n",
        "            state = tf.cast(state, dtype=tf.float32)\n",
        "        else:\n",
        "            state = pure_state(h_arr)\n",
        "        \n",
        "        # Calculamos la feature\n",
        "        if input_type == 'rho2':\n",
        "            rho_input = rho_2_tf(state, rho_2_arrays_kkbar_tf) \n",
        "        elif input_type == 'rho1+rho2':\n",
        "            rho_2_input = rho_2_tf(state, rho_2_arrays_tf) # ! CAMBIAR POR rho_2_arrays_kkbar_tf SI ES RHO2KKBAR\n",
        "            rho_1_input = rho_1_tf(state, rho_1_arrays_tf) \n",
        "        else:\n",
        "            rho_input = rho_1_tf(state, rho_1_arrays_tf)\n",
        "        \n",
        "        # Calculamos la enegia\n",
        "        if include_energy:\n",
        "            energy = state_energy(state, h_arr)\n",
        "\n",
        "        # OUTPUTS\n",
        "        # Caso input eigvals\n",
        "        #input_shape = (basis.m, 1)\n",
        "        #rho_2_input = tf.linalg.eigvals(rho_2_input)\n",
        "        #rho_2_input = tf.sort(tf.math.real(rho_2_input), axis=-1)\n",
        "\n",
        "        # Caso PCA\n",
        "        #input_shape = (num_gen, 1)\n",
        "        #rflat = np.array([np.ndarray.flatten(x) for x in rho_2_input.numpy()])\n",
        "        #rho_2_input = np.dot(rflat, P)\n",
        "\n",
        "        # Generación de dataset\n",
        "        # Tradicional (rho2 tipo matricial)\n",
        "        if input_type == 'rho1' or input_type == 'rho2':\n",
        "            if include_energy:\n",
        "                datasets.append(tf.data.Dataset.from_tensor_slices(((rho_input, energy), h_labels)))\n",
        "            else:\n",
        "                datasets.append(tf.data.Dataset.from_tensor_slices(((rho_input), h_labels)))\n",
        "        else:\n",
        "            datasets.append(tf.data.Dataset.from_tensor_slices(((rho_1_input, rho_2_input, energy), h_labels)))\n",
        "        # Rho2 flatteneada, requerido para DF\n",
        "        #datasets.append(tf.data.Dataset.from_tensor_slices(((tf.reshape(rho_2_input, (gpu_batch_size,basis.m*basis.m))), h_labels)))\n",
        "        #datasets.append(tf.data.Dataset.from_tensor_slices(((rho_1_input, rho_2_input), h_labels)))\n",
        "        #datasets.append(tf.data.Dataset.from_tensor_slices(((rho_1_input, rho_2_input, state), h_labels)))\n",
        "    ds = tf.data.Dataset.from_tensor_slices(datasets)\n",
        "    dataset = ds.interleave(\n",
        "        lambda x: x,\n",
        "        cycle_length=1,\n",
        "        num_parallel_calls=tf.data.AUTOTUNE,\n",
        "    )\n",
        "\n",
        "    return dataset, label_size\n",
        "\n",
        "\n",
        "#batch_size = 32\n",
        "#dataset = dataset.shuffle(buffer_size=num_samples).batch(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Filleo de dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8moZIlfabZuy"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Dividimos los datasets\n",
        "train_size = int(0.8 * num_samples)\n",
        "\n",
        "train_dataset = dataset.take(train_size)\n",
        "val_dataset = dataset.skip(train_size)\n",
        "\n",
        "\n",
        "batch_size = gpu_batch_size\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "val_dataset = val_dataset.batch(batch_size)\n",
        "\n",
        "#beta_val = beta_input[train_size:]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cardinality no funciona con los datasets generados por GPU\n",
        "\"\"\"\n",
        "val_size = tf.data.experimental.cardinality(val_dataset).numpy()\n",
        "print(\"Validation Dataset Size:\", val_size)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PCA\n",
        "\"\"\"\n",
        "# Correr una vez para definir la transformacion y lyego volver a correr la gen de dataset\n",
        "num_gen = 10\n",
        "rflat = np.array([np.ndarray.flatten(x) for x in rho_input.numpy()])\n",
        "rflat = rflat - rflat.mean()\n",
        "rflat = rflat / rflat.std()\n",
        "U, S, Vh = np.linalg.svd(rflat)\n",
        "print(S)\n",
        "\n",
        "# Determinado automáticamente\n",
        "num_gen = np.where(S < 0.1)[0][0]\n",
        "print(num_gen)\n",
        "\n",
        "Z = np.dot(rflat.T, rflat)\n",
        "eigenvalues, eigenvectors = np.linalg.eig(Z)\n",
        "D = np.diag(eigenvalues)\n",
        "P = eigenvectors[:,0:num_gen]\n",
        "Z_new = np.dot(Z, P)\n",
        "Z_new.shape\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DNN y CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYEEjNB-7b8y"
      },
      "source": [
        "### Definición de modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gen_dnn_model(label_size, input_type, include_energy: bool, res = 1):\n",
        "    if input_type == 'rho1':\n",
        "        rho_layer =  tf.keras.layers.Input(shape=(basis.d, basis.d, 1), name='rho')\n",
        "    elif input_type == 'rho2':\n",
        "        rho_layer =  tf.keras.layers.Input(shape=(basis.m, basis.m, 1), name='rho')\n",
        "    else:\n",
        "        rho_1_layer =  tf.keras.layers.Input(shape=(basis.d, basis.d, 1), name='rho1')\n",
        "        rho_2_layer =  tf.keras.layers.Input(shape=(t_basis.size, t_basis.size, 1), name='rho2')\n",
        "\n",
        "    if include_energy:\n",
        "        energy_layer = tf.keras.layers.Input(shape=(1, ), name='energy')\n",
        "\n",
        "    if input_type == 'rho1' or input_type == 'rho2':\n",
        "        flatten_rho = tf.keras.layers.Flatten()(rho_layer)\n",
        "        #flatten_rho2 = tf.keras.layers.BatchNormalization()(flatten_rho2)\n",
        "        if include_energy:\n",
        "            dense1 = tf.keras.layers.concatenate([flatten_rho, energy_layer]) \n",
        "        else:\n",
        "            dense1 = tf.keras.layers.concatenate([flatten_rho]) \n",
        "    else:\n",
        "        flatten_rho_1 = tf.keras.layers.Flatten()(rho_1_layer)\n",
        "        flatten_rho_2 = tf.keras.layers.Flatten()(rho_2_layer)\n",
        "        #flatten_rho2 = tf.keras.layers.BatchNormalization()(flatten_rho2)\n",
        "        dense1 = tf.keras.layers.concatenate([flatten_rho_1, flatten_rho_2, energy_layer])    \n",
        "\n",
        "    local_size = label_size\n",
        "    l = 4 + (res-1)\n",
        "    layer_s = [64//i*2 * res for i in reversed(range(1,l))]\n",
        "    for i in range(0,l-1):\n",
        "        dense1 = tf.keras.layers.Dense(layer_s[i], activation='sigmoid')(dense1)\n",
        "        #dense1 = tf.keras.layers.Dense(layer_s[i], activation='sigmoid', kernel_regularizer=tf.keras.regularizers.L1(0.001))(dense1)\n",
        "        #dense1 = tf.keras.layers.Dropout(0.1)(dense1)\n",
        "\n",
        "    output = tf.keras.layers.Dense(local_size)(dense1)\n",
        "    if input_type == 'rho1' or input_type == 'rho2':\n",
        "        if include_energy:\n",
        "            model = tf.keras.models.Model(inputs=[rho_layer, energy_layer], outputs=output)\n",
        "        else:\n",
        "            model = tf.keras.models.Model(inputs=[rho_layer], outputs=output)\n",
        "    else:\n",
        "        model = tf.keras.models.Model(inputs=[rho_1_layer, rho_2_layer, energy_layer], outputs=output)\n",
        "\n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "# NOT SUPPORTED FOR RHO1+RHO2\n",
        "def gen_cnn_model(label_size, input_type, include_energy: bool, res = 1):\n",
        "    if input_type == 'rho1':\n",
        "        rho_layer =  tf.keras.layers.Input(shape=(basis.d, basis.d, 1), name='rho')\n",
        "    elif input_type == 'rho2':\n",
        "        rho_layer =  tf.keras.layers.Input(shape=(basis.m, basis.m, 1), name='rho')\n",
        "    else:\n",
        "        rho_1_layer =  tf.keras.layers.Input(shape=(basis.d, basis.d, 1), name='rho1')\n",
        "        rho_layer =  tf.keras.layers.Input(shape=(t_basis.size, t_basis.size, 1), name='rho2')\n",
        "\n",
        "    if include_energy:\n",
        "        energy_layer = tf.keras.layers.Input(shape=(1, ), name='energy')\n",
        "\n",
        "    # CNN\n",
        "    # Factor de cantidad de filtros\n",
        "    lf = 8 * res\n",
        "    conv_limit = 2 \n",
        "    conv_rho = tf.keras.layers.Conv2D(lf*2**conv_limit, (2, 2), activation='relu')(rho_layer)\n",
        "    #conv_rho = tf.keras.layers.BatchNormalization()(conv_rho)\n",
        "    for j in [(2**conv_limit - 2**k) for k in range(1,conv_limit)]:\n",
        "        conv_rho = tf.keras.layers.Conv2D(lf*j, (3, 3), activation='relu')(conv_rho)\n",
        "        #conv_rho = tf.keras.layers.BatchNormalization()(conv_rho)\n",
        "    \n",
        "    # A rho1, en el caso 1+2 solo le aplicamos un filtro, porque es muy chica. Luego concatenamos\n",
        "    if input_type == 'rho1+rho2':\n",
        "        conv_rho1 = tf.keras.layers.Conv2D(lf*2**conv_limit, (2, 2), activation='relu')(rho_1_layer)\n",
        "        flatten_rho = tf.keras.layers.Flatten()(conv_rho)\n",
        "        flatten_rho_1 = tf.keras.layers.Flatten()(conv_rho1)\n",
        "        flatten_rho = tf.keras.layers.concatenate([flatten_rho, flatten_rho_1])\n",
        "\n",
        "    else:\n",
        "        flatten_rho = tf.keras.layers.Flatten()(conv_rho)\n",
        "\n",
        "    # DNN\n",
        "    #flatten_rho = tf.keras.layers.BatchNormalization()(flatten_rho)\n",
        "    if include_energy:\n",
        "        dense1 = tf.keras.layers.concatenate([flatten_rho, energy_layer]) \n",
        "    else:\n",
        "        dense1 = tf.keras.layers.concatenate([flatten_rho]) \n",
        "\n",
        "    local_size = label_size\n",
        "    l = 2 \n",
        "    layer_s = [16//i*2 * res for i in reversed(range(1,l))]\n",
        "    for i in range(0,l-1):\n",
        "        dense1 = tf.keras.layers.Dense(layer_s[i], activation='relu')(dense1)\n",
        "        #dense1 = tf.keras.layers.Dense(layer_s[i], activation='sigmoid', kernel_regularizer=tf.keras.regularizers.L1(0.001))(dense1)\n",
        "        #dense1 = tf.keras.layers.Dropout(0.1)(dense1)\n",
        "\n",
        "    output = tf.keras.layers.Dense(local_size)(dense1)\n",
        "    \n",
        "    if include_energy and input_type != 'rho1+rho2':\n",
        "        model = tf.keras.models.Model(inputs=[rho_layer, energy_layer], outputs=output)\n",
        "    elif input_type == 'rho1+rho2' and include_energy:\n",
        "        model = tf.keras.models.Model(inputs=[rho_1_layer, rho_layer, energy_layer], outputs=output)\n",
        "    else:\n",
        "        model = tf.keras.models.Model(inputs=[rho_layer], outputs=output)\n",
        "    model.summary()\n",
        "\n",
        "\n",
        "    return model\n",
        "    \n",
        "\n",
        "\n",
        "# Custom loss functions\n",
        "def base_vec_to_h(h_labels):\n",
        "    #h_labels = tf.stack(h_labels)\n",
        "    g_arr = tf.map_fn(gather_elements, h_labels) # Construyo la matrix g desde los labels\n",
        "    h_arr = two_body_hamiltonian_tf(t_basis, basis.m, en_batch, g_arr, rho_1_arrays, rho_2_arrays, k_indices_tf)\n",
        "\n",
        "    return h_arr\n",
        "\n",
        "def base_vect_loss(base_pred, base_true):\n",
        "    h_true = base_vec_to_h(base_true)\n",
        "    h_pred = base_vec_to_h(base_pred)\n",
        "    return tf.math.reduce_mean(tf.norm(h_pred - h_true, axis=(-2,-1), ord='fro'))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiWk9piJtNIZ"
      },
      "source": [
        "### Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhJCHf0fQdRl",
        "outputId": "1821cf27-9ff5-4d67-e9f5-956d20eda5e2"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import RMSprop, Adam, Nadam, Lion\n",
        "\n",
        "def dnn_fit(dataset, label_size, input_type, include_energy, cnn = True, loss = 'MSE', num_epochs = 50, res = 1):\n",
        "    if cnn:\n",
        "        model = gen_cnn_model(label_size, input_type, include_energy, res = res)\n",
        "    else:\n",
        "        model = gen_dnn_model(label_size, input_type, include_energy, res = res)\n",
        "\n",
        "    # Dividimos los datasets\n",
        "    train_size = int(0.8 * num_samples)\n",
        "\n",
        "    train_dataset = dataset.take(train_size)\n",
        "    val_dataset = dataset.skip(train_size)\n",
        "    \n",
        "    batch_size = gpu_batch_size\n",
        "    train_dataset = train_dataset.batch(batch_size)\n",
        "    val_dataset = val_dataset.batch(batch_size)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(),\n",
        "                loss=loss,\n",
        "                metrics=['accuracy', 'mean_squared_error'])\n",
        "\n",
        "    # Train the model\n",
        "    device_name = tf.test.gpu_device_name()\n",
        "\n",
        "    with tf.device('/gpu:0'):\n",
        "        #history = model.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset)\n",
        "        history = model.fit(train_dataset, epochs=num_epochs)\n",
        "\n",
        "    return model, val_dataset, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "cvpE_X1iTXcB",
        "outputId": "eff0e5f5-5b26-46ea-ec6b-491d1de9944c"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "#plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "#plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.plot(history.history['mean_squared_error'][75:], label='Training MSE')\n",
        "plt.plot(history.history['val_mean_squared_error'][75:], label='Validation MSE')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE')\n",
        "plt.title('MSE vs. Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# MIN LOSS = 0.0128 c/fund 50epochs MSE\n",
        "##         = 0.0118 s/fund 50epochs MSE\n",
        "##         = 0.0039 s/fund 50epochs MSE m=4 d=6\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRlZuRUNa6Yb",
        "outputId": "85850559-311b-4cf4-ea5b-465a9ee8a7af"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Assuming you have a validation dataset (val_dataset)\n",
        "iterador = iter(val_dataset)\n",
        "sample = next(iterador)\n",
        "next_sample = next(iterador)\n",
        "input_data = sample[0]  # Assuming your dataset provides input data as the first element\n",
        "actual_values = sample[1]  # Assuming your dataset provides actual labels as the second element\n",
        "\n",
        "# Predict using the model\n",
        "predictions = model.predict(input_data)\n",
        "\n",
        "#mean_squared_error(predictions, actual_values)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Diferencias en error RMSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def dnn_error_coef(model, val_dataset):\n",
        "    # Assuming you have a validation dataset (val_dataset)\n",
        "    iterador = iter(val_dataset)\n",
        "    sample = next(iterador)\n",
        "    next_sample = next(iterador)\n",
        "    input_data = sample[0]  # Assuming your dataset provides input data as the first element\n",
        "    actual_values = sample[1]  # Assuming your dataset provides actual labels as the second element\n",
        "\n",
        "    # Predict using the model\n",
        "    predictions = model.predict(input_data)\n",
        "\n",
        "    #mean_squared_error(predictions, actual_values)\n",
        "\n",
        "    # Vemos algunos valores\n",
        "    for e in val_dataset:\n",
        "        for i in range(0, 4):\n",
        "            print(e[1][i])\n",
        "            print(predictions[i])\n",
        "        break\n",
        "        \n",
        "    # Veamos el MSE de los valores de G      \n",
        "    #RMSE_pred = mean_squared_error(actual_values, predictions, squared=False)\n",
        "    #RMSE_rand = mean_squared_error(actual_values, next_sample[1], squared=False)\n",
        "    #print(RMSE_pred, RMSE_rand)\n",
        "    #rint(RMSE_rand/RMSE_pred)\n",
        "    # Veamos los errores en términos de MSE\n",
        "    if predictions.shape[1] == 1:\n",
        "        norm_pred = np.mean(np.abs(predictions.T-actual_values))\n",
        "        norm_rand = np.mean(np.abs(next_sample[1]-actual_values))\n",
        "    else:\n",
        "        norm_pred = np.mean(np.linalg.norm(predictions-actual_values,ord=2, axis=1))\n",
        "        norm_rand = np.mean(np.linalg.norm(next_sample[1]-actual_values,ord=2, axis=1))\n",
        "    print(norm_pred, norm_rand)\n",
        "    print(norm_rand / norm_pred)\n",
        "    # Veamos los errores en norma 2\n",
        "    if predictions.shape[1] == basis.m: # caso vectorial\n",
        "        norm_pred = base_vect_loss(predictions, actual_values)\n",
        "        norm_rand = base_vect_loss(next_sample[1], actual_values)\n",
        "    print(norm_pred, norm_rand)\n",
        "    print(norm_rand / norm_pred)\n",
        "\n",
        "    return(norm_rand / norm_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "iterador = iter(val_dataset)\n",
        "sample = next(iterador)\n",
        "next_sample = next(iterador)\n",
        "input_data = sample[0]  # Assuming your dataset provides input data as the first element\n",
        "actual_values = sample[1]  # Assuming your dataset provides actual labels as the second element\n",
        "\n",
        "# Predict using the model\n",
        "predictions = model.predict(input_data)\n",
        "predictions.shape\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Análisis rho2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reconstruye rho a partir de G\n",
        "# Codigo medio copiado de gen_dataset, not good\n",
        "def rho_reconstruction_tf(g_arr, h_type, state_type):\n",
        "    size = basis.m*(basis.m+1)//2\n",
        "    h_labels = g_arr\n",
        "    # A partir de aca es todo igual, salvo la parte de generación random\n",
        "\n",
        "    ## Caso G proporcional a ones\n",
        "    if h_type == 'const':\n",
        "        label_size = 1 \n",
        "        g_arr = [np.ones((basis.m, basis.m))*g_seed for g_seed in h_labels]\n",
        "        g_arr = tf.constant(g_arr, dtype=tf.float32)\n",
        "\n",
        "    ## Caso generico\n",
        "    elif h_type == 'random':\n",
        "        label_size = basis.m*(basis.m+1)// 2 # CASO GENERICO elementos independientes de una mat de m x m\n",
        "        # Construimos la mat G\n",
        "        triag = tfp.math.fill_triangular(h_labels, upper=True)\n",
        "        g_arr = triag + tf.transpose(triag, perm=[0,2,1])-tf.linalg.diag(tf.linalg.diag_part(triag))\n",
        "\n",
        "    elif h_type == 'vect':\n",
        "        symmetry = True # Necesario para la invesión de BCS\n",
        "        indices = np.abs(np.arange(basis.m)[:, np.newaxis] - np.arange(basis.m))\n",
        "        g_arr = [np.repeat(x,2)[indices] if symmetry else x[indices] for x in h_labels]\n",
        "        h_labels = tf.constant(h_labels, dtype=tf.float32)\n",
        "        g_arr = tf.constant(g_arr, dtype=tf.float32)\n",
        "\n",
        "    ## Caso reducido\n",
        "    elif h_type == 'gaussian':\n",
        "        g_arr = gen_gauss_mat_np(h_labels[:,0], h_labels[:,1], basis.m)\n",
        "        h_labels = tf.constant(h_labels, dtype=tf.float32)\n",
        "        g_arr = tf.constant(g_arr, dtype=tf.float32)\n",
        "\n",
        "    elif h_type == 'gaussvect':\n",
        "        label_size = 2\n",
        "        vect_arr = gen_gauss_plus_vect(h_labels[:,0], h_labels[:,1], basis.m)\n",
        "        indices = np.abs(np.arange(basis.m)[:, np.newaxis] - np.arange(basis.m))\n",
        "        g_arr = [np.repeat(x,2)[indices] for x in vect_arr]\n",
        "        g_arr = [g_arr[k] - np.diag(np.diag(g_arr[k])) for k in range(gpu_batch_size)]\n",
        "        h_labels = tf.constant(h_labels, dtype=tf.float32)\n",
        "        g_arr = tf.constant(g_arr, dtype=tf.float32)\n",
        "\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "    # Construimos los hamiltonianos basados en g_arr\n",
        "    h_arr = two_body_hamiltonian_tf(t_basis, basis.m, en_batch, g_arr, rho_1_arrays, rho_2_arrays, k_indices_tf)\n",
        "\n",
        "    # Calculamos los estados\n",
        "    if state_type == 'thermal':\n",
        "        state = thermal_state_tf(h_arr*beta) \n",
        "        state = tf.cast(state, dtype=tf.float32)\n",
        "    else:\n",
        "        state = pure_state(h_arr)\n",
        "\n",
        "    rho_input = rho_2_tf(state, rho_2_arrays_kkbar_tf)\n",
        "    \n",
        "    return rho_input\n",
        "\n",
        "# Vemos algunos valores\n",
        "def dnn_rho_reconstruction_error(model, val_dataset, h_type, state_type):\n",
        "    iterador = iter(val_dataset)\n",
        "    sample = next(iterador)\n",
        "    next_sample = next(iterador)\n",
        "    input_data = sample[0]  # Assuming your dataset provides input data as the first element\n",
        "    actual_values = sample[1].numpy()\n",
        "    predictions = model.predict(input_data)\n",
        "\n",
        "    # Calculamos los rho\n",
        "    rho_pred = rho_reconstruction_tf(predictions, h_type, state_type)\n",
        "    rho_true = rho_reconstruction_tf(actual_values, h_type, state_type)\n",
        "    rho_rand = rho_reconstruction_tf(next_sample[1].numpy(), h_type, state_type)\n",
        "\n",
        "    \n",
        "    rho_2_s = lambda x: np.sort(np.linalg.eigvals(x))\n",
        "\n",
        "    # Analisis RMSE\n",
        "    #RMSE_pred = mean_squared_error(rho_2_true, rho_2_pred, squared=False)\n",
        "    #RMSE_rand = mean_squared_error(rho_2_true, rho_2_rand, squared=False)\n",
        "    #print(RMSE_pred, RMSE_rand)\n",
        "    #print(RMSE_rand/RMSE_pred)\n",
        "    # Printeamos algunos valores\n",
        "    for i in range(0, 2):\n",
        "        print(\"true: \" + str(rho_2_s(rho_true[i])))\n",
        "        print(\"pred: \" + str(rho_2_s(rho_pred[i])))\n",
        "\n",
        "    print(rho_true, rho_pred)\n",
        "    norm_pred = np.mean(np.linalg.norm(rho_true-rho_pred, axis=(1,2)))\n",
        "    norm_rand = np.mean(np.linalg.norm(rho_true-rho_rand, axis=(1,2)))\n",
        "    print(norm_pred, norm_rand)\n",
        "    print(norm_rand / norm_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main exe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_samples = 000\n",
        "h_type = 'randomsymm'\n",
        "g_init = 0.01\n",
        "g_stop = 2.5\n",
        "state_type = 'thermal'\n",
        "input_type = 'rho2'\n",
        "include_energy = True\n",
        "cnn = True\n",
        "loss = 'MSE'\n",
        "num_epochs = 20\n",
        "arb = False # Usamos el hamiltoniano arbitrario? O 2 cuerpos?\n",
        "res = 2 # Resolución de la topología de la red (a mayor res, más parámetros)\n",
        "\n",
        "\n",
        "#dataset, label_size = gen_dataset(h_type, g_init, g_stop, state_type, input_type, include_energy = include_energy, num_samples = num_samples, arb = arb)\n",
        "# DNN\n",
        "model, val_dataset, history = dnn_fit(dataset, label_size, input_type, include_energy, cnn = cnn, loss = loss, num_epochs = num_epochs, res = res)\n",
        "print(dnn_error_coef(model, val_dataset))\n",
        "#print(dnn_rho_reconstruction_error(model, val_dataset, h_type, state_type))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparación con BCS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Caso G = G"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "en_batch = [np.arange(0, basis.m) - basis.m//2 + 1/2 for _ in range(0,gpu_batch_size)]  # ojo si lo cambie en H\n",
        "en_batch = tf.constant(en_batch, dtype=tf.float32)\n",
        "\n",
        "energ = np.array(en_batch[0])\n",
        "e_mean = np.mean(energ)\n",
        "\n",
        "# Calcula rho2 ha partir del delta dado, en formato [delta]. Es por el optimize, perdon\n",
        "@nb.jit(nopython=True)\n",
        "def bcs_delta(delta: np.ndarray, m = basis.m):\n",
        "    delta = delta[0]\n",
        "\n",
        "    lambda_k = lambda k: np.sqrt((energ[k])**2 + delta**2)\n",
        "    f_k = lambda k: 1/2 * (1 - (energ[k])/lambda_k(k))\n",
        "    r_k = lambda k: delta/(2*lambda_k(k))\n",
        "   \n",
        "    rho = np.zeros((m, m))\n",
        "    for k in range(0, m):\n",
        "        for kp in range(0, m):\n",
        "            p = f_k(k)**2 if k == kp else 0.0\n",
        "            rho[k, kp] = r_k(k) * r_k(kp) + p\n",
        "\n",
        "    return rho \n",
        "        \n",
        "        \n",
        "# Calculamos g_BCS a partir de la rho2 calculada por BCS más cercana a rho dada\n",
        "def g_bcs(rho_init):\n",
        "    rho_dist = lambda x: np.linalg.norm(bcs_delta(x)-rho_init)\n",
        "    opti = scipy.optimize.minimize(rho_dist, 1, method='Nelder-Mead')\n",
        "    delta = opti.x\n",
        "    lambda_k = lambda k: np.sqrt((en_batch[0][k] - e_mean)**2 + delta**2)\n",
        "    G = 1/(np.sum([ 1/(2*lambda_k(x)) for x in range(0, basis.m)])) \n",
        "\n",
        "    return G\n",
        "\n",
        "# Cargamos elementos del conjunto de validación\n",
        "iterador = iter(val_dataset)\n",
        "sample = next(iterador)\n",
        "input_data = sample[0]  \n",
        "actual_values = sample[1]\n",
        "predictions = model.predict(input_data)\n",
        "\n",
        "# Ordenamos los valores de G con el fin de plotear\n",
        "g_ids = actual_values.numpy().argsort()\n",
        "predictions_sort = predictions[g_ids]\n",
        "g_true_sort = actual_values.numpy()[g_ids]\n",
        "\n",
        "predictions_sort = predictions_sort.T[0]\n",
        "rho_pred = rho_reconstruction_tf(predictions_sort, h_type, state_type)\n",
        "\n",
        "# Calculamos ahora G BCS\n",
        "rho_actual = rho_reconstruction_tf(actual_values.numpy()[g_ids], h_type, state_type)\n",
        "g_bcs_sort = [g_bcs(x) for x in rho_actual.numpy()]\n",
        "rho_bcs = rho_reconstruction_tf(g_bcs_sort, h_type, state_type)\n",
        "\n",
        "rho_error = lambda x: np.linalg.norm(rho_actual.numpy()-x, ord=2, axis=(1,2))\n",
        "\n",
        "plt.rcParams['axes.labelsize'] = 16\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "plt.rcParams['legend.fontsize'] = 16\n",
        "plt.rcParams['axes.linewidth'] = 1.5\n",
        "plt.rcParams['figure.dpi'] = 300\n",
        "\n",
        "plt.plot(g_true_sort, rho_error(rho_pred), label='CNN')\n",
        "plt.plot(g_true_sort, rho_error(rho_bcs), label='BCS')\n",
        "\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel(\"G\")\n",
        "plt.ylabel(r\"Error de reconstrucción $\\rho^{(2)}$\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bcs_deltak_rho(delta_r, state_type='thermal'), rho_init\n",
        "#print(actual_values)\n",
        "bcs_opti_cost(sample[1][idx], delta_r, h_type='gaussvect', state_type='thermal')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "G = 1\n",
        "sigma = 2\n",
        "G * np.exp(-np.arange(basis.m//2)**2/(2 * sigma))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.abs([x if x<0 else 0 for x in np.diff(term)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "auxterm = np.zeros(basis.m)\n",
        "term = [1,2,3,6,5,6,7,2]\n",
        "for i in range(0,basis.m-1):\n",
        "    if term[i] > term[i+1]:\n",
        "        auxterm[i] = 1\n",
        "print(auxterm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Caso G = G(k-k')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcula rho_2 en función del delta_k dado\n",
        "import scipy.optimize\n",
        "from sympy import symbols, Function, diff, lambdify\n",
        "import sympy\n",
        "\n",
        "energ = np.array(en_batch[0])  \n",
        "\n",
        "# Dado delta_k simétrico (basis.m//2) devuelve rho asociada\n",
        "@nb.jit(nopython=True)\n",
        "def bcs_deltak_rho(delta_k, m = basis.m, state_type = 'gs'):\n",
        "    delta_k = np.concatenate((delta_k, np.flip(delta_k))) # impongo simetría\n",
        "\n",
        "    sq = lambda k: np.sqrt(energ[k]**2+delta_k[k]**2)\n",
        "    vk = lambda k: np.sqrt(1/2 * (1 - energ[k]/sq(k)))\n",
        "    uk = lambda k: np.sqrt(1/2 * (1 + energ[k]/sq(k)))\n",
        "    fk = lambda k: 1/(1+np.exp(beta*sq(k))) if state_type == 'thermal' else 0\n",
        "    ukvk = lambda k: uk(k)*vk(k)*(1-2*fk(k))\n",
        "    vksq = lambda k: vk(k)**2 * (1-2*fk(k)) + fk(k) \n",
        "\n",
        "    rho = np.zeros((m, m))\n",
        "    for k in range(0, m):\n",
        "        for kp in range(0, m):\n",
        "            p = vksq(k)**2 if k == kp else 0\n",
        "            rho[k, kp] = ukvk(k)*ukvk(kp) + p\n",
        "\n",
        "    return rho \n",
        "\n",
        "# Pasemos a la inversion, es decir, obtener G(delta_k)\n",
        "\n",
        "## Metodo exacto (caso vectorial)\n",
        "### Generamos las funciones para escribir M_ij\n",
        "def gen_sympy_func(m):\n",
        "    uv_s = symbols(f'uv0:{m}')\n",
        "    g_s = symbols(f'g0:{m}')\n",
        "    d_s = np.zeros(m, dtype=object)\n",
        "    for i in range(m):\n",
        "        d_s[i] = np.sum([g_s[np.abs((i-j))//2] * uv_s[j] for j in range(m)])\n",
        "\n",
        "    funcarr = np.zeros((m//2, m//2), dtype=object)\n",
        "    for i in range(m//2):\n",
        "        for j in range(m//2):\n",
        "            funcarr[i, j] = lambdify(uv_s, diff(d_s[i], g_s[j]), 'numpy')\n",
        "\n",
        "    return funcarr\n",
        "\n",
        "M_funcarr = gen_sympy_func(basis.m)\n",
        "\n",
        "def bcs_build_M(delta_k, m=basis.m):\n",
        "    delta_k = np.abs(np.concatenate((delta_k, np.flip(delta_k))))\n",
        "    sq = lambda k: np.sqrt(energ[k]**2+delta_k[k]**2)\n",
        "    vk = lambda k: np.sqrt(1/2 * (1 - energ[k]/sq(k)))\n",
        "    uk = lambda k: np.sqrt(1/2 * (1 + energ[k]/sq(k)))\n",
        "    ukvk = lambda k: uk(k)*vk(k)\n",
        "\n",
        "    M = np.zeros((m//2, m//2))\n",
        "    uv_vals = [ukvk(k) for k in range(basis.m)]    \n",
        "    for i in range(m//2):\n",
        "        for j in range(m//2):\n",
        "            M[i, j] = M_funcarr[i, j](*uv_vals)\n",
        "\n",
        "    M = np.linalg.inv(M)\n",
        "    return M\n",
        "\n",
        "## Inversion numérica, calculo de la función de costo a partir de autoconsistencia\n",
        "#@nb.jit(nopython=True)\n",
        "def bcs_opti_cost(g, delta_k, m=basis.m, state_type='gs', h_type='vect'):\n",
        "    if h_type == 'gaussvect':\n",
        "        G, sigma = g[0], g[1] # g = (G, sigma)\n",
        "        g_s = G * np.exp(-np.arange(m//2)**2/(2 * sigma))\n",
        "    else:\n",
        "        g_s = g # nada que hacer en el caso vectorial\n",
        "\n",
        "    # El choclo es por numba\n",
        "    delta_k = np.concatenate((delta_k, np.flip(delta_k)))\n",
        "    sq = lambda k: np.sqrt(energ[k]**2+delta_k[k]**2)\n",
        "    vk = lambda k: np.sqrt(1/2 * (1 - energ[k]/sq(k)))\n",
        "    uk = lambda k: np.sqrt(1/2 * (1 + energ[k]/sq(k)))\n",
        "    fk = lambda k: 1/(1+np.exp(beta*sq(k))) if state_type == 'thermal' else 0\n",
        "    ukvk = lambda k: uk(k)*vk(k)*(1-2*fk(k))\n",
        "    vksq = lambda k: vk(k)**2 * (1-2*fk(k)) + fk(k) \n",
        "\n",
        "    term = np.zeros(m)\n",
        "    for i in range(m):\n",
        "        if state_type == 'thermal':\n",
        "            #term[i] = delta_k[i] - sum([g_s[np.abs((i-j))//2] * delta_k[j] / (2 * sq(j)) * np.tanh(beta * sq(j)/2) for j in range(m)])\n",
        "            term[i] = delta_k[i] - sum([g_s[np.abs((i-j))//2] * ukvk(j) for j in range(m)])\n",
        "        else:\n",
        "            term[i] = delta_k[i] - sum([g_s[np.abs((i-j))//2] * ukvk(j) for j in range(m)]) # ojo con un factorcito por aca\n",
        "\n",
        "    #return term\n",
        "    return np.linalg.norm(term, ord=2)\n",
        "\n",
        "# Implementación de la función de inversión mediante ambas estrategias (exacto y numérico)\n",
        "def bcs_rho_g(rho_init, h_type = 'vect', state_type = 'gs', exact = False, energ_f=0, actual_energy=0, just_delta = False):\n",
        "    # Buscamos delta_k   TODO: CASO TERMICO ENERGIA\n",
        "    dist = lambda delta_k: np.linalg.norm(bcs_deltak_rho(delta_k, basis.m, state_type)-rho_init, ord=2) + energ_f * (delta_energ(delta_k, state_type)-actual_energy)**2 + 0 * np.linalg.norm([x if x<0 else 0 for x in np.diff(delta_k)])\n",
        "    bounds = [(0.1, 50) for _ in range(basis.m//2)] # Bounds de delta_k, TODO determinar o acotar\n",
        "    opti = scipy.optimize.dual_annealing(dist, bounds=bounds, maxiter=1000)\n",
        "    #print(opti)\n",
        "    delta_k = opti.x\n",
        "\n",
        "    if just_delta:\n",
        "        return 0, delta_k, opti\n",
        "\n",
        "    if exact and state_type == 'gs' and h_type == 'vect':\n",
        "        M = bcs_build_M(delta_k)\n",
        "        g_rebuild = M @ delta_k[:basis.m//2]\n",
        "        delta_k = delta_k[:basis.m//2]\n",
        "\n",
        "    else: \n",
        "        if h_type == 'gaussvect':\n",
        "            #g_dist = lambda g: bcs_opti_cost(g, delta_k, basis.m, state_type=state_type, h_type='gaussvect')[:2]\n",
        "            #optig = scipy.optimize.root(g_dist, np.random.rand(2), method='broyden1', options={'maxiter': 1000})\n",
        "            #print(optig)\n",
        "            g_dist = lambda g: bcs_opti_cost(g, delta_k, basis.m, state_type=state_type, h_type='gaussvect')\n",
        "            bounds = [(g_init, g_stop), (0.01, 2.1)]\n",
        "            optig = scipy.optimize.dual_annealing(g_dist, bounds=bounds, maxiter=1000)\n",
        "   \n",
        "        else:\n",
        "            g_dist = lambda g: bcs_opti_cost(g, delta_k, basis.m, state_type=state_type, h_type='vect')\n",
        "            bounds = [(g_init, g_stop) for _ in range(basis.m//2)]\n",
        "            optig = scipy.optimize.dual_annealing(g_dist, bounds=bounds, maxiter=1000)\n",
        "\n",
        "        #print(optig)\n",
        "        g_rebuild = optig.x\n",
        "\n",
        "    return g_rebuild, delta_k, opti\n",
        "\n",
        "# Calculo de energía en funcion de delta\n",
        "def delta_energ(delta_k, state_type = 'gs'):\n",
        "    delta_k = np.concatenate((delta_k, np.flip(delta_k)))\n",
        "    sq = lambda k: np.sqrt(energ[k]**2+delta_k[k]**2)\n",
        "    vk = lambda k: np.sqrt(1/2 * (1 - energ[k]/sq(k)))\n",
        "    uk = lambda k: np.sqrt(1/2 * (1 + energ[k]/sq(k)))\n",
        "    fk = lambda k: 1/(1+np.exp(beta*sq(k))) if state_type == 'thermal' else 0\n",
        "    ukvk = lambda k: uk(k)*vk(k)*(1-2*fk(k))\n",
        "    vksq = lambda k: vk(k)**2 * (1-2*fk(k)) + fk(k) \n",
        "\n",
        "    if state_type == 'thermal':\n",
        "        h0 = np.sum([2*energ[k]*vksq(k) for k in range(basis.m)])\n",
        "        hi = np.sum([delta_k[k]**2/(2*sq(k))*(1-2*fk(k)) for k in range(basis.m)])\n",
        "        return h0 - hi\n",
        "    else:\n",
        "        return 0 # TODO: RW CASO GS\n",
        "\n",
        "    # Termino de energía, el más fácil\n",
        "    t1 = np.sum([energ[j] * 2 * vk(j)**2 for j in range(basis.m)])\n",
        "\n",
        "    # Término de interacción, calcularemos G en función de este delta\n",
        "    M = bcs_build_M(lambda k: uk(k) * vk(k))\n",
        "    h_labels = M @ delta_k[:basis.m//2]\n",
        "    #h_labels = gex\n",
        "    indices = np.abs(np.arange(basis.m)[:, np.newaxis] - np.arange(basis.m))\n",
        "    g_arr = np.repeat(h_labels,2)[indices]\n",
        "    rho = bcs_deltak_rho(delta_k, basis.m).T\n",
        "    terms = np.multiply(g_arr, rho)\n",
        "    \n",
        "    t2 = np.sum(terms)\n",
        "\n",
        "    return t1-t2\n",
        "\n",
        "\n",
        "# Reduce el constraint en energia hasta alcanzar una distancia tolerable en rho\n",
        "def opti_delta(rho_init, exact, actual_energy, energ_f=0.1):\n",
        "    max_iter = 1\n",
        "    tol = 0.2\n",
        "    dist = lambda delta_k: np.linalg.norm(bcs_deltak_rho(delta_k, basis.m)-rho_init) \n",
        "    gf, deltaf = np.zeros(basis.m//2), np.zeros(basis.m//2)\n",
        "    i = 1\n",
        "    while i < max_iter:\n",
        "        g, delta, opti = bcs_rho_g(rho_init, exact, energ_f/i, actual_energy=actual_energy) # vamos de a poco reduciendo el constrain en energia\n",
        "        if dist(delta) < tol:\n",
        "            gf, deltaf = g, delta\n",
        "            #print(opti)\n",
        "            break\n",
        "        if dist(delta) < dist(deltaf):\n",
        "            gf, deltaf = g, delta\n",
        "        i += 1\n",
        "\n",
        "    return gf, deltaf\n",
        "\n",
        "# Función auxiliar: calculemos rho2 para esos otros g de la inversion. Recuperamos rho??\n",
        "def rho_reconstruction_vect(gex):\n",
        "    return rho_reconstruction_tf([gex], 'vect', state_type)[0]\n",
        "\n",
        "# Dado delta, obtengo G via la inversión y reconstruyo rho\n",
        "# delta -> G -> rho_reconstruction_tf\n",
        "def rho_reconstruction_from_delta(delta_k):\n",
        "    delta_k = np.abs(np.concatenate((delta_k, np.flip(delta_k))))\n",
        "    sq = lambda k: np.sqrt(energ[k]**2+delta_k[k]**2)\n",
        "    vk = lambda k: np.sqrt(1/2 * (1 - energ[k]/sq(k)))\n",
        "    uk = lambda k: np.sqrt(1/2 * (1 + energ[k]/sq(k)))\n",
        "\n",
        "    M = bcs_build_M(delta_k[:basis.m//2])\n",
        "    gex = M @ delta_k[:basis.m//2]\n",
        "\n",
        "    return rho_reconstruction_vect(gex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dex = [2.73278064, 5.16334309, 5.13024812, 2]\n",
        "np.linalg.norm(bcs_opti_cost(gex, dex, state_type=state_type, h_type=h_type))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.linalg.norm(bcs_opti_cost(gex, dex, state_type=state_type, h_type=h_type))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(2,100):\n",
        "    if np.all([gi > 1 for gi in sample[1][i]]):\n",
        "        print(sample[1][i],i)\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Autoconsistencia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "idx = np.random.randint(0,100)\n",
        "#idx = 1\n",
        "\n",
        "# Cargamos los valores\n",
        "iterador = iter(val_dataset)\n",
        "sample = next(iterador)\n",
        "input_data = sample[0][0][idx]\n",
        "rho_init = input_data\n",
        "rand_idx = np.random.randint(0,100)\n",
        "rho_rand = sample[0][0][rand_idx]\n",
        "actual_energy = sample[0][1][idx]\n",
        "\n",
        "# Compatibilizamos los dos casos\n",
        "if h_type == 'gaussvect':\n",
        "    s = sample[1]\n",
        "    actual_values = gen_gauss_plus_vect(s[:,0], s[:,1], basis.m)[idx]\n",
        "    p = model.predict(sample[0])\n",
        "    prediction = gen_gauss_plus_vect(p[:,0], p[:,1], basis.m)[idx]\n",
        "else:\n",
        "    actual_values = sample[1][idx]\n",
        "    prediction = model.predict(sample[0])[idx]\n",
        "\n",
        "\n",
        "# Calculamos el delta por autoconsitencia\n",
        "def auto_delta(delta_k, m = basis.m, state_type = 'gs'):\n",
        "    delta_k = np.concatenate((delta_k, np.flip(delta_k)))\n",
        "    sq = lambda k: np.sqrt(energ[k]**2+delta_k[k]**2)\n",
        "    vk = lambda k: np.sqrt(1/2 * (1 - energ[k]/sq(k)))\n",
        "    uk = lambda k: np.sqrt(1/2 * (1 + energ[k]/sq(k)))\n",
        "    fk = lambda k: 1/(1+np.exp(beta*sq(k))) if state_type == 'thermal' else 0\n",
        "    ukvk = lambda k: uk(k)*vk(k)*(1-2*fk(k))\n",
        "    vksq = lambda k: vk(k)**2 * (1-2*fk(k)) + fk(k) \n",
        "\n",
        "    gt = actual_values.numpy()\n",
        "    term = np.zeros(m//2)\n",
        "    for i in range(m//2):\n",
        "        if state_type == 'thermal':\n",
        "            term[i] = sum([gt[np.abs((i-j))//2] * delta_k[j] / (2 * sq(j)) * np.tanh(beta * sq(j)/2) for j in range(m)])\n",
        "        else:\n",
        "            term[i] = sum([gt[np.abs((i-j))//2] * ukvk(j) for j in range(m)]) # ojo con un factorcito por aca\n",
        "\n",
        "    return term\n",
        "    \n",
        "gex = actual_values.numpy()\n",
        "delta_r = np.random.rand(basis.m//2)\n",
        "for k in range(0, 100):\n",
        "    delta_r = auto_delta(delta_r, state_type=state_type)\n",
        "\n",
        "dist = lambda delta_k, rho: np.linalg.norm(bcs_deltak_rho(delta_k, basis.m, state_type=state_type)-rho, ord=2) # delta_energ(delta_k)-sample[0][1][0])**2+\n",
        "#rho_error_from_delta = lambda delta, rho: np.linalg.norm(rho_reconstruction_from_delta(delta) - rho, ord=2)\n",
        "rho_error_from_ge = lambda ge, rho: np.linalg.norm(rho_reconstruction_vect(ge) - rho, ord=2)\n",
        "\n",
        "print(np.linalg.norm(rho_init, ord=2)/np.linalg.norm(delta_r, ord=2))\n",
        "print('>>Autoconsitencia a partir de g true')\n",
        "print(f'Delta {delta_r}')\n",
        "print(f'Dist {dist(delta_r, rho_init)} de rho')\n",
        "print('\\n>>Resultados de la inversión')\n",
        "gex, dex, opti = bcs_rho_g(rho_init, h_type, state_type, exact = False, energ_f=0.01, actual_energy=actual_energy)\n",
        "print(np.linalg.norm(rho_init, ord=2)/np.linalg.norm(dex, ord=2))\n",
        "\n",
        "print(f'Delta {dex}')\n",
        "print(f'Dist {dist(dex, rho_init)} de rho')\n",
        "print(f'Energia {delta_energ(dex, state_type=state_type)}')\n",
        "#print(f'Reconstruccion rho {rho_error_from_delta(dex, rho_init)}')\n",
        "print('\\n>>Resultados random')\n",
        "print(f'Dist {dist(delta_r, rho_rand)} de rho random')\n",
        "print(f'Reconstruccion rho random {rho_error_from_ge(actual_values.numpy(), rho_rand)}')\n",
        "print('\\n>>Resultados predicción')\n",
        "print(f'Reconstruccion rho {rho_error_from_ge(prediction, rho_init)}')\n",
        "print('\\n>>Valores de G')\n",
        "print(f'True {sample[1][idx].numpy()}')\n",
        "print(f'Pred {gex}')\n",
        "print(f'Random {sample[1][rand_idx]}')\n",
        "print(f'Energia real {actual_energy}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dado delta, mejor G?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gen_sympy_func(m, label_size, h_type):\n",
        "    uv_s = symbols(f'uv0:{m}')\n",
        "    if h_type == 'randomsymm':\n",
        "        g_s = symbols(f'g0:{label_size}') # gs (semilla)\n",
        "    else:\n",
        "        g_s = symbols(f'g0:{m}')\n",
        "\n",
        "    d_s = np.zeros(m, dtype=object)\n",
        "\n",
        "    if h_type == 'vectnosymm' or h_type == 'vect':\n",
        "        for i in range(m):\n",
        "            d_s[i] = np.sum([g_s[np.abs((i-j))] * uv_s[j] for j in range(m)])\n",
        "\n",
        "    elif h_type == 'randomsymm':\n",
        "        # Generamos la matriz G_kk'\n",
        "        g_mat = random_fermi_arr_inv(g_s, m, obj = True)\n",
        "        for i in range(m):\n",
        "            d_s[i] = np.sum([g_mat[i,j] * uv_s[j] for j in range(m)]) \n",
        "    \n",
        "    else:\n",
        "        raise ValueError\n",
        "    \n",
        "    funcarr = np.zeros((m, label_size), dtype=object)\n",
        "    for i in range(m):\n",
        "        for j in range(label_size):\n",
        "            funcarr[i, j] = lambdify(uv_s, diff(d_s[i], g_s[j]), 'numpy')\n",
        "\n",
        "    return funcarr\n",
        "\n",
        "M_funcarr = gen_sympy_func(basis.m, label_size, h_type)\n",
        "\n",
        "def bcs_build_M_thermal(delta_k, label_size, m=basis.m):\n",
        "    delta_k = np.concatenate((delta_k, np.flip(delta_k)))\n",
        "    sq = lambda k: np.sqrt(energ[k]**2+delta_k[k]**2)\n",
        "    vk = lambda k: np.sqrt(1/2 * (1 - energ[k]/sq(k)))\n",
        "    uk = lambda k: np.sqrt(1/2 * (1 + energ[k]/sq(k)))\n",
        "    fk = lambda k: 1/(1+np.exp(beta*sq(k))) if state_type == 'thermal' else 0\n",
        "    ukvk = lambda k: uk(k)*vk(k)*(1-2*fk(k))\n",
        "    vksq = lambda k: vk(k)**2 * (1-2*fk(k)) + fk(k) \n",
        "\n",
        "    M = np.zeros((m//2, label_size))\n",
        "    uv_vals = [ukvk(k) for k in range(m)]    \n",
        "    for i in range(m//2):\n",
        "        for j in range(label_size):\n",
        "            M[i, j] = M_funcarr[i, j](*uv_vals)\n",
        "\n",
        "    return M\n",
        "\n",
        "\n",
        "#print(delta_r)\n",
        "gex, dex, opti = bcs_rho_g(rho_init, h_type, state_type, exact = False, energ_f=0.01, actual_energy=actual_energy, just_delta=True)\n",
        "print(opti)\n",
        "delta_o = dex\n",
        "print(gex)\n",
        "M = bcs_build_M_thermal(delta_o, label_size)\n",
        "print(dex)\n",
        "#print(M @ np.repeat(actual_values, 2)) # deberiamos recuperar delta\n",
        "#conv = lambda x: np.insert(x, 0, 0)\n",
        "conv = lambda x: x\n",
        "\n",
        "eps = 0.001\n",
        "print(M.shape)\n",
        "#delta_o = np.concatenate((delta_o, np.flip(delta_o)))\n",
        "lin_const = scipy.optimize.LinearConstraint(M, delta_o-eps, delta_o+eps)\n",
        "#cost = lambda g: np.linalg.norm(g-np.repeat(actual_values,2)) caso simetrico\n",
        "print(actual_values)\n",
        "cost = lambda g: np.linalg.norm(g-conv(actual_values))\n",
        "bounds = [(0.01, 50) for _ in range(label_size)]\n",
        "\n",
        "opt = scipy.optimize.minimize(cost, np.random.rand(label_size), constraints=lin_const, bounds=bounds, method='SLSQP') #COBYLA, SLSQP, \n",
        "#opt = scipy.optimize.differential_evolution(cost, bounds=bounds, constraints=lin_const)\n",
        "#opt = scipy.optimize.shgo(cost, bounds=bounds, constraints=lin_const, iters=2, n=1000, sampling_method='halton')\n",
        "print(opt, opt.x)\n",
        "print(actual_values)\n",
        "print(M @ opt.x - delta_o)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora la idea es hacer todo lo anterior, de manera sistemática"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO Reducir repetición de código con lo anterior\n",
        "\n",
        "# Cargamos valores\n",
        "iterador = iter(val_dataset)\n",
        "sample = next(iterador)\n",
        "input_data_arr = sample[0][0].numpy()\n",
        "actual_energy_arr = sample[0][1].numpy()\n",
        "actual_values_arr = sample[1].numpy()\n",
        "prediction = model.predict(sample[0])\n",
        "#conv = lambda x: np.repeat(np.insert(x, 0, 0),2) caso simetrico\n",
        "conv = lambda x: np.insert(x, 0, 0)\n",
        "\n",
        "def min_opti_g(idx):\n",
        "    input_data = input_data_arr[idx]\n",
        "    rho_init = input_data\n",
        "    actual_energy = actual_energy_arr[idx]\n",
        "    #actual_values = conv(actual_values_arr[idx])\n",
        "    actual_values = actual_values_arr[idx]\n",
        "\n",
        "    # Optimizamos para hallar el delta\n",
        "    gex, dex, opti = bcs_rho_g(rho_init, h_type, state_type, exact = False, energ_f=0.01, actual_energy=actual_energy, just_delta=True)\n",
        "\n",
        "    delta_o = dex\n",
        "    M = bcs_build_M_thermal(delta_o, label_size)\n",
        "\n",
        "    eps = 0.01\n",
        "    lin_const = scipy.optimize.LinearConstraint(M, delta_o-eps, delta_o+eps)\n",
        "    cost = lambda g: np.linalg.norm(g-actual_values)\n",
        "    bounds = [(0.01, 200) for _ in range(label_size)]\n",
        "\n",
        "    #print(M.shape, delta_o)\n",
        "    opt = scipy.optimize.minimize(cost, np.random.rand(label_size), constraints=lin_const, bounds=bounds, method='SLSQP') \n",
        "    #print(opt)\n",
        "    #print(actual_values)\n",
        "    #print(M @ opt.x - delta_o)\n",
        "    return opt.x, actual_values, prediction[idx]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "min_opti_g(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "\n",
        "min_actual_g2 = []\n",
        "min_num_g2 = []\n",
        "min_pred_g2 = []\n",
        "\n",
        "def task(i):\n",
        "    return min_opti_g(i)\n",
        "\n",
        "with ProcessPoolExecutor() as executor:\n",
        "    # Submit all the tasks to the executor\n",
        "    futures = [executor.submit(task, i) for i in range(200)]\n",
        "    \n",
        "    # Use tqdm to display progress\n",
        "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
        "        num, actual, pred = future.result()\n",
        "        min_actual_g2.append(actual)\n",
        "        min_num_g2.append(num)\n",
        "        min_pred_g2.append(pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "min_actual_g = min_actual_g2\n",
        "min_num_g =  min_num_g2\n",
        "min_pred_g = min_pred_g2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "op1 = lambda g: [np.linalg.norm(x) for x in g]\n",
        "op2 = lambda g: [np.linalg.norm(x) for x in g]\n",
        "\n",
        "for i in range(len(min_actual_g)):\n",
        "    min_actual_gm = op1(min_actual_g)\n",
        "    min_num_gm = op2(min_num_g)r\n",
        "    min_pred_gm = np.array(op2(min_pred_g))\n",
        "\n",
        "sortids = np.array(min_actual_gm).argsort().astype(int)\n",
        "x = np.array(min_actual_gm)[sortids]\n",
        "plt.plot(x, np.array(min_num_gm)[sortids], label='BCS')\n",
        "plt.plot(x, np.array(min_actual_gm)[sortids])\n",
        "plt.plot(x, min_pred_gm[sortids], label='ML')\n",
        "#plt.yscale(\"log\")\n",
        "#plt.ylim(0,5)\n",
        "plt.legend()\n",
        "plt.xlabel(\"Norma G_true\")\n",
        "plt.ylabel(\"Norma G_BCS\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "difop = lambda g: np.linalg.norm(np.array(g)[sortids] - np.array(min_actual_g)[sortids], axis=-1)\n",
        "plt.plot(x, difop(min_num_g), label='BCS')\n",
        "plt.plot(x, difop(min_pred_g), label='ML')\n",
        "plt.legend()\n",
        "plt.xlabel('Norma G_true')\n",
        "plt.ylabel('Norma diferencia G-G_true')\n",
        "plt.yscale(\"log\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, x in enumerate(min_actual_gm):\n",
        "    if np.linalg.norm(x) < 10:\n",
        "        print(x, min_num_gm[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pre_const = lambda g: bcs_opti_cost_alt(g, dex, basis.m, state_type=state_type, h_type=h_type)\n",
        "nl_const = scipy.optimize.NonlinearConstraint(pre_const, -0.1, 0.1)\n",
        "cost = lambda g: np.linalg.norm(g-np.repeat(actual_values,2))\n",
        "bounds = [(0.1, g_stop+0.5) for _ in range(basis.m)]\n",
        "\n",
        "#opt = scipy.optimize.minimize(cost, np.random.rand(basis.m), constraints=nl_const, bounds=bounds, method='SLSQP')\n",
        "opt = scipy.optimize.differential_evolution(cost, bounds=bounds, constraints=nl_const)\n",
        "print(opt)\n",
        "print(pre_const(opt.x), actual_values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Buscamos delta_k   TODO: CASO TERMICO ENERGIA\n",
        "#dist = lambda delta_k: np.linalg.norm(bcs_deltak_rho(delta_k, basis.m, state_type)-rho_init, ord=2) + 0.01 * (delta_energ(delta_k, state_type)-actual_energy)**2\n",
        "#bounds = [(0, 50) for _ in range(basis.m//2)] # Bounds de delta_k, TODO determinar o acotar\n",
        "#opti = scipy.optimize.dual_annealing(dist, bounds=bounds, maxiter=1000)\n",
        "#delta_k = opti.x\n",
        "#print(delta_k, delta_r)\n",
        "\n",
        "g_dist = lambda g: bcs_opti_cost(g, delta_r, basis.m, state_type=state_type, h_type='vect')\n",
        "bounds = [(g_init, g_stop), (0.01, 2.1)]\n",
        "#optig = scipy.optimize.dual_annealing(g_dist, bounds=bounds, maxiter=1000) broyden1!!\n",
        "scipy.optimize.root(g_dist, (1,1), method='hybr', options={'maxiter': 10000})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scipy.optimize.roots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "auto_delta(delta_r, state_type=state_type), delta_r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "g_dist = lambda g: bcs_opti_cost(g, delta_r, basis.m, state_type=state_type, h_type='vect')\n",
        "bounds = [(g_init, g_stop) for _ in range(basis.m//2)]\n",
        "optig = scipy.optimize.dual_annealing(g_dist, bounds=bounds, maxiter=1000)\n",
        "print(optig)\n",
        "optig.x, sample[1][idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dex, auto_delta(dex, state_type=state_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rbcs = bcs_deltak_rho(dex, state_type=state_type)\n",
        "np.linalg.eigvals(rbcs), np.linalg.eigvals(rho_init)\n",
        "plt.plot(np.linalg.eigvals(rbcs))\n",
        "plt.plot(np.linalg.eigvals(rho_init))\n",
        "plt.yscale('log')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nn = [0,1,2,None,np.inf,-1,-2]\n",
        "for x in nn:\n",
        "    d = dex\n",
        "    print(np.linalg.norm(d, ord=x))\n",
        "\n",
        "print(np.linalg.norm(d, ord=2)*2-np.linalg.norm(d, ord=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bounds = [(0, 50) for _ in range(basis.m//2)]\n",
        "scipy.optimize.dual_annealing(dist, bounds=bounds, maxiter=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dist = lambda delta_k: (bcs_deltak_rho(delta_k, basis.m, state_type)-rho_init).numpy().flatten()\n",
        "op = scipy.optimize.root(dist, np.random.rand(4), method='lm', tol=1e-8, epsfcn = 0.1)\n",
        "op, delta_r\n",
        "#delta_k = op.x\n",
        "#type(dist(delta_r))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "op = scipy.optimize.fsolve(dist, np.random.rand(basis.m//2))\n",
        "op"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rho_init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dist = lambda delta_k: np.linalg.norm(bcs_deltak_rho(delta_k, basis.m, state_type)-rho_init)+(np.linalg.norm(delta_k, ord=0)-basis.m//2)**2\n",
        "bounds = [(0,10) for _ in range(4)]\n",
        "opti = scipy.optimize.dual_annealing(dist, bounds=bounds, maxiter=10000)\n",
        "delta_k = opti.x\n",
        "print(dist(delta_r))\n",
        "\n",
        "opti, delta_r\n",
        "#g_dist = lambda g: bcs_opti_cost(g, delta_r, basis.m, state_type=state_type, h_type='gaussvect')\n",
        "#optig = scipy.optimize.dual_annealing(g_dist, bounds=[(0,10), (0,10)], maxiter=10000)\n",
        "#optig, sample[1][idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.linalg.norm(delta_k, ord=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nn = [0,1,2,None,np.inf,-1,-2]\n",
        "for x in nn:\n",
        "    print(np.linalg.norm(delta_r, ord=x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "opti +"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#opti = scipy.optimize.differential_evolution(dist, bounds=bounds)\n",
        "#opti\n",
        "opti = scipy.optimize.direct(dist, bounds=bounds, maxiter=1000)\n",
        "print(dist(delta_r))\n",
        "opti"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "g_dist = lambda g: bcs_opti_cost(g, delta_r, basis.m, state_type=state_type, h_type='gaussvect')\n",
        "boundss = [(0.01,20) for _ in range(2)]\n",
        "optig = scipy.optimize.dual_annealing(g_dist, bounds=boundss, maxiter=1000)\n",
        "sample[1][idx], optig, delta_r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bcs_opti_cost(sample[1][idx], delta_r, state_type=state_type, h_type='gaussvect')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "g_dist = lambda g: bcs_opti_cost(g, delta_r, basis.m, state_type=state_type, h_type='gaussvect')\n",
        "opti = scipy.optimize.minimize(g_dist, np.random.rand(2), method='Nelder-Mead', tol=1e-8)\n",
        "opti, sample[1][idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "g_dist(sample[1][rand_idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(np.linalg.eigvals(rho_init))\n",
        "\n",
        "plt.plot(np.linalg.eigvals(bcs_deltak_rho(dex, basis.m)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "delta_k = dex\n",
        "# Calculamos el sistema de ecs\n",
        "delta_k = np.abs(np.concatenate((delta_k, np.flip(delta_k)))) # pues el resultado son los delta indep\n",
        "sq = lambda k: np.sqrt(energ[k]**2+delta_k[k]**2)\n",
        "vk = lambda k: np.sqrt(1/2 * (1 - energ[k]/sq(k)))\n",
        "uk = lambda k: np.sqrt(1/2 * (1 + energ[k]/sq(k)))\n",
        "\n",
        "M = bcs_build_M(lambda k: uk(k) * vk(k))\n",
        "M @ delta_k[:basis.m//2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "arr = []\n",
        "for i in range(0, 100):\n",
        "    arr.append(rho_error_from_ge(actual_values.numpy(), sample[0][0][i]))\n",
        "np.mean(arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rho_init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "maxval = 100\n",
        "# Cargamos elementos del conjunto de validación\n",
        "iterador = iter(val_dataset)\n",
        "sample = next(iterador)\n",
        "\n",
        "input_rhos = sample[0][0].numpy()[:maxval]  \n",
        "input_energies = sample[0][1].numpy()[:maxval] \n",
        "actual_values = sample[1].numpy()[:maxval]\n",
        "input_data = sample[0]\n",
        "predictions = model.predict(input_data)\n",
        "\n",
        "# Ordenamos los valores de G con el fin de plotear\n",
        "g_ids = actual_values[:,0].argsort()\n",
        "g_ids = np.mean(actual_values, axis=-1).argsort()\n",
        "predictions_sort = predictions[g_ids]\n",
        "g_true_sort = actual_values[g_ids]\n",
        "rho_pred = rho_reconstruction(predictions_sort)\n",
        "rho_actual = input_rhos[g_ids]\n",
        "\n",
        "# Calculamos ahora G BCS\n",
        "rho_bcs_arr = []\n",
        "for l in tqdm(range(actual_values.shape[0])): # equiv al batch_size\n",
        "    rho = input_rhos[l]\n",
        "    actual_energy = input_energies[l]\n",
        "    gex, dex = opti_delta(rho, actual_energy,0)\n",
        "    rho_bcs = rho_reconstruction(gex)\n",
        "    #print(rho_bcs)\n",
        "    rho_bcs_arr.append(rho_bcs)\n",
        "\n",
        "rho_bcs = np.array(rho_bcs_arr)[g_ids]\n",
        "\n",
        "rho_error = lambda x: np.linalg.norm(rho_actual-x, ord='fro', axis=(1,2))\n",
        "\n",
        "plt.plot(g_true_sort[:,0], rho_error(rho_pred), label='DNN prediction') # ploteamos segun el primero\n",
        "plt.plot(g_true_sort[:,0], rho_error(rho_bcs), label='BCS')\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel(\"g\")\n",
        "plt.ylabel(\"Rho2 reconstruction error\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(np.sort(np.mean(actual_values, axis=-1)), rho_error(rho_pred), label='CNN prediction') # ploteamos segun el primero\n",
        "plt.plot(np.sort(np.mean(actual_values, axis=-1)), rho_error(rho_bcs), label='BCS')\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel(\"g\")\n",
        "plt.ylabel(\"Rho2 reconstruction error\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rho_error = lambda x: np.linalg.norm(rho_actual-x, ord=2, axis=(1,2))\n",
        "\n",
        "plt.plot(g_true_sort[:,0], rho_error(rho_pred), label='DNN prediction') # ploteamos segun el primero\n",
        "plt.plot(g_true_sort[:,0], rho_error(rho_bcs), label='BCS')\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel(\"g\")\n",
        "plt.ylabel(\"Rho2 reconstruction error\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Análisis para G cte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generacion de elementos, rho2 a partir de ellos, y comparación con la predicción\n",
        "# Nuevamente, el resultado depende pura y exclusivamente del modelo, y no de los ptos tomados\n",
        "h_labels = np.linspace(0.1,1,512)\n",
        "g_arr = [np.ones((basis.m, basis.m))*g_seed for g_seed in h_labels]\n",
        "g_arr = tf.constant(g_arr, dtype=tf.float32)\n",
        "h_arr = two_body_hamiltonian_tf(t_basis, basis.m, en_batch, g_arr.numpy(), rho_1_arrays, rho_2_arrays, k_indices_tf)\n",
        "\n",
        "# Estados térmicos\n",
        "state = thermal_state_tf(h_arr*beta) \n",
        "state = tf.cast(state, dtype=tf.float32)\n",
        "# Estados puros\n",
        "#state = pure_state(h_arr)\n",
        "\n",
        "rho_2_input = rho_2_tf(state, rho_2_arrays_kkbar_tf)\n",
        "predictions = model.predict(rho_2_input).T\n",
        "G_err = np.abs(predictions-h_labels).T\n",
        "plt.plot(h_labels, G_err)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ploteo de varios elementos de val_dataset\n",
        "# No sirve de mucho, depende del modelo y no la muestra\n",
        "max_plt = 10\n",
        "idx = 0\n",
        "for e in val_dataset:\n",
        "    predictions = model.predict(e[0])\n",
        "    pred_ids = predictions.T.argsort()\n",
        "    predictions_sort = predictions[pred_ids][0]\n",
        "    G_true_sorted = e[1].numpy()[pred_ids].T\n",
        "    G_err = np.abs(predictions_sort-G_true_sorted)\n",
        "    plt.plot(predictions_sort,G_err)\n",
        "    idx += 1\n",
        "    if idx > max_plt:  \n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modelos Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tenemos que trabajar con DataFrames para trabajar con xgboost, por eso inicialmente desempaquetamos el dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow_decision_forests as tfdf\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def rf_fit(dataset):\n",
        "    # Generación de dataset\n",
        "    ds_f = {} # features\n",
        "    ds_l = {} # labels\n",
        "\n",
        "    # Generamos las etiquetas\n",
        "    for i in range(basis.m*basis.m):\n",
        "        ds_f[f'{i}'] = []\n",
        "    # Generacion de labels TODO: Escribir todo en función del label size y fue\n",
        "    if label_size == 1:\n",
        "        ds_l['g'] = []\n",
        "    elif label_size == 2:\n",
        "        ds_l['g'] = []\n",
        "        ds_l['sigma'] = []  \n",
        "    else:\n",
        "        for i in range(0, label_size):\n",
        "            ds_l[f'l{i}'] = []\n",
        "\n",
        "    # Poblamos el DF\n",
        "    for e in list(dataset.as_numpy_iterator()):\n",
        "        # Elementos de rho2\n",
        "        for i in range(0,basis.m*basis.m):\n",
        "            ds_f[f'{i}'].append(np.ndarray.flatten(e[0])[i])\n",
        "        # Labels\n",
        "        if label_size == 1:\n",
        "            ds_l['g'].append(e[1])\n",
        "        elif label_size == 2:\n",
        "            ds_l['g'].append(e[1][0])\n",
        "            ds_l['sigma'].append(e[1][1])\n",
        "        else:\n",
        "            for i in range(0, label_size):\n",
        "                ds_l[f'l{i}'].append(e[1][i])\n",
        "\n",
        "    ds_l = pd.DataFrame(ds_l)\n",
        "    ds_f = pd.DataFrame(ds_f)\n",
        "\n",
        "    # Spliteamos los datasets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(ds_f, ds_l, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Entrenamos\n",
        "    regressor = xgb.XGBRegressor(objective='reg:squarederror', max_depth=20)\n",
        "    regressor.fit(X_train, y_train)\n",
        "    predictions = regressor.predict(X_test)\n",
        "\n",
        "    # Evaluamos\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    print(f'Mean Squared Error: {mse}')\n",
        "\n",
        "    return regressor, X_test, y_test, y_train\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Análicemos los resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rf_error_coef(regressor, X_test, y_test, y_train):\n",
        "    predictions = regressor.predict(X_test)\n",
        "    # Printeamos algunos valores\n",
        "    for i in range(0, 10):\n",
        "        print(predictions[i], y_test.to_numpy()[i])\n",
        "\n",
        "    if label_size == 1:\n",
        "        actual_values = y_test.to_numpy()\n",
        "        norm_pred = np.mean(np.abs(predictions-actual_values.T))\n",
        "        norm_rand = np.mean(np.abs(y_train.to_numpy()[:len(actual_values)]-actual_values))\n",
        "    elif label_size > 1:\n",
        "        norm_pred = np.mean(np.linalg.norm(predictions-y_test.to_numpy(),ord=2, axis=1))\n",
        "        norm_rand = np.mean(np.linalg.norm(y_train.to_numpy()[:len(predictions)]-y_test.to_numpy(),ord=2, axis=1))\n",
        "        \n",
        "    print(norm_pred, norm_rand)\n",
        "    print(norm_rand / norm_pred)\n",
        "    return norm_rand / norm_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Análisis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ejemplo de uso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset, label_size = gen_dataset('const', 0.1, 5, 'thermal', 'rho1')\n",
        "# DNN\n",
        "#model, val_dataset = dnn_fit(dataset, label_size)\n",
        "#dnn_error_coef(model, val_dataset) \n",
        "# RF\n",
        "regressor, X_test, y_test, y_train = rf_fit(dataset)\n",
        "rf_error_coef(regressor, X_test, y_test, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Barrido en intervalos de G para G cte\n",
        "g_init_range = np.linspace(0.01,10,20)\n",
        "err_arr = []\n",
        "for g_init in g_init_range:\n",
        "    print(g_init)\n",
        "    dataset, label_size, input_type = gen_dataset('const', g_init, g_init+0.5, 'gs', 'rho1')\n",
        "    # DNN\n",
        "    model, val_dataset, history = dnn_fit(dataset, label_size, input_type)\n",
        "    err = dnn_error_coef(model, val_dataset)\n",
        "    # RF\n",
        "    #regressor, X_test, y_test, y_train = rf_fit(dataset)\n",
        "    #err = rf_error_coef(regressor, X_test, y_test, y_train)\n",
        "    err_arr.append(err)\n",
        "\n",
        "plt.plot(g_init_range,err_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.xlabel('G init')\n",
        "plt.ylabel('Loss coef')\n",
        "plt.plot(g_init_range,err_arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Misc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from xgboost import plot_tree\n",
        "import matplotlib \n",
        "xgb.plot_tree(regressor, num_trees=20)\n",
        "fig = matplotlib.pyplot.gcf()\n",
        "fig.set_size_inches(150, 100)\n",
        "fig.savefig('tree.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Para G cte, error en función de G. Sí, es cualquier cosa\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pred_ids = predictions.T.argsort()\n",
        "predictions_sort = predictions[pred_ids]\n",
        "G_true_sorted = y_test.to_numpy()[pred_ids].T[0]\n",
        "G_err = np.abs(predictions_sort-G_true_sorted)\n",
        "plt.plot(predictions_sort,G_err)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Spliteo de DataFrames y generacion de Datasets\n",
        "label = 'h_labels'\n",
        "\n",
        "def split_dataset(dataset, test_ratio=0.30):\n",
        "  \"\"\"Splits a panda dataframe in two.\"\"\"\n",
        "  test_indices = np.random.rand(len(dataset)) < test_ratio\n",
        "  return dataset[~test_indices], dataset[test_indices]\n",
        "\n",
        "\n",
        "train_ds_pd, test_ds_pd = split_dataset(df)\n",
        "print(\"{} examples in training, {} examples for testing.\".format(\n",
        "    len(train_ds_pd), len(test_ds_pd)))\n",
        "\n",
        "train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label=label, task=tfdf.keras.Task.REGRESSION)\n",
        "test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_ds_pd, label=label, task=tfdf.keras.Task.REGRESSION)\n",
        "\n",
        "# Entrenamiento\n",
        "model = tfdf.keras.RandomForestModel(task = tfdf.keras.Task.REGRESSION)\n",
        "model.compile(metrics=[\"mse\"]) \n",
        "model.fit(x=train_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tfdf.model_plotter.plot_model_in_colab(model, tree_idx=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(metrics=[\"mse\"])\n",
        "evaluation = model.evaluate(test_ds, return_dict=True)\n",
        "print()\n",
        "\n",
        "for name, value in evaluation.items():\n",
        "  print(f\"{name}: {value:.4f}\")\n",
        "\n",
        "predictions = model.predict(test_ds)\n",
        "\n",
        "for e in test_ds:\n",
        "    for i in range(0, 10):\n",
        "        print(e[1][i])\n",
        "        print(predictions[i])\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "logs = model.make_inspector().training_logs()\n",
        "plt.plot([log.num_trees for log in logs], [log.evaluation.rmse for log in logs])\n",
        "plt.xlabel(\"Number of trees\")\n",
        "plt.ylabel(\"RMSE (out-of-bag)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testeo barrido en G código anterior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num = 100\n",
        "g_range = np.linspace(0.01,20,num)\n",
        "rho_range= {}\n",
        "gpu_batch_size = 2\n",
        "\n",
        "# Construccion de parametros y matrices auxiliares\n",
        "#rho1_size = m1_basis.size\n",
        "rho2_size = m2_basis.size\n",
        "rho2kkbar_size = basis.m\n",
        "fund_size = basis.size\n",
        "hamil_base_size = basis.d*(basis.d+1)//2\n",
        "rho_1_arrays = rho_1_gen(basis)\n",
        "rho_1_arrays_tf = tf.constant(rho_1_arrays, dtype=tf.float32)\n",
        "rho_2_arrays = rho_2_gen(basis, nm2_basis, m2_basis)\n",
        "rho_2_arrays_tf = tf.constant(rho_2_arrays, dtype=tf.float32)\n",
        "rho_2_arrays_kkbar = rho_2_kkbar_gen(t_basis, rho_2_arrays)\n",
        "rho_2_arrays_kkbar_tf = tf.constant(rho_2_arrays_kkbar, dtype=tf.float32)\n",
        "k_indices = get_kkbar_indices(t_basis)\n",
        "k_indices_tf = gen_update_indices(t_basis, gpu_batch_size)\n",
        "\n",
        "batch_size = 2\n",
        "indices = tf.constant(get_kkbar_indices(t_basis))\n",
        "indices_tf = gen_update_indices(t_basis, batch_size)\n",
        "en_batch = [np.arange(0, basis.m) for _ in range(0,batch_size)]\n",
        "en_batch = tf.cast(en_batch, dtype=tf.float32)\n",
        "G_batched = [np.ones((basis.m,basis.m)) for _ in range(0, batch_size)]\n",
        "\n",
        "#h_arr = two_body_hamiltonian_tf(t_basis, basis.m, en_batch, g_arr.numpy(), rho_1_arrays, rho_2_arrays, k_indices_tf)\n",
        "#(h0, hi) = (t[0][0].numpy(), t[1][0].numpy())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_g(g):\n",
        "    #print(g)\n",
        "    ## CONST\n",
        "    #G_batched = [g * np.ones((basis.m,basis.m)) for _ in range(0, batch_size)]\n",
        "    ## GAUSSIAN\n",
        "    h_labels = np.array([[g, 1] for _ in range(0, gpu_batch_size)])\n",
        "    g_arr = gen_gauss_mat_np(h_labels[:,0], h_labels[:,1], basis.m)\n",
        "    h_labels = tf.constant(h_labels, dtype=tf.float32)\n",
        "    G_batched = tf.constant(g_arr, dtype=tf.float32)\n",
        "\n",
        "    G_batched = tf.cast(G_batched, dtype=tf.float32)\n",
        "    t = two_body_hamiltonian_tf(t_basis, basis.m, en_batch, G_batched, rho_1_arrays, rho_2_arrays, indices_tf)\n",
        "    state = pure_state(t)\n",
        "    #print(fund)\n",
        "    #print('rho')\n",
        "    #Toda la matriz\n",
        "    rho = rho_2_tf(state, rho_2_arrays_kkbar_tf)\n",
        "    #Solo el bloque kkbar\n",
        "    #rho = rho_2_kkbar(basis, fund, ml_basis, mll_basis, t_basis)\n",
        "    #Rho1\n",
        "    #rho = rho_1(basis, fund).todense()\n",
        "    r = np.sort(linalg_d.eigvals(rho[0]).real)\n",
        "    #print(r)\n",
        "    return (g, r)\n",
        "\n",
        "# Version sincrónica\n",
        "rho_range = {}\n",
        "\n",
        "for g in g_range:\n",
        "    print(g)\n",
        "    rho_range[g] = compute_g(g)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ploteamos\n",
        "rho_range = dict(rho_range)\n",
        "rho_range = dict(sorted(rho_range.items()))\n",
        "x_axis = list(g_range)\n",
        "values = list(rho_range.items())\n",
        "size = len(values[0][1])\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "\n",
        "# Plot using matplotlib\n",
        "# Use LaTeX to format all text\n",
        "\n",
        "plt.rcParams['text.usetex'] = False #True\n",
        "plt.rcParams['axes.labelsize'] = 30\n",
        "plt.rcParams['xtick.labelsize'] = 20\n",
        "plt.rcParams['ytick.labelsize'] = 20\n",
        "plt.rcParams['legend.fontsize'] = 20\n",
        "plt.rcParams['axes.linewidth'] = 1.5\n",
        "\n",
        "plt.cla()\n",
        "plt.figure(figsize=(8, 5))\n",
        "#%matplotlib qt\n",
        "%matplotlib inline \n",
        "for k in range(1,size):\n",
        "    plt.plot(x_axis, [values[j][1][k] for j in range(0,num)], linewidth=2)\n",
        "\n",
        "#plt.xlabel(r'$G/\\epsilon$', fontsize=18)\n",
        "#plt.ylabel(r'$\\lambda^{(2)}$', fontsize=18)\n",
        "plt.xlim(0, 20)  # Set x-axis limits from 0 to 6\n",
        "plt.ylim(0, 5)  # Set y-axis limits from 5 to 12\n",
        "\n",
        "#matplotlib.use('Agg')\n",
        "#matplotlib.use('GTK3Agg')\n",
        "\n",
        "plt.tick_params(axis='x', which='both', bottom=True, top=True, labelbottom=True)\n",
        "\n",
        "# Enable minor ticks on the x-axis\n",
        "plt.minorticks_on()\n",
        "\n",
        "# Customize the appearance of minor ticks on the x-axis\n",
        "plt.tick_params(axis='x', which='minor', width=1.5)\n",
        "plt.tick_params(axis='x', which='major', width=1.5)\n",
        "plt.tick_params(axis='y', which='major', width=1.5)\n",
        "\n",
        "plt.show()\n",
        "matplotlib.pyplot.savefig('filename.png')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "oapxWkD16fHg"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
